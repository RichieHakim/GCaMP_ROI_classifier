{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/josh/opt/anaconda3/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.45, 0.45), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1g0lEQVR4nO29W6w123XX+RtzVtVaa+/9Xc8lsYNJnG5DtyN1N24rIIEQEkI4eQkvtJKWEC1Z8ktQg8QDJ+SBp0iBh0j9wsORsAAJYiJAIg8RIUS0okgN2EQJ8UVOjp3Esc+xz+W77dtaVTXn6IdZtdasWrPWqv2d7zt7fWfXX9rae9d1Vs1/jTHmmGOMKarKhAljYK67ARNeHExkmTAaE1kmjMZElgmjMZFlwmhMZJkwGs+NLCLyGRH5uoi8ISKvPa/7TPjgIM/DzyIiFvg94K8A3wa+CPyUqn71md9swgeG5yVZfhR4Q1W/qaol8AXgJ57TvSZ8QMie03V/APjj6P9vA3926OBC5jqX4842Wf8hm42q7JODMur4dqsktm+2SXz/WAJH2zR1bHxF1XBuOKD7XDufRzfHJ+4XQ6IzNpfftCVoj+6zxefF7X7i33tXVV9Jteh5kWX7rXWfBRH5HPA5gDlH/LnZj9Hbn7ywqoJXML39zTaxdt0ReM+WmvUK6pubmM112u3SCFsjmzYYA953r9Nsa68vImDtZh+Ac2hVo84hJuzvX3P9PM09121Z36d5Jkg/Tx+J96DOgXOd5+28X7NRMP/h/J//0dClnxdZvg18LPr/TwBvxgeo6uvA6wC3zUvrN7CTJLB5kb730uLOjdDfpgbwXUKsry2RVvaKmub8lijtS/V+mzwQOsSY8Ls9DgJRpKfx+x0ftT+0UTvHxs8zys70vkvyhih9idN5PwPvvsXzIssXgU+IyMeB7wA/Cfyf+04SkfCAIuGrjF9KnxzqUa/rjlifG6P/PyDe0xfmItLtoFbyeLMhTAJx+9Z/t0Tp3CDdUZt2Sqf94v2mPQ1p23ZiTNjf/3iia7XX7xzXu0dLova4tVTcgedCFlWtReRvAb8KWODzqvqVUSf3O6b/MsY1YPf+lChut/fvF0uYzi0G7pFqr0mc21c9LdoOjUnXV7vt/j3vpqMi2/NSbbCy/53x/CQLqvorwK88xYnjjhODtB+Dke0vic1L6nyFsbjXIQnTVxkNYaKvcRBxp/rufZPSoE+ExtbqX7MjdRv1sd/cZ1vitteObLS1dNmD50aWp0Hc6J1f39D5zneM11Z84xsjDxBs+JLY7IvvsSZMvK+9xlUdDbGkaknRV3ViumTcUqUjnr13raS0gmDstu+1PUc9ONC+TZXA4ZAl9ZADSKqEhE2Db6x/HfhqeiOgLYnRa89o43LHNdaE2dc5QyqmZ+skr9X/yFTXhOlIz/i9DL2jCAdCFt10rt1NmCFjc3NA4sWJQYzvDB07nd7eu0Vs07Siv738wJB8y3jtqbzOvsRHMWg/RSosqSpSLoTobzWNH2Nt6JrGd+NRla6U2YPDIIsSRjZ7DPKdfo8YKUN1iCjrNkTXayWSMV3fCKzVWfq+A/6YyO+zk+yJ0VtoT2hvrAZFdhBxy0iP1Fv0DjejLrMeXe7CYZBFCE6kHWqoY5Q6lx5qRsdu+SqGrt1Kor5I9gYkDCmHVE9qlBF3wq4R1F4JmUJqVAQbf07sp+ndV/q+ogTECOz4Fg6DLMigU20L0Ve2doU3HbRG56vpdlDfv7GB3TI+VRXpS5K4Y3oqqjlpI+r758U20h51u25fYig/SPz24+m1VVvDFtJ+oJE4CLKIRC5t2BLl647xkQWP3fgkekTZ5YmMCbNFTtvsc9E9omt3PJ7tffrD0nguqT8KaR2JPXW7vm6kLtbtTBFmF/pu/Hio3Ed87RdqNNRDZwi7tbNVHY1Povma14j9Gc2XnJJaW51Po9KsTb7crWu09+8QWbv741FIa2gn0CFM/A52jb769lF/6N2fe4K0H2jkKPQwydK8/E7nrHVub1tfB+9xnPUddOtOShh+ySmE9h7tvRsbalCabQ11TecZ+lMGo4z4ns+n/54GCRY793rTHmNMgBcnrDISzy20/bJHeB9Ho7WB+kSJCdUivm8bitBuH2pXNCJLdWrXpmrbMdIpmbrm1txRz0Uw8tpwKJIlcnHv8l/sHTL3kfiSO7ZAjNbwa19mX6L02rU1YzsoBfz62I5auYIK2KWS4zbs8+n07aKtY/fgMMjSYFC/7hjubXVaT4Wp2RwHO/wTzX2lvUbkG0m2c1fbE3ZBq9ZEpDst4Tcz0skO6z9PyjgfUJWDH1o0otty/+/AQaghhW6j+w0fI036I6KE2upggIBjvrAO4Z5GDSakyVa4w4AqS5H9adTwlsoSs3dEdCCSRbvjfzHbL3THC0kNPYHk0LEf0LRT7fWv314z0aZBVbHPGbjjmYbaE+9fo3VUDrU79f4iSTjGwD0MsrTu/sTE3fqQPV9858XEvo5+bEjvnDVSnbprZBGPKiKbZKitfdui4zDc4UcZ9AQngp72GcxdGytWPXavfwoORA0hbBEFNqJy/XJ84iW1l4gfNB6Z9Pd53RbBROLdmM01TMI/k/Cn9I8Z7cpPzUjvUVHPDO19BnxQKRyGZGnRi4EdROTy3vJL9G2X9d+y/SX3g7LX7YgkE4nOH7pHTy0NIRkE1ZnzGeG1bY/pj3b69+jvi4LJt9T9HlIehmQJomV7c8rYbbd1XnR3uLsdl9LzV/SvmTJ2dzj1to3DBDF2jOC6xw34Onrbk/eNPpirTkxu+W9GGOqHI1lSaRB95reBPqlhXjvUbbCeaU1FnkWxK53jehJlXOpFd+5o3ZwdMS99X8++jh4MXE+SbNv73ZIsNZ1wFRwOWVrsMnL7YQd9tDbHrpni9jpsrrWewYZtouzonF3ZB1vxsaY7gTk6rqXp+KRRO3TOwPn7QiT2fRiHR5YGKVsilcax81xIitakA2zAs9k9JuH5HTsj3HfYDcxpRY1c/7mrE5MB2aNV4CYe5oXz4MKAMbkjgkwNSR9DCvtibDv3jI/pR+P3kRx2D7x8rztjWQbV5z71E6vhsU66vq/phYiUa9D5SvY9cBtE1KoSNl/urgi15D1bpOJo6Km/2F7qp4PGX+qOc1Q1nd/bqJr2A9i0a/w80vo6vefY69wbISEPiizDX9UVRGvimkMYGmomt8eR9JogVap9a6M9Ct1sg6pibMWl9NocZSAMStCBeaynCt8cwOGQJQ5K7hmofc9oytsbX0dJ2BZj2zDgCQUQa5qvvx2q9+aIEm0GuhF47X2kGzS1Rcpeeu5Q+unee8NO+2hvgFWEAyFL5HpOpGSE7XFIJeljYOPGdmx8N1chzoBN0nZUPzh8J+LheF0DdMMqhzIo0RBprx71ceZldL1eFYf+NTptSLWruX/8jC+IzSJ7O3bv0BE2+0eSZK//IvL4xumr+/T/1tRDG5rQhlXuI68RRCUQZU9k4KjQywEvc+yTScb49HAgZGHnzOfW1xOJ5jUSE2txCGHqOp3JtERdlq5KjOyMsS7+OPRy0/iOOtly3rXSy1pEEo67PXbJTkkT3SsVe7wPB0EWYcBI7OjlntRoz+0HE/X1e48w63O0ZxdE+zrzJ9H9OzPA0VB1ZzhBTNQrqEMR2RpiDzrV9o0gt6RLYvg+YgBxEGRRhkdC+9TPJl2i+zJSieZbndl+vS16aaIiEiL9++i7+Pfkae8zIFPD9z4B+++hNeL7pBmUEH375ClwEGQBttned7f3/Rujr5t+eeuXbLsjrQ7iZLGmPVt+ibi0WHTernuv9+2L1utLs16KqWggs4QH2sTvDNx3jATchQOZdR6BNuxviCiJUVFq9nnr/33ByyLbs8r98E/tGY39a/XicDZk2EWmkdMIMWI7pPdsQ7ZK245kZkAPByFZhODD2NreRJQNieLt+ZZt13wn/6dvv6R0ed+fEW7cPWZk1YFOG6NnijFsh2xPqK6N3r5R3j1wMLxiyFv9YvlZUsZtP28XNj6O1vBsX1ZsMySGvVhJGrphf2J2OUZsxA4dsx5JbX+t4WS/GQWtL9sjYCpdhW4Ht0avav8ZI9LHknBILQ34aPbhcNRQStwPHmuG41oS0DbM0vvhaLBdRl/7suNgJK/rn1Ht2JXWsqMUyM543qTTUrthpYmh+dNi75ki8nkReVtEvhxtuy8ivyYiv9/8vhft+xkJ9fq/LiJ/9albFqOJ4kp+CamOio3PJqpuXQt2l5+i774fGya5jg/u5uW010ypi3ZyL/4BkraDugGbwkdSL25z+77aip9N+9bHtd7oK0bYjaHZPwU+09v2GvDrqvoJ4Neb/xGRTxLKmP5Ic84/FtlXomcPWuPLRaGUO6XAAKF8uMbQix98cX3C9O+tPd2fItiOOaQtY7Tf/vX0xcbnk/I8x2GX7XPiNZzXD09NVYBI3buHvWRR1d8AHvQ2/wTwz5q//xnw16LtX1DVlar+AfAGoY7/fsSqIv464gm2Mf6B9rj2i45HUIm41s153U7rvPx1Gxq7I5IUYu3aqTdkvO6MSoukUZKM0HmG0TE5qWu093Nuo6biDIbn5O7/PlV9C0BV3xKRV5vtPwD85+i4bzfbdiJ8DYncnlQUP8PW+1ZntTElPefV4JC6P5Jov+q2+E5sbEapFLtiaHaqr/ZebSVNSeQe7Zhdj9GZI4pHhT0VGDsddzosE3jWBm7qjsmeFZHPiciXRORLlS43RYDjUUwcxT/gE4mu1z3ObKTJeijcHyZ3L9D9DRsnWF+Exzk3djvvJibjXqO1nwuVeMaOioyP731Igx0eS9RGHXf3j6PB05LleyLykaaBHwHebrbvrdnfQlVfV9VPq+qnc5lvJgaHbIK+wdYileraHB9+t0lqkVqLZ5P7o6RYIohJ190fwFpl9RLiUiptlGHZGqaxDQLr99S3s7pqtWsvJe8X2VFjnHJPS5ZfBv5m8/ffBP5dtP0nRWQmoW7/J4D/uu9irVNu2MiMDNzUTDEDqimSTq1hu+V9TRGmhZFQ097IZvbY+22p1N6/LyXaNvR/YGMrpFJg4rZr+EjW7U2FccbP35M8sVRN2kSxxNpTb26vzSIivwj8JeBlEfk28A+Anwd+SUQ+C3wL+OtNg78iIr8EfBWogZ9W1f0V74ZCE5qHXtewXR+fDpCK9XGHPL143eR2K1sjlTFfv2qbj9NTV0NJc522m6guTO9e7eRoWwwxCpraSZR++2N7LL52//xnUTRZVX9qYNdfHjj+54Cf23vnPmKv4pCja4Sx18kJSoQtrMMiex2312geiolJbbvCRGcnFKFncLY1auM1CgYJnPAC9xq5c/sYHIa7fwf6gU5xQFNyVrq/SEN7ndYR1eYeDZUwjzFEznUcTCL4euhc37tn66LfEfYYlnaJ6t61+yKHYd+Gu1KcS+wXGuERPxx3fwLJEhz94d6QQbzrwXf5E1L51btGUWOx656Do5jY1tgm1t65nT6p+vu22rGbDochWRqGd0MDBpieigMxwvoLj1VSa/PYrj2zmZBLGKQRBm0DuNIoqdvOzbW2/B2pDtwa8o9Xz/0h+latl7HJaA0OgizKjq8k5fTaMWeTzIfu5+qkwibj+3UM1QGPajI5/2rk2RjHV+u09tyt2/eH0YkRVp8wV5l5Pgiy7EQqKauPgS9zrcbaxRpSkfGxQbw5Mdxam3LsrQ0RH99vU+w1NT1P7EDY5Thfi9/YKQ32dfDg/h2z22OmDQ6XLHEwdRRyuTPiCwZHQZ2cm3ChtcELpHOBfJOw1koAM1CTv72koXsvjXwXA1MXHQwErO9ciSRCaoIxXHfk/NEeHI6BOzBbnFzWpHWk7XJTD0X5r/d3yTboEOwXDmrjbtqf1PxKdM+rhAA8NfrOwMQ764dCDO3fhQORLK2ntXnxMUnUgwrq6Liwgc1XvZ4Har2sKYfYgIdyXyWBXaW0hjIC+x7hHf6PjtHcN96jJLDU8VvtHZAm/XtuTVamrpvAYZBFifwWid3tvqjAMPQMxIRdsTOGJNq+5Qgc8BB3yNH37zSk6oxy6LUjfqb4Ou3CloattZT6I6FUR8ft2ORB2c7i4VvPDuMWCI1wOGoohdjD2vPqtg+4b1TQHtOZKIsmFZMe41SVBBIdPODL2ZqUi2fCh3w2kbrrB1F1rvO8kJrX6uEwJItE6w2tjcD2y406PuV1VR9yiJtN8ZAxrqaQFOWJEVDHqbdrOiA+vudh3iJK37Aeune0Pc6t3pUL1C/rsTWP1sdTRPW3OAyywCYVZD3x1YrIxCRdi8jtvp6u7Kekxgs9dNJAEmqk/X2VVI+EH2iLMPtgZFsN+mil+D1+mK6TMWpP77zB0dJIHARZkjZ4ZNz1F+Ye9LzGaqolzC4/TdQ5Yk1Epoi4jcHaSWAfGorukgAthkIY+ym4sB5ud+aG2CMRxgyT+36fkcQ5CLLEHlwZEpPt19cv9LPD7Z7y0yTRxK0kF/VsJI1W9fZ6iX30DMa1sZvIiZLo+DViX1JicnCIJFdVJ33VvLcKaIODIAuw9RV1hpTxQ7nGSSWmu67iAHYuAdyW3soypCigyJsIud6cUF0jYtCy3NgccbzuDjtAmyF7MrbE2u5QvG1Wm0w3VlX4PRKvva70ktfGSMIIh0MW6BAGtqfbxTfGrKXr/+gFJ6/J0YsuQxoJ0l6v/f/4CD1e4I9m6MziMxNsABFEFXNZY0+XmNMLdFWGKk7ONRFs236WFNE7UO2SDa4WLzMCfdW16zrdaYxhHBZZYE2Y1FwO0NR168WeNv6JvrHZpnm2pIgJQpYhmYUix989obq/oLyTUS0M9VzweXM/hfxMWbxTMHsnw5xewuUSXa4QGinX89u0hBmckgiN25ZI+5bRid7PGKQIkxxtMUDqHg6HLH0pYQySbZqnqqGTve8axCnPbBw81P5vGiM2LwJJZgW6mOHnBeUrCy5eyVjdM5S3oD5SfAFoML6LRwZXFAAUmcGYZqRRC2KaYkK9Cb8OUdq/e2qro3J9lAorpqs696mjHeRJEqb1lici9HbhcMjSQKwJqqG1IdqOhiD6q6pRAb4T3NPxicRFlE1zPWtgNkOOFujRnPr2nOp2TnnbcvmSYfmyUN7zuBOHvVWR5w7vBe8M57cL1GQgBUczwyy3WGuRsoLaIVWFrspg09Cb7OyRKKWiRARFG+IlXsoAGfYZvuv30fftbJVY602bDOAgyCJ0A43EWmRWwHyG5hltro6sKmRl0eUS8b3C7O2oJ3bU2WYyz5qgdooCf2tBfWfB6qWcy/uW5UvC8iWlfqXk5P4FL5+c831Hp9zKVqy8Zely3jh5mYdyB8jwNkNFmANmWSOVg1UZOryu0aYqZSd2JFWpITV0dvQWQ08brv0Qy6SH+Wmwx9g9CLJgBJnPGrsiEEWPF/iTGb7IgtErYJc15nyFGIMuV7BcdrL5WtulMwIyDVHyHF3McLfmrO7nXLxkuXxVWL7i4dUVP/DyY/703bf5E4uH/MniPY7Niid+wcP6GCPK76wKlstjTG3Ilga7zLGZwa5c8MqsqjU5404bDCBPoXUI7vDAbtlB/RHNUxLlxZlINBa5dQud5VDkuKOC6s6M8k6Gy5uCPgr5uad4kpPNcszjc6hrfCP615BmfA1rO0XyPJDv1pzyds7qjmV1T1jdV/Tlko+8/Jj/5aU3+d9OvsX3Z4941Z5ixPOOuwXA/eKCO8eXfO/2jPIspzgR6jOLKIhXJDPBDmqN6HboS9+X0SVB0p7oT2L28oDW2OePiv9fnxN7wq9er+AwyGINevsYv8hx84zqds7ynmV1z+BmzTEKxangZsLMGoraIxeXcJHW/a3tQ5YFdXY0o741o7xtKW8L5V3F3at45aVT/qe7b/Ojt77Jp+bf4r6puWUslXrglFO34G52wf3FBY9vLyifZNRHhnpuMKViSoMag1oDpiEMpKuE2+FpgI2nepzPBHZIgwG11Spu9Rrm3nbV00vgIMiimaG6t8DNLW5uKW8ZVncNq7vgFoq3gEC9ELw1oBl2NSc/X2DKqnMtab+cvECKHGYF/vYR1d05q3s5qztCeRvqEyU/rrg3v+Qj88d8NHvIR63jSAqsCJWWeDWUaln5DOcN3gutoSTa/DiPVA5xvmND7QoyggHnXYtUxkIiwR24WkRcaxc9ZeTcYZDFCquXC1wu+FyojoX6GOpjpT726NwjhcfNc0RNYzfk2PNjTN8oazywWuT4eVBp9a0w6lneMZR3heq24o8dR/OKWVYzNxVzU5GLocJx5j3vOeE9f8wDd8KD6piHywXleUFxKdilYkuPXTrMZY2sSiirdcn1tQe4N0QGho3SsRiaDO0tBNGZfTcReaPyI0k1uAMHQRZvhdUti8/A50GC1IsgVfTEMb+14mSx4mF+zKqeNWSxZBdzctWuC14EzQx+llEvLPWxpTwxVCcS1M8dpbrtsMcVi6JibitycVg8XpWleh55w/fcCd+p7vNWeZd3lic8PlsgpxnZmZBdaiDMssYsS2RZomWVrCzVmSfqG6cpw/Rp42XbCdM4cyEi0dAkaNJbPoCDIAuANC5wDVoGtYoaRaynyGtuz5d4hUceLuws2AkyY3Fk16pBjeALwWdCPRfqeSCem4ObQ71Q3C2PnNTM5xVHeUUmngtf8N36Lrm8x6mf8059mz+u7vPGxav84elL/NF796i+e8Tiu4bF28riPUfxaIU9XSEXwZtLXYcXn5gv6mBfXErn2K5fJCZbnK3YLRgdp7GYzXX2kPCFGQ2Jgl0pagRXhHindcyTgDWema155ficRV7z+GTOWXGCmozqJAueVgWfC24GrghSyc3AzzyaKZop5B47d8zmJSfzFfOswohyVs/4ZvkK79S3eFCf8HZ1izcv7/CHj+/z4NEJ+r0Zx28aTt70zN+rmT1YYR6dI5crdLmEstq46wfmeMbO02xhKCMg8lLH8zqD191HmBdl1lmckl941AquEIwDUwumElxtcN5gRLk/u+Bjx49Y3bH8rvkoj7hNfdxIFgWfa1BdC4fMHfmsZpY7TBPba0XJM0dhHYu8Ym6DjfGkXnDpgzv/UbngweqIt89OePTgBPNezuJtw/F3PcdvluQPLjCnl+jZBVo16ielenpq5cq2SSp/eSA7IbXSa3zv9bWGCPMixbOIU4pHFVIrpraY2oAXxBlWJuOsmPNwvuB2vuSj80fcsZfU3vIVFU5PFuAF9YIpHPN5xclixTyrWWQVuXVk4jHiMbL9UpYu46yaUath5TIuqpyLVcHZ+RzOMrJzIbsAW4Z2AkHV9GNmh7CvgwYDqbrBTv3A9H2z3Z1tse/lKQOf4FDIUnuyB+eY1Qy7KrDLDFMZbBkCmy5nOY+PFtQnj3k1f8KPzL7DzFQsbMV3bt9h5cLQdpbVvDQ/5+XinJkJKsaIkotjbsIQ+8IVrHzGk3rBaT3jrJrxZDXnvMxZljlVZXG1xV9kZGcmkGWpgSiGtRG9xt7Y2AGn2vr8EbPIqTzloXsnSm8kCfMUOAiy4B3y5BxT1eSVw1QzxOXYyqJWqBeWi6M5D+8cceELcqn5/uwxf+r4e9zKl1y6nNJnLGzF/fycl/NTLIpD8GrIxWHEr/++8AUrn/NY55xXBafLGWfnc+plBrUEqbYymAqk8a2pAZcbbG67ZGkL8vT8HZ0vfY8Lfydhhjq2UT37/Dnx31sBVVcceR0IWTRMDhLc5zhl5jx2laOS4wrDRZHzrcU9/tv8B8nFUanl1M2xeHLx+CbAu1LLmZtjUDyCU8PKZ1RqqdTiG8v5ST3jSTnnvCy4WBbUFxlynoU4cQVxAhJGUVUtmDIM1/1lhsmzxrVf02YdpiLbtuKHYwkzNlY2Qhum2U8O2zns7ecJ9eu9RNfZh8Mgi2oYfvqQiG6cQ1Yl5qwAjnGzAjczXC4WfH3+Kpl4budLClNjCSQx4nEqrHwGzAHwKoFU1ZzH1ZzaWzLjKIzjoi44K2dcljnVMhAlP2s8tNFIzC0U8YJdQX1pyGaWLAuJ9mptmP1OxdTEiKP3U86zFAYkwHalyRGk64VxSrw9Efs8hIMhC85tpvedR2qHVDX5Iqc4zZg9MtRHhkfzE74q38edxZJ7swsK6yidpfThUVpjtkWtlkfLBY8v59TOYK3HilI5y/KywJ1nmDNL8cSQXbAmi5rw47NGgLjQTtk1t7NrNftU9P4YJLMAEhJnl3RJhXGuPbqb7fuky5gChB8D/jnw/YAHXlfV/0dE7gP/Cvgh4A+B/0NVHzbn/AzwWcL07/+tqr+65yYbN3bzhWqj92VVkV04ilOhfmTwec5Df5vT4wXvHR0xyxxlbaldG26p0XtQnDOsLnP8eY6s2qlgxVSCXQrFUrBLyC7BLnXts1EJ3mSfC7ZUijMlu1RM6aHelFjVyLfS5j6pRrVmYz+J3cTs7PXeDuQqr43V+Dyiju5cI3GvdntLoGb/YHpNhDGSpQb+rqr+lojcAv6biPwa8H8R6vf/vIi8Rqjf//ekW7//o8B/FJE/pfuqVrZiuQmCDikegqxqsvOaYmFwhaBGKOuM+sRyepxzVji0MlCbtSd3rUok2B723FCcCdkS8IQllSuwpWJXiqkUU4OpFfGN3SSs56rEQX7hyS4dpnQbg1bj5PdNjtHa1Q6sk9xS4ZUj0I9F3spP3mewpiL0ekPwZApvAmOqVb4FtKXXT0Xka4QS6z9BKHkKoX7//wv8PaL6/cAfiEhbv///23evpKtcFVM67KWnOA3L3ZpSqM+F+ljQLENqMFXo1LWB2nrKXZAa2YViy43kEAe2UkwZhsXr2JRaMc6DB58bfC6YWjErFwKdzlfIqkL9JrRznR+USk3pV61K4SqFfuKJxD3hlklvcjsM7+czPYvSpr1G/BDwZ4D/wjOu39/52tqvsDXInCdbOtQE725+KVQLwc0FtQTJ0EqH5rf4VkqAXXnsygdfiTQqriGGxAajgqk9UtYbMd04w6RyUNVh0nBVQtWkg7Rfpe3bBP3swnRnpDIN+ykwQ+ftTTlhmyhtsLYa0oluOzCaLCJyAvwb4O+o6pMdX0tqx5bcFZHPAZ8DmMtx2Ng2vP1C29TRlcOaGqkVf2nIC8MsF9zMoBLUh6kUu/KYVVAV4jxSe3BNqS/XqIzMrldUlTYdQwRt51qqeh2Ive5859DGnlLngyHe2iqRF3drRBGntPS+8i1CeL9FgDFR98k5o8S9OtdtZ6j7oQ57MIosIpITiPIvVPXfNpu/JyIfaaTKlev3q+rrwOsAd+zL3da2yVvOIXUTEO09sjJYY9As/PisCch2HlM1QUhlHSRA29ktSSLborMeYyNB1i+9dlBVm5FZrM/jYfLAfMxw9Fo3jqWDXv6ORiTdWduF/eTrXDcR2hmOHzdSGzMaEuCfAF9T1V+Idv0yoW7/z7Ndv/9fisgvEAzcUfX7W6y/0OZrlrbjl2UwehsVpUawkbUvLoxSqGu0dqiParnFMJG7Pl6HqEUdyLZVxy2R85Nqe/Kee563RafDE4FLu66/NR0wOBLaMRf1DEZDfx74G8DvishvN9v+Ps+yfn+jDjpfjPNIXW8v4dK3BTTqvDafKFmyNPGSTGL5l2Z5PHV+1JxN8u/ePbZ8HKnrpEIpgU4y2NB9RsbrPo2hHWPMaOg3Sdsh8Kzr98fwPhiRZmDRKtpQAN/t9CGDLdVZvhs3u0Xa9zGPAuOizzZt0U5cSlLqxRiKc0k8/9on1Ms4WO8biYPw4CqJRrcjjRTaDm0lSBvFD+sO71QsiEgwetQA9PN89rroUx2bmIMZJFEsYQaDnnrOul7Nub4N0/GhxIWNdj3HAA6CLDG6ejstJTq5wQRPbWxTrO2eEaOJrRd2lTIXI0IVpdeuNYbKXey4bmz8dt7TgBG878NIxcDswkGRZZfY3npJRhBs+GLaRPXouBRhds20JqfxI1WQipoHOhUhUy5zJVKjfSLECWkp+ycRgZ98R4kO7+dSpa693j+SMAdBFiEmwbYVvzMCLMqsS4UzprDr5XRe7pAqAFCPSLaZ14Jtw1o9MVdG1fbv1ZXr1MOLMcaZ1kqveGgcx9bEcbwjCPMU06CHgb3Go9+I/30vdjt/eM+XHGOPqtNIXT5TvJ8yp1cpsBhBrhxI/BwgIu8A58C7192WK+BlPpzt/UFVfSW14yDIAiAiX1LVT193O8biJrb3hVVDEz54TGSZMBqHRJbXr7sBV8SNa+/B2CwTDh+HJFkmHDgmskwYjWsni4h8RkS+LiJvNIHf1w4R+byIvC0iX4623ReRXxOR329+34v2/UzT/q+LyF+9hvZ+TET+k4h8TUS+IiJ/+7m0WVWv7Yfgq/8G8MNAAfwO8MnrbFPTrr8IfAr4crTtHwGvNX+/BvzD5u9PNu2eAR9vnsd+wO39CPCp5u9bwO817Xqmbb5uyfKjwBuq+k1VLYEvELIDrhWq+hvAg97mnyBkMdD8/mvR9i+o6kpV/wBosxk+MKjqW6r6W83fp0CcgfHM2nzdZPkB4I+j/8dlAlwPOtkMQJzNcDDPsCsDg/fZ5usmy6hMgAPHwTxDPwNj16GJbXvbfN1kGZUJcCD4XpPFwNNkMzxv7MrAaPa/7zZfN1m+CHxCRD4uIgUh7fWXr7lNQ2izGWA7m+EnRWQmIh/nitkMzwIjMjDgWbT5AEYeP06w3r8B/Ox1t6dp0y8SUnYrwlf4WeAl4NeB329+34+O/9mm/V8Hfuwa2vsXCGrkvwO/3fz8+LNu8+TunzAaz00NHaKzbcL7w3ORLCJiCarlrxDE+BeBn1LVrz7zm034wPC8JMtBOtsmvD88r+j+lNPnz8YHxFUUjo/kf//T/2O+3ler50IzLvyM03rGqsyRUjBlW1PFh4T3tkYK+uJ5Zw4Upzx8VwdicJ8XWfY6fTSqovCp/3Wmv/nvP4JpBN277pLfKl/mS+c/zG+++z/wxrdeZf5HM47eVG59p2b+5gX20Rn65BQ9vwjLzSWWzf3A8BQVnQ4V/1H/9R8N7XteauhKTh9FqdTh8RgE27z8Si21N6Bh2btQEFAgM2gWlsgLiedmXE7O84Lqh4Io+/C83vCVnG0KVOpxzQs3sK5f69rkKFHUhFIbPjOQ2WipubaExtWT1yeMx3NRQ6pai8jfAn6VEIbweVX9ytDxXpVz9TgqPJ4LVS78jEtXUHkDcSUvw6bGSls9QSSkeu4u7DHhfeK5pa+q6q8AvzLmWIfw2FtOUcDzwB/x3foOT+oZZW3BCeKkW4O2n7f8PLL+JnRwELnODuGRn+EwLH3OO+42b5e3uagLyjoDL50qlDppm2vBQZBl5XO+Ub6Kx+DU8Ngd8ag+4qya4Vyob6tCqDQ5EeXacBBkufAFv3P+J5mZmpmpWfmMR+WCZZ2HFU8BtaEkaWdQ3tgqa9yAEcl14iDIsnQ5Xz/9Pm7nS+4WlzgVzuoZK2cDWQQ0U3wmDWEEbStMirl6RYH+qGki2SgcBFlqNTxaLtbLuxhR6mapO2uVKve4mcHNhepIqE4yTFUgqwLKEnEOyR1aQadEeh8Nudp1jdfFDWWSTmNwEGRRFS6rDCMzMuMpTI1XITeePK8pZxl+bqmPCEvuLi2myjGXM0xZhRXQm3r6Yfg8TBixFsmzZlEJHxx+cUXMfZUlbzAOgizeC6sqxxrlss4hC9Ilt455XrMqHOXCUR8L5dJgKsFUGXY5Iy/rsDBDXa87WWu/6fRYmliLFAXSFiusa6iqbdJMhEniIMiiCmVpKTKDV8GgFMaRGU9dGOqFQRUqJ5S+LXpsEJ2BCFlugyvaCMgK1Ie5ooYopsiR+QzyApkVUOSgilQ1WlVIWaGrFZQl6s1TV0b6sOMgyIIKrm7mgYDMuPVimKaZf7RGeSJQGsVnGdCsHkXB3EBe+01l7rJqHDIeMYLMZ8jxMXo0x88KdNbU7l85pKyQi+WaYIKbPMEDOBCygK8MzgXJ0totM+NY2IrChmV3rfE8sZ5lNmOlOWEWyYAWSOnJXCCMVDXimlGStcjJCXrnBHdrhptnuHlYrs6uHOYyx1oTjOSqXi9VdK2z2AeKgyELdSCLNmpoYSvu5pcAnNsZx1lJZjyZ8TwyyrkXlpKhYhBvwoqtqljdrM4hWQZFjrt7QnVvTnXLUi8M9Sx4hPMLS36ekVuzIdrlEr9aNQUMd4ysbiAOhCwCteCdWauiE7taL7l7YQsuXEFhaua2YpbVvGM8Z9mClSkwzmBXFlPPmNUeW4e1iXRW4I8KynszlvcyyltCfSTUi7DuYX0aVkeDAlMtsFVYK0DqGq3q630nB4jDIIsHe2Go5hkXZc5ZPePMzVj5nCNThgW9G9slF0/WLJgpwKkXVqsCuxIgQ+2CorCoEdxRRn1kWN6xrO4J1Qm4meLzsAK8GkEUbGXILnPM5TysQFKVzRpFbrM6yCRhDoMs4iE7M/gi43S24J28xqDk4rmbX3BkSnJTc0IYUmfG4RG8CrUzXJSGS5fhc6FeZMxuWdRAPQ+SZHUHyruKO3a0ayf6C4M4wZSCXRqyowx7VGCqOjj6GvuFaTi9xmGQxUF+Cr4wVPOcx8UCG5GCHO6bityumJuKuanwGsjifLB1VszxhaWeG6pbYTk8N4P6SKnuOOzdkuN5RV0b6triJKcuBbsKaqk+MrijDFnlmMsC8lWzQETj6JuG0wdCFg/FqaKZoJJx6Re8XQf7pfQZq1lGNbNBwohbG8D3igtqb3AqPDTKclawPM4oLw2aKTrzmKOao6OSu8eX5MbzZDnj/HKGM2Fy0tsQqulzoZ5bzDxHFjOkrpuS6D4Mq73hphu8B0EW46B40izk4IXSZZROeM8bKmdYHWWsfMbd/JLjbLUmze1sCfOgmuZZzekirABflhlFUXMyX3G7WLHIKo6yktJbVs5yfjkLIzAJtfE1a5flNfhFhlRzjPeIarMeokeMDxLmBqujgyCLOGX2xGGcxVRgSkGcpdQZj7xQO0uthnJuuacGnwm5OHJx3M0vycVzK19xPi8oXUathpN8xcvFObezSxzBf/NueYIVRb00AVXNhKKAz8DNhLqySJ0jbhYW5ayqjaF7w+2XgyFL/rgKK6iWBlsG34k4S1nOeFKZYMhWOWfzGXeLSxa2YmZrMnEY8RzbkmNbAmDEs7AVd+wlual5WB3zqD7mvdUxT5Yzqsscc2mwS7CrZrE0CavD+8IEp50rwnrPzeqsrMqu/XIDCXMgZPFkj5bYS0t2nmMvM0ydNROGhpXLufBCVVmWZc7FouCkWHGSr7iVrVjYkuMsGL9HpmzUVI0RpVLL23qbh+UR71wcc3ExQ84t2YVgLwW7DGtBQ1BFMpdAVJ8hTjGqId5KFS1LRGUzf3TDCHMQZMF7zOk5uswwFxlmNUPcDFNnwW2vhpXmVJXhSWUpa8tyEQzfugirus9MTS6OI1Nyy15SasaFL3hcH/HW8g7fObvDe49OcI8L8lNDdiZkF5Bd6maleYLvxRWm2RbygYzzSB3mkbQsb+z80cGQRS+WYb3lLMNUNYVTbFUEdeBNMHwrS10Jly4Mf8tm8rH2Fo9QqWXpcy58wYP6mHdXJ7x1eZvvPL7D6aMj5FHO7ImQnwrZOdiVkq20Ewwu2iSz5YKrDTLLkLpo1piuw+JVZXkjwxkOhCyKXlyg7QLdqwJb1ZjlHKmOED9DnMXUwsoZKg2jJfWCNr4Wj1B5y2WWc2ZnvHkZpMm7j08oH87JH1ryJ0J+Bvm5YleKqcNq89L09ToovCGLeINTwOfY2mNqFxx12kwJ3LBwhoMgi6qGsAIjYURb1eDDavEZMDOC+AJRCyqIDxKmqgxPnGFVZSzrjEfZgtyG8Ia3T0+CNHmYM39omD0MvpxWmphSEa+I6+Yi+UzwRTtKCv+bwqDzDG1GSJQVau2NU0cHQRZUQyxJE4OrAGWzfp8xZNY2X38Bahs3vaGuJKilhaVc5Rjj8SqoE/xpTv7IMnsozB4oi4ee/MwhdVgRVbwidW8BTCO4mQ2SKm+zCkK6rBRhSE1VI1URJEtZ3qjZ6cMgCzTBR83fvunEZhLPNFFt+GPEFxhnsaVQ1g1plga/snhAKsGshNkTYfYI5g898weO2buXmPMVGINm7QKYm3AGIKjBkwKkiZVp55EyweQGX1hskUOeh/CH9WQjN8J2OSCyxC86Io6soEn7yLImCV5BnEUcmCoMf+tlIIBdCbaE4rEyf6jMHlQUj0vM4wtkWYI1SLzCq2vIYiQsu2vASlB13gqamaCqPGvbpj2+DdvsLrH64cXhkCWGBtGuDqglDFklGL8ZgNe1hDEVuDlkF00aSRVGObPHnsW7FfmDS+RihVyu0KoCmuV4m2JAVHWoxGANkueIESwgLkdmGV5pbJtmFdcPufTYhcMkCzQ1T0IukIqEUEcaB17tgiu+LjBVhpsJrginBS+wMntUkz+4xDw6g1UZiNK47L3q2oWvVR3SQ4q8mQMKUse0kkSbeN3aI3WbPtKzdeRmjIoOlywt1AcpUJYbwgDWNp3qlHpu8bOQpWhLHwhzWSNVk0u0DjXwDWHCNUOc7UaCAUHCtKutew91KF8mVYjRlbLaEK85/yYQBV4EssC6U4ENYYxgnceUDrPI8XOLimCqIAHsZbWxRyB0bFUF4jjXTSxTH8IovW5WUVeF2oVgKNiooVUZhs51HQjnb0bVJ3gRyNKqo7h8gjZpHq6J5K9m+DILBmczwpGyDiOoWOU4v8kpaq+zvqZD1UPZ3Md5KHKkal5RY69oVUU17G6OVIEXgSwtVMNXXNdhYk8aB55ziPeYKgfbjnKaoXZVh6zDul6rjHXRnyFp4FzjP/GIc2iedfc532QxuhtXQOjFIQus7Qv1jWNNNUiA2iGrErIsDGmNgbpRO1XV2Cd+o3qGiNI4B1sfj1rbRMw1PhTvN2rsBgZyv2BkaTvGbySAc0EC1BZsFZLKjEG9X9sWVNVGZezr3Fjt+d6oR0cQ7kOMvYVNDnJxyVYlNUNfLctAnLKC1QpdrsLvsgzS5WkN0ca+Wf/cYKLAuNKm/xT4TG/ba8Cvq+onCEuTvAYgIp8klDH9keacf9zU8X/28FEn1jVaVviGKNr+Lit8WaF1dXVDtK1t20ok78LPDSUKjCCLHvLikk1nqm9sjcYA1aru+FHWxz71PW4uQWI8bdHk971Qo4h8TkS+JCJfqlg9ZTPYfP2wJk37876JMqGDZ23gjl6oMa7df1vuv7/ebOaS0tsnPCs8rWQ5vMUl1zaGTqrjOeFpyXKwi0tOeH7Yq4ZE5BeBvwS8LCLfBv4B8PPAL4nIZ4FvAX8dQFW/IiK/BHwVqIGfVr1JgYcfbuwli6r+1MCuvzxw/M8BP/d+GjXhMHHd6zq/uLiBy9VMZHla3EADeiLLhNGYyDJhNCayTBiNiSwTRmMiy4TRmMgyYTQmskwYjYksE0ZjIsuE0ZjIMmE0JrJMGI2JLBNGYyLLhNGYyDJhNCayTBiNiSwTRmMiy4TRmMgyYTQmskwYjYksE0ZjIsvzwocw+n8iy4TRmMjyvPAhTBWZyDJhNCayTBiNiSwTRmMiy4TRmMgyYTQmskwYjYksE0ZjIsuE0ZjIMmE0JrJMGI0xtfs/JiL/SUS+JiJfEZG/3Wy/3vr9NxXXOEE5RrLUwN9V1f8Z+HPATzc1+q+/fv9NQ0uUayLMmNr9b6nqbzV/nwJfI5RYP4z6/TcJ7eTkNU1SXslmEZEfAv4M8F94n/X7n1nt/puGa5zNHk0WETkB/g3wd1T1ya5DE9u2nlBVX1fVT6vqp3NmY5sx4RoxiiwikhOI8i9U9d82mw+vfv+E54oxoyEB/gnwNVX9hWjXVL//hmHMEjJ/HvgbwO+KyG832/4+U/3+G4cxtft/k7QdAlP9/huFyYM7YTQmskwYjYksE0ZjIsuE0ZjI8iHMHHxeED2AZCgReQc4B9697rZcAS/z4WzvD6rqK6kdB0EWABH5kqp++rrbMRY3sb2TGpowGhNZJozGIZHl9etuwBVx49p7MDbLhMPHIUmWCQeOayeLiHymCex+Q0Reu+72AIjI50XkbRH5crTtYAPUP7CgelW9th/AAt8AfhgogN8BPnmdbWra9ReBTwFfjrb9I+C15u/XgH/Y/P3Jpt0z4OPN89gPuL0fAT7V/H0L+L2mXc+0zdctWX4UeENVv6mqJfAFQsD3tUJVfwN40Nt8sAHq+gEF1V83WUYFdx8I3leA+geFZxlU38d1k2VUcPeB42Ce4VkH1fdx3WR5kYK7DzpA/YMIqr9usnwR+ISIfFxECkIm4y9fc5uGcLAB6h9YUP0BjDx+nGC9fwP42etuT9OmXwTeAirCV/hZ4CVCmu7vN7/vR8f/bNP+rwM/dg3t/QsENfLfgd9ufn78Wbd58uBOGI3rVkMTXiBMZJkwGhNZJozGRJYJozGRZcJoTGSZMBoTWSaMxkSWCaPx/wMyTc7d+xQhjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABWL0lEQVR4nO29XaglS3Ye+K2IyNznp+re7tvdGve0hdUCDUzLL9YYyYONMRhjWZhpv9hIBmODwC8ytsEPblkMfhLIfhAMDPMgcGMNeCQLbJh+EGg0woMwM/bII2RLLdFS67/lVv+ou2/dqnP2zoyINQ8RK3JFZOTeu/pW3dplnQWn6pz8jYxcsX6+9ZPEzHigBzqHzKsewAO9PvTALA90Nj0wywOdTQ/M8kBn0wOzPNDZ9MAsD3Q2vTRmIaLvJKLPENFniegTL+s+D/TeEb0MnIWILIBfBfAXAHwOwM8B+B5m/uUXfrMHes/oZUmWbwfwWWb+DWaeAPw4gI+/pHs90HtE7iVd9yMAflf9/TkA37F18Giu+No8AkTIZWl3SuZR8ze3+4jSL8cuRM0vImmZ6+tRczfmdP1jlK9BAGBM+rEG7AyiM2VsxAwK6QchADECkZd7GCrnszVgQ4AB2BKiAdgAbNMPLDciQD0FU/ozEigCiIDx+WeOQATeufvPX2bmD/Ue52UxS28Wq1dGRH8bwN8GgCu6xZ+6/R/SJDGn/wEUFRmbt21IrrG+ST6HrM0TnWcuBH3zfB2zXCdfk0MsL4yZ070NLddLN0ljNCcEc74GEYGur0E31+A3bjF/4BaHD4xgSzATw04R7ukM9/Ye9OQZeH8ADgdwCKDBAcMIur1GfOMG4dEO/sYhXFvMtwbTLcHfEuZHwPQ+hn/Tg3YRIAYZrt4EewP2BEwG9pmFuyPsvgLc/n7E7e8fQFPEz/zf/+Nvbz3Oy2KWzwH4RvX3HwXwn/UBzPwjAH4EAN60H+SWUWCMWvQRPduqMIZiiuWc/Ft73foCABHYmLTSTMN8kQGOQDSATWPS4yvUkzpll0gFSsdFhj0EDO8EgAAzRZiDh9nPwDSnc5wFYwcyBBoGYHDgqxE8uiRZLIENAE5SwR4YcSDYe0IcLSITMEaQDSDDMMQAMWAjeCTEHSE4RhwtKBq4e8JwN8BMnTlS9LKY5ecAfAsRfRTA7wH4bgB//egZ+YXKSqykAhGoeeFl1QOAJcDa+npyrLpuJaEMASEU6VQYRl8fAEcGmZiHkRirMOApkvFbC5gsmZhB9zOGmMS+mTwwzSAflus6BxqHpHYGBx4H8M4i7hzikNUQARQZxgM4ENgkhmFHmMmALYMIsDbCGM7/RzgTEaLBYecwXztMvIO7NzjcGdjpuKR8KczCzJ6I/g6An0Jak59k5k8fOaG8XK1a9ItciX2tVprjyzlanVT3i0AAQAZsUBiRleQgIhSrhU6om2P2C1GtLmMEHSbQnO5FU5YoxgDOAuMAdjZJE2fAgy02Dg8G7BY7jLLNAWKYiWD3QHSEaAE/EKInEBkQpbkabMD14GGI8cwO2NuIu3sHf2MwPSLY6fhjvizJAmb+SQA/edbBWoroa4RQi36h5m9mBoUAbl9Yvu5KncXmXsKIcl19nqEi6Z4bZtAqKN+HZl/bPiHfc3Dg6x14NyBeOcTBgh2lZ6Jkm8JQ+p8B47PkM8g7AQqAmQF7IPAziwDAjxFhMAhDkizXg4c1EYYA5nTtMDL8DYFPcMNLY5bnJmtrJpAXE8L2qjVUVAtnSVNWsTHIcng5nigx1Sk/S6kPApZxadUmJIy2IV0So5kszUJ9nrofDw7xZkR4NMJfWYSrZJskCcKgmP6H8HMADDOiy4ySGcgesj1DALFF2BnwLsLHAD/aPG2MyECMWXI7IFwBcTg+LZfDLMBazfSMyfw3icfU7Co2T0s9O8PQ9rHGLPuszRJJXUOYRNlVhfLvNI7AOICcK25zerZYH5vVT9w5+GuHcEXwVwbRAiYAFCkxChOo8YTDSIiOkutMVFQTzwS2DDZANMnWmieHu2lAYIIPNkkWFxGuIubHBrTW7BVdDrNsMAYzL4aoSAugSAzSL09IX6Nn6AILo8g123OyDUPWrF332Lzs5p7iitNuBG6uwVdjrXpmnwxaAGwNYG1SPTuLOBDCaBAdELPXY0JCa+RvUUtskPAWk44t+AojSaCYGC39TgiTxdO7HfZuSIwCwAwR8SbAWxSptUWXwSy8AFDFqEXt8bBBkibWJsxD9gN9zEOv9BBqb8g0qkpJEZaxaDIEhM72sr/BayhLkasd4qMrxJtxGZaPoL0BaM6ejwUPydMJo8mSYjFUKQJsEpPEAcnjyQAcU1I/iCgSR7bJT2KYpAV5MpjjiDAEGMuwLsANAdEw4lU4iYJeBrMo0q5zclWRXnLk5CKr47orfDkg/d+6zS2gpz0hoFZ/WCGJ6fjW6NbMam3CRnYj+OYK8WpA3GW7KTAMALLNWPONKDBMYMSQEVabGIQFrXXJvogD0u8WIEFgfWIKcJY6YhTLtT2lZ4mEyABdezgXYTMscI7pfnHMAiiJkW0TRgLHmGnxbDTY1lMl4jZr+0DbKO0L7+ExzEmatdfXzKE9NsFTrnaJUW7GJDGGhOEYRHCgci+KnLwda2F8hPEMMzGMZbBN0iUMhLDLxmdmgLgDwhUjuuQyu/vFG0rgooL/KW8L+WTDxWscncf1OMMSZw/pOMtcBrP0jEyNrTAnXKSh4gHJi2r3c0ZggRorUUxVGFM8lcIoCYthSvcnfX0FzLEwlNhQVoA0hzhm99dSsa0SpgNQiPn5sofmLcwUYC0ltWMpqyAg7FDcWs6ubtgxeExwfloPVJiFDcAuMRwrO4aUJjUmYjd43AwzLCWw7vVgFqD7sgEs6sjavl0h6mkL0u8BaqJO5HxgkTpFRZg6nAB0JZGMp2L3PD4zZ4DPM8hHmMmDDgG0PyzQfh6D0YCiJcSBcoAPMDMhKoOWsloRSRN2QHR5DMkWRrQAq6Aim8xcY4S78nh0c8DNkMbwdNrhfnbF6N2iy2AWouJ1rGwRINkBLbYhkVlgWd1aTQDl5VdgmnJ1i+Qhs/a2kF+kPreDGldkKAOEEZjTscZn43cOoGlOiO3swcIsISR4Hwk3IWawNUkFjRZ2AuIEwGQ7xaBEi5kJ0THiVUyqh1TgkLMnlB+ELYNuPK5vD3h8fcDtOOHxcMAzP+LJfodnT6/A/hXA/e+aeqitVlXK/a3c2vYc8VK0yywqo+fZdNzgco54VEAt4arjsr3jQ3KPQ0yLAEh/Z2iffQBidp05AiGmNIXsEZrZgYKFCQwKWb2EJCQpAiYQInEyYi3AuwjahWWKIoFD+slcCBoiHr1xjw8/fgcfuHpWVM478w6Hw4D4dEjS6ghdCLMsRiVpWL3FUDQAJsCcbSRCTzKJoZxtDHi/7NPqr0WNs0vNxW6SF9xhNLFfvAdP2UYZXIrzyLiNKVKEPRLDhASIsCGQMZnJOLm9WaXoWBAFLFiLY/Auwt56DKNHCIQYDThmF94lHGUcPR5dH/BNb34F3/r483jLPcOX/SN8aXqML9BjcCSQJ9D0OjALo8SBVoiqkgqVqtFQvrxU9UILafsEWNIOyKwRXG2DKPAvBRW3iYiWcwEQx5SLEgbQbgQPaZrZmkW15agzQkjH5rHS4BYVK/wv2iQAhheMhR2AXcDV9YTrccZ+dpgmhwgDNsmGub454AO3d/jI7dv4tjd+B99x81k8NhN+8fAR3IUxTwmBZoJ5PSQL1niHokratOfkF8U91aVJpIvB2rPqIb5HqBsi0MCccwlrudqlPJTBZnAsgmefmH4GVikTLkmikrNCSoootJYFbxkj7C7gepxxO055SgheeTU7F3DtZrxvuMcju8ctzRgQsY8DnvgrvHPYIdw7DHuCeT0ki1IdmimU9KiMzVZ6SDoCcFwSCcMoD2llHKu4k+zvUmbOVgphHEC3N+CbK/BuQLgewM7AzAF0CDDMyRMKobj1NDjQOCbGutkh7lxOcEqgpE6bjEPCWMJVBHYRwxCwcx5XboaPJiX2GYsQlmeMTLgPA94O1/i98CYsGL95+BB+++lb+Oo7N6CnDu4pwc7HX9NFMEuB9oE6DgTUUHwIWWxHVFltVTS3gfSBxZ5RqQerUILs18yhbSbtYekwgUprIGtAwwDOMH+4cfBXFmwJdm/gAPCcIQCVsgnnUh7L1Q7hZkDYGcSRFpwkS5Y4AmHHCFfJVjG7gGHw2DmP0QZcDzM4P/qErF6ylDlEh6/Ot/hd8wHM7PBbdx/AF58+wvTOiPEpYXiW3PRjdBHMomkrr7baroOAwCpA2LtOG08qKmnjnoW2suIad5ysTZLFuZSU3VyyJGXL9URqjkOSKjdXiI9G+BsH/8hivjGYbwhhRwijxIU4uc9DxkuGAEuMg3cwxPDRYDAR5DycDQjR4GaYMdq0mL40PcYTf40nfodff/sDePvJDcxTB3tPsIecdXeELoJZCM0LEybQwJfaV6RAJ3OugvP1ccJEEeCcfN2maq6o2bcC54zJEL8BBpcYpdyHUyzIc+KNOYLm5FIn49UAowVd7ZIkut3B3wzwt5lRbgnzDSHu1ozCQ7JVhiE98/3s4KOBMxE75zHYgMgEQ4yd9bjK+uVL+0d4e7rC1+6u8eSda/DbI4ZnKcPOzHg9mKVQL1v+iM0gGAb38JVOembygGKGxFU44dS9VkPK0kmub01BmosqCwzjYwL/LMEeAmjKqQmc1M+isnbwj8bEKLeZUR4R/FWOA42cAooDI44RGCKcC4UpvHeYA+NmnDEwwVHEYAMMMVxOLN4Hhy/d3+IPntzi8HQHurOwzwzsPqVTGo/XJJ+llRKt7bBFbZ6JplYqsSn6O92ik3y9AerpfZXBm5mNQwRRTChpiIAPCbFlTtIEgNl70L2C+Y1JCU83O4RHI6Y3HabHBtNjgr8h+GsgXDP8NSNex1wPlOB7O6YkbHm2lPFGuJ8GhGjgbFJPgw3JO4oGB+/w5NkVpqcj6N4mTIVRRaiPaWPgUphF06kRC2UktsSJWmNVR5CzzcNZbYikKdt0hLpNilLU9Yx03m4wgPcpGZsImKmMhaYZOEyJsawBhgF8NSLeDJgfOUyPDA5vGkxvLFHlcMWIjzyG2xnWRYQcOSbDIGIEJrD6maLB7C2MSYxiTMQcLA6HAfPBge8taG9hppxxR0ixI/G4TqzPy2OWU6TsBVbucgX9l5zZJVhYkqcyg7RG7dFk7E7QUZ8j6DCFCLYR5MMSWMzRZfa+SBU2AyinOsbBIOwM/FWWJlfJ44lXjHgdYG88Ht3uYQ1jPzvMsxjqVLvHMf0dJRaV40TxYIGDgdkb2ImyuiGA8pzMSDkx4XVRQ9rQbA1TnbfSOy+fU6B/TRUD5ckRyH5jKJVUaROo5JpyXLmPGOQxhRKKa5zTGELeHsIypo4KTas9pxY4BoZU70M5wToEg+Btdo+5SJhyu8mC7y3MwWREFhhy1JpycpQkSInUtAdgeMoY7nL57BG6DGbRJEwhMHtWE710gapUVaD/lrFKGQbXDNNDbeXY9h497KY3biIAITFiyLmM8n/kDOvb9HcOHC55Lvk6TABFsGOQW2yTGE1mFpNsDcPJXjcR1kVwNOCDgX1qMbxDcHfA8Cy7wzHFmnT+rtzTHoDhLsLdxdeDWSpQDtr9zQlAQG2T9DwdBfevGCufV1Evb7dHx4zo9jmYc1VjWGJQ+RqVmrM2YTGWEK1ZUFqTmCCdA7A38LPFwQ5pDQSzEokxGsTJIBws7FOL8W3C+DYwPmGMT2NO9gaYEsgXbYpES2mJPTDsfYC7Dyef9SKYZZN0fkuZeFMYpmSvtcFAQUaBJWYj+4Q0o6ioNIAGcKtVmbZXukHPUga7YQAYArlckjo6xCGVckhOLUzKQzF7kz2ZZI8Ywymt1PJSvwzATxZ8sDDPLMavGey+CuzejhjfiRif+BK5ZkqlrTGrajNLGmeAufepjPZEXOyymSXTqu44mmSftC43qxWsE7x17KaNPeXzKmqlTqPKVgyj83SbYOjK8yJT1QnFwWRGyXGgvOrtRGBP2T4mxCHCuAiyXOwVjgSeDcydxfi2wfg14OorEbuvBQzvzHBPpzQnJs1XStdMYzOTh9nnOuvZZ7DwNWAWQXALU0jGWJYqRAQmA+plXjZ1QYVa+6PcrPm7YZRKYghaXOyojei3Rpz18chaQzNerhNilzyhOKjV7gG7z8zFSdJQNAiewINBdAxYXmRWILgnFsMTwu5riVGuvuIxvj3BPJ1A+0NKCrcGUuTGxiQ7STEJe58M8NdCDVVqJA+Y4rLdmARiaVUB1C96q4BM4jBCPcNWG8s95lIJUqWIXpiqayg320JY6p1sTkEYbFE/0QFgwByAQThBmGUmmAkpsGiQ8mojlbrm8Qlh9zXG7m3G7iszxj/Ywzzbgw4TcEhpCyQhCSO4DwPeLxl7Epw9QZfBLMBiN6CWLhAIXR2zIu21bJWkyvlExQ5hfW6PWmbIx59NmgkVM7JJ3Z/YLs9mQjaODyhZcmwBMxLiTDkBG2CzJHLbfTJkr94O2H11hnv7APO1p+D7fZIWs0dV9CaUk8RYKgxOxcgyXQ6zKA+oAtc6dc7VOa0xisaWkHMUclsnHTXGrRy/xShaArXpnO25OlhZEsQzopyDjMZztk8WJqHsVaWyjxRITPksKSHKTgx3D7j7iOFpxPhkhn16AD3bA/tDUimzzykd+RmDml/guRkFuCRmAZYkJo2QqoY73WKvJq9E2z1CJardJnhrQ1QzjMq/LdfSE7vlcus8lxxc5FYNlp5xCdewhwhnURg/JWRzQVPFW9JutdszhqcB7ukMezeD7qdSXsLzrBgllHtyARrV3D0HowCXxCzamNUejZRXaFUkx3RSKTdTMKtbNftbm+YYo2ydp6TcSg3KOGMC6cinrDk7+MTJEl6KkvOSUxyAZNcMJsVvsqRydwHunQnmyX2xTXjO2XcZ/EO7aGTRCfNqSfg6SRZmBot+1bQR1KvKOtptmXqxH4rq5dc7a3tIu78bNlIan5J0BcFFflnqJWjpwhHYH2CIQDGCDgPMlUuqx8dUgG+QY1gEGlNJCMvcMGAOKaqNmCLcfJjAk2rb1AKcrSSsxnM+6HgRzJLiJwHE2RjT8Zdj9kM+VxecreqZ1fFlWo7kyGiovzq2w5jUnqvPmZscxYLDMPgwJQkzzbDjADMOSarOPp1rbUrcHgdQZMRoE+MkqBvm4FMKREgBSp6mVLTWPrt+nnZ8QprZT9BlMIumI21LC+kH04bxOccDa9dWBy81ndLpcu65ep8XFZHqm31OaUhBRvYpiZucS71dOKVhkne5UU+OKR2WgjX4zGC5q2YVDuktGo1Sb0EKG3SSWYjokwD+MoAvMvMfz9veAvAvAXwTgN8C8NeY+at53/cD+F4kO/zvMvNPnRyFQPc9RpEXIsZaj4mOuNTAYigDWFp/5f2r7H4hhae0YxK7SF+36pObx1Qxoh4fZ7edU20PSxwpBx0534OkunGak5EuHoxfGA2zL8+1STqVo5m3Ak2cwSxnRNLwzwF8Z7PtEwB+hpm/BcDP5L9BRB9DamP6rfmc/yX38T9KBKTMeDrStkt+xP0EalFbgKfOI2WjjzXDKTd6yyAu2M3WMTEzS9vSrLei8/XEPuMQgHlOtsYhezLe56bNEexD2ne/Bz29A955Bn7nKfjJ0/T/02cLngJsz12eP86R+8LkMq9irx07P9NJZmHmnwXwlWbzxwH8aP79RwH8FbX9x5n5wMy/CeCzSH38j5Nw9qkfecA0sIWBWv1cni6fkztkk2At6prCpNXqOmL0lckuSLNZp0ecMhobaCAxiWK4EJLU8B48Z3dYGGqaEoMoF1xLjNULP7YQhI4Bk4q+Xpvlv2Lmz+ebfp6IviFv/wiAf6eO+1zedppOWefaEOvA6pSlS5EeQtYm+0BeaGGYXJK6IQWKG1+p+8wkHFNQ09qEp7Q2wVZ0W4+1uZdm9rYeuyrNfR4EWY2lLeavKhUklHLici/awO3Jse4YqOndr3acvMkKOxB7p3hEjcGqW3aY2tvqlcs241zuqRilkM6q27KbJE2iY1ByxkYoj7OcoxYNq+vrArtjZbRdhs0L4NgYj9HXyyxfIKIPZ6nyYQBfzNtP9uwX4rZ3f9qYdjYrrUttY8Kei7jcTJ0XS0yo2tcx8lpwMDGkin4f88D0S+u9OEFsG+nVpcbY1mCl0ApPaQ1sWVTNXDwPnWPg9uhTAP5m/v1vAvjf1fbvJqIdpb793wLg/z3rimXlhFp/NkarXuGSrghltJWkqC37I2Mg7FNonk/lcUhNsrQEsdnOGdySfNUbbw9EbL2PJqwAYBlLs62yk4BieJ+VyaftNO21tfbgCTrHdf4xAH8OwAeJ6HMA/jGAHwLwE0T0vQB+B8BfBQBm/jQR/QSAXwbgAXwfM5/IGd+gGJeu2/JAvc5LHBf0VLvfEYsEacE5zXAAOPT1Z0vdEpGWUdKBz/Wo1bmtZKyk0uLBHRnkely9e51jiDd0klmY+Xs2dv35jeN/EMAPPtcotCGm9bbuBClRXFIfYFjuWYrdKS7bROK0HSdLnXM0C8NkoEy/tKrjwioUsQHGncsoTdeHVRPFXvhCxmZRqaWzAEytbntjfYne0AulhGLz6oNSEjkl1wwz2w4lLxdY1NiWrSCkGQZcpWoW+0czzKnB6+BcGV9tZHbd2dYGsY3R3WtHLxUP+X6kJW0bpyq3apyBXvhE23xH6CKYhYAK60hplAokakT90gEh927p6fxjK0UmizmVZhyjVp/rMbbxlq24kqbGKG3xjVWg85hq27I11LhW6ao6RLGl+jboIphFG2DdIKCG6/M+kvNkv/6/PU5fR84TQ5hUSODUGOWaOeus2EXt9U+R8kyKhGOuuzro1a+YkNvrnLJNgCqPeJNhzqDLYBZgFdsoIFsLRontooxZ/V3Dcky6SPpfvwQ9OfqlAcsxveCiUSmZpXNUOB1s3CK9mtX3iCoD+hga25OmbXxIHyNuc06npN7xJ+gymIWa4BwW1SIqqa9rFS4jhiJRP05zirZEfw9Q4w1p1NoBW6u2CQt0pam2e7bUmQ6UCrDXM2TlWtluWbpuPx+jXwazIK/YsEyiMEhVTKaoDYa16ZRVPQ/Qf3HHJqvFQJTbqrPnVh/FEtXQMpx+6b3orxzbo55UFDRXI8oc61zjdrHIM8k1n3NRXQizKJIXIRMs6kZEv3hAwljt52HkMgZrKdO7z3MPr7mvppZhtCRsGKZq79G+qNZea+8hi0MYJddOMywIIddYdRZH46V1k7iO0IUwi6CR65eXbJbULaCzc/uSuQge6EiZLUbpGNWcLlCy5AtJvElfpzcezfjVvY5gHD2bqSGi/D0DBHA0SZXLouoU+HefUddoadR7gy6DWRh1rolWOyLuZV+Do/SkSqGQepi3UqZNAN8kFbijRlpovKMwX/tSjqidKllrSy31/lahA4oxfwAjSxfdCLp3TjMHC051HuR/GcwC9A0zoZ4UqLAXWjOMTESTblhhDie8Dg2a6S+pbT/CBqL6nLB6uqd65i17Iku3VNDaYE4dgG4FHrb3ez0ki7JT2ihwiwk0D1wmxzYlJNX3herjK0ZpQgEFzZRjdIMgub+oJ6FGwq2QYHVc//FVKaxIKpG0Evzr0RbwpvGXngrMx3AbXjjx1ZPLYBahnjHaA4/0RCgSK/9UyP9oLEUzTO8laYNaZ9NX/Vg6oYNTsRzNsKr2mMR1bz7sWdVNKbXWZsBVUWUxsOUcfVw8nS13EcxSYkPtJG55K6eM0w6dyi+trtuqxFUkWHVxEskj8ar8+8n7qRd46tizxv51HPu8dBHMAmAJpmlR3D2uBqIApJfT2y5k6CQzoV2RwFH8gUytnlYduxs3dbU9bVhHmjMSXVxfJblWyU7Aup9vu5C2FlbP6O4+6UIXwixK3+qtGvDqrO5W7JftDa0M2S0XF7UYP7lGDVXqozpeMfzKJqmukT0raYy4ER2uymgVk1UBTQ0GatpYeEfH1aELYZYF2td6e0Wt3aLSFJaVhmJPlO16MrYkVp7wXiS4Uk9GPhRBdUqFRmgbj6RykzvX7pbKKsN+FTw0plLbVS5xT5Js2UjZ/uoa4x26EGZBV1V0YWsVSRWXUYv3qsXp1orZQnUVw6yoctWXbypW6qDNL5F7tTEjKDdbsgHVM3dzUNLO6lqLZCAQKLv4RzwwjQ9JnGgLC+rQaXZ6D4hwpmEmsaIsgao6IC2V8rGl25L2CDS1E9TaFEBiknZCpQbJZrS0k0VXPU/zbLp4rUikVoXo4rZzwhK9L7PlsawO7YUYzqDLkSzAMim9FZr3l/XdivNm5S+bz185m7C7dntFMlhbUjgLqRdQJWnLtYQhCobSAQWlzgnN8zzvy+09r6i8JvhajfMIXRSzrDwcHdM59tKPWfzHgnI96ngTXcANWFIn9D23SL0kSROgVhLqcUraQSsFmo99LuecQLn1+S2m0hjTm49wdO97SN0SD2AtWdoVoc7v7mtjNpp6k3NswqRTttxjS0U091zZXu2LPodWxn0HRGvSJ8o4ZCxxXWrzPOO4CMlSW/sKsRR7Q3kalVHWegA9uP3UZPT2N9hHVQ3Qswu2YlcigU4h0qpTU3U9bfQ36GthGHlgfW7DTKt8oMj9stvXBu5v1Y8Yp2iAp01Ul6vfN+MzQkekSl2yusDpZ1UO9v6Wa7RAmn6pzYuqVcyGKy9jFtWk7qdDEvmCeZ+6loYpzrDpLodZzmnik6kfW6EuIHc0mnqGntbHrdIKtC3Vu29DK7dWXbdXmnp2iEJJGqBR6bD1HMg8bXmIR+gimKWbOsm87ohw5OWuIsRQWMY5npA6ZvX6mwjuCgeRfR0qY5A+Lhozkf9VIlXbtaENRq6oUbWrdiAN9ebp3EVzEcwCoI+wMq/3945r7YAmzL9apcoDqWygc7wloVYCbAF9+VhGVgkdOL8F4s4m7cVou00SoaoxNBK2Nx8n6HKYBegbeUD/YVqmOVekNiv7uRlF1JFtJIryOIqIj3WPly0pVGyjMyTgWdH5tomjPu7YPJ24/+UwS6tzM21mnR2TNPlaqzhPqwK2jOate+rr9F58J5ILKEO77b8i0m8jyl4laGGZl4phJLDYO7cH/WsJeMppaOgymGWDUeTvrpknkyQPuxEoXDGbfskyeT2pcmwSO1gJb92HuUAAxRsC1pKz8WDacR9NTFox8/ZCqjL6X0cDt6t3tRhd7T9i9J0wOAudYIRC7YvYMgZl28ZKr6LL7Xm9SLts7wB/Eu0maYu2BdJtSSslGauI9Qm6DGYBTopSIqq+4nEUdHsO0bo9HFp7RSdWO0m5rdxb20PaCG4N92a8x1I1ShB1cADldAnv67FIGYz21oCV+tQpDufQxcD9AGosoLv9TGzgXEbREeXVro5E0WM5Rr1j5F4dKVC9NEFte8FQTVvlL00UvHvu1xNuwCVJFgXnHyWNyfSO1ZC4HKNXeyuNei6vZrYNJinG8zF8ZsNw7ZJI0DZg2oYywCvG28zBaa6tVegqGn/GOC+DWfQL7tkiMS41vCLq22OUwZpqgFXKoI6T9I6VL3y1L7eDg2wmRol6bGyCcxmmSo/U1wUUGovUfIgpla32QMFjYQJNbbjhjN50F6GGGKhXaWtEdlb1crJinF7UOZ9fkqbk2LLvdGBQM6fuVH2WuusBiwA2u3Z3jWBag2w6Ai7MeQ6jaA9QM4pE+4/QSWYhom8kon9DRL9CRJ8mor+Xt79FRD9NRL+W/3+/Ouf7ieizRPQZIvqLp+6RHmLD62jBM03ywlRL9JJmqM+3FhiG9KPdSWP6nS1blaTvkduoY55rm6d3rRZsE6zlmLfVqkudlSfBVTKoMuP04mrvq3/Ec+rNZSc00NI5ksUD+AfM/N8C+FMAvo9Sj/4X2r8/PeeGUdnbL2qEufoBsLw0pUakJekq+NeuyN42tQo5RPUTamkoCdxKglXjon49USWptOHbeY5iwOrvVrcGazsvIdb3UHNXP/dxdjjJLMz8eWb++fz7OwB+BanF+sfxgvr3E7D00E83qleANiK1qmnUyyq5OZ/LucKvfBfw9EOvV6B+WceoUZ8lSNoyiQLOtvJ1Vy9TPW+rVjfxFv0B84bKdbZyiRt6LgOXiL4JwJ8A8O/xIvv3y6rs2QHHIq5qf/WYrVEpIri9Zhr8co783XpIsm8Ycn5sQJXJt2IEZagOQ7Vry1DuGrgy9l7IovWWmuvXG+VYs07nlPuvz1rR2cxCRI8A/CsAf5+Znxxx1Xo7Ol5l3bu/oJFoJrQNfnX17cZY2vwOoN/kpjdgDfw1H4eoEqG2wEEFJuprngyQNmPfdPsV01S5MOXa/TnpSt8z6SxmIaIBiVH+BTP/67z5XfXv56Z3f6Va2hcgD9ckG5cH1xD7BsRdfeGrR+qldPN4mwpAUnYBQljV/5TLtsZyK/GMqfN2tsIFmrQEMqZuCNAr3d1q36rvdQbYeI43RAD+GYBfYeYfVrs+hRfZv1/noeYvq1ffCMq0dKZs3NfWxmhWcLd+6Igx2r2OkPZognxxXXkaHdAMyqZYYUja0+oENnuR8/I8qiqyMoChbBJavre08vTEK9wwvjWdI1n+NIC/AeAXiegX8rZ/hBfZv38LE9joVV+6Ym+tmN4LPiJ5tofV5I6QAUxM/ex6E9uqzDPut4ovnaEa2piOzlEWidfNnTmFw7xbA5eZ/y227Z8X07+/B5YxrxrmACiWexe5PAZEtdROok5tzDZOcbWN8haCPm1piLMyGrdees+Q14nV6vll7Cu3+FhsR9TS1nPr8W0FYjfoMuB+oAbf8mdhutn5+oOYvWtsMMxqBWu7qL1HqDgiA1/ClAZAYy9sPIu+bq1Ka0N+dcxzRMwrfKaVHK13JWpLSKTRmQxzEXB/oZ6RtSUVtiK7W7aHonMnh7PdUUo15Ge5UP+nM74tzKT7bFjwpG6OShdQ66idnufVIspl/+k5uQzJQkAvd2PFED0p0Bi1q3qZXq6IbO+t5B7OkUX/SmS3q3nLoxAvq/csW/gPUGy1ksOrX7KMaQuX2XLRW6DznJSLTJfBLMjfEcoi/6SbC9ReQ4unSL2Mhsr1atx68at78FKg3rrr7Xmt0dm+lJ4kUM9bbc/3Lln6zWfuqg81tKTuUxm6oqIbzKZ4UC8oNvRq6Rym6W7v4y1fNx37vnOHuvdq3ebnGVsvheCIQb81zlMq+hjR13viiyQi+hKAZwC+/KrH8hz0QfyXOd4/xswf6u24CGYBACL6D8z8J1/1OM6lP4zjvXw19EAXQw/M8kBn0yUxy4+86gE8J/2hG+/F2CwPdPl0SZLlgS6cHpjlgc6mV84sRPSduQrgs0T0iVc9HgAgok8S0ReJ6JfUthdbzfBix/veVGC02fHv5Q9SQsqvA/hmACOA/wjgY69yTHlcfxbAtwH4JbXtnwL4RP79EwD+Sf79Y3ncOwAfzc9j3+PxfhjAt+XfHwP41TyuFzrmVy1Zvh3AZ5n5N5h5AvDjSNUBr5SY+WcBfKXZ/HG8oGqGF038HlRgAK9eDX0EwO+qv09XArw6qqoZAOhqhot5hmMVGHiXY37VzHJWJcCF08U8Q1uBcezQzraTY37VzHJWJcCF0BdyFQO+nmqGl03HKjDy/nc95lfNLD8H4FuI6KNENCKVvX7qFY9pi15sNcMLpPesAuMCPI/vQrLefx3AD7zq8eQx/RiAzwOYkVbh9wL4AFJN96/l/99Sx/9AHv9nAPylVzDeP4OkRv4TgF/IP9/1osf8APc/0Nn00tTQJYJtD/Tu6KVIFkotNn4VwF9AEuM/B+B7mPmXX/jNHug9o5clWS4SbHugd0cvK7u/B/p8hz5Ad1GwcP/drX2zuYSGAnhBAajZro8lfV4+h1lt5xpN0LXOJYtfndvsqofWQhUbYxTJXeqK8rbqUy75H1LncF1rRPq6pJ9Xj089c95Mck85+IQmeRL/4Mu8kYP7spjlJOjDuouC+xD/9298fLtlRM6qr8oW9NfO5HgygDW5fEIK1nPZqy7RyC3QaRwBl6cghqX8YiuLX1fwNeMr10Yusc1jlAZCNI7AOKTSj8MEnqb04lQ7j1KOEQLY+3UlQCluz8fqz+bIvaQOSc6Vc6wtn+ZtKxHLs8aI/+Ppj/72+sETvSxm+fqAquoLYjUzVH1VmkZ5iQEYRFIXxKV0o9QUtRV51qI03ZOVrgq+iJrWpd12XLy0y2gliFmYtNw7hGXFdPq3VM8lL1iXpm4VhMlcxeZ8PV5hlKqoLTHduQUyL4tZCtgG4PeQwLa/fvQMWSnycD4uX4wf3MJIud1GKVSXSQohvRTOBe3yItWqAbD8LQyiGIXz94CqmmN1fvVNJLl++83B3JSIYMsYCZReqLwn7vStbaQk4YQEFcmmpBe01LAWGFz9bJr5hRmpud4ReinMwsyeiP4OgJ9CSkP4JDN/+qyTdWWcrChgXfTedB5gzoX0REkVnUtZ9Wx6hao4X1QhK4ZLPXfj8QmXl96uark/UEkxQn6RhpC+y9ioI814veKzotaynZbHutw7N2NUjHJOAd5LK19l5p8E8JNnHp0n0qLqaSH6vFNaKaon/7HsMNlesGJLagbrtNvitbpYqR91Pqv7FRUkE64+wFDZIL3+sh0mKdflzpdQFFOUc5R6Kt8NEHUVGfB+sb/kOOcWRpSxhfMin5dR65wtfwoB4DzJZEAWy4pVHoQ2yES0Upk0vcItqG0u3HS+JHktIvpb9QXUUk0X3+fzAKwZpXxbiKv/ixGsbSLNMOV+MUkVPU1ynDbcB5ee3eZnFRXMDMx10T1Zkwx6axdjvqjj020+XnUgcaFiqctDxGVlivGZX3zdjYmWFhftdxahDxOmWBr0rLo6tRJMu9a6ncbWtYHlGUJYvQQt5crvTY+YShL2Ol/p5kKrgTTtNoTp5Vml/VrrzZ1JFyJZlNdCBLCpvZ+O8Vl5GfnvIvqVO1l5VK1ebrsHsFI9Wy0pYv0pO71dWmgwUdKmPTdbURnP4BZJoTpkis2xcsmFceUraHrs65uk+wwuHas7Q+T9SWSc7mt9EcyScCSuW0kQpfFzLF4HmMHSmjwu4nwl9kvfV67EfqHSsFhhFe1kb3VtEo+r1w2h3SbjPSbhs9oiACwGbgHwDIC4MKLgSHK8NrzlHsUW4eYeeX56TGVOMwpwIcyySVurpTH2KAIgvVrMysOpmEY8qY5XsfKKpGkOepJJqb4N26Mwc7FdOq3PlkGm/0IABQug8aA4AlA2STGwFylLEYtBq8+LAtb1gM/z2pJdDrPIV97zn/Vq6Rh67d/e9xvTNKu9NO9rvYvumDIDZHFfHSXNlJ2DfIJXVEl1TMFFYjI4tco1JhvgpmJWCiEhvL2GPczFDS84lBjgjWoudtDsgaCwHWES4cNXibN8XXTK1z/RggvIk6+cTmpfcD5+1cj42L167rZcX7wL8YhiUjuo1ElWHV7dR6/8yGscBaj7AutjiRcQLgSATLXIVs+gGJ5LFywZhjgL2Qs9QRfBLESUVqhMjnRz1M18e5wfeflYlbzUyJWtVvAHYDlGie+yetsXAySbQyZUe0L6XjFPdOOxlDFX9lFtOFeS0MQFA1k9p2a0BtJH7emt5opoDQUoKs93xsepLoJZJKjH3gPzLCl8y/4W4i7nJZFMEbW0KF5NXvGCn0jDPx1kE9Et2ztNAGXliodWTWte3Rrcq8ZcPSZt2z/WFiCx9eZqSbi43G171pr5xGNSqlTua2rmODen6UKYBYA1IDbgaBM4BywBP0CtUO3VYIHDw8YDqxepGxXql0VERUSLUbrgIGuJJvsXu6rtstm8MNnGtFYB7XFA+ho9c3k2MVpXEvJYp8kz4PvqWmfQZTALkPW4KUFDUS3VCyGTGKR1g0OzooUiAwirgJ+gvRrcIq27THqpLIhsVlM8+3rMxtSpE8AiTY4YjOXZxMCFdN1WEkU9Q6VmZa46DLNiXvH4JNIu49X80arhI3QZzMJY3FlKtgtJOF/iMSFD6kxrvS7ehvwOLK5zjOm7x/lFp9wOWtxaOWdQRiTHjGXU9k75+KYAZq3aalXl1urWbrYE9LKX082/6TBKYXyRvk2vXQIApvJx0V68qZKgZ0iYy4H7T1FPb8vvJZ+jA67JpBoCicGZcz2q8EKUYBtV0qEYt/nvdM8jK1HGVpBVXqRGm1QlQFuHitppX6LEgQpsb9ZjKYZ4wwhVI2U67YE2dBmShaCMruWFl3iGUCc7rRiBYsDBoko8kl7/Min6MyqyQq1NQcsA5cHYxWsSo1SvavnGUDWesJyrt+tgppJGqxiXjLUNZrbeDlDGXRi3PEezqHqdtiubitfR+Q26DGbRpOM5lUg36+OA1cS2OpmIAOdWsD/nCU6JSkl8k0UyKmXCyQAIqzhUCRJSLC95UQtIXpoYqTJGjbhCxXlatWUz12qbWRhBST7Kc8AyHnk2HcIwEWCzbnMvc2tshnzjWQbxZTCL2CxCrbG49c3hjdSDHnXFLSn7I18veVYyrrX3lGI96rN3meGqVduSGMxtvu3WZ2OA45iMGlsatxpnaUWfJBwBycNsgUBiiQ0cm7aKLoNZAJSUhHYSdT5IJ4m7SiNs8AMd4q9SH8U97gULRZ2gI5r1S9fuvagVq2wQ7d2oZKwVBCAqRD8rFkym6+Vptaa9vOwUkE5cR7pv8eS0F6bjYmcsuAthFl5W9ZGcFM0wWqxXto2JNcNU5yv3sUWGe5loQqZWS8Wo1PYLUZ3OqbPogDWWovEfuX8jPddo7DpdovuMEnAU+ynygmRnN7qo3WPXaugymIUbjKAnaluRrT6aUMSvnszKq1GgnQ7inROL6kHtMuEiPTTDmVhJslJxIMZzkZzi6lNfFZwI7BWpJ6GSPGbSEisovCVnDQocUcaLjgTdoMtgFqAEukjl4a5qhYSULYHIIJOTgNSLItCymiWI14v/tCscjRHcuOzFMG4nWLYFrlxr1vfzyW4o1QoF56EEYgSsjftKrca1dCnJ10qC6Uw9a5PEczbl5Kp0hFV66gm6HGbJ1CZYl0iyTlLqqBmRTL2XKNc6596rMID2sgSIa/CKqnRE37MdA7BgK87VSVUlvUF5Lq2t1anwqTwgTXHx9NKiWcbaZYstCafocpjllOumDN0lzdAukIYOwcu1JHodYi1VtHfRYCCViwxUHonOmy2jVRjIsmmNvhYD3DnQbrekOTKnXBMbwT5Lwlxe0mbxJ+O1fqGrRHdgOVeQ22KML+Ov6FiMSdHlMMs5A+bFoFwwCZREoCXm03hBjbHa/Wp8k+jU3S/xpJ5Lq9WHlIZIgZcAaOMAGgbwVSplTSoyRbtp9suqZ84qKS5YUGYsrkDKHAXP6o+Il7Fo1FZyVvL4jqZ5HqHLYBZq4hRCVVZb8yFLeemRk0QRknhLC4RFZVA2L1gbi3lD/ffWsFtvBUDJpxE7hHOJiTUgk22HcUC8GUt8ieacjI4J5H16nhaqb+8LbEIMvUzCFYN0zj1l6F4GsyBJCdJllgq6LplhwjxDM2zBWTJV5ZxA49EoQ1Tv095PFdXuZM2VS+WXYFQNkDGL2AcW1VAMUAMeHcL1AHYE8gwKEZYINPuiPqpnU4xeJajrccrCaCF+NdYyJB360JH5E3QZzJKNLwQsksBaZfnHZfvgUipjA59XUqGVJpp62/K5Bahr1QyaVaeZsEw4FuxFGEZUkhS+GQM2BB4swrVFdEklmDmCfATdHVHFogIBlHKQNjYVGcwqjULUco/kGuKFvTbeEGPR7W3GW2+lSH2xFrk9XEQms7VbdGpBT9VsJYifsqu6qROJYahRlWbOwFlkUGTQsXel839b9SFuPzeg3kZGXnkWjih5t4aOg6GZLoRZVGJRz51UTMQhgmhdClHUUCdKXa7T5oHo1Yq161xlp0FupXAf9XLqhCKN4yi8h5LRS3OA2QeQl7FwwYJWrm0n4gwgLRh0Fo1aFL14UhmntgPJgNxpB+NimEXcQ5mUym3Ux0VVnK5yaQGsoXGddd+7L23kc5TzUNkiZV+J7CqG6QX58oqlqsIQoMAwkwcHSX1Adn95McB71N6jTWXIz7SZj8vZKxKDVspDLJZjjtBlMIuGrMsmZT9oxhCR3OhjbWymDbEgolvGXemIBHUeqRfifUY9sYQJkLGOnj2oSzrIACYzipSLGAMWtz0ChAgKSarQ7EEhM6W1wMApWUvl6lTjlE0bNla6Rw4/VOUOBjp+VuE0J+gymMUoD0fp4ZK1P89LSqM+rwOIpe1pxa+a3MixorYECteZcfKymMEcAY/6fGka1Ll/xaw6K8/ZxCSZwZkopQz4pJIwe5APCXMBQMaAXQbtvG+KwWyt/qogqlKPEvYQXpFSWIMkEbE4AQz0mb+hy2AWiJgUfCM/sKQE5CgvbTFHpkW6HGeUAty1asKm/9mlDLQChE35BirKDIhNYGo1JeMnqgA4diZLlsV+gY8ghCRRgAoSkKw8HCaApjIfpU2GPG9rl4h6RFi8NaA2fnupp69P1JmzwaZIR4x1auSRaDGztO5SG8VWaGyi4joC6WWOA3g3As6kF8sMI71P3ARMM3iaEhbUAQ+rcdvUNIevRvCja4SbAXFMrjJbAoXE1GYKgEmLHZRcahZDkzm50/cT6P6QpKtPZai67VdKG+2Ag4ZQMrQiZ4lSx9YqVf/a5LMwg7dqlUtTnyOoZWOQFnh8GIqYrstAlvuU/Tc7hJsRPBpEZ0CR4YxJlSd3OS6TQ/wVwyiMJkmq3F1pGIDdCP9oxPzGgHBlEEYCE2A8w84Me688L2sQdw5htzy78Qw3OhhnQXd74J7Bh2ld5ywqq5yIujymtd9KwphZJLfO9tugy2CWltrcFEAZoI3HAaDKKSkoZxOy3yIi8OAQrwf4W4e4MwhjYhYmwsAMExPKyt4Xz4qApcmfW7ovwVnw4IBxgH/zCtP7RxzeMAg7QhjTiWYi2AlwA4ENAZYQrUHYGcSRAAKYCGZmsCVYZ2Az4yIyME9AyKpSt/zaIg3nt9LDUEKzX0TUmYg+CeAvA/giM//xvO0tAP8SwDcB+C0Af42Zv5r3fT/SVzQCgL/LzD916h4SG6qo1auaITLaKw9PysAj2LpEtGGUYtdQFs0A4CxiRlX9tYHfJQM0oasDaPKgyQF7Ul6UAWXpwc6mAJ9LaoRHh7CzmN434v4ti+lNQnQA59kmD5gZCPfJhmGXniuMSU2xSRqERgI7IDqRWgRjDDCNKYYknlobIjijbrmo4jPagwmdI1n+OYD/GcD/qrZ9AsDPMPMPUfqIwycA/EMi+hhSG9NvBfBfA/g/iei/YeYTtrbSuacetFJD2Z2VZCed66p0czewJkngZgBbQhwN/BVhvib4a4AiwcwG9t7Aji4ZvTkEId0T+GoEX+3AOwseLKIz4CFJpjga7N9vcXg/YXoDYJdqpsGA8QTyWKSIEXUgP4RoEz+zNYg2SaCBAEcEOszpZ06uPc/z2uaTuWrnU2y3MvUmgZzHZx3AGczCzD9L6bt7mj4O4M/l338UwP8F4B9CfagRwG8SkXyo8f85ehNC6QCd75k2t3aKbgF2xuqRyWpR2er8mLAO4yPszIhTEv1gwM4MIwFHZ0FXu6K2MA4ItyPC7QB/bRGukkSKQ5IicQDmR4T5MTDfcmIWm65lJoKZCMSEsCPYA2AnhpkT7B8dUmFmHnd0hHBlQJy8KnMYYPYe5jAD0ww6uAVe8H4lLSr105Hgq+DlBn29Nkv1oUYi0h9q/HfquM0PNZLq3X9lHhUwjNkXA6yC34/0fqsMuZYag7kyTmWSfIA5BLg7CzBA0aTSn/sIcwggH5OaubkCWwveWcSrAdMbA+bHFtMjwnxL8LeJSdgAbBlhZIRrBu8iMESQS+ML9w58b2A8EO+S0UuB4faJYeJACGNiOmSnJqkog3BFsHsLu7Owewuzd8DgQHsL2h/AMdS4jLjQTfJXmS+JvalQyBa9aAO3Z2l1R8C6d//wDVxAI32xFrLupVW2pZqaWvwBSEhl03GBZg9zNyUR7x3MnPa7+5DcWyCpGpNc4HBl4a8tDm8aHN40mN4E5scM/yiCR3Vty4CLMC7Cuohh9IjR4EBAZId4sMmWMcmTs4cIe+cRRwvyBnFHiI6yyw14R8COYAeGG5LqtIOFHSyMZO/50J8LFQtb24cb3ypo6Otlli8Q0YezVHkBH5dkpKY4DWzdxjYgYfhGTbVXK/myAncDbU/Z6thphnmKHOAbYa8c2BJMDu7FK4dw7eCvsrq5IvhrwvQm4fA+YH4jIj72GB5NcC4iRkKMlLE3rv5nBsSdYgBsFziEPMMeQo4dEeIh3S/szKLebPKq2GYVtjMwk4W7cnDOwDAD+8OCyQjpyDVw2oPq0NfLLJ9C+kDjD2H9ocb/jYh+GMnAPfNDjSiJO1Viji4jbSvwBIVt+tvrBGsAOd2QqrhNWy/Ehwk4TKA7A7sbYXYjeBwSQDdYhCuH6U2H6ZGBv6bELLfA9CZjfn+Ae2PCm4/u8YHbO1iK2PsBh2ARogEzJW83WIRgEINZ4jCUftim/ykyaPKwBy4gnZkHULAI1wZMBnBIamqXLmBmwHiD4SrN1eBjQn+Zk/Hbpn1oEjV1IoAodI7r/GNIxuwHiehzAP4xEpP8BBF9L4DfAfBX8wv6NBH9BIBfRoqqfN9pTwhJDLYN8lpcZfPcxSM6ShrAqzyqLLZDQGQG+RyniRF8e4U4WsyPHA5vWOzfSp5SuALCNcO/6TG+74D3P77DW9d3+ODVUwDAk+kaz/yIOVjM0aT/A+C9RfAGPBuQpyWHRR41o7YlL9eYxOxxALEDU1ZHO0IcEtMk4zzZM2Z2sPsRJsea+HAoj0lbUiXDEN201obO8Ya+Z2PXn984/gcB/OCp665IMsxoWfEph1VlpmvDVrahzygV42nVJQJGu5W9Dz0QIe5cYpQ3DfZvEfYfSAZrvEpG6/Bowhu3ezzeHTCaAB8tPBtM0WIOFodgMfn8+2GAnyzi3oEOBmZPMAcC5cdjWrwfSeAGMwynYCOFERQGULRgSiBfHAC4bAgDCSO6HTBMI+gwYUUtOl6259hb/80UukwEF1ggak06psHczxvVIrckQi195CqGVJ0Y5Lrp76TmErNkQ/b9wPTBALoOMGPAOHo8uj7gzas9bt0EQxH74DBFh3s/YO8dDrPDHCy8NzWj3JvkLh8AI1mQtCC3xAzMCXSjaQY5B5qvQOEawIgwJrc7DjJXAJDsKH9tYe8czFbmW8ltETWd5/MMVXRZzCLSpeMBbxq1GmTaKiTP+yoQr41baoM6o7H+xmJ6ZDC/kYxY+8aMcTfDuYCdCxhtgAHDs8HsB3g22HuHu8OI/TQgeIMYDeJswAcDOiSQz90T7D1g9xlfmZLrDABw2RCNKmA4zylN2VnY0cLOuWs3Y2lrWlQZ6hRNDUO0sqP1Kl+LQKImya/YyKpfFYG1kWht9DZ5H/rcVUKUpCu4xChxdPA3FvNjwvyIEW8DHt0ccDXkdmPECEx4No/gORmwh9lhmh2mg0PcWyAQEAkUCPaQgDh3R3B3gHvGSbrMDOMT0wAAO7OkTAgyyzH9PnuYKSTwzgMUFrVKAbATYPcBdJjrZPaqDGYdgX79ap11Vhyw5vKt7a2bDSw5HU1joNLitA2oiciWCLRLwJu/Isy3gL9l2FuPN673GEyEjwaBCSEaTJHgg8U0O/jZwh8ssLcw9wYUshEbUyzIHgjuHhjeYYzvMOyUJBxFLvm4cbDpszASXgixILPkQ7JffGIwM2e3m1KE2swMu/eg/VwSqfT8lTBHbLzL3rx26DKYhah8WWuV89qjttYHqA23yhgW1bJkxNHWd3ZEpdkc57ELGutcwJXzGEzAHC0O3sEzJU8nM0qYDTAZmINJcL4s6AiYORmz8mMCUiiBs3QgIIx5fGEAxatcp2yAiapvBNgpwh0Y4R5F/bh7hruPoENI3lxowDmtvp8jeKjpQpgFpRaIQqgqClfZ9JHXqsg0E9GWkjq1Uotxm3EIAa/0eRI9Lu57Ujs76+EyXhOMwQFACAbBpx/2BggpSGgCCiAtdgRlQK6yKWgJHmIgxMEUY9MYA3I5k49yOkMEaI5w9xHRmcJs7hDh7n1K09TQfacGu0uvTd2QRIwRl6Kt3Cx4lU3fa5AsJaP598JkJWstlYyyFKPnZB+aTe4n55f+KTZlykVHRcSDAOcibtyUXGRr4EzEHA3upwGRk22CSKBI2Q9WTJFBOM4AXHTpR5ooEyuGgdxzhLVmwSVDTBI4RpgpwN4HDEYkFadQwbMZNM0ryL8bkO20GTtFF8IsWR1UJQ2mQl1XcSFt9ALl3NLRSX+n0LmEyA4upS5KxeAdkh2ApGrIOfBuRNylJKg4pJfKQ8TNbsI3XD3Fh8Z3MFDAzBa/9vQb8NtI0mWKBPYENow4JjyfQpYokREsQBlIYyfR5sX2AC3AbrQJ3ndDWiSWCJiyBPQBZj9jMAQ7xyS9mJPhey/MskSeu4yi5rQgvK+NzQL08zHarDhtnLYGr6QF6g9Gii3kUkZb3A0pYuwMjLewPgKHeTnOWvD1iHid0hvDSIgDA2PE490Bf3T3VXzz7ov4kHsCC8aNmXDnR9wdRoRAiMYClsEUES0BIcHxFLKUMgmGD9cEe5NAObcH7J5LvAiAijovKtMCKQnrMAP3DOsjjLjZQM7XPQCHKX8MPK7hAx2QBWr78IwuFpfBLIy+0dVbBW0ZKuShFQJZKgCzrs+qhQeTMuJ2BpgJZsy2TF5dvMtR5Z3NjJLUBlnGtZvxprvDH3Fv45uHJxgA/N74FbwxfhiDewPWMryL4AQ7J2k3p3RJEq/FiscDxJFhxjxOLEguWNRV4h4zG9iDhZlsskeYswEbVYA1q6lpXhKhTn3YqykLPmrPZLoMZgFW0H0b/CtlIemgvG3JZ0krt/GOcmGXlF4kpklGJDGSShqHCofgwSKMOZFJ0gey8RFzeNgAGPLk+mjBTDAmpSGwTX1SyESE2SIeUs1QsmWQJQinoniPcg/ExSgm5mLfsE2R5mRwZxtMSeHyewilKK6Ks1W0uNNdnOlEHO5CmKURi8CC5PaQ2KpCL7ubbauMlgRONynPNQ4mMc3gSosMNqbYE3FEypktoSrCzBYBVLJuAhM8G4RIMIbhhgCi5GZbYkyDxcGMCIQUPAy0qBvDKdpsaEFhI0pmHpP6sUtitzD9MnU5whzUNwqYl49UoGaMtv1IUdmvTWG8plZ3SqK2GLySia6Pl2IqSVsAltgSURLbc4CZUvmopeRukjQmzFWCfOWKCgrD4g1xJNz7AV+eH+H3h/fhfeYeN+Sx5xGGGKPLTGIijIkYXcBgIp5NQ05LIHCkBdEVF9pTQWEFxbUTZO3Azil7zk4xFc/rWmjBpUIOOlJEyg+2pYlzF5ntIeO6huoIXRSz1E2HY8MoVAcWNcTfrooo8h5gnpOHBMDkLkt2zJjO5FMDncElRrke4K9zfU9WD0wAB4Onhx0+t38fdjny9z77DF/1t3CUjF+fc1c0HhOZMHkHPzuErMooLABdCgEIussY7hjuPiGxCZFNaZ1mSj/ks2GfS1fYGtAcYQ4JAqAYwRxBXiHVMretqpEW7IbOUkHAJTGLRlqBjHu00b5OJvpW3ov6HAuAnCsSQC6Xp+ZgHUVOdUO5FEQM22KvMABPeLYf8fm7N2GJEUF4v3uGt/01BhPweNgjskEEwYAxWg9DjDlY3E0D9sSQDwRQ9pCEUewBCaY/AMNdxPAkwO49zEEANqhkpqwuXS5Iu3IJ3s9qiGJUOcZhjUdpJDsGlJ7DW8HXhi6HWYRKMrXCVDbUaRU9lh6xTdqB/M7e5xxVX3/NgwgUcr2xNNURxDUgBewOBvtnI37fPkZkQmDC3W7EfRgBAKMNiJwkiSGGI8E4uPykAWdtURgGhWHcPsLdRbinU2KU+ympT3nWUjNlUtL4kMpNKFqQjzDeZXV0Wp30P7MTN1NPhS6HWaS5jCQDaRGqgl9LumReZUCKFkeofFvtUS2TV4qy5nmRRIZABwezG2DnWGwIid/YicB3BsEMeMaEL2TkLLKByUzhKMLDJCQXKBKGmcCCtCkmNB6l0Mwekl3i7hnumYe5m0GHaUFiZQ5MwouSR5Rc8ugINBpQcKli8mByQT3Xi6VMQFTIn16Upk4026DLYRYob6gtW9BwviYdete93YQqHa2YTB9nLTD6bL8MsFOE8SarhlSm4UAgNgjTgGeR8AUk7+jxeMghAA/JutYME9v8EU6xHURhSIads2G7T8isudsrvEQxi3PFqE0b8iWzd8dWAWvHAoWaScq4TksV4FKYhbFkvXUs+Krxb0uCPp5b0aivSZQmyvuUK3Lv4XYWuyExh/fJY4kTYO8TojofBjybDA77AY9zkvbjYQ/PFpGpqKLIhIN3JWm7EAEwWOJOyIyTmae4wpHXKGzeR3OAvfepG4OPMD7CHHzzcYlQ0NluC1ZNZxi3wKUwC7BmkibFsXIDdZsIiQXpFh2aYdoie5lILDYP55xXc5jhnmbcxjuY2YA8So0yG8DtCdNkMR2u8DWfDG7c5KFmZknXJuwzs9TPlRmlBA5zmkJhkpilX1AqomYYmgPs0wNM/vYRAqfOUT4sbnWbltoW6bUfxTiDLodZeqQ7UDbpkxXDNA+7WckIrER0kVqzB+0nGCI45tyf1sH4VLyecA7A7k1yfT3hYAfcXe9wPSTvxxSkNxnB4kqTickmsJzVBidU1i4ShgU/qX5yeoU1wJCj5jk8QQcPgi9zQznIWEkmYT5rV89d/i59el8XnIUINA51tyajOmgDlSQpJCJWislU/Kh8IEJI2SpVjMmYhH5ODR7j04/b24SG5nPt5EBswYbgbw3mycFHg9EGWPk2IxMME6INGCLlwGBA9IQQUhoD+RwjGpCqDofczGdIxipxqo2mcUzdoyRiPqQYEU1JdZLMWYjJzvF+yd0VqRxCsanWU88ALM75tOrFMAucA8GDM2xf0gyIS4/c0o60nKY8HdW+tFJphvLkxX4YXsQ+UBr1iKRxhxk8SIZavtx8BWBEdBbT+wj7yWDyFs5EOJK2qykMMDCBXUh/DwHzaMCBEL0BjUCcqOS2SAjCDA7ylXcCwNc78M0OcbS5MxTB3BPsHJY+dCGkecu2V/Uda9X3v0dMsqRO93m5DGYRIqVegCWHpRhpKiTf1AWtiqQkVnSsG3FmvKVjYwSzS2DdNC8qoKi0tPrdlcVwZ2D3BBwsJu+wy4nchhgRgEFSS85ERJdKRzgS5kiInkBzaqsRc92PxH9KsFDuO7jUYuzaIeakLBcYxjUfohKJImrImHpxaYbRi0Yy/3ulNw1dCLNwCatXWfjaAxqG/qlAWRElKg2gFHvn9mO6q2PbHLlQlEmPyfic57pBoUvfFzBTXEo4pmzwDhZxTJ/WYib4bNiWNExH4F0ygMNM4IO4vChMA0tJHWSbrKRWOJM6Q43Jfoo7Cx5dUlkxgucGV5GFJR/wzHOkv4GwLMgcuH1tkp8kn6UpIhNqa5NXVBokZ90sTJY/B8ON+1hFs9s2qJ34Ew8udXkiSq7qHGD3MbW+uDfwB4dplwrKDBhTtMULEukyunThGAn3gyvpB9LAp4JkzIKbsMn9/p1Ey5Mai4OFcTYlc6Oev2LvNR+tyr5XtQiLej+DLoNZwCsmWK38rZYaer/EjtqPY+sWn9UKlMyx1OSwGNJaXGewj30K/dPewQwW7s5hfMdh/BphP454lvGV63FGiCltIT1HupYPCYfxktSdUWIKGdENnFzgkOM8En7I0XLY3MeFKQUYfSy2TXfuxOgPWDwe1Ytlc56P0IUwSw3vA7U0qYxT3S+unKxwA6kPIlp9HFwH1igCGCTcnyLahWH0l+KFyaTbNhHIEJwzuPpqSmkAGewx4B1zjcPVMqVcMBcA2ZUOc8rSI08l/zaptNR9CjGWwnwAqR2IBDQjEAeT2nJMGYRr0lGrVvYBWTqm6L0YvJUaPiOdUugymEXzSWWUqcCgXu29B2xxA/laeqOb074GpJOGxDGnrMkuYdiYujxITImcgxkcxrdHxDGpObYGezfgMOdMJjGsY7NyPaW6olxRaGfOeSz5I1XCKNJtm+aMoyQGoNHBTDk8IfVBPXhfhU5YYmedAr18k/X5HboMZgFWD7KC+HWZ6inEUbeRaCsd9cQI9tKrGpDj9Bj0J2l8SiUYnlqMO0K4IkRncwdKVl+UlzEh5dXG3E9uJVkE2wmlMXJ59JyCYGIuIgsBtJ+Sm5wxllUye6tqTIaOWxtFjGGcVkmXwywaoRULXfbp9lZtAk+Psv3S+56zTpRK6k21BxUiUR9LCkRhPENFVdBhhnuWYklxSJ0IwnV2gW2KG0rdUXRcWpjSrCLPueyUfCwFYqUJT0ZhKRvqNPtkuOZyVvYqLCDP2Ca1KySbiMtHRzUaftYCxCUxi1AL8Wc6mVTc9snVnlVzaNVaow1QlmbL6/tUuEbOujP7Ge6ZwTimzkzhXrwcgtQCsQXiKO29kLs1LWmUZkoeFqSEo6QZpOQnTjdPqkn31ut9pKsduw6HSJ/glqF0NuIRuhxmETVjFz3abbMhxmwvyUmT2CAJ+CiMV74GkkEo1tsbo0/bLIBImtylKqOnyZVO5aRsgXig/HZF7SWEdr4h+NsEwJk5JT0NzwB3nyoMi1QRfGj1/WpGqjsiFfvJjKK/K6mk4oohckVnacGuc3B1SusGXQ6ztNgAHUlLOMUo5ZpZ7FuzgE65RHWFVmqbSJ+f1dmCw+TVHUxmmPSy3T0BcKk3/xxVln5SUfZNh0NEqkTMgN5wF2HvQ/JufFzKOiQI2KrZznQItA8gfzMgvVIKIX8bSUljQ0hdpTP4WEpqqAbwNugymIUa+H6LtKrRx24xzVaDZc7X0WGEXstPoH5hvHyFnZFfTg5AWmbQnI4tkoIySivxJU45vnYGzMxwdwHuPvdTmebS7ak2uE8sCECpTlqi1ZHTBzI2qOr2lDPrNmKNhc5pQPiNSK3Y/wiSX/kjzPw/vdj+/crAUl8372IvwFpctond+fzVB6qAXICV1Yw1YAwrhHjVfw5YwCxjCkCHHISjEMGzSzVIzCkaPPuUfW8teOcwIkWs42BS0lJgmEOAvZ9B+1yjLFHjhkFY4z56PGpbJUE0FFCQ6kZ95XOrBLATdI5k8QD+ATP/PBE9BvD/EdFPA/hbeJH9+5svcJUJ0V5S7xN1zKia0+h8XXFBW+9AfUZ3/SHJBgoX26kwmwolpINSh0sJOub4EvtQ8lBodskYvrPLC+fcmXLKjHKYUorB7Bf1Jx3HdUqoNrLlkeSzetV8ZrBRUwMT6GaP1Sd+N+icbpWfByCt198hol9BarH+cbzI/v0rgy6TwgEKlRgI1jgJACCsDLwataSlV0tpKcZFzUmHqCpAKTZNC2zpUIDXiLPYDL64v6QTm9KgEnNInxhJL2BTwgQVPJ+7SuooeJEMUBhQS3Fhtq2WYC+81jl/8OFPAPj3eJf9+6ve/XRb1fiUY3ollVIFACTPoE1o2piMQgLhG5W/IcitAcqkq34vBQ0OQUm3JkUgG5TqAZN080jjOijxn41Jgd+rr5PJF8oUKLioRYVet3NT5d8q0qq42r6AlNWHRY/Q2cxCRI8A/CsAf5+ZnxzhxN6O1RusevfbD3ILx3eBotaO0UVUCpCq3GG5nk5eztcubqZBk6+qJI7EjSRACazHpiog0/1oUZEcAJ+ZSewC3T1cu7557NRIxKV+qp6PqmfwMikrd79SxRrH0n/LuI/QWcxCRAMSo/wLZv7XefML7t+vKKdFllbqgJICccEUxNWWL6IKQ7SQPbCgltqQlResAS4yy0uRVSr9CXteihjXGtSKmUnkPlolkkowb11jY5aVJklezI2ljYWxenOX503PZdneHtfiVyeSn06GHCm9rX8G4FeY+YfVrk8h9e0H1v37v5uIdkT0UZzbv7+lrKu59Y70xKsvppLUzajEn6VMRI5Rej1mqFyiyTpOpDEOXVarPjHHAp71mvxl0c/iCssz6G9Jt2i0tmf02Nu/hURl6Z+i1ppt2k4SymOh/P1InnMHhiN0jmT50wD+BoBfJKJfyNv+EV50//6WlE5l5hwTaeB/mUjmZVW0WWC9IFlVjdesfM6rO38VrSq+Ku6pSoaW+JFW+bER/cACemlPziAjsvWarcZqO/D8ll2mbZpWorQBVWAZy7wAe8foHG/o36JvhwAvqn8/oXB4IfVgIrZLvkmbOScpBO2E6mRsqTGChS5zXUVqmRPaa3I/FaCOnTR9/isGyKps1Smy59GVzk3ZfVeMqz239lrFMBabqAskKkwJWAxqtZAqe6cnJTt0GQhuBuWqpsbdw9RnezUAxQp4U5PH2g4RzARIDIPag+KihhYGjfLSRI25znTpfjC9NAftNeljgRy7ynZXDED0tXQQEubNx1Jul7GVS8wlcTuCedWEvU8arNugC2EWxdXSDkyrE0096L4lY1QMJ23SsH7vPimQuL4fS1mG4BlHQgsVk7f5N22x+tGwRqx7/RbVwn20WiRfASZz7vDWF0DKGFs38DhdBrPwGjQrL7IcU9sX1VRXLyFPVPOV+YInVC2xQn0fjTXE3OvErF1UEekl17WljWSiMnZ9H2YAHSbseDElNVS71m0xnZ6H9t49ym4+2VyTPfcPAy6FWYDk/lq7PLihJjbDFWaQ1Ek6rkxEy2zZ86hyYVah+LB85lcTZ9d0I2ayxQhdtSP71ddly3MKRtN+BVWVbJA2ctV9tqgECVVoYW3wNrACn/7e0EnX+T0hPUHA+sU9B+mXuNLnZv0SV/vl593Si7hGj3o5xd37tzjKCWT7DKJzPx/yMomIvgTgGYAvv+qxPAd9EP9ljvePMfOHejsuglkAgIj+AzP/yVc9jnPpD+N4L0MNPdBrQQ/M8kBn0yUxy4+86gE8J/2hG+/F2CwPdPl0SZLlgS6cXjmzENF3EtFniOizOZf3lRMRfZKIvkhEv6S2vUVEP01Ev5b/f7/a9/15/J8hor/4Csb7jUT0b4joV4jo00T0917KmCXK+Sp+kAL7vw7gmwGMAP4jgI+9yjHlcf1ZAN8G4JfUtn8K4BP5908A+Cf594/lce8AfDQ/j32Px/thAN+Wf38M4FfzuF7omF+1ZPl2AJ9l5t9g5gnAjyMlfL9SYuafBfCVZvPHkRLTkf//K2r7jzPzgZl/E4AkqL9nxMyfZ+afz7+/A0An1b+wMb9qZvkIgN9Vf3eTuy+EqgR1ADpB/WKe4VhSPd7lmF81s/QCKK+be3Yxz9Am1R87tLPt5JhfNbO8++Tu946+kBPT8cIT1F8AHUuqz/vf9ZhfNbP8HIBvIaKPEtGIVMn4qVc8pi16uQnq74Les6T6C/A8vgvJev91AD/wqseTx/RjSFWYM9Iq/F4AHwDwMwB+Lf//ljr+B/L4PwPgL72C8f4ZJDXynwD8Qv75rhc95gcE94HOplethh7oNaIHZnmgs+mBWR7obHpglgc6mx6Y5YHOpgdmeaCz6YFZHuhsemCWBzqb/n804My7EHVgfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5614, loss_val: nan, pos_over_neg: 1.0447083711624146 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.3778, loss_val: nan, pos_over_neg: 3.3395397663116455 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 7.1618, loss_val: nan, pos_over_neg: 1.6413317918777466 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.9915, loss_val: nan, pos_over_neg: 3.036532163619995 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.881, loss_val: nan, pos_over_neg: 9.184853553771973 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.8503, loss_val: nan, pos_over_neg: 15.330060005187988 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: 13.769256591796875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.6605, loss_val: nan, pos_over_neg: 13.378978729248047 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.6281, loss_val: nan, pos_over_neg: 15.212271690368652 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.587, loss_val: nan, pos_over_neg: 23.29730987548828 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.5421, loss_val: nan, pos_over_neg: 42.011573791503906 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.5588, loss_val: nan, pos_over_neg: 43.178348541259766 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.5145, loss_val: nan, pos_over_neg: 39.56291580200195 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.4964, loss_val: nan, pos_over_neg: 45.86143493652344 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.4766, loss_val: nan, pos_over_neg: 59.46516036987305 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.4857, loss_val: nan, pos_over_neg: 57.12859344482422 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.449, loss_val: nan, pos_over_neg: 90.30148315429688 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.4409, loss_val: nan, pos_over_neg: 107.88484954833984 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.4304, loss_val: nan, pos_over_neg: 115.69216918945312 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.4285, loss_val: nan, pos_over_neg: 105.70407104492188 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.4205, loss_val: nan, pos_over_neg: 87.18276977539062 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.4159, loss_val: nan, pos_over_neg: 85.43584442138672 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.4051, loss_val: nan, pos_over_neg: 124.76329803466797 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.3884, loss_val: nan, pos_over_neg: 97.57488250732422 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.3768, loss_val: nan, pos_over_neg: 166.6060028076172 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.3642, loss_val: nan, pos_over_neg: 160.82420349121094 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.34, loss_val: nan, pos_over_neg: 192.2418670654297 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.3748, loss_val: nan, pos_over_neg: 179.4260711669922 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.3451, loss_val: nan, pos_over_neg: 168.7216339111328 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.3476, loss_val: nan, pos_over_neg: 152.32601928710938 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.3396, loss_val: nan, pos_over_neg: 209.3761749267578 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.3279, loss_val: nan, pos_over_neg: 337.53271484375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.3124, loss_val: nan, pos_over_neg: 142.59823608398438 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.3512, loss_val: nan, pos_over_neg: 234.0596160888672 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.2939, loss_val: nan, pos_over_neg: 238.94461059570312 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.2963, loss_val: nan, pos_over_neg: 397.1705322265625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.295, loss_val: nan, pos_over_neg: 157.38270568847656 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.269, loss_val: nan, pos_over_neg: 262.0244140625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.2674, loss_val: nan, pos_over_neg: 192.04788208007812 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.2672, loss_val: nan, pos_over_neg: 304.905517578125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.2769, loss_val: nan, pos_over_neg: 166.5602569580078 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.2598, loss_val: nan, pos_over_neg: 230.6359405517578 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.2502, loss_val: nan, pos_over_neg: 178.95559692382812 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.2587, loss_val: nan, pos_over_neg: 238.79042053222656 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.2399, loss_val: nan, pos_over_neg: 442.5587463378906 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.2564, loss_val: nan, pos_over_neg: 195.85414123535156 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.2208, loss_val: nan, pos_over_neg: 139.24087524414062 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.2223, loss_val: nan, pos_over_neg: 117.82904815673828 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.2592, loss_val: nan, pos_over_neg: 168.0215301513672 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.2304, loss_val: nan, pos_over_neg: 533.1426391601562 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.2135, loss_val: nan, pos_over_neg: 163.00454711914062 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.2004, loss_val: nan, pos_over_neg: 189.39364624023438 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.2205, loss_val: nan, pos_over_neg: 229.6129608154297 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.2215, loss_val: nan, pos_over_neg: 105.06242370605469 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.2035, loss_val: nan, pos_over_neg: 437.5560607910156 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.1942, loss_val: nan, pos_over_neg: 524.2943115234375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.2081, loss_val: nan, pos_over_neg: 322.4298095703125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.1971, loss_val: nan, pos_over_neg: 420.36248779296875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.1871, loss_val: nan, pos_over_neg: 114.42517852783203 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.2049, loss_val: nan, pos_over_neg: 298.29388427734375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.1916, loss_val: nan, pos_over_neg: 497.9173583984375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.1877, loss_val: nan, pos_over_neg: 148.88473510742188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.1962, loss_val: nan, pos_over_neg: 388.10296630859375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.1655, loss_val: nan, pos_over_neg: 303.6028137207031 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.1648, loss_val: nan, pos_over_neg: 241.15823364257812 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.1747, loss_val: nan, pos_over_neg: 213.5969696044922 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.172, loss_val: nan, pos_over_neg: 341.55487060546875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.1577, loss_val: nan, pos_over_neg: 445.1236572265625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.1635, loss_val: nan, pos_over_neg: 291.833251953125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.1682, loss_val: nan, pos_over_neg: 659.9988403320312 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.1495, loss_val: nan, pos_over_neg: 270.26434326171875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.1697, loss_val: nan, pos_over_neg: 221.44383239746094 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.1647, loss_val: nan, pos_over_neg: 522.3010864257812 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.15, loss_val: nan, pos_over_neg: 510.1427307128906 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.1677, loss_val: nan, pos_over_neg: 790.5547485351562 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.1433, loss_val: nan, pos_over_neg: 388.18243408203125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.1575, loss_val: nan, pos_over_neg: 311.9070129394531 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.136, loss_val: nan, pos_over_neg: 296.4540710449219 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.1496, loss_val: nan, pos_over_neg: 370.6463928222656 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.1262, loss_val: nan, pos_over_neg: 493.8175354003906 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.1473, loss_val: nan, pos_over_neg: 409.2738342285156 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.1293, loss_val: nan, pos_over_neg: 2194.76806640625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.1505, loss_val: nan, pos_over_neg: 280.5133056640625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.1351, loss_val: nan, pos_over_neg: 405.61724853515625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.1414, loss_val: nan, pos_over_neg: 387.1517639160156 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.1371, loss_val: nan, pos_over_neg: 1773.7943115234375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.1295, loss_val: nan, pos_over_neg: 324.0937805175781 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.1371, loss_val: nan, pos_over_neg: 814.1448364257812 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.1325, loss_val: nan, pos_over_neg: 528.6254272460938 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.1231, loss_val: nan, pos_over_neg: 709.9321899414062 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.1335, loss_val: nan, pos_over_neg: 400.6474304199219 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.1367, loss_val: nan, pos_over_neg: 475.19140625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.1455, loss_val: nan, pos_over_neg: 2108.8720703125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.1198, loss_val: nan, pos_over_neg: 2168.650146484375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.1278, loss_val: nan, pos_over_neg: 311.5567626953125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.1319, loss_val: nan, pos_over_neg: 638.3256225585938 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.1259, loss_val: nan, pos_over_neg: 570.3151245117188 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.1312, loss_val: nan, pos_over_neg: 446.7638854980469 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.1323, loss_val: nan, pos_over_neg: 756.4742431640625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.1088, loss_val: nan, pos_over_neg: 358.8970031738281 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.116, loss_val: nan, pos_over_neg: 200.2230224609375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.1252, loss_val: nan, pos_over_neg: 372.704833984375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.1164, loss_val: nan, pos_over_neg: 325.8014221191406 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.1265, loss_val: nan, pos_over_neg: 306.5794677734375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.1175, loss_val: nan, pos_over_neg: 286.9820251464844 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.1186, loss_val: nan, pos_over_neg: 232.87803649902344 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.1061, loss_val: nan, pos_over_neg: 879.042236328125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.114, loss_val: nan, pos_over_neg: 357.3091735839844 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.0904, loss_val: nan, pos_over_neg: 676.982421875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.1108, loss_val: nan, pos_over_neg: 366.6370544433594 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.1195, loss_val: nan, pos_over_neg: 338.3680114746094 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.1228, loss_val: nan, pos_over_neg: 323.7328186035156 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.0979, loss_val: nan, pos_over_neg: 673.1757202148438 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.0871, loss_val: nan, pos_over_neg: 282.44866943359375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.0998, loss_val: nan, pos_over_neg: 492.8987731933594 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.1082, loss_val: nan, pos_over_neg: 446.33770751953125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.1198, loss_val: nan, pos_over_neg: 571.6319580078125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.1049, loss_val: nan, pos_over_neg: 272.1192932128906 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.1103, loss_val: nan, pos_over_neg: 377.12457275390625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.102, loss_val: nan, pos_over_neg: 362.31585693359375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.1112, loss_val: nan, pos_over_neg: 599.1998901367188 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.1005, loss_val: nan, pos_over_neg: 163.7645721435547 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.1092, loss_val: nan, pos_over_neg: 209.45928955078125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.1022, loss_val: nan, pos_over_neg: 855.9915771484375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.1198, loss_val: nan, pos_over_neg: 184.00347900390625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.1092, loss_val: nan, pos_over_neg: 265.0415954589844 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.0893, loss_val: nan, pos_over_neg: 439.3561706542969 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.0904, loss_val: nan, pos_over_neg: 517.3865356445312 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.1126, loss_val: nan, pos_over_neg: 223.5556182861328 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.122, loss_val: nan, pos_over_neg: 240.52743530273438 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.1017, loss_val: nan, pos_over_neg: 351.4296875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.1078, loss_val: nan, pos_over_neg: 182.03358459472656 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.1067, loss_val: nan, pos_over_neg: 535.2627563476562 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.0869, loss_val: nan, pos_over_neg: 498.54620361328125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.1066, loss_val: nan, pos_over_neg: 241.61083984375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.0953, loss_val: nan, pos_over_neg: 279.10235595703125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.0965, loss_val: nan, pos_over_neg: 447.0877990722656 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.0957, loss_val: nan, pos_over_neg: 419.3135986328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.0898, loss_val: nan, pos_over_neg: 502.5914001464844 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.0994, loss_val: nan, pos_over_neg: 311.58270263671875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.0861, loss_val: nan, pos_over_neg: 341.84393310546875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.0798, loss_val: nan, pos_over_neg: 340.3411560058594 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.0968, loss_val: nan, pos_over_neg: 475.9255065917969 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.0987, loss_val: nan, pos_over_neg: 496.98779296875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.0717, loss_val: nan, pos_over_neg: 590.3491821289062 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.0802, loss_val: nan, pos_over_neg: 685.1510620117188 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.0839, loss_val: nan, pos_over_neg: 364.9896545410156 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.0882, loss_val: nan, pos_over_neg: 341.9855651855469 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.0821, loss_val: nan, pos_over_neg: 304.1137390136719 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.0706, loss_val: nan, pos_over_neg: 948.617431640625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.0812, loss_val: nan, pos_over_neg: 327.23919677734375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.0795, loss_val: nan, pos_over_neg: 1591.5638427734375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.0728, loss_val: nan, pos_over_neg: 936.0488891601562 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.0769, loss_val: nan, pos_over_neg: 359.9535217285156 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.0893, loss_val: nan, pos_over_neg: 771.5518188476562 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.0788, loss_val: nan, pos_over_neg: 866.5228271484375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.079, loss_val: nan, pos_over_neg: 539.6994018554688 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.0737, loss_val: nan, pos_over_neg: 932.593017578125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.0838, loss_val: nan, pos_over_neg: 483.3731994628906 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.0848, loss_val: nan, pos_over_neg: 683.7586059570312 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.0813, loss_val: nan, pos_over_neg: 1233.5386962890625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.072, loss_val: nan, pos_over_neg: 943.22021484375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.0707, loss_val: nan, pos_over_neg: 639.5836791992188 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.0665, loss_val: nan, pos_over_neg: 716.0609130859375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.0741, loss_val: nan, pos_over_neg: 335.1690673828125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.0743, loss_val: nan, pos_over_neg: 295.1044616699219 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.0633, loss_val: nan, pos_over_neg: 1031.24072265625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.0654, loss_val: nan, pos_over_neg: 687.462646484375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.0556, loss_val: nan, pos_over_neg: 1057.663330078125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.0837, loss_val: nan, pos_over_neg: 220.76300048828125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.0749, loss_val: nan, pos_over_neg: 229.0476531982422 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.0697, loss_val: nan, pos_over_neg: 277.1019287109375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.0675, loss_val: nan, pos_over_neg: 841.930419921875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.0784, loss_val: nan, pos_over_neg: 299.1085205078125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.0774, loss_val: nan, pos_over_neg: 971.7303466796875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.055, loss_val: nan, pos_over_neg: 350.5407409667969 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.0549, loss_val: nan, pos_over_neg: 394.1139221191406 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.0601, loss_val: nan, pos_over_neg: 522.348876953125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.0729, loss_val: nan, pos_over_neg: 296.738525390625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.0443, loss_val: nan, pos_over_neg: 1040.54638671875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.0601, loss_val: nan, pos_over_neg: 287.0633239746094 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.0714, loss_val: nan, pos_over_neg: 333.0135192871094 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.0654, loss_val: nan, pos_over_neg: 549.490478515625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.0636, loss_val: nan, pos_over_neg: 458.9458312988281 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.0787, loss_val: nan, pos_over_neg: 391.1045227050781 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.0611, loss_val: nan, pos_over_neg: 457.8114929199219 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.0733, loss_val: nan, pos_over_neg: 281.6115417480469 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.0773, loss_val: nan, pos_over_neg: 374.2673645019531 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.0684, loss_val: nan, pos_over_neg: 379.35455322265625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.067, loss_val: nan, pos_over_neg: 463.35986328125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.0538, loss_val: nan, pos_over_neg: 325.5542907714844 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.0675, loss_val: nan, pos_over_neg: 394.2484436035156 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.0561, loss_val: nan, pos_over_neg: 421.3294677734375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.0534, loss_val: nan, pos_over_neg: 213.2724609375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.0681, loss_val: nan, pos_over_neg: 512.46728515625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.0494, loss_val: nan, pos_over_neg: 345.7333679199219 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.067, loss_val: nan, pos_over_neg: 539.2222900390625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.0526, loss_val: nan, pos_over_neg: 404.81890869140625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.0549, loss_val: nan, pos_over_neg: 305.9976806640625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.0645, loss_val: nan, pos_over_neg: 1131.072265625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.0608, loss_val: nan, pos_over_neg: 742.1919555664062 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.0422, loss_val: nan, pos_over_neg: 1102.361083984375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.0462, loss_val: nan, pos_over_neg: 1162.9951171875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.0596, loss_val: nan, pos_over_neg: 579.115966796875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.0681, loss_val: nan, pos_over_neg: 616.576904296875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.0612, loss_val: nan, pos_over_neg: 267.4844970703125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.0473, loss_val: nan, pos_over_neg: 1370.538330078125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.061, loss_val: nan, pos_over_neg: 668.75439453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.047, loss_val: nan, pos_over_neg: 673.1507568359375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.0436, loss_val: nan, pos_over_neg: 662.4059448242188 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.0706, loss_val: nan, pos_over_neg: 449.2174072265625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.0549, loss_val: nan, pos_over_neg: 488.5371398925781 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.0413, loss_val: nan, pos_over_neg: 1600.236572265625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.0634, loss_val: nan, pos_over_neg: 647.94287109375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.066, loss_val: nan, pos_over_neg: 486.79730224609375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.061, loss_val: nan, pos_over_neg: 496.6512756347656 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.056, loss_val: nan, pos_over_neg: 827.323974609375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.0586, loss_val: nan, pos_over_neg: 400.4051818847656 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.06, loss_val: nan, pos_over_neg: 846.1201782226562 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.0461, loss_val: nan, pos_over_neg: 585.1315307617188 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.0556, loss_val: nan, pos_over_neg: 898.7090454101562 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.0454, loss_val: nan, pos_over_neg: 2254.171630859375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.0393, loss_val: nan, pos_over_neg: 1039.251220703125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.0425, loss_val: nan, pos_over_neg: 680.4540405273438 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.0467, loss_val: nan, pos_over_neg: 393.8467102050781 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.0503, loss_val: nan, pos_over_neg: 1086.8123779296875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.0467, loss_val: nan, pos_over_neg: 411.5586853027344 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.0501, loss_val: nan, pos_over_neg: 779.8031616210938 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.0524, loss_val: nan, pos_over_neg: 852.1635131835938 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.0581, loss_val: nan, pos_over_neg: 498.5189514160156 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.0507, loss_val: nan, pos_over_neg: 878.1129760742188 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.0424, loss_val: nan, pos_over_neg: 519.0023803710938 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.0619, loss_val: nan, pos_over_neg: 548.3627319335938 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.0441, loss_val: nan, pos_over_neg: 663.0958862304688 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.0538, loss_val: nan, pos_over_neg: 1773.2025146484375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 1076.4547119140625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.0544, loss_val: nan, pos_over_neg: 554.8545532226562 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 863.6202392578125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.0527, loss_val: nan, pos_over_neg: 2158.2822265625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.0394, loss_val: nan, pos_over_neg: 436.1489562988281 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.0497, loss_val: nan, pos_over_neg: 460.65203857421875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.0492, loss_val: nan, pos_over_neg: 480.2988586425781 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.0471, loss_val: nan, pos_over_neg: 440.5875549316406 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.0427, loss_val: nan, pos_over_neg: 349.9509582519531 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.0428, loss_val: nan, pos_over_neg: 361.1853332519531 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.0325, loss_val: nan, pos_over_neg: 777.71728515625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.0594, loss_val: nan, pos_over_neg: 2358.869140625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.0329, loss_val: nan, pos_over_neg: 750.19140625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.0476, loss_val: nan, pos_over_neg: 520.2777099609375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.0535, loss_val: nan, pos_over_neg: 667.0003662109375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.035, loss_val: nan, pos_over_neg: 1105.5936279296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.0261, loss_val: nan, pos_over_neg: 510.42657470703125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.0471, loss_val: nan, pos_over_neg: 485.95654296875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.0343, loss_val: nan, pos_over_neg: 325.2689514160156 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.0613, loss_val: nan, pos_over_neg: 262.666015625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.0334, loss_val: nan, pos_over_neg: 293.1493225097656 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.052, loss_val: nan, pos_over_neg: 305.0567626953125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.0343, loss_val: nan, pos_over_neg: 385.33099365234375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.0236, loss_val: nan, pos_over_neg: 270.41650390625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.0456, loss_val: nan, pos_over_neg: 180.11773681640625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.0253, loss_val: nan, pos_over_neg: 275.3757019042969 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.0542, loss_val: nan, pos_over_neg: 157.3345184326172 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.0426, loss_val: nan, pos_over_neg: 575.8424682617188 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.0388, loss_val: nan, pos_over_neg: 997.0798950195312 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.0541, loss_val: nan, pos_over_neg: 192.09466552734375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.0434, loss_val: nan, pos_over_neg: 386.3359069824219 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.0287, loss_val: nan, pos_over_neg: 576.4227905273438 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.0532, loss_val: nan, pos_over_neg: 239.31573486328125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.0517, loss_val: nan, pos_over_neg: 690.172119140625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.0413, loss_val: nan, pos_over_neg: 586.1885986328125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.035, loss_val: nan, pos_over_neg: 194.2322998046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.0287, loss_val: nan, pos_over_neg: 541.2302856445312 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.029, loss_val: nan, pos_over_neg: 244.7694091796875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.0445, loss_val: nan, pos_over_neg: 469.5073547363281 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.0522, loss_val: nan, pos_over_neg: 375.9588317871094 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.0342, loss_val: nan, pos_over_neg: 558.8865356445312 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.0331, loss_val: nan, pos_over_neg: 299.59906005859375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.0358, loss_val: nan, pos_over_neg: 291.46136474609375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.0363, loss_val: nan, pos_over_neg: 206.1731414794922 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.0486, loss_val: nan, pos_over_neg: 286.50189208984375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.0389, loss_val: nan, pos_over_neg: 482.79107666015625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.0412, loss_val: nan, pos_over_neg: 438.80596923828125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.0269, loss_val: nan, pos_over_neg: 268.8370361328125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.0346, loss_val: nan, pos_over_neg: 236.72897338867188 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.0369, loss_val: nan, pos_over_neg: 919.4022827148438 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.0268, loss_val: nan, pos_over_neg: 788.6759033203125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.0343, loss_val: nan, pos_over_neg: 544.2269287109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.0415, loss_val: nan, pos_over_neg: 350.9344787597656 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.0331, loss_val: nan, pos_over_neg: 321.3683776855469 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.0325, loss_val: nan, pos_over_neg: 283.15362548828125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.0336, loss_val: nan, pos_over_neg: 1003.0910034179688 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.0298, loss_val: nan, pos_over_neg: 751.6046752929688 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.0317, loss_val: nan, pos_over_neg: 1099.1058349609375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.0332, loss_val: nan, pos_over_neg: 259.2654724121094 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.0327, loss_val: nan, pos_over_neg: 480.8229675292969 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.0349, loss_val: nan, pos_over_neg: 310.291015625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 484.72711181640625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.0342, loss_val: nan, pos_over_neg: 726.8878784179688 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.0362, loss_val: nan, pos_over_neg: 870.4984741210938 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 608.7975463867188 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.0194, loss_val: nan, pos_over_neg: 455.5055847167969 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.0309, loss_val: nan, pos_over_neg: 376.1914367675781 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.0298, loss_val: nan, pos_over_neg: 583.1547241210938 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.0204, loss_val: nan, pos_over_neg: 946.8668823242188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.0316, loss_val: nan, pos_over_neg: 652.010986328125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.033, loss_val: nan, pos_over_neg: 371.0552673339844 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.0278, loss_val: nan, pos_over_neg: 1513.21337890625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.0296, loss_val: nan, pos_over_neg: 1158.2325439453125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.023, loss_val: nan, pos_over_neg: 390.5525207519531 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.0394, loss_val: nan, pos_over_neg: 504.6603698730469 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.0367, loss_val: nan, pos_over_neg: 460.50494384765625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.0244, loss_val: nan, pos_over_neg: 577.4094848632812 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.0271, loss_val: nan, pos_over_neg: 439.1418762207031 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.0374, loss_val: nan, pos_over_neg: 436.30413818359375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.0334, loss_val: nan, pos_over_neg: 765.9389038085938 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.0304, loss_val: nan, pos_over_neg: 327.9260559082031 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 450.2501220703125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.015, loss_val: nan, pos_over_neg: 413.98577880859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.0428, loss_val: nan, pos_over_neg: 429.80560302734375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.0223, loss_val: nan, pos_over_neg: 461.87701416015625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 418.90008544921875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 384.951171875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.0047, loss_val: nan, pos_over_neg: 780.9829711914062 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.0258, loss_val: nan, pos_over_neg: 358.01605224609375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.0436, loss_val: nan, pos_over_neg: 311.3695068359375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.0252, loss_val: nan, pos_over_neg: 736.0186767578125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.0158, loss_val: nan, pos_over_neg: 652.140869140625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.0379, loss_val: nan, pos_over_neg: 312.00103759765625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.0227, loss_val: nan, pos_over_neg: 987.192626953125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.02, loss_val: nan, pos_over_neg: 618.2185668945312 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.0339, loss_val: nan, pos_over_neg: 338.9792785644531 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.0334, loss_val: nan, pos_over_neg: 529.982177734375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 535.4049072265625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.0409, loss_val: nan, pos_over_neg: 713.0674438476562 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.0228, loss_val: nan, pos_over_neg: 611.02392578125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.0384, loss_val: nan, pos_over_neg: 436.9829406738281 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.0202, loss_val: nan, pos_over_neg: 602.2349243164062 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.0092, loss_val: nan, pos_over_neg: 1023.5556030273438 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.0211, loss_val: nan, pos_over_neg: 559.3497924804688 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.0386, loss_val: nan, pos_over_neg: 462.798583984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.0354, loss_val: nan, pos_over_neg: 372.4967041015625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.0197, loss_val: nan, pos_over_neg: 1038.1435546875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 787.003173828125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.0301, loss_val: nan, pos_over_neg: 415.5928955078125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.011, loss_val: nan, pos_over_neg: 1562.9964599609375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.0336, loss_val: nan, pos_over_neg: 526.05029296875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.0197, loss_val: nan, pos_over_neg: 316.454833984375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 1456.7515869140625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.0255, loss_val: nan, pos_over_neg: 1009.991943359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 521.692626953125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.0232, loss_val: nan, pos_over_neg: 568.2914428710938 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.0053, loss_val: nan, pos_over_neg: 729.4674072265625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.0277, loss_val: nan, pos_over_neg: 236.23780822753906 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.0108, loss_val: nan, pos_over_neg: 377.6046142578125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.0268, loss_val: nan, pos_over_neg: 1044.2569580078125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.0173, loss_val: nan, pos_over_neg: 1287.7646484375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.0328, loss_val: nan, pos_over_neg: 296.39349365234375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.0179, loss_val: nan, pos_over_neg: 631.3807373046875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.0184, loss_val: nan, pos_over_neg: 460.04443359375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.0255, loss_val: nan, pos_over_neg: 971.6778564453125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.0022, loss_val: nan, pos_over_neg: 626.0994873046875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.0429, loss_val: nan, pos_over_neg: 1173.125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.0188, loss_val: nan, pos_over_neg: 1234.5743408203125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.0193, loss_val: nan, pos_over_neg: 331.999755859375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.0175, loss_val: nan, pos_over_neg: 661.44091796875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.0213, loss_val: nan, pos_over_neg: 774.9170532226562 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.0374, loss_val: nan, pos_over_neg: 690.686767578125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.011, loss_val: nan, pos_over_neg: 1757.1719970703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.029, loss_val: nan, pos_over_neg: 375.9028625488281 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.007, loss_val: nan, pos_over_neg: 462.12091064453125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.0289, loss_val: nan, pos_over_neg: 432.74932861328125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.0409, loss_val: nan, pos_over_neg: 565.8624267578125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.02, loss_val: nan, pos_over_neg: 1093.6138916015625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 784.063232421875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.0301, loss_val: nan, pos_over_neg: 974.6931762695312 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.0299, loss_val: nan, pos_over_neg: 977.817138671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.0251, loss_val: nan, pos_over_neg: 584.2345581054688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.0141, loss_val: nan, pos_over_neg: 998.0592041015625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.008, loss_val: nan, pos_over_neg: 431.26031494140625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.0254, loss_val: nan, pos_over_neg: 539.3221435546875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 394.4039611816406 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.0245, loss_val: nan, pos_over_neg: 513.1856689453125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.0304, loss_val: nan, pos_over_neg: 544.3283081054688 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.0198, loss_val: nan, pos_over_neg: 527.997314453125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.0222, loss_val: nan, pos_over_neg: 887.74951171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.0318, loss_val: nan, pos_over_neg: 863.8207397460938 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.025, loss_val: nan, pos_over_neg: 296.92144775390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.0138, loss_val: nan, pos_over_neg: 608.3357543945312 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.0352, loss_val: nan, pos_over_neg: 533.307861328125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.0388, loss_val: nan, pos_over_neg: 220.11660766601562 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.0158, loss_val: nan, pos_over_neg: 288.90771484375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.0208, loss_val: nan, pos_over_neg: 920.631591796875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.009, loss_val: nan, pos_over_neg: 203.1959991455078 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.0236, loss_val: nan, pos_over_neg: 291.3397216796875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.0145, loss_val: nan, pos_over_neg: 737.6439819335938 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.0078, loss_val: nan, pos_over_neg: 466.16790771484375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.0122, loss_val: nan, pos_over_neg: 314.90069580078125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.0087, loss_val: nan, pos_over_neg: 435.19000244140625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.0182, loss_val: nan, pos_over_neg: 675.5269165039062 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.0118, loss_val: nan, pos_over_neg: 641.8616333007812 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.0102, loss_val: nan, pos_over_neg: 504.7388610839844 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.0194, loss_val: nan, pos_over_neg: 451.6847839355469 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.0308, loss_val: nan, pos_over_neg: 394.28680419921875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9961, loss_val: nan, pos_over_neg: 872.5829467773438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.026, loss_val: nan, pos_over_neg: 786.3048095703125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.0016, loss_val: nan, pos_over_neg: 702.2551879882812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.0211, loss_val: nan, pos_over_neg: 952.213134765625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.0214, loss_val: nan, pos_over_neg: 2436.2236328125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.0211, loss_val: nan, pos_over_neg: 340.3417053222656 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.0079, loss_val: nan, pos_over_neg: 350.2194519042969 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 521.0626831054688 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.003, loss_val: nan, pos_over_neg: 368.9083251953125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 438.6371154785156 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.0209, loss_val: nan, pos_over_neg: 336.5673522949219 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.0238, loss_val: nan, pos_over_neg: 875.6814575195312 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.024, loss_val: nan, pos_over_neg: 433.07025146484375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.02, loss_val: nan, pos_over_neg: 370.3247985839844 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.0214, loss_val: nan, pos_over_neg: 334.5733947753906 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.0204, loss_val: nan, pos_over_neg: 698.5596313476562 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.0418, loss_val: nan, pos_over_neg: 854.0936279296875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.0388, loss_val: nan, pos_over_neg: 860.8716430664062 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.0101, loss_val: nan, pos_over_neg: 1435.486572265625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.0046, loss_val: nan, pos_over_neg: 608.7584838867188 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.0159, loss_val: nan, pos_over_neg: 348.542724609375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.0107, loss_val: nan, pos_over_neg: 664.6883544921875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.0152, loss_val: nan, pos_over_neg: 737.8226318359375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.0186, loss_val: nan, pos_over_neg: 378.5908203125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.0078, loss_val: nan, pos_over_neg: 910.477294921875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.0242, loss_val: nan, pos_over_neg: 386.6105651855469 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.0074, loss_val: nan, pos_over_neg: 276.2027282714844 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.0031, loss_val: nan, pos_over_neg: 478.1535339355469 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.0199, loss_val: nan, pos_over_neg: 699.0308227539062 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.0121, loss_val: nan, pos_over_neg: 485.3247985839844 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.0075, loss_val: nan, pos_over_neg: 584.1499633789062 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9981, loss_val: nan, pos_over_neg: 1415.682373046875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.0162, loss_val: nan, pos_over_neg: 670.695556640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.0125, loss_val: nan, pos_over_neg: 283.9752197265625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.0002, loss_val: nan, pos_over_neg: 749.8148193359375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.012, loss_val: nan, pos_over_neg: 295.84393310546875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.0136, loss_val: nan, pos_over_neg: 385.15966796875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.0232, loss_val: nan, pos_over_neg: 305.1569519042969 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.0188, loss_val: nan, pos_over_neg: 1780.5045166015625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.0175, loss_val: nan, pos_over_neg: 299.8678894042969 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.0105, loss_val: nan, pos_over_neg: 465.5108947753906 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.0028, loss_val: nan, pos_over_neg: 53082.59765625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.006, loss_val: nan, pos_over_neg: 558.9940185546875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.023, loss_val: nan, pos_over_neg: 411.5485534667969 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.0126, loss_val: nan, pos_over_neg: 958.1452026367188 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.0132, loss_val: nan, pos_over_neg: 652.39306640625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.0092, loss_val: nan, pos_over_neg: 602.1067504882812 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.0082, loss_val: nan, pos_over_neg: 719.3194580078125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.015, loss_val: nan, pos_over_neg: 362.0550537109375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.0052, loss_val: nan, pos_over_neg: 3842.864013671875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9873, loss_val: nan, pos_over_neg: 1423.817626953125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.0358, loss_val: nan, pos_over_neg: 564.4572143554688 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.009, loss_val: nan, pos_over_neg: 1964.6011962890625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.0148, loss_val: nan, pos_over_neg: 2019.6968994140625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.0141, loss_val: nan, pos_over_neg: 603.6923217773438 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.017, loss_val: nan, pos_over_neg: 654.0411987304688 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.0239, loss_val: nan, pos_over_neg: 1678.7911376953125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 746.6355590820312 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.0106, loss_val: nan, pos_over_neg: 434.109375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.0003, loss_val: nan, pos_over_neg: 1993.2427978515625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 6268.3603515625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 639.841796875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.0182, loss_val: nan, pos_over_neg: 582.9955444335938 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.0237, loss_val: nan, pos_over_neg: 607.0393676757812 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.0073, loss_val: nan, pos_over_neg: 914.5455322265625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.0204, loss_val: nan, pos_over_neg: 469.5321044921875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.003, loss_val: nan, pos_over_neg: 640.8543701171875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.0094, loss_val: nan, pos_over_neg: 1220.12451171875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.0198, loss_val: nan, pos_over_neg: 502.40869140625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.0018, loss_val: nan, pos_over_neg: 739.9905395507812 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.0064, loss_val: nan, pos_over_neg: 411.8140563964844 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.0219, loss_val: nan, pos_over_neg: 967.5386962890625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9889, loss_val: nan, pos_over_neg: 322.7231750488281 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.0113, loss_val: nan, pos_over_neg: 458.5781555175781 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9874, loss_val: nan, pos_over_neg: 670.353271484375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9999, loss_val: nan, pos_over_neg: 514.2391967773438 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.0128, loss_val: nan, pos_over_neg: 352.28387451171875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.0026, loss_val: nan, pos_over_neg: 674.0067138671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.006, loss_val: nan, pos_over_neg: 444.4919738769531 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.0128, loss_val: nan, pos_over_neg: 741.7863159179688 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.0023, loss_val: nan, pos_over_neg: 351.08489990234375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.0128, loss_val: nan, pos_over_neg: 1074.291748046875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 772.287353515625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.0005, loss_val: nan, pos_over_neg: 273.9508972167969 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.0108, loss_val: nan, pos_over_neg: 343.5426330566406 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 567.9720458984375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.0159, loss_val: nan, pos_over_neg: 600.4698486328125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.0008, loss_val: nan, pos_over_neg: 999.142333984375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.0102, loss_val: nan, pos_over_neg: 682.1726684570312 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.0228, loss_val: nan, pos_over_neg: 606.8903198242188 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.0059, loss_val: nan, pos_over_neg: 669.612548828125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.0066, loss_val: nan, pos_over_neg: 395.2066345214844 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.0176, loss_val: nan, pos_over_neg: 475.0638427734375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 1577.7000732421875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.019, loss_val: nan, pos_over_neg: 634.8281860351562 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.0064, loss_val: nan, pos_over_neg: 551.70263671875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.0176, loss_val: nan, pos_over_neg: 571.6422119140625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.0172, loss_val: nan, pos_over_neg: 14167.009765625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 952.8821411132812 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.0012, loss_val: nan, pos_over_neg: 615.519775390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.0142, loss_val: nan, pos_over_neg: 845.437744140625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.0125, loss_val: nan, pos_over_neg: 383.51885986328125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.0085, loss_val: nan, pos_over_neg: 483.46923828125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.0126, loss_val: nan, pos_over_neg: 529.8058471679688 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.0093, loss_val: nan, pos_over_neg: 1656.3577880859375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.0041, loss_val: nan, pos_over_neg: 288.1192626953125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9991, loss_val: nan, pos_over_neg: 635.0138549804688 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 952.4185180664062 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.0122, loss_val: nan, pos_over_neg: 411.2643127441406 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.0039, loss_val: nan, pos_over_neg: 751.8464965820312 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9935, loss_val: nan, pos_over_neg: 12334.6240234375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.0075, loss_val: nan, pos_over_neg: 623.6766357421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.0157, loss_val: nan, pos_over_neg: 528.523681640625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.0134, loss_val: nan, pos_over_neg: 626.3558349609375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.0046, loss_val: nan, pos_over_neg: 405.39678955078125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9938, loss_val: nan, pos_over_neg: 593.8229370117188 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.0078, loss_val: nan, pos_over_neg: 483.1908874511719 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9888, loss_val: nan, pos_over_neg: 2101.742919921875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9998, loss_val: nan, pos_over_neg: 2141.473876953125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 745.818359375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.0042, loss_val: nan, pos_over_neg: 310.7790832519531 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.0073, loss_val: nan, pos_over_neg: 587.8541870117188 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 2596.74853515625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.0002, loss_val: nan, pos_over_neg: 888.1239013671875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.0089, loss_val: nan, pos_over_neg: 543.33349609375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.0198, loss_val: nan, pos_over_neg: 729.2456665039062 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.0103, loss_val: nan, pos_over_neg: 649.2247314453125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.002, loss_val: nan, pos_over_neg: 294.13214111328125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.0118, loss_val: nan, pos_over_neg: 996.8489379882812 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.0011, loss_val: nan, pos_over_neg: 2155.902099609375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.0031, loss_val: nan, pos_over_neg: 579.5631713867188 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.0018, loss_val: nan, pos_over_neg: 698.5416870117188 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9857, loss_val: nan, pos_over_neg: 491.5179748535156 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9995, loss_val: nan, pos_over_neg: 691.8154296875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9974, loss_val: nan, pos_over_neg: 617.2742919921875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 488.15167236328125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.007, loss_val: nan, pos_over_neg: 728.3468017578125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.0085, loss_val: nan, pos_over_neg: 887.3841552734375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.0012, loss_val: nan, pos_over_neg: 657.9371948242188 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.0075, loss_val: nan, pos_over_neg: 519.8361206054688 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.0002, loss_val: nan, pos_over_neg: 1116.2716064453125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.0118, loss_val: nan, pos_over_neg: 1395.30224609375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9992, loss_val: nan, pos_over_neg: 366.7082824707031 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 498.9996337890625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.0059, loss_val: nan, pos_over_neg: 599.3072509765625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.0106, loss_val: nan, pos_over_neg: 1136.9849853515625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 630.8951416015625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9984, loss_val: nan, pos_over_neg: 2713.24853515625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.006, loss_val: nan, pos_over_neg: 777.2307739257812 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 1242.844970703125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9936, loss_val: nan, pos_over_neg: 1099.277587890625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9803, loss_val: nan, pos_over_neg: 506.83154296875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9997, loss_val: nan, pos_over_neg: 396.0524597167969 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9936, loss_val: nan, pos_over_neg: 736.550048828125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9899, loss_val: nan, pos_over_neg: 633.5733642578125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.0041, loss_val: nan, pos_over_neg: 2012.0068359375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9983, loss_val: nan, pos_over_neg: 444.1365661621094 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9954, loss_val: nan, pos_over_neg: 1031.600830078125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9974, loss_val: nan, pos_over_neg: 766.8360595703125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9781, loss_val: nan, pos_over_neg: 679.872314453125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9791, loss_val: nan, pos_over_neg: 721.589111328125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 575.6448974609375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 663.1730346679688 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 1103.2823486328125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9918, loss_val: nan, pos_over_neg: 773.22412109375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9792, loss_val: nan, pos_over_neg: 552.970703125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9849, loss_val: nan, pos_over_neg: 1322.9189453125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.0062, loss_val: nan, pos_over_neg: 639.7711791992188 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9962, loss_val: nan, pos_over_neg: 479.792236328125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9933, loss_val: nan, pos_over_neg: 427.4188232421875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9979, loss_val: nan, pos_over_neg: 577.4010620117188 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9982, loss_val: nan, pos_over_neg: 492.20880126953125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 640.0450439453125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 966.9716796875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.0162, loss_val: nan, pos_over_neg: 610.6343383789062 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9894, loss_val: nan, pos_over_neg: 589.075927734375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.0068, loss_val: nan, pos_over_neg: 786.7639770507812 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9904, loss_val: nan, pos_over_neg: 1290.3717041015625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9913, loss_val: nan, pos_over_neg: 971.5384521484375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9921, loss_val: nan, pos_over_neg: 6179.298828125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9997, loss_val: nan, pos_over_neg: 668.631591796875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.0163, loss_val: nan, pos_over_neg: 364.81109619140625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.0114, loss_val: nan, pos_over_neg: 510.75018310546875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.0047, loss_val: nan, pos_over_neg: 345.0652160644531 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 817.3894653320312 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 702.5608520507812 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 1626.9815673828125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9932, loss_val: nan, pos_over_neg: 618.5602416992188 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9976, loss_val: nan, pos_over_neg: 557.5751342773438 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9921, loss_val: nan, pos_over_neg: 430.80029296875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 440.37115478515625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.0091, loss_val: nan, pos_over_neg: 857.6136474609375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.0054, loss_val: nan, pos_over_neg: 664.828369140625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 935.0451049804688 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 572.7852172851562 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9907, loss_val: nan, pos_over_neg: 1183.2138671875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.0045, loss_val: nan, pos_over_neg: 469.8479309082031 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9865, loss_val: nan, pos_over_neg: 543.1097412109375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9979, loss_val: nan, pos_over_neg: 352.37677001953125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.0023, loss_val: nan, pos_over_neg: 529.85986328125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9924, loss_val: nan, pos_over_neg: 1620.6666259765625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9967, loss_val: nan, pos_over_neg: 736.677978515625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.0145, loss_val: nan, pos_over_neg: 520.8748779296875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9853, loss_val: nan, pos_over_neg: 1027.973876953125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.0005, loss_val: nan, pos_over_neg: 778.9596557617188 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.0234, loss_val: nan, pos_over_neg: 263.931640625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9981, loss_val: nan, pos_over_neg: 358.7725524902344 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.012, loss_val: nan, pos_over_neg: 1326.798828125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 858.8084106445312 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9956, loss_val: nan, pos_over_neg: 526.5324096679688 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 1054.9241943359375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.002, loss_val: nan, pos_over_neg: 692.3777465820312 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.0116, loss_val: nan, pos_over_neg: 833.784423828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9952, loss_val: nan, pos_over_neg: 930.1884765625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9979, loss_val: nan, pos_over_neg: 580.6739501953125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9871, loss_val: nan, pos_over_neg: 461.4008483886719 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.003, loss_val: nan, pos_over_neg: 4795.59228515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9808, loss_val: nan, pos_over_neg: 1481.595458984375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 592.300537109375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9843, loss_val: nan, pos_over_neg: 1162.41552734375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.0137, loss_val: nan, pos_over_neg: 489.2010803222656 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9903, loss_val: nan, pos_over_neg: 519.7482299804688 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 713.852783203125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.01, loss_val: nan, pos_over_neg: 434.8901672363281 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9842, loss_val: nan, pos_over_neg: 1264.494873046875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 740.26953125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9959, loss_val: nan, pos_over_neg: 1611.9014892578125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.0077, loss_val: nan, pos_over_neg: 1089.072509765625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9761, loss_val: nan, pos_over_neg: 2961.24072265625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.0109, loss_val: nan, pos_over_neg: 457.7064208984375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9851, loss_val: nan, pos_over_neg: 641.6893310546875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 1718.5003662109375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9837, loss_val: nan, pos_over_neg: 5634.64208984375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9882, loss_val: nan, pos_over_neg: 1052.9459228515625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9941, loss_val: nan, pos_over_neg: 1229.6126708984375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 1582.34423828125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 1070.4698486328125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 798.6128540039062 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9989, loss_val: nan, pos_over_neg: 766.283935546875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9914, loss_val: nan, pos_over_neg: 914.8311157226562 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.995, loss_val: nan, pos_over_neg: 638.9437866210938 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.0016, loss_val: nan, pos_over_neg: 618.5375366210938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9977, loss_val: nan, pos_over_neg: 4546.6201171875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9826, loss_val: nan, pos_over_neg: 1286.3905029296875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.0018, loss_val: nan, pos_over_neg: 675.0919799804688 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9915, loss_val: nan, pos_over_neg: 942.1113891601562 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9828, loss_val: nan, pos_over_neg: 253.60394287109375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 658.3189697265625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9992, loss_val: nan, pos_over_neg: 1161.23291015625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 1122.792236328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9767, loss_val: nan, pos_over_neg: 758.2197875976562 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 960.7869873046875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9931, loss_val: nan, pos_over_neg: 2287.396240234375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 565.8602905273438 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9799, loss_val: nan, pos_over_neg: 740.9690551757812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9949, loss_val: nan, pos_over_neg: 924.193359375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9879, loss_val: nan, pos_over_neg: 636.3250122070312 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.0041, loss_val: nan, pos_over_neg: 562.8175048828125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9987, loss_val: nan, pos_over_neg: 551.3480224609375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9853, loss_val: nan, pos_over_neg: 991.2622680664062 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9846, loss_val: nan, pos_over_neg: 1513.2845458984375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9871, loss_val: nan, pos_over_neg: 438.4792175292969 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9944, loss_val: nan, pos_over_neg: 4423.146484375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9872, loss_val: nan, pos_over_neg: 1644.1328125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 934.8778686523438 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9894, loss_val: nan, pos_over_neg: 560.70556640625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9965, loss_val: nan, pos_over_neg: 861.8132934570312 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 699.5509033203125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9839, loss_val: nan, pos_over_neg: 617.258056640625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.0113, loss_val: nan, pos_over_neg: 1019.3549194335938 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9953, loss_val: nan, pos_over_neg: 823.9791259765625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9896, loss_val: nan, pos_over_neg: 449.01220703125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.0032, loss_val: nan, pos_over_neg: 1119.423828125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 638.7005004882812 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.0051, loss_val: nan, pos_over_neg: 1424.0810546875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.002, loss_val: nan, pos_over_neg: 793.966552734375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9954, loss_val: nan, pos_over_neg: 1637.8704833984375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 694.2789306640625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 1739.205322265625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9927, loss_val: nan, pos_over_neg: 1225.93310546875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.0061, loss_val: nan, pos_over_neg: -2879.77099609375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 495.3711242675781 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 606.903076171875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9838, loss_val: nan, pos_over_neg: 835.533935546875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9927, loss_val: nan, pos_over_neg: 357.6175842285156 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9812, loss_val: nan, pos_over_neg: 445.4422607421875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9863, loss_val: nan, pos_over_neg: 803.453125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.0019, loss_val: nan, pos_over_neg: 288.0970458984375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 3085.9580078125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 1168.8812255859375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9895, loss_val: nan, pos_over_neg: 631.4215698242188 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.0065, loss_val: nan, pos_over_neg: 494.2590637207031 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [20:04<100396:57:48, 1204.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.9875, loss_val: nan, pos_over_neg: 469.95880126953125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9861, loss_val: nan, pos_over_neg: 772.0654296875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9805, loss_val: nan, pos_over_neg: 310.6705322265625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 632.7103881835938 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9986, loss_val: nan, pos_over_neg: 1758.383056640625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9873, loss_val: nan, pos_over_neg: 1936.077392578125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9882, loss_val: nan, pos_over_neg: 689.5416870117188 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.0079, loss_val: nan, pos_over_neg: 534.0975952148438 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 1215.5550537109375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 1119.2432861328125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9989, loss_val: nan, pos_over_neg: 915.7656860351562 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9977, loss_val: nan, pos_over_neg: 2230.77294921875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9935, loss_val: nan, pos_over_neg: 1698.4368896484375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9887, loss_val: nan, pos_over_neg: 853.6455078125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.0001, loss_val: nan, pos_over_neg: 672.1329345703125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9967, loss_val: nan, pos_over_neg: 1618.4080810546875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9999, loss_val: nan, pos_over_neg: 1540.5980224609375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9679, loss_val: nan, pos_over_neg: 3234.64599609375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9921, loss_val: nan, pos_over_neg: 889.5944213867188 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9887, loss_val: nan, pos_over_neg: 1082.2647705078125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9949, loss_val: nan, pos_over_neg: 633.7324829101562 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9999, loss_val: nan, pos_over_neg: 1203.9326171875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.978, loss_val: nan, pos_over_neg: 938.41943359375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9936, loss_val: nan, pos_over_neg: 559.7515258789062 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 1236.3876953125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9844, loss_val: nan, pos_over_neg: 1322.3509521484375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9939, loss_val: nan, pos_over_neg: 1191.2103271484375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 1823.971435546875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9737, loss_val: nan, pos_over_neg: 1372.831787109375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 1095.6265869140625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9963, loss_val: nan, pos_over_neg: 1339.690673828125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.0097, loss_val: nan, pos_over_neg: 482.0512390136719 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9947, loss_val: nan, pos_over_neg: 696.2477416992188 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9859, loss_val: nan, pos_over_neg: 986.9342651367188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.0071, loss_val: nan, pos_over_neg: 1360.665283203125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.987, loss_val: nan, pos_over_neg: 719.37890625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9812, loss_val: nan, pos_over_neg: 785.8077392578125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 1283.38623046875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9738, loss_val: nan, pos_over_neg: 680.6217651367188 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9961, loss_val: nan, pos_over_neg: 337.9303894042969 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9922, loss_val: nan, pos_over_neg: 668.7075805664062 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 698.581298828125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.0085, loss_val: nan, pos_over_neg: 446.8855285644531 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.992, loss_val: nan, pos_over_neg: 760.2935180664062 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 6651.677734375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 516.298828125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9813, loss_val: nan, pos_over_neg: 780.6212158203125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 749.2164916992188 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 8267.9697265625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.0013, loss_val: nan, pos_over_neg: 578.846923828125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 563.7952880859375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9851, loss_val: nan, pos_over_neg: 3993.560791015625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9883, loss_val: nan, pos_over_neg: 652.3406982421875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 632.6306762695312 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 1164.10498046875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 1539.1484375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9831, loss_val: nan, pos_over_neg: 925.8917236328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 575.99169921875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9705, loss_val: nan, pos_over_neg: -7848.08203125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 15429.1533203125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9751, loss_val: nan, pos_over_neg: 847.3267211914062 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 686.9932250976562 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 1479.008056640625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 670.9856567382812 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9961, loss_val: nan, pos_over_neg: 592.7208251953125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9913, loss_val: nan, pos_over_neg: 740.1051025390625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9941, loss_val: nan, pos_over_neg: 608.6661376953125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 465.9967041015625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9923, loss_val: nan, pos_over_neg: 1727.575439453125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 4593.44384765625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9874, loss_val: nan, pos_over_neg: 335.87298583984375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9818, loss_val: nan, pos_over_neg: 677.199951171875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9837, loss_val: nan, pos_over_neg: 1213.970703125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9852, loss_val: nan, pos_over_neg: 514.5719604492188 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 391.6199035644531 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9741, loss_val: nan, pos_over_neg: 441.7149658203125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9866, loss_val: nan, pos_over_neg: 697.9732055664062 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.0123, loss_val: nan, pos_over_neg: 688.0787963867188 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9872, loss_val: nan, pos_over_neg: 638.0595703125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9785, loss_val: nan, pos_over_neg: 1023.1522216796875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 824.3236694335938 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9833, loss_val: nan, pos_over_neg: 827.9112548828125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9745, loss_val: nan, pos_over_neg: 1041.10009765625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9905, loss_val: nan, pos_over_neg: 673.9204711914062 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9868, loss_val: nan, pos_over_neg: 870.5256958007812 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9899, loss_val: nan, pos_over_neg: 636.2478637695312 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9824, loss_val: nan, pos_over_neg: 944.4832763671875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9889, loss_val: nan, pos_over_neg: 1335.2286376953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 458.9210205078125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9735, loss_val: nan, pos_over_neg: 384.53082275390625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.991, loss_val: nan, pos_over_neg: 1147.65576171875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 481.1462097167969 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9796, loss_val: nan, pos_over_neg: 559.3778686523438 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9722, loss_val: nan, pos_over_neg: 1061.6339111328125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1312.9197998046875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9759, loss_val: nan, pos_over_neg: 739.24658203125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 601.40087890625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 1529.624267578125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9678, loss_val: nan, pos_over_neg: 1324.8895263671875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9901, loss_val: nan, pos_over_neg: 908.1839599609375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9771, loss_val: nan, pos_over_neg: 1405.2017822265625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9772, loss_val: nan, pos_over_neg: 741.8477172851562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9853, loss_val: nan, pos_over_neg: 678.5794677734375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9755, loss_val: nan, pos_over_neg: 557.8493041992188 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 3301.474609375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9624, loss_val: nan, pos_over_neg: 1122.82177734375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9809, loss_val: nan, pos_over_neg: 723.4447631835938 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9989, loss_val: nan, pos_over_neg: 2168.793212890625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.993, loss_val: nan, pos_over_neg: 702.9869384765625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9796, loss_val: nan, pos_over_neg: 416.7487487792969 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 1245.0238037109375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9892, loss_val: nan, pos_over_neg: 5216.251953125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9826, loss_val: nan, pos_over_neg: 699.7113647460938 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9898, loss_val: nan, pos_over_neg: 958.27978515625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9877, loss_val: nan, pos_over_neg: 977.1764526367188 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9799, loss_val: nan, pos_over_neg: 1446.2904052734375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 1137.6038818359375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9986, loss_val: nan, pos_over_neg: 398.0068664550781 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 675.094970703125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9869, loss_val: nan, pos_over_neg: 836.86474609375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 1649.87255859375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9809, loss_val: nan, pos_over_neg: 1027.2110595703125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9877, loss_val: nan, pos_over_neg: 806.773193359375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 752.9053344726562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9765, loss_val: nan, pos_over_neg: 373.3514099121094 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9914, loss_val: nan, pos_over_neg: 478.8707275390625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 821.047119140625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 1136.5592041015625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 977.599609375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9993, loss_val: nan, pos_over_neg: 1174.6563720703125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9793, loss_val: nan, pos_over_neg: 1026.0428466796875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9932, loss_val: nan, pos_over_neg: 1025.682861328125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9882, loss_val: nan, pos_over_neg: 468.5310974121094 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9712, loss_val: nan, pos_over_neg: 499.6485290527344 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 538.0055541992188 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 940.30859375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9761, loss_val: nan, pos_over_neg: 1819.2005615234375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9897, loss_val: nan, pos_over_neg: 492.61260986328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.988, loss_val: nan, pos_over_neg: 810.0043334960938 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 1645.2650146484375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 851.0156860351562 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 3134.5517578125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.982, loss_val: nan, pos_over_neg: 862.4802856445312 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 376.8258972167969 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9665, loss_val: nan, pos_over_neg: 949.5859985351562 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9977, loss_val: nan, pos_over_neg: 1625.1573486328125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9785, loss_val: nan, pos_over_neg: 1549.0799560546875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.979, loss_val: nan, pos_over_neg: 5791.6767578125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9722, loss_val: nan, pos_over_neg: 1341.2021484375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.97, loss_val: nan, pos_over_neg: 658.91650390625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9911, loss_val: nan, pos_over_neg: 514.5269775390625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.976, loss_val: nan, pos_over_neg: 620.8794555664062 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 319.5826721191406 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 1562.0616455078125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 17164.671875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9673, loss_val: nan, pos_over_neg: 1011.3389892578125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 1518.2052001953125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 1551.9732666015625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 1873.181640625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9778, loss_val: nan, pos_over_neg: 437.23101806640625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 1530.506103515625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9745, loss_val: nan, pos_over_neg: 2226.062255859375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9795, loss_val: nan, pos_over_neg: 1939.699462890625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9852, loss_val: nan, pos_over_neg: 1375.575927734375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9783, loss_val: nan, pos_over_neg: 538.0488891601562 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9826, loss_val: nan, pos_over_neg: 865.1670532226562 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 509.67303466796875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9715, loss_val: nan, pos_over_neg: 1474.1998291015625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 508.40966796875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.981, loss_val: nan, pos_over_neg: 1293.025634765625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9776, loss_val: nan, pos_over_neg: 805.3519897460938 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9753, loss_val: nan, pos_over_neg: 1058.2003173828125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.99, loss_val: nan, pos_over_neg: 498.79437255859375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9779, loss_val: nan, pos_over_neg: 1057.5216064453125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.985, loss_val: nan, pos_over_neg: 529.5321044921875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 917.1822509765625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9821, loss_val: nan, pos_over_neg: 1169.546630859375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.978, loss_val: nan, pos_over_neg: 856.83837890625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9731, loss_val: nan, pos_over_neg: 638.5450439453125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9993, loss_val: nan, pos_over_neg: 691.8851318359375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 632.3707275390625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.975, loss_val: nan, pos_over_neg: 944.8721923828125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 1078.6890869140625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 727.0203247070312 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9883, loss_val: nan, pos_over_neg: 988.8619995117188 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9763, loss_val: nan, pos_over_neg: 2642.31298828125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 1085.6593017578125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.98, loss_val: nan, pos_over_neg: 599.451171875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9858, loss_val: nan, pos_over_neg: 646.1713256835938 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 732.798583984375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9863, loss_val: nan, pos_over_neg: 463.0094299316406 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 602.6416015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9877, loss_val: nan, pos_over_neg: 1041.7828369140625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9643, loss_val: nan, pos_over_neg: 1182.92724609375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9759, loss_val: nan, pos_over_neg: 1215.00927734375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 452.0303039550781 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9717, loss_val: nan, pos_over_neg: 568.6091918945312 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9639, loss_val: nan, pos_over_neg: 1132.272216796875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9873, loss_val: nan, pos_over_neg: 908.4934692382812 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 852.081787109375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9765, loss_val: nan, pos_over_neg: 933.4096069335938 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 6002.93798828125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9694, loss_val: nan, pos_over_neg: 3122.68017578125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 549.9210205078125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9944, loss_val: nan, pos_over_neg: 639.3357543945312 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.965, loss_val: nan, pos_over_neg: 7861.5234375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 2645.697998046875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9778, loss_val: nan, pos_over_neg: 587.5460815429688 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 815.8924560546875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9762, loss_val: nan, pos_over_neg: 763.750732421875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9788, loss_val: nan, pos_over_neg: 461.8069763183594 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9789, loss_val: nan, pos_over_neg: 1184.5350341796875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 986.392333984375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9816, loss_val: nan, pos_over_neg: 988.4703369140625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 736.7063598632812 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9764, loss_val: nan, pos_over_neg: 1228.529541015625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9859, loss_val: nan, pos_over_neg: 447.4761962890625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9718, loss_val: nan, pos_over_neg: 1442.0445556640625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.979, loss_val: nan, pos_over_neg: 804.4490356445312 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9828, loss_val: nan, pos_over_neg: 5055.88671875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9677, loss_val: nan, pos_over_neg: 2238.234375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 580.0635986328125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9722, loss_val: nan, pos_over_neg: 1196.841552734375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 1063.4736328125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9694, loss_val: nan, pos_over_neg: 767.97509765625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9845, loss_val: nan, pos_over_neg: 2106.756591796875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 652.819091796875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 1607.9405517578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 2850.692626953125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9557, loss_val: nan, pos_over_neg: 1105.625732421875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9763, loss_val: nan, pos_over_neg: 1278.074951171875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9847, loss_val: nan, pos_over_neg: 771.2838745117188 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.971, loss_val: nan, pos_over_neg: 1062.711181640625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.991, loss_val: nan, pos_over_neg: 2462.49072265625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.971, loss_val: nan, pos_over_neg: 2554.848876953125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9788, loss_val: nan, pos_over_neg: 602.814208984375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9801, loss_val: nan, pos_over_neg: 729.6902465820312 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9834, loss_val: nan, pos_over_neg: 956.7289428710938 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9803, loss_val: nan, pos_over_neg: 1220.564208984375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9833, loss_val: nan, pos_over_neg: 958.101806640625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.978, loss_val: nan, pos_over_neg: 396.1448059082031 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9829, loss_val: nan, pos_over_neg: 829.7393188476562 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9747, loss_val: nan, pos_over_neg: 458.4014892578125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 612.3461303710938 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9777, loss_val: nan, pos_over_neg: 489.8993835449219 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9897, loss_val: nan, pos_over_neg: 968.3895874023438 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9709, loss_val: nan, pos_over_neg: 595.4220581054688 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9807, loss_val: nan, pos_over_neg: 916.4125366210938 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 1202.8458251953125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9825, loss_val: nan, pos_over_neg: 2188.975341796875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9639, loss_val: nan, pos_over_neg: 1641.7030029296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9724, loss_val: nan, pos_over_neg: 357.1915283203125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9749, loss_val: nan, pos_over_neg: 689.91064453125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9773, loss_val: nan, pos_over_neg: 859.2175903320312 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 1430.8817138671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9689, loss_val: nan, pos_over_neg: 1198.0089111328125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9768, loss_val: nan, pos_over_neg: 1353.5460205078125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9924, loss_val: nan, pos_over_neg: 9147.45703125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 999.033447265625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 333.26458740234375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9783, loss_val: nan, pos_over_neg: 1136.052001953125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9748, loss_val: nan, pos_over_neg: 1373.11376953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9797, loss_val: nan, pos_over_neg: 553.9606323242188 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.982, loss_val: nan, pos_over_neg: 534.4692993164062 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 479.22137451171875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9699, loss_val: nan, pos_over_neg: 709.4196166992188 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9798, loss_val: nan, pos_over_neg: 658.6203002929688 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9797, loss_val: nan, pos_over_neg: 2738.8115234375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9644, loss_val: nan, pos_over_neg: 1664.9930419921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9589, loss_val: nan, pos_over_neg: 944.00537109375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9717, loss_val: nan, pos_over_neg: 505.8251953125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9813, loss_val: nan, pos_over_neg: 1447.5550537109375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9879, loss_val: nan, pos_over_neg: 577.3998413085938 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9869, loss_val: nan, pos_over_neg: 593.8554077148438 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.965, loss_val: nan, pos_over_neg: 1666.4521484375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 796.0234985351562 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9839, loss_val: nan, pos_over_neg: 905.9382934570312 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9775, loss_val: nan, pos_over_neg: 825.4689331054688 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 610.896484375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9729, loss_val: nan, pos_over_neg: 948.7988891601562 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9682, loss_val: nan, pos_over_neg: 508.6755065917969 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.987, loss_val: nan, pos_over_neg: 562.1314086914062 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9775, loss_val: nan, pos_over_neg: 857.8858032226562 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9619, loss_val: nan, pos_over_neg: 750.599365234375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 1866.9029541015625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9769, loss_val: nan, pos_over_neg: 1986.521240234375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9913, loss_val: nan, pos_over_neg: 232.84912109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9755, loss_val: nan, pos_over_neg: 478.8553771972656 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9758, loss_val: nan, pos_over_neg: 324.46026611328125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9852, loss_val: nan, pos_over_neg: 3307.375244140625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.975, loss_val: nan, pos_over_neg: 1033.1656494140625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9906, loss_val: nan, pos_over_neg: 481.8027038574219 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9699, loss_val: nan, pos_over_neg: 1854.3489990234375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9793, loss_val: nan, pos_over_neg: 1259.497314453125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9674, loss_val: nan, pos_over_neg: 345.2754211425781 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9719, loss_val: nan, pos_over_neg: 759.5073852539062 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9958, loss_val: nan, pos_over_neg: 380.3766784667969 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 706.6402587890625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 1740.458740234375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9692, loss_val: nan, pos_over_neg: 2995.749755859375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9617, loss_val: nan, pos_over_neg: 1761.1358642578125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9837, loss_val: nan, pos_over_neg: 696.5675048828125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 1185.3621826171875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 586.330810546875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 460.251220703125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9692, loss_val: nan, pos_over_neg: 523.074462890625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9684, loss_val: nan, pos_over_neg: 1284.4139404296875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9779, loss_val: nan, pos_over_neg: 520.9823608398438 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9724, loss_val: nan, pos_over_neg: 463.5460205078125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 4147.9189453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9776, loss_val: nan, pos_over_neg: 1406.49462890625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9755, loss_val: nan, pos_over_neg: 877.779296875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9747, loss_val: nan, pos_over_neg: 425.17236328125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9764, loss_val: nan, pos_over_neg: 2120.573974609375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 1697.191162109375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 763.9672241210938 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 455.0662841796875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9789, loss_val: nan, pos_over_neg: 1878.7562255859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 939.6615600585938 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9652, loss_val: nan, pos_over_neg: 749.1170654296875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 526.7742309570312 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 1187.348388671875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9683, loss_val: nan, pos_over_neg: 357.8694152832031 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9692, loss_val: nan, pos_over_neg: 841.5701904296875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9759, loss_val: nan, pos_over_neg: 707.94140625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9774, loss_val: nan, pos_over_neg: 1428.4688720703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 645.9702758789062 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9703, loss_val: nan, pos_over_neg: 551.0911254882812 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 1990.3326416015625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.964, loss_val: nan, pos_over_neg: 6301.95166015625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 521.3844604492188 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 1717.418212890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9713, loss_val: nan, pos_over_neg: 936.2583618164062 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9608, loss_val: nan, pos_over_neg: 1121.7945556640625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9764, loss_val: nan, pos_over_neg: 567.8416748046875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9753, loss_val: nan, pos_over_neg: 960.857666015625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9779, loss_val: nan, pos_over_neg: 1620.406982421875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 970.1314697265625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9795, loss_val: nan, pos_over_neg: 433.4027404785156 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 3913.786865234375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 4515.2890625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9749, loss_val: nan, pos_over_neg: 8994.0126953125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9643, loss_val: nan, pos_over_neg: 508.52056884765625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9772, loss_val: nan, pos_over_neg: 732.0601196289062 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9632, loss_val: nan, pos_over_neg: 1328.044921875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9676, loss_val: nan, pos_over_neg: 667.2401733398438 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9772, loss_val: nan, pos_over_neg: 1014.1170043945312 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 502.0700378417969 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 286.74896240234375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9744, loss_val: nan, pos_over_neg: 870.512939453125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9669, loss_val: nan, pos_over_neg: 922.410888671875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 605.6013793945312 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9828, loss_val: nan, pos_over_neg: 1138.597412109375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9647, loss_val: nan, pos_over_neg: 1923.8277587890625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9786, loss_val: nan, pos_over_neg: 953.470703125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9768, loss_val: nan, pos_over_neg: 453.6964416503906 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9797, loss_val: nan, pos_over_neg: 551.3418579101562 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9816, loss_val: nan, pos_over_neg: 1188.3787841796875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 946.6058959960938 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9737, loss_val: nan, pos_over_neg: 780.218994140625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9636, loss_val: nan, pos_over_neg: 394.9588928222656 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9783, loss_val: nan, pos_over_neg: 529.536376953125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9765, loss_val: nan, pos_over_neg: 1367.9495849609375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9673, loss_val: nan, pos_over_neg: 562.7493896484375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.967, loss_val: nan, pos_over_neg: 743.1942749023438 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 978.0079956054688 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9601, loss_val: nan, pos_over_neg: 841.6701049804688 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9768, loss_val: nan, pos_over_neg: 410.2208557128906 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9718, loss_val: nan, pos_over_neg: 512.779052734375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 1156.7916259765625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9746, loss_val: nan, pos_over_neg: 751.9068603515625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 566.1837158203125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9607, loss_val: nan, pos_over_neg: 557.264404296875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9711, loss_val: nan, pos_over_neg: 1011.8155517578125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9634, loss_val: nan, pos_over_neg: 606.95703125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 494.7795715332031 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9897, loss_val: nan, pos_over_neg: 486.4346618652344 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 1202.2496337890625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.967, loss_val: nan, pos_over_neg: 547.0509643554688 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9763, loss_val: nan, pos_over_neg: 365.3199157714844 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 343.45758056640625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9676, loss_val: nan, pos_over_neg: 1213.845703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 696.6041259765625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 552.7570190429688 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9639, loss_val: nan, pos_over_neg: 560.32177734375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9699, loss_val: nan, pos_over_neg: 1099.6044921875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 638.0543212890625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9645, loss_val: nan, pos_over_neg: 447.4938049316406 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9835, loss_val: nan, pos_over_neg: 391.8464660644531 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 4520.119140625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9674, loss_val: nan, pos_over_neg: 497.76092529296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9719, loss_val: nan, pos_over_neg: 677.8692016601562 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9724, loss_val: nan, pos_over_neg: 762.1080322265625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 522.0728149414062 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9586, loss_val: nan, pos_over_neg: 1014.978271484375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9742, loss_val: nan, pos_over_neg: 464.874755859375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9718, loss_val: nan, pos_over_neg: 404.5363464355469 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 739.2767333984375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 646.8580932617188 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9795, loss_val: nan, pos_over_neg: 1302.1578369140625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 922.47509765625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.964, loss_val: nan, pos_over_neg: 951.628173828125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9673, loss_val: nan, pos_over_neg: 688.96875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 725.5242309570312 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9888, loss_val: nan, pos_over_neg: 437.7524719238281 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9735, loss_val: nan, pos_over_neg: 450.69781494140625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9666, loss_val: nan, pos_over_neg: 1369.7144775390625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9648, loss_val: nan, pos_over_neg: 3835.99560546875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 1188.7506103515625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9746, loss_val: nan, pos_over_neg: 7406.42041015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9645, loss_val: nan, pos_over_neg: 3120.013916015625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 1151.62353515625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 827.2958984375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9631, loss_val: nan, pos_over_neg: 745.8223876953125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 1986.5555419921875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9666, loss_val: nan, pos_over_neg: 903.6984252929688 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9592, loss_val: nan, pos_over_neg: 1317.2152099609375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 508.4697265625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.977, loss_val: nan, pos_over_neg: 416.3586730957031 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9813, loss_val: nan, pos_over_neg: 552.0960083007812 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9766, loss_val: nan, pos_over_neg: 1004.97265625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 462.0447692871094 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9549, loss_val: nan, pos_over_neg: 3320.1181640625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1127.823974609375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9588, loss_val: nan, pos_over_neg: 788.3720092773438 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 488.1983642578125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 602.4476318359375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1030.4886474609375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 412.1879577636719 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9713, loss_val: nan, pos_over_neg: 390.13726806640625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 1233.0308837890625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 874.751220703125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9622, loss_val: nan, pos_over_neg: 504.50067138671875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 515.1923828125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9728, loss_val: nan, pos_over_neg: 449.8815612792969 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9612, loss_val: nan, pos_over_neg: 449.74273681640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 580.4939575195312 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9676, loss_val: nan, pos_over_neg: 903.388671875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 1178.62109375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9697, loss_val: nan, pos_over_neg: 308.0848388671875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9708, loss_val: nan, pos_over_neg: 837.8548583984375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9659, loss_val: nan, pos_over_neg: 1242.908203125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9743, loss_val: nan, pos_over_neg: 1540.0098876953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9677, loss_val: nan, pos_over_neg: 814.2664794921875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9791, loss_val: nan, pos_over_neg: 749.692138671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9732, loss_val: nan, pos_over_neg: 3404.0810546875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9628, loss_val: nan, pos_over_neg: 1310.4049072265625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 1011.6948852539062 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9775, loss_val: nan, pos_over_neg: 1221.2928466796875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9643, loss_val: nan, pos_over_neg: 3457.349365234375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 12768.3115234375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9697, loss_val: nan, pos_over_neg: 404.7817687988281 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 605.9224853515625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9702, loss_val: nan, pos_over_neg: 1139.44775390625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 1742.3363037109375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9647, loss_val: nan, pos_over_neg: 496.8490295410156 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9592, loss_val: nan, pos_over_neg: 839.8062133789062 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9564, loss_val: nan, pos_over_neg: 1320.5130615234375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9725, loss_val: nan, pos_over_neg: 780.1524047851562 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 703.2794799804688 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9821, loss_val: nan, pos_over_neg: 662.8763427734375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 1306.54833984375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 800.7947998046875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9623, loss_val: nan, pos_over_neg: 1609.025634765625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9524, loss_val: nan, pos_over_neg: 1083.283935546875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9581, loss_val: nan, pos_over_neg: 1044.728271484375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.967, loss_val: nan, pos_over_neg: 691.0792236328125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 720.40966796875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9474, loss_val: nan, pos_over_neg: 2525.342041015625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 3214.02978515625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1317.2322998046875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 942.0738525390625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9612, loss_val: nan, pos_over_neg: 570.2174072265625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9568, loss_val: nan, pos_over_neg: 4640.125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9723, loss_val: nan, pos_over_neg: 1599.954345703125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 1055.3841552734375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9696, loss_val: nan, pos_over_neg: 907.3294067382812 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9605, loss_val: nan, pos_over_neg: 870.5505981445312 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 809.7474975585938 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9622, loss_val: nan, pos_over_neg: 1030.9771728515625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9513, loss_val: nan, pos_over_neg: 599.5807495117188 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9817, loss_val: nan, pos_over_neg: 7224.55712890625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9683, loss_val: nan, pos_over_neg: 2376.810302734375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9757, loss_val: nan, pos_over_neg: 688.2020874023438 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 3964.36474609375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9674, loss_val: nan, pos_over_neg: 1285.2430419921875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 2082.731201171875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9592, loss_val: nan, pos_over_neg: 598.3094482421875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 1603.3421630859375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9656, loss_val: nan, pos_over_neg: 767.6441650390625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9789, loss_val: nan, pos_over_neg: 457.29718017578125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9606, loss_val: nan, pos_over_neg: 1200.744873046875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9646, loss_val: nan, pos_over_neg: 2130.322021484375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9552, loss_val: nan, pos_over_neg: 1105.4166259765625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9712, loss_val: nan, pos_over_neg: 751.512451171875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 470.40277099609375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9738, loss_val: nan, pos_over_neg: 946.0438842773438 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9647, loss_val: nan, pos_over_neg: 369.2502136230469 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9619, loss_val: nan, pos_over_neg: 731.0771484375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 470.96978759765625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 529.594482421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 943.843994140625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 887.41162109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 936.0270385742188 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9626, loss_val: nan, pos_over_neg: 469.7942810058594 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9532, loss_val: nan, pos_over_neg: 576.9178466796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 587.2633056640625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 839.05859375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 589.7533569335938 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9714, loss_val: nan, pos_over_neg: 948.5181274414062 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9746, loss_val: nan, pos_over_neg: 690.265869140625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9769, loss_val: nan, pos_over_neg: 903.802978515625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.983, loss_val: nan, pos_over_neg: 696.721435546875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 620.91796875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.964, loss_val: nan, pos_over_neg: 829.524169921875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9703, loss_val: nan, pos_over_neg: 588.1602172851562 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 742.8314819335938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9588, loss_val: nan, pos_over_neg: 769.0362548828125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9675, loss_val: nan, pos_over_neg: 1512.1510009765625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9702, loss_val: nan, pos_over_neg: 881.3226928710938 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 1347.671875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 632.6713256835938 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 4244.26953125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9624, loss_val: nan, pos_over_neg: 690.6947021484375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9514, loss_val: nan, pos_over_neg: 697.7433471679688 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1737.59619140625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9685, loss_val: nan, pos_over_neg: 3982.197021484375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 673.8309326171875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9777, loss_val: nan, pos_over_neg: 726.1320190429688 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9585, loss_val: nan, pos_over_neg: 2247.962646484375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9553, loss_val: nan, pos_over_neg: 296183.3125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9659, loss_val: nan, pos_over_neg: 2436.38818359375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 715.6333618164062 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9655, loss_val: nan, pos_over_neg: 1218.845947265625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: -3969.79296875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 2281.530029296875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 711.8531494140625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 2051.347412109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: -4557.1279296875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: -515017.875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1258.205322265625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 1166.129638671875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9655, loss_val: nan, pos_over_neg: 855.7432861328125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9651, loss_val: nan, pos_over_neg: 2511.8310546875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9725, loss_val: nan, pos_over_neg: 517.4549560546875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 4197.2431640625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 1286.38720703125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9642, loss_val: nan, pos_over_neg: 1157.4814453125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 2733.314208984375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 999.3907470703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9615, loss_val: nan, pos_over_neg: 1442.4149169921875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9736, loss_val: nan, pos_over_neg: 1282.9268798828125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 11696.013671875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9736, loss_val: nan, pos_over_neg: 811.56201171875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9723, loss_val: nan, pos_over_neg: 1712.741455078125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9656, loss_val: nan, pos_over_neg: 1080.309326171875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 913.7352294921875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9705, loss_val: nan, pos_over_neg: 769.8062744140625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9611, loss_val: nan, pos_over_neg: 2817.762451171875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9607, loss_val: nan, pos_over_neg: 1328.258056640625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.961, loss_val: nan, pos_over_neg: 1279.1083984375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9583, loss_val: nan, pos_over_neg: 785.1640625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.963, loss_val: nan, pos_over_neg: 1029.4671630859375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 2185.88818359375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 751.76904296875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 1307.439453125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 820.748291015625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9563, loss_val: nan, pos_over_neg: 701.924072265625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9594, loss_val: nan, pos_over_neg: 1102.3328857421875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9568, loss_val: nan, pos_over_neg: 2187.98876953125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 1009.7761840820312 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 11661.1650390625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9654, loss_val: nan, pos_over_neg: 2835.457763671875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9608, loss_val: nan, pos_over_neg: 604.6597290039062 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 1280.874267578125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 537.4255981445312 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9697, loss_val: nan, pos_over_neg: 519.0641479492188 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 558.7068481445312 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1011.2106323242188 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 494.88677978515625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 2112.949951171875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 398.3558044433594 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 290.4610900878906 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 972.3668212890625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1478.8951416015625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 448.6534118652344 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9656, loss_val: nan, pos_over_neg: 2277.450927734375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 488.0547180175781 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: 783.10400390625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 497.8551330566406 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 419.9805908203125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 788.6354370117188 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9481, loss_val: nan, pos_over_neg: 959.4708862304688 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9665, loss_val: nan, pos_over_neg: 585.2483520507812 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 806.2801513671875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 486.4857177734375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9669, loss_val: nan, pos_over_neg: 542.2825317382812 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.959, loss_val: nan, pos_over_neg: 541.7899169921875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 602.3126220703125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 461.54681396484375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 734.4088745117188 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9668, loss_val: nan, pos_over_neg: 790.1854858398438 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9544, loss_val: nan, pos_over_neg: 811.9652709960938 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9714, loss_val: nan, pos_over_neg: 892.7135009765625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 637.9096069335938 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 871.8743896484375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9679, loss_val: nan, pos_over_neg: 1199.1195068359375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 4600.462890625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 890.80908203125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9591, loss_val: nan, pos_over_neg: 5504.2109375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 2510.045166015625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9539, loss_val: nan, pos_over_neg: 746.1331787109375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9504, loss_val: nan, pos_over_neg: 2426.5517578125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 4879.02880859375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 553.228515625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9651, loss_val: nan, pos_over_neg: 717.2623291015625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 1275.279296875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 990.8941650390625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9758, loss_val: nan, pos_over_neg: 1640.0068359375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: -11573.78515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 1369.968017578125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9589, loss_val: nan, pos_over_neg: 1477.1622314453125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 680.2684326171875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 1807.206298828125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9576, loss_val: nan, pos_over_neg: 777.9884643554688 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 1189.127197265625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9466, loss_val: nan, pos_over_neg: 820.2384033203125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9623, loss_val: nan, pos_over_neg: 624.1328125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1801.5367431640625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1885.7322998046875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 443.67474365234375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 1447.322265625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.96, loss_val: nan, pos_over_neg: 8363.587890625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 2132.083740234375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 1082.0228271484375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9605, loss_val: nan, pos_over_neg: 575.5985107421875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 838.27783203125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 472.874267578125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 803.6258544921875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9704, loss_val: nan, pos_over_neg: 961.8768310546875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 1965.4407958984375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 1118.4678955078125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9698, loss_val: nan, pos_over_neg: 506.23828125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 1465.3804931640625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 732.2445678710938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 436.22998046875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 924.322021484375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1419.9583740234375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9645, loss_val: nan, pos_over_neg: 637.22509765625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9634, loss_val: nan, pos_over_neg: 1040.20458984375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 950.6099853515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9533, loss_val: nan, pos_over_neg: 917.0897827148438 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9624, loss_val: nan, pos_over_neg: 721.452880859375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 2030.283935546875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 1586.4093017578125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: -24419.587890625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9539, loss_val: nan, pos_over_neg: 6483.4462890625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 14997.5771484375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9585, loss_val: nan, pos_over_neg: 1146.3111572265625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 1420.204833984375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9704, loss_val: nan, pos_over_neg: 1601.3232421875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 1545.6761474609375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9686, loss_val: nan, pos_over_neg: 1268.9254150390625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9578, loss_val: nan, pos_over_neg: 786.5825805664062 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9524, loss_val: nan, pos_over_neg: 3806.65576171875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 3471.994384765625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 660.742919921875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 3314.91064453125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9703, loss_val: nan, pos_over_neg: 1817.2921142578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 927.0296630859375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9543, loss_val: nan, pos_over_neg: 952.655517578125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 2991.224609375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9525, loss_val: nan, pos_over_neg: 3403.689208984375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9594, loss_val: nan, pos_over_neg: 489.39215087890625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 646.7020874023438 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 1688.8878173828125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9597, loss_val: nan, pos_over_neg: 5895.12158203125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 1418.540283203125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 661.1383666992188 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 2200.21240234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 3191.883056640625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9513, loss_val: nan, pos_over_neg: 660.2813110351562 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 1909.222412109375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.954, loss_val: nan, pos_over_neg: 1007.8494262695312 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9685, loss_val: nan, pos_over_neg: 996.0789184570312 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 3909.733154296875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 1378.0133056640625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 450.6241760253906 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 1434.15576171875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 5036.44580078125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 1033.6033935546875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 1193.52392578125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9644, loss_val: nan, pos_over_neg: 675.296142578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1983.935791015625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 639.9163818359375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [39:59<99882:36:26, 1198.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.9515, loss_val: nan, pos_over_neg: 898.3248291015625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 1315.1134033203125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 1578.4510498046875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 759.9706420898438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9578, loss_val: nan, pos_over_neg: -269886.21875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 1101.626220703125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1212.3043212890625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 1486.691162109375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9626, loss_val: nan, pos_over_neg: 1935.41796875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 816.0136108398438 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 2685.3955078125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 1424.27001953125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 1007.8795166015625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 1199.0927734375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 1070.1007080078125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 1705.08349609375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 719.0358276367188 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9633, loss_val: nan, pos_over_neg: 1277.9774169921875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9637, loss_val: nan, pos_over_neg: 985.5642700195312 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9668, loss_val: nan, pos_over_neg: 15415.9130859375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1100.745361328125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9616, loss_val: nan, pos_over_neg: 1217.9166259765625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 885.1468505859375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 1325.717529296875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 1400.3780517578125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1043.866943359375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 1581.3206787109375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 2716.7841796875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 686.2576904296875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 464.1960754394531 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 371.7525634765625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: 786.47265625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 513.1532592773438 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9632, loss_val: nan, pos_over_neg: 649.6608276367188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 2583.822998046875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 1377.674560546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 835.5100708007812 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 518.4733276367188 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 642.1566162109375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1157.27197265625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 1014.2764892578125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 1057.9716796875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9576, loss_val: nan, pos_over_neg: 1513.804931640625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 5403.34228515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.968, loss_val: nan, pos_over_neg: 31765.1640625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 1223.6053466796875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9663, loss_val: nan, pos_over_neg: 1369.6419677734375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 1347.02880859375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 1628.74609375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 756.1696166992188 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 663.552734375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 614.8692626953125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9648, loss_val: nan, pos_over_neg: 1222.52734375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9596, loss_val: nan, pos_over_neg: 1219.9525146484375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 2156.175048828125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 813.4017944335938 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9515, loss_val: nan, pos_over_neg: 2887.8017578125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 1123.6185302734375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 748.7470703125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 713.3355712890625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 1635.6240234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 794.7320556640625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9588, loss_val: nan, pos_over_neg: 584.4747924804688 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9636, loss_val: nan, pos_over_neg: 3825.06396484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 159049.0 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.946, loss_val: nan, pos_over_neg: 1546.226806640625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9678, loss_val: nan, pos_over_neg: 526.2426147460938 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 4894.5380859375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 2520.0712890625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.957, loss_val: nan, pos_over_neg: 496.53070068359375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 807.8234252929688 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 1120.7161865234375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 2389.380615234375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 871.2636108398438 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 967.8319702148438 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 580.412841796875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 2528.23388671875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 1002.6829223632812 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 948.2163696289062 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 929.9120483398438 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9594, loss_val: nan, pos_over_neg: 654.8511962890625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 778.741455078125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9523, loss_val: nan, pos_over_neg: 608.773681640625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 625.234130859375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 677.2163696289062 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9583, loss_val: nan, pos_over_neg: 798.3263549804688 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 1271.94921875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 2429.376953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 997.812744140625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9583, loss_val: nan, pos_over_neg: 675.6461791992188 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9603, loss_val: nan, pos_over_neg: 799.535400390625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 731.5720825195312 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 4186.01123046875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9663, loss_val: nan, pos_over_neg: 768.928466796875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 550.9379272460938 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 1008.6574096679688 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 4665.27294921875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 1204.718994140625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 867.858642578125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 1006.3767700195312 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9629, loss_val: nan, pos_over_neg: 1251.1087646484375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9607, loss_val: nan, pos_over_neg: 1509.0972900390625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 781.47705078125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9597, loss_val: nan, pos_over_neg: 1416.4951171875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1988.9091796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 767.012939453125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 1394.352294921875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 32425.33984375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 940.2904052734375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 1016.434326171875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 417.75897216796875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1874.093017578125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9617, loss_val: nan, pos_over_neg: 1170.6910400390625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9786, loss_val: nan, pos_over_neg: 1647.4954833984375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 617.9005737304688 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 528.5689697265625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 1124.2291259765625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9549, loss_val: nan, pos_over_neg: 798.12353515625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 455.1931457519531 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9495, loss_val: nan, pos_over_neg: 743.5391235351562 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 924.8502807617188 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 2237.84130859375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 869.3904418945312 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 833.8619995117188 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9616, loss_val: nan, pos_over_neg: 929.8959350585938 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 1044.5474853515625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.948, loss_val: nan, pos_over_neg: 2394.527587890625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 994.7643432617188 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 4087.9052734375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.953, loss_val: nan, pos_over_neg: -11571.8291015625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 5463.74560546875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 5034.705078125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 1816.13330078125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9589, loss_val: nan, pos_over_neg: 1052.8199462890625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9529, loss_val: nan, pos_over_neg: 619.7344970703125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 801.9523315429688 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9606, loss_val: nan, pos_over_neg: 1382.4490966796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 2366.36181640625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9615, loss_val: nan, pos_over_neg: 695.364501953125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 450.88250732421875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9514, loss_val: nan, pos_over_neg: 1003.4681396484375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 755.3826904296875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9576, loss_val: nan, pos_over_neg: 614.8388061523438 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 498.4332580566406 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 441.0530090332031 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 280.06207275390625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 661.6527709960938 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 928.6913452148438 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 712.7547607421875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 774.9961547851562 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 882.9647827148438 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 846.48779296875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9386, loss_val: nan, pos_over_neg: 748.4624633789062 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 1172.0419921875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 2394.00390625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 3492.283447265625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 1100.4149169921875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 1772.5474853515625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 1126.887451171875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 739.6817016601562 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 1587.4263916015625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.948, loss_val: nan, pos_over_neg: 1002.6571655273438 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9494, loss_val: nan, pos_over_neg: 1002.3563232421875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1321.1397705078125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9529, loss_val: nan, pos_over_neg: 50281.3515625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 1654.76123046875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 764.0595703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 584.7869262695312 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9663, loss_val: nan, pos_over_neg: 729.7976684570312 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 469.6231689453125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 989.9741821289062 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 438.4841003417969 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 875.4435424804688 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 734.89892578125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 883.6021728515625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 789.2786865234375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 670.4700317382812 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9428, loss_val: nan, pos_over_neg: 5101.294921875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1427.018310546875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1868.5921630859375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 753.4912719726562 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 932.6312866210938 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: -19841.7265625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 1632.9779052734375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 1298.3316650390625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 1367.239013671875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 944.6310424804688 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 5251.31884765625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 4244.24658203125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9611, loss_val: nan, pos_over_neg: 5711.03662109375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 1512.8360595703125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9579, loss_val: nan, pos_over_neg: 3126.837646484375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 2665.362548828125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 4673.98583984375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 4579.15380859375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 4884.82958984375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 3082.379638671875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 1996.89794921875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9695, loss_val: nan, pos_over_neg: 1725.4793701171875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1100.8916015625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 950.3292846679688 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 6552.16943359375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 2283.80322265625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 2242.800048828125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 1401.7359619140625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 946.8248901367188 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9648, loss_val: nan, pos_over_neg: 1462.888671875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1623.95166015625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 543.083740234375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 1260.3734130859375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9644, loss_val: nan, pos_over_neg: 1731.337646484375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 823.72265625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 1699.1092529296875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 4249.50634765625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 578.8226318359375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9495, loss_val: nan, pos_over_neg: 814.8133544921875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 755.1744384765625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9564, loss_val: nan, pos_over_neg: 1170.653564453125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 3502.35888671875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1661.1234130859375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 1019.2472534179688 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 1310.3314208984375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 879.1287231445312 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 922.6629638671875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9435, loss_val: nan, pos_over_neg: 1075.571533203125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 890.2555541992188 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 2912.87255859375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 575.4324340820312 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9449, loss_val: nan, pos_over_neg: 1803.376953125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 1589.302490234375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9433, loss_val: nan, pos_over_neg: 2577.4833984375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 940.821044921875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1233.8905029296875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1711.09814453125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 19706.578125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1258.025634765625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 515.0302734375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1585.341064453125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 3771.37451171875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 968.881591796875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9591, loss_val: nan, pos_over_neg: 1298.946533203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 831.6747436523438 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 605.8475341796875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9553, loss_val: nan, pos_over_neg: 996.0657958984375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 1063.8603515625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9492, loss_val: nan, pos_over_neg: 955.973388671875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 737.7120361328125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1201.0677490234375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1085.6529541015625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9611, loss_val: nan, pos_over_neg: 1091.72900390625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 616.3096923828125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 675.6611938476562 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 1435.60888671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9483, loss_val: nan, pos_over_neg: 853.1909790039062 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 2412.2939453125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9514, loss_val: nan, pos_over_neg: 605.1121215820312 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 710.1233520507812 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1662.5439453125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 890.9502563476562 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 833.1682739257812 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 709.6034545898438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9538, loss_val: nan, pos_over_neg: 15773.7294921875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 1217.700439453125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 942.5308227539062 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 698.2373046875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 1293.70654296875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9539, loss_val: nan, pos_over_neg: 934.2161254882812 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 1551.6512451171875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1601.721435546875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 1533.317138671875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 1539.0804443359375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 1794.1060791015625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9525, loss_val: nan, pos_over_neg: 899.0068969726562 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: 1064.7022705078125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 178956.671875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 1290.1510009765625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 1049.53564453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1502.8448486328125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9483, loss_val: nan, pos_over_neg: 3760.54296875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1482.71484375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 393.4430847167969 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 587.6080932617188 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9523, loss_val: nan, pos_over_neg: 1006.152099609375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 943.5359497070312 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9492, loss_val: nan, pos_over_neg: 474.5882873535156 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 545.236572265625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 1320.4544677734375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 586.9022827148438 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9543, loss_val: nan, pos_over_neg: 645.6513061523438 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9557, loss_val: nan, pos_over_neg: 580.4657592773438 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 1486.63134765625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9596, loss_val: nan, pos_over_neg: 745.4214477539062 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1201.2772216796875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 797.9249267578125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9651, loss_val: nan, pos_over_neg: 815.4697875976562 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9695, loss_val: nan, pos_over_neg: 1095.822021484375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 1906.451416015625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.954, loss_val: nan, pos_over_neg: 2221.53955078125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 1944.85546875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 2106.761962890625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 2360.235107421875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 2067.797119140625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 1843.8319091796875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 873.0645751953125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9474, loss_val: nan, pos_over_neg: 1804.56640625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 2211.385009765625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 1282.3829345703125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1106.7210693359375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 1233.7210693359375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9481, loss_val: nan, pos_over_neg: 942.2764892578125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 989.709716796875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 2134.124267578125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 1050.3382568359375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9468, loss_val: nan, pos_over_neg: 1326.4652099609375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: -15824.947265625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.948, loss_val: nan, pos_over_neg: -3608.968017578125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 1212.9271240234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1621.12255859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1366.66796875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 6586.2333984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 1025.92822265625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 555.3233032226562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1996.767578125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 1806.4869384765625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 552.5572509765625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 549.0469970703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9513, loss_val: nan, pos_over_neg: 813.1569213867188 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9613, loss_val: nan, pos_over_neg: 911.6792602539062 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 1921.3367919921875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9613, loss_val: nan, pos_over_neg: 845.2313842773438 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 602.1646118164062 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1169.043701171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 675.0516357421875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 1040.09130859375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9723, loss_val: nan, pos_over_neg: 587.3792114257812 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 823.0771484375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 943.9797973632812 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 648.8018798828125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1237.47119140625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 761.6224975585938 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 682.0423583984375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1320.9739990234375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 1045.2098388671875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 6372.2119140625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.95, loss_val: nan, pos_over_neg: 630.4369506835938 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 904.0635375976562 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: -2847.295166015625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 2122.93505859375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 565.1217041015625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 1155.7777099609375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 2413.240234375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 571.737060546875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 294.0021667480469 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 746.531494140625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 418.6093444824219 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.947, loss_val: nan, pos_over_neg: 734.0158081054688 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 570.1149291992188 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9563, loss_val: nan, pos_over_neg: 592.4583740234375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 486.69012451171875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 893.9168701171875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 510.93646240234375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9544, loss_val: nan, pos_over_neg: 456.8091735839844 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 674.7470703125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9504, loss_val: nan, pos_over_neg: 539.9193115234375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 986.370361328125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1016.111572265625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 1513.4246826171875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 4268.736328125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 924.5595703125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 872.685302734375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9502, loss_val: nan, pos_over_neg: -12469.5234375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 3343.236572265625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9463, loss_val: nan, pos_over_neg: 3791.0849609375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 6449.1552734375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 2869.7255859375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 9734.986328125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9603, loss_val: nan, pos_over_neg: 1271.72021484375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 795.8388061523438 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: -7309.498046875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 1677.724853515625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1184.147216796875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 2556.500732421875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 1600.7847900390625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 1709.017822265625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 2947.873291015625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 976.4673461914062 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 1332.2109375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 731.9481811523438 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 2106.74609375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 2317.48583984375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 1235.09912109375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 938.6694946289062 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 2872.79931640625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 2473.155029296875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 684.1209716796875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 721.2564086914062 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 8651.24609375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 951.95458984375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 609.0593872070312 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 1142.822265625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 4511.8046875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1571.9342041015625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 1022.5827026367188 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 732.9413452148438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9502, loss_val: nan, pos_over_neg: 2172.448486328125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: 544.9813232421875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 2439.751708984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 1624.826904296875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1570.5836181640625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 1257.6180419921875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 4910.19482421875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 1246.1009521484375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 711.1602783203125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 702.7492065429688 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 1415.9869384765625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1849.071533203125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 1022.0372314453125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 1257.4383544921875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 878.2894897460938 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.961, loss_val: nan, pos_over_neg: 2545.816650390625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 3021.902099609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 5059.19140625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 2795.567138671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9502, loss_val: nan, pos_over_neg: 1179.118408203125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9605, loss_val: nan, pos_over_neg: 1442.2847900390625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9495, loss_val: nan, pos_over_neg: 2759.798828125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 500.65374755859375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 14667.2099609375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 2893.49169921875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1673.11767578125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.949, loss_val: nan, pos_over_neg: 1097.5499267578125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 2694.774169921875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 1571.5587158203125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 1262.5372314453125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9463, loss_val: nan, pos_over_neg: 1220.4066162109375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 902.1112670898438 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9591, loss_val: nan, pos_over_neg: 995.1166381835938 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 2491.517333984375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1394.1295166015625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 1420.4322509765625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 1827.8702392578125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 1252.028076171875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 1919.2552490234375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 912.9569702148438 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1615.2847900390625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 1295.4539794921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 945.2794799804688 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 862.626708984375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 1407.288330078125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1652.2520751953125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 4719.25439453125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9452, loss_val: nan, pos_over_neg: 4321.6875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 17845.626953125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 1883.4708251953125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1987.6904296875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 2813.150634765625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 1731.98095703125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 1197.2259521484375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 701.3738403320312 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 5131.2919921875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 5061.50244140625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 1361.5057373046875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: -2760.792236328125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9546, loss_val: nan, pos_over_neg: 2485.984130859375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 2050.5029296875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 1939.9114990234375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1842.037109375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1072.8316650390625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 942.671875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9444, loss_val: nan, pos_over_neg: 1021.0314331054688 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 4744.27685546875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: -29680.193359375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 1032.21240234375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 3407.66748046875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9435, loss_val: nan, pos_over_neg: 3516.709716796875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9481, loss_val: nan, pos_over_neg: 1396.1983642578125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1592.060791015625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 1960.2271728515625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 1925.9541015625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: -5852.3828125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 2028.4923095703125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 2006.1243896484375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 2456.636962890625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 2457.5498046875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9532, loss_val: nan, pos_over_neg: 71954.5546875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 3110.738037109375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 744.6602172851562 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9489, loss_val: nan, pos_over_neg: 693.9307861328125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 1199.011962890625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 13459.078125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9533, loss_val: nan, pos_over_neg: 1463.426025390625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 2656.905029296875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 839.0164794921875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 4083.205078125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: -2946.3310546875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 2262.989501953125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 2562.481201171875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: -7965.56298828125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 2108.85693359375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 1177.59228515625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: -23662.388671875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 4155.6279296875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: -40862.1640625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 1086.39697265625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 1133.4918212890625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 2213.27001953125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1241.497314453125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 907.7267456054688 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 616.5338745117188 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 3297.71533203125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 9250.0400390625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.95, loss_val: nan, pos_over_neg: 865.5392456054688 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 3627.389892578125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 1946.3790283203125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 735.5993041992188 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 1468.1087646484375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 4048.956787109375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 567.2258911132812 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 553.64013671875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 758.280517578125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 1293.603515625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 1312.6402587890625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 701.0588989257812 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 613.1643676757812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 891.6689453125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1103.5546875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 428.3733825683594 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1941.6676025390625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 2725.63525390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 909.7896728515625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 1835.8541259765625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 1251.625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 3341.005615234375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 739.6395263671875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1211.5877685546875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 6590.642578125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 1619.4681396484375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1405.4759521484375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 1841.7105712890625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 813.8285522460938 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 5291.30517578125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 1028.2066650390625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 881.4107055664062 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 709.8555297851562 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 2612.191162109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 1520.558349609375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 558.7875366210938 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 692.3318481445312 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 2618.724609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 1171.4075927734375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 761.5125732421875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 730.8960571289062 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.948, loss_val: nan, pos_over_neg: 1044.87109375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9493, loss_val: nan, pos_over_neg: -8203.1279296875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 5464.6142578125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9581, loss_val: nan, pos_over_neg: 586.455810546875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 1436.4366455078125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 809.5343017578125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 1106.6170654296875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 3584.83154296875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.938, loss_val: nan, pos_over_neg: -7889.84130859375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 970.04541015625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.945, loss_val: nan, pos_over_neg: 2080.252685546875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 659.1241455078125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 800.4706420898438 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1020.1441650390625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 2217.770263671875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 1282.9757080078125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.942, loss_val: nan, pos_over_neg: -41352.34375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 2421.274658203125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 1945.3580322265625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9386, loss_val: nan, pos_over_neg: 1012.53955078125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: -24516.115234375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: -6287.23681640625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1240.5079345703125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9616, loss_val: nan, pos_over_neg: 618.7766723632812 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: -121373.203125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 1133.696044921875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 910.9207153320312 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 1039.2222900390625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 3714.235107421875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 2356.830078125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9523, loss_val: nan, pos_over_neg: 1639.6005859375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.949, loss_val: nan, pos_over_neg: 3022.525634765625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 2901.513427734375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 4367.296875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 1055.1490478515625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 2141.05517578125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: -31796.720703125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 3335.42431640625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9461, loss_val: nan, pos_over_neg: 2381.3232421875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 1469.4571533203125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 3365.14990234375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 2312.551513671875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1910.2734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 3005.161376953125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: -5211.00732421875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 1204.2952880859375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9386, loss_val: nan, pos_over_neg: 1279.739013671875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1778.655029296875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1877.78662109375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: -13535.17578125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 8663.2685546875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 1614.288818359375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1181.5313720703125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 16726.17578125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 2142.3115234375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 1921.097900390625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 5381.26123046875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 2969.383056640625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1574.511962890625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 657.3654174804688 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9354, loss_val: nan, pos_over_neg: 958.7848510742188 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 899.5418701171875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 2640.749755859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 4130.03515625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 2098.70751953125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9332, loss_val: nan, pos_over_neg: 1491.6488037109375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 1617.9654541015625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 1751.481689453125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9563, loss_val: nan, pos_over_neg: 1450.055419921875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 814.7297973632812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 723.555908203125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 4231.46875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9433, loss_val: nan, pos_over_neg: 21970.83203125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 709.8941650390625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 1807.574951171875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.935, loss_val: nan, pos_over_neg: -2892.8740234375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 3343.4111328125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 624.8284912109375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 711.3740234375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9332, loss_val: nan, pos_over_neg: 2014.1527099609375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 962.080322265625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 2173.735595703125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 2689.752685546875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 3055.505615234375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 1701.72412109375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 4117.3671875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 644.8296508789062 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 991.0694580078125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: 918.6112060546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1893.2828369140625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1815.3311767578125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 1155.5634765625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 8086.25 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 738.68115234375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 1791.4007568359375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 793.433349609375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 3354.14501953125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9474, loss_val: nan, pos_over_neg: -4301.67919921875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2519.873291015625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 718.6262817382812 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 896.2802734375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 137681.84375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1099.232177734375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 2145.1435546875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 2226.798828125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1853.751953125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 1195.5888671875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9352, loss_val: nan, pos_over_neg: 898.253662109375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 870.0838623046875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9552, loss_val: nan, pos_over_neg: 1456.495361328125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 659.2472534179688 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 863.207275390625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 1067.7861328125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 3920.90283203125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 1889.2808837890625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: 676.6846923828125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 519.982421875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9428, loss_val: nan, pos_over_neg: 2437.455078125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: -42518.1953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 1409.002685546875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 864.9756469726562 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 772.0787963867188 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 1400.1741943359375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 3715.62109375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1075.2244873046875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9494, loss_val: nan, pos_over_neg: 1003.0125732421875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: -54367.625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 981.4904174804688 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 2645.778564453125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 3165.1435546875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2024.2000732421875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 2416.658203125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 593.2432861328125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 1457.5411376953125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 3742.18798828125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.936, loss_val: nan, pos_over_neg: 1335.3018798828125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1107.2178955078125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 937.306884765625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 3031.439697265625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 7424.205078125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 1668.220947265625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 477.1262512207031 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 520.8916625976562 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [59:50<99619:48:23, 1195.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 1034.48193359375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 675.9262084960938 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 1689.462890625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 959.8406372070312 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 17270.59765625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 848.999267578125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 720.552978515625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.947, loss_val: nan, pos_over_neg: 412.9375305175781 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1021.880126953125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 1310.8663330078125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 367.4347229003906 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 643.34716796875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1498.1304931640625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 2631.7568359375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 910.299560546875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 760.3705444335938 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 5036.26025390625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1456.8609619140625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 1936.0679931640625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 1432.9130859375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 669.3054809570312 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.957, loss_val: nan, pos_over_neg: 956.2821655273438 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 1036.4647216796875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 1882.1065673828125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 2385.0771484375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 1048.6630859375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 898.3321533203125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: 2174.47021484375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 1383.8812255859375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9503, loss_val: nan, pos_over_neg: 666.0303344726562 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: -96521.4765625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 775.9369506835938 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 1283.634033203125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9448, loss_val: nan, pos_over_neg: 530.7635498046875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 1087.49658203125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 1728.5426025390625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 2247.215087890625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9543, loss_val: nan, pos_over_neg: 1708.3687744140625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: 1017.3115844726562 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 1705.478759765625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1171.8592529296875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: -47169.60546875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1551.78759765625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 2600.05517578125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 936.7288208007812 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 802.1216430664062 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 2557.684814453125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: -47184.30078125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 2835.0146484375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1587.4591064453125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 1092.2972412109375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 1157.6304931640625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 4111.79638671875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 1746.058349609375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1863.479736328125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 37199.0390625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9561, loss_val: nan, pos_over_neg: -5380.22509765625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 962.3994140625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 1275.60205078125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 682.4556274414062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 3045.716064453125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 1828.2662353515625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1900.950927734375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 960.6256713867188 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1281.864990234375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1438.2735595703125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 576.2628173828125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 1015.811767578125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 5075.91162109375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 597.55859375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 488.5722961425781 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 1363.348388671875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 2235.04150390625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 816.052490234375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 658.570068359375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 1720.6771240234375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 2329.523681640625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 983.8165893554688 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 479.2088928222656 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 1256.3460693359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 1024.8109130859375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 1046.89111328125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1094.9169921875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1752.81689453125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 2222.6796875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 417.1174621582031 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 801.8638916015625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 2178.4931640625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 6845.9365234375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 2940.63232421875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.947, loss_val: nan, pos_over_neg: 707.5148315429688 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 964.2506713867188 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 3217.747802734375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 624.5259399414062 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 1191.466796875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9492, loss_val: nan, pos_over_neg: 494.88677978515625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 1233.7347412109375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 2886.986328125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 792.4373779296875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 362.46575927734375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1273.4210205078125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 580.9761962890625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 302.2047119140625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 647.7006225585938 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 753.03125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1338.5887451171875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 557.3758544921875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 556.2548828125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 1431.85888671875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 593.8934936523438 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1065.5389404296875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 1181.1136474609375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1092.46142578125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 1192.6070556640625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 2219.18017578125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2318.119140625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 544.2958374023438 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 567.3057861328125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 886.101318359375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 467.3658752441406 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 750.98046875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 1507.0452880859375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9449, loss_val: nan, pos_over_neg: 807.263427734375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1461.47021484375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 579.8969116210938 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 3634.47607421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 2752.833984375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1262.68310546875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 2929.50390625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 1249.7628173828125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9468, loss_val: nan, pos_over_neg: 2197.49951171875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 10607.0732421875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1084.11865234375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 937.9573974609375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 933.9763793945312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 81908.9921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1107.8477783203125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 800.00390625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 1519.3173828125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1801.09423828125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 945.4783935546875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 2606.357177734375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 1203.3795166015625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1120.3951416015625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1165.663330078125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 570.7871704101562 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 1776.2088623046875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 1495.179931640625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 1265.8154296875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 3168.447265625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 484.6595153808594 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.939, loss_val: nan, pos_over_neg: 1175.967529296875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.936, loss_val: nan, pos_over_neg: 684.843505859375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9435, loss_val: nan, pos_over_neg: 517.09716796875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 1481.580810546875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1582.9259033203125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 935.1693115234375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.946, loss_val: nan, pos_over_neg: 1332.2894287109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 634.1516723632812 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 904.9555053710938 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 1134.565185546875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 593.6141357421875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9416, loss_val: nan, pos_over_neg: 1021.4427490234375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 599.3093872070312 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2671.944580078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 4321.357421875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 857.1209106445312 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 1011.8388061523438 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1891.4783935546875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 4104.78076171875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 2218.31298828125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 795.2024536132812 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9416, loss_val: nan, pos_over_neg: 1226.4180908203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 943.32763671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 4833.322265625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 694.3887939453125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1176.6748046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9448, loss_val: nan, pos_over_neg: 1319.1658935546875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9449, loss_val: nan, pos_over_neg: -34871.34765625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9354, loss_val: nan, pos_over_neg: 2164.496826171875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 1596.359619140625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 1057.521240234375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 934.8358154296875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 4852.03076171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 5842.83837890625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 2184.80126953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 604.61279296875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 3089.62353515625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.936, loss_val: nan, pos_over_neg: 1523.3748779296875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1257.7293701171875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 2771.3134765625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9447, loss_val: nan, pos_over_neg: 839.8101806640625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 11213.591796875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 4824.48046875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 2308.500732421875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 784.847900390625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 2127.12158203125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1710.9229736328125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 1688.5443115234375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.94, loss_val: nan, pos_over_neg: -90212.4140625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 814.4810180664062 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 29411.28515625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 1547.884521484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 2789.320068359375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 10929.6064453125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 2856.712646484375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 688.0177001953125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 8257.9462890625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: -60664.21484375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1049.330810546875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 1187.9073486328125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 1568.613525390625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 5465.99658203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 3484.298828125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 1006.0107421875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 51328.1015625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 8284.7001953125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 1531.08203125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 713.9505615234375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1898.8704833984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 637.9634399414062 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 721.783935546875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 4020.04345703125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 1092.258056640625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 1220.836669921875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 1802.81787109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 4096.6123046875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: -11101.998046875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 1572.19970703125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 710.2046508789062 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 685.5861206054688 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 2249.60986328125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 1234.4566650390625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 739.0968017578125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 1030.4527587890625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.945, loss_val: nan, pos_over_neg: 2466.20068359375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 5961.88818359375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 1712.194580078125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 8115.6962890625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1187.8446044921875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1022.0902099609375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1596.92529296875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 2403.89013671875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 865.4744262695312 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 1196.1207275390625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 776.5533447265625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1363.8721923828125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 1363.590576171875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 2219.392578125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 629.35986328125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 571.9256591796875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 1616.5054931640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 4312.5263671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 462.4133605957031 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 1170.1787109375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1408.548095703125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 1096.77001953125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 5066.728515625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 2669.868896484375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 1359.66259765625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 1234.168701171875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1944.24609375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 2315.323486328125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 872.4493408203125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 729.8818359375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 1308.5928955078125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: 2343.657470703125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 4212.48095703125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 1516.9844970703125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 902.5718383789062 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 1070.0802001953125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 2453.529541015625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9433, loss_val: nan, pos_over_neg: 4478.5712890625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 841.8355712890625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 736.4594116210938 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2725.114990234375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 7889.21337890625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 3521.117431640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 764.404296875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 650.7237548828125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 594.6361694335938 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 1355.9024658203125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1226.017578125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 705.9122314453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 3889.253173828125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 1137.194091796875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 825.7752075195312 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 960.485595703125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 762.3161010742188 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 623.3145751953125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1354.460693359375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 565.2747192382812 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2335.9296875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 1308.747802734375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 2675.04541015625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 925.3875122070312 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 702.5111694335938 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 607.8994140625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 942.847900390625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1154.3128662109375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 1252.6263427734375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 880.4989624023438 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 1206.5032958984375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 987.626220703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1537.42431640625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 1008.6504516601562 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 891.9098510742188 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1827.550048828125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 8772.0400390625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 3418.27490234375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 1044.2794189453125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 498.2697448730469 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1414.512939453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 9376.4990234375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 1000.4574584960938 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 1495.819580078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 1091.058837890625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1390.865478515625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 975.70849609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 2240.76806640625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 3231.962646484375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: 1412.0771484375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 521.1834716796875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 2842.403564453125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 1444.81689453125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 1441.0921630859375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 705.7520751953125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 1477.8033447265625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 978.7978515625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 8131.26171875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 2346.878173828125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: -53545.62890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1370.4420166015625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 3116.654296875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 3500.761474609375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 4506.99462890625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1829.8062744140625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1418.6214599609375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 3088.11962890625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 50190.81640625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 3839.55078125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: -5669.6953125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9553, loss_val: nan, pos_over_neg: 3041.990234375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 1377.223876953125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 6737.92236328125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 5354.13720703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 963.852294921875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 857.0932006835938 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 811.4088134765625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1097.7611083984375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 2275.051025390625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 1023.2720947265625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 692.5277709960938 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 1027.26025390625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 57074.0078125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: -16529.970703125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 4040.960693359375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1249.0528564453125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1388.575927734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1598.767333984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 903.0115966796875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 684.9647216796875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 608.1091918945312 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 4357.04541015625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: 1483.07275390625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9332, loss_val: nan, pos_over_neg: 1823.018310546875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 1467.6334228515625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 2976.7724609375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2065.6611328125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 769.7755737304688 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 616.2876586914062 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 586.5523681640625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 2312.8056640625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 515.3346557617188 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1282.170654296875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1621.3759765625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 795.2073974609375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 941.5923461914062 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 973.5557861328125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 791.5270385742188 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 5995.81787109375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1004.2501831054688 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 1262.513916015625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2158.411865234375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 4151.87890625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2042.9588623046875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 975.6299438476562 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 1399.4031982421875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 3300.540283203125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1411.3289794921875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 657.7373046875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 3520.884033203125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 5502.46142578125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 708.8303833007812 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 932.6707153320312 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 3213.159912109375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1756.3580322265625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 2017.2684326171875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1263.198974609375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9385, loss_val: nan, pos_over_neg: 822.3671264648438 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 1144.3980712890625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 1262.6502685546875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1346.75341796875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1033.703125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 666.7343139648438 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 1239.4705810546875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 688.6974487304688 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 11738.3828125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 780.9234619140625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 1200.760986328125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 1984.808837890625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 990.3109130859375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 837.6861572265625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 788.9046630859375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 1700.0103759765625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 966.126220703125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 13775.6884765625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 8459.6015625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 920.3148193359375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 861.4931030273438 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 994.1204223632812 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 666.2506713867188 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 2104.409912109375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 626.7174682617188 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 431.1825256347656 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1413.138671875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1273.21435546875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 1681.3017578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1224.034423828125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9321, loss_val: nan, pos_over_neg: 1132.2237548828125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: -2336.535888671875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1408.869140625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 7143.4970703125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 893.5311889648438 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 6272.26513671875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 848.4804077148438 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1188.6177978515625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1487.5750732421875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 3465.474365234375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 2779.07275390625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 4740.16552734375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 3835.8681640625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 3937.79541015625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1274.4361572265625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 10867.5888671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 3103.571044921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 1309.8433837890625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1839.878173828125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 580.0485229492188 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1829.4886474609375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 806.4744873046875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 719.6541137695312 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 740.3463134765625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 4916.109375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 865.37548828125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1022.0111083984375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 1182.7047119140625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1844.918212890625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1023.041748046875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 1655.90625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 780.7415161132812 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 965.0693969726562 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 4993.07666015625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 21066.2578125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 985.26611328125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 597.9442138671875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1355.9244384765625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1738.6031494140625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 2414.142822265625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 3486.274658203125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.929, loss_val: nan, pos_over_neg: -15409.12109375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 890.0615234375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 1050.311767578125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 942.8245239257812 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1245.5458984375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 1360.27197265625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 2495.047119140625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 1215.8287353515625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 1096.3648681640625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1569.7913818359375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 3520.581787109375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 1290.297119140625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 699.1799926757812 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 1112.5130615234375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 921.8214111328125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 10376.9052734375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 1280.4114990234375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 892.2373657226562 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 2383.135498046875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1754.758056640625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 756.6754150390625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 812.5122680664062 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9445, loss_val: nan, pos_over_neg: 6001.81787109375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1283.1396484375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9343, loss_val: nan, pos_over_neg: 1095.75830078125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 510.69720458984375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1851.5521240234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 8678.3525390625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 1513.5548095703125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 1192.748046875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 8274.736328125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1097.9879150390625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1311.9097900390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: -12780.31640625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 3631.6650390625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 1001.7014770507812 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 2961.439208984375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 1090.93994140625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 2323.123779296875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 1445.953857421875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9354, loss_val: nan, pos_over_neg: 667.8142700195312 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1002.0311279296875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 3368.029541015625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1029.3765869140625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 1622.0657958984375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 1460.4088134765625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1152.2877197265625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 2478.662353515625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: -7482.90380859375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 2043.4515380859375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 10989.4931640625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 8392.333984375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 46834.08984375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 4842.79296875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 8268.9052734375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1203.104736328125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1415.569091796875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1297.453125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 942.88818359375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 1721.5189208984375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1664.9736328125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 1873.44775390625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 2171.052490234375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1621.897216796875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1087.569580078125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 722.2381591796875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1451.9732666015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 813.7532958984375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 1050.9312744140625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 5332.02294921875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 2722.942138671875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 1789903.875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 2816.533447265625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1463.282470703125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 2278.649169921875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 14058.341796875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 2833.265625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 1780.015625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1539.0828857421875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9343, loss_val: nan, pos_over_neg: 3238.00732421875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 4809.52490234375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1079.4810791015625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 13150.798828125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 2186.376708984375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: -25727.185546875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 13554.4453125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 2424.934326171875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 9256.3525390625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1426.6920166015625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 1310.150146484375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 1164.7200927734375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1584.717041015625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 2948.381103515625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 2674.318115234375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 1481.5772705078125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 686.3499755859375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1572.7557373046875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 1142.0367431640625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 522.8118286132812 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1608.1624755859375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 675.86083984375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 1711.2086181640625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 915.2471923828125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1087.042236328125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 2002.7882080078125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 2804.33251953125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: 1006.5231323242188 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 504.07611083984375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 3157.619384765625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 2580.65869140625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 695.549560546875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 885.184814453125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 374.9563293457031 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 1577.3607177734375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 2054.73193359375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 1239.3145751953125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1054.0711669921875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 882.3514404296875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 2608.46435546875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 1142.235595703125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 367.0581359863281 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1022.2699584960938 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1016.412109375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 5271.5673828125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 3177.868896484375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 885.5523681640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 989.8805541992188 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 1039.693115234375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 926.41650390625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 983.0025634765625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 860.1963500976562 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 822.387451171875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 612.775634765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 862.1099243164062 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 3155.101318359375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 4153.52001953125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 875.3779907226562 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 506.45086669921875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 1690.995361328125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 3554.208251953125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 1596.824462890625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 465.1056213378906 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 1041.5555419921875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 2036.850830078125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 3866.702880859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 2806.659912109375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1279.3936767578125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1862.64013671875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1973.5496826171875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 2049.565185546875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 4049.035888671875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 992.770751953125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2408.304443359375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 1088.30126953125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 860.5865478515625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 4268.40234375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: -3878.201904296875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 3107.37060546875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 892.7011108398438 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1408.802734375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 1241.8643798828125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 1329.803955078125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1628.4837646484375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.932, loss_val: nan, pos_over_neg: 2326.516845703125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1085.3394775390625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 759.8336791992188 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 1455.1290283203125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1297.6905517578125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 697.4153442382812 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 873.1921997070312 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 799.1104736328125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1777.463623046875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1119.9305419921875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 725.8153076171875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 675.487548828125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1769.6422119140625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1526.0045166015625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1457.2359619140625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 891.0604248046875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 1857.7294921875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1444.6861572265625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 892.5005493164062 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 16322.376953125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 4481.8828125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1094.5936279296875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 876.356689453125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.945, loss_val: nan, pos_over_neg: 782.2476196289062 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9352, loss_val: nan, pos_over_neg: 730.614501953125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 2534.55859375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 1177.1199951171875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 1056.003662109375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1184.91796875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 846.4423828125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 4244.1103515625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 1234.522216796875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 960.574951171875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 774.185302734375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 650.0800170898438 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 3079.971923828125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 9694.5947265625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 3743.438232421875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 7662.6728515625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 2924.32666015625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 647.850341796875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 1213.6971435546875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 896.9457397460938 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 662.1317749023438 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1407.35400390625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 1438.0467529296875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1826.8740234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 81046.4296875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 1280.172119140625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 938.9808349609375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 795.2916259765625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1053.6014404296875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 814.4799194335938 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1036.90478515625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1059.4105224609375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 2083.69873046875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.932, loss_val: nan, pos_over_neg: 1743.3404541015625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 2107.720458984375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 883.1650390625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1979.830322265625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 2110.80224609375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 1020.7837524414062 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:19:53<99868:51:47, 1198.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 48492.91796875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 995.76318359375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 873.7741088867188 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1251.0792236328125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 8663.65625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 1154.2357177734375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 1127.1385498046875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 672.5905151367188 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 1064.2772216796875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 4653.38916015625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 3039.26953125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 970.346435546875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1235.2481689453125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 768.8197021484375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 1035.2459716796875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 5866.29052734375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1632.6014404296875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 2603.80615234375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 503.3785095214844 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 1513.229248046875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1214.6639404296875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2359.0234375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1965.81005859375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 3131.961669921875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1778.5911865234375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1536.21728515625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1833.085205078125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1507.3463134765625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9321, loss_val: nan, pos_over_neg: 699.5138549804688 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 2047.0721435546875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 1419.607666015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1493.1329345703125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1228.322509765625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 2532.5966796875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: -10132.2900390625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: -26472.46484375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 2946.6240234375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 2554.068359375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 902.687255859375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1869.5389404296875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: 938.3373413085938 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 1508.7235107421875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1502.18359375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 7628.58154296875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 2316.334716796875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 557.7623291015625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 2546.18310546875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 2814.12548828125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 3133.483642578125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 2540.983642578125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1104.4453125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 3074.88623046875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 2788.3798828125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 935.8934326171875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1310.021240234375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 2313.303955078125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 2399.023193359375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 1459.658447265625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.928, loss_val: nan, pos_over_neg: -53354.83203125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 3500.63525390625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 1616.4786376953125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 944.766845703125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1979.406494140625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 2029.9342041015625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 761.9880981445312 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 584.5611572265625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 1360.44921875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 23739.15234375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 1142.7696533203125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 776.3368530273438 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1333.1395263671875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 5893.6435546875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 8962.8798828125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 5436.1396484375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1568.5433349609375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 5607.044921875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 4241.80126953125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 1178.460693359375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: -58669.70703125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.928, loss_val: nan, pos_over_neg: 2093.128173828125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 4441.58056640625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 2891.11474609375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1592.381591796875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1186.788330078125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 2734.578125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 1039.6014404296875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 1161.35693359375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 6047.5478515625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 962.0239868164062 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 2575.94580078125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 3560.5380859375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1885.0618896484375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 3614.07080078125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1372.879638671875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 5459.341796875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1781.243408203125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9461, loss_val: nan, pos_over_neg: 1728.1446533203125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 2211.4765625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 522.3208618164062 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 2524.11474609375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1933.2613525390625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 826.2820434570312 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 4074.3037109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1028.72509765625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1511.4736328125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 1853.746337890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1018.6098022460938 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 780.8635864257812 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 539.12744140625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 927.0059204101562 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 927.0687255859375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 2638.095458984375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 1584.7330322265625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 1535.123046875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 834.3878173828125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 1410.6295166015625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 4659.568359375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1428.6419677734375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1543.1495361328125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 5307.2744140625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2091.84716796875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 4991.7900390625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 1132.7230224609375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 860.5669555664062 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 10212.19921875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 3660.558837890625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 1453.6585693359375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 3383.529052734375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 451.4265441894531 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 2407.169677734375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 718.1851196289062 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1291.0699462890625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 3553.0732421875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1490.787109375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 11931.9755859375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1735.3572998046875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 2077.329833984375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1074.8358154296875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 2135.67138671875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 4063.100341796875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1756.7713623046875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1353.531494140625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 6380.6884765625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 979.5673828125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 4277.36962890625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2304.937255859375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 815.9732055664062 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 1103.44775390625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 740.3276977539062 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 3043.6767578125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1307.162353515625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1859.6259765625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 10738.5908203125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 2868.9345703125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 3600.160888671875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1476.0684814453125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: -4399.38134765625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1286.59912109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 1603.5838623046875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1754.577392578125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 13742.099609375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1274.819580078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 11788.3427734375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: -11803.6591796875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 4837.205078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 9032.8974609375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1494.657470703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 1700.3974609375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: -12964.1171875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 2408.90185546875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 1097.01171875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 908.1986083984375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2492.382080078125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2857.278564453125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.932, loss_val: nan, pos_over_neg: 1212.229248046875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 2330.336181640625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: -15678.8291015625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: -15929.90625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1453.334716796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 8120.2275390625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1238.7010498046875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1445.409423828125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 940.3510131835938 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 1457.8245849609375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1077.8021240234375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 1575.0277099609375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1518.95947265625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1852.7542724609375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 746.6199340820312 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 2214.05712890625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1097.090576171875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1712.2701416015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2346.6767578125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1420.8990478515625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 4377.14453125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 3926.538330078125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 4577.11083984375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 2371.7861328125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 2197.348876953125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 701.3472290039062 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 805.0717163085938 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1678.4610595703125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1009.1533813476562 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 842.16796875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: -15829.111328125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 2082.912841796875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 4123.119140625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1188.5416259765625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 643.4766845703125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1192.2178955078125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1151.3345947265625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1156.795654296875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 1315.5692138671875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 943.7960205078125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 2567.231689453125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 7121.7333984375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1445.1904296875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 37984.125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 11207742.0 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 4038.879638671875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 2758.21435546875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1889.5045166015625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 858.49951171875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1874.628662109375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 4532.849609375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 872.4972534179688 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 1503.7645263671875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 1174.355224609375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 1193.9095458984375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 2963.826416015625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 6394.75439453125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 2122.63037109375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 1032.4022216796875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1269.089599609375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 1056.6251220703125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 618.9592895507812 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 726.036865234375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1419.5068359375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 1601.678466796875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2248.9365234375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2598.068359375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 693.0538330078125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 803.1018676757812 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1245.2491455078125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 8715.8486328125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1415.7529296875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 1225.007568359375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 750.9813842773438 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 11752.0185546875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 949.5243530273438 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 675.8772583007812 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1563.0947265625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1521.6580810546875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 870.2276611328125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1093.904541015625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 837.3035888671875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 38093.6171875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1407.1971435546875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 2347.5048828125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 772.4057006835938 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1402.91943359375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1712.57275390625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1567.0185546875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 3901.6591796875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1706.458251953125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: -5301.60546875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2831.136962890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 982.0672607421875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 792.4070434570312 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 5741.19140625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 8572.6123046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 18680.72265625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 1303.18994140625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1120.8807373046875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 15147.1064453125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1416.0030517578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 7127.421875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1678.6617431640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 736.6547241210938 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2959.71728515625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 50629.5859375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 995.106689453125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 965.7114868164062 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 14231.625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 5565.7734375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2527.094482421875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 2751.365234375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 1363.8319091796875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 4834.12451171875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 1228.0418701171875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 1442.3543701171875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 1728.450927734375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 3873.818115234375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.93, loss_val: nan, pos_over_neg: -22205.27734375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 4695.86669921875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1161.9892578125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 7404.8603515625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 1928.6573486328125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1368.6595458984375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 1951.6961669921875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: -12785.771484375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 2310.660400390625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1759.395751953125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 1085.6761474609375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 7426.447265625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 4292.0615234375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 10762.4609375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 2958.269287109375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1507.7305908203125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2065.871826171875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 599.9520874023438 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 2345.46875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 1949.1583251953125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 3384.99658203125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 3065.729736328125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 5030.46337890625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 2933.730224609375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 2241.4375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 2123.963623046875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 3414.027099609375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1953.538330078125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 630.9663696289062 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1107.48974609375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 8551.2080078125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1079.839599609375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1275.7423095703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1779.116943359375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 3659.75 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 952.55908203125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 484.59466552734375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1149.4451904296875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 543.2969360351562 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 553.3681030273438 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 837.3947143554688 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 1221.5748291015625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: -22289.609375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 2189.264892578125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 1304.2674560546875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 1166.185302734375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2268.1005859375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1251.9002685546875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1152.2374267578125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 1295.2769775390625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 3020.142822265625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 714.6735229492188 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 674.9212036132812 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1575.5555419921875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 4390.65576171875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 5741.13818359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 922.6244506835938 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1331.209716796875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 667.33056640625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 2346.536865234375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1469.2677001953125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 633.9132690429688 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 710.10888671875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 2606.386474609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: -18514.140625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1045.3106689453125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1406.5091552734375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 1704.674560546875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1410.8814697265625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 4211.7119140625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 3402.40185546875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1564.83447265625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1587.732177734375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1503.713623046875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 2595.22265625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 5763.62890625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1275.5299072265625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 910.214111328125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 2336.952880859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 4871.208984375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 5235.5810546875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 4212.85498046875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 982.9132690429688 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 2862.254150390625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1200.1728515625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 1115.02294921875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 2685.5791015625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 873.0157470703125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1538.718505859375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 996.6250610351562 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 1862.9678955078125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 604.6114501953125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 129667.8125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1691.54345703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 6616.1328125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 3414.452880859375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: -13706.232421875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1421.7359619140625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 14642.0712890625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 791.0582885742188 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 1801.300048828125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 2280.310791015625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 2515.6806640625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 1209.798583984375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 3754.55126953125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 1625.51123046875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 4647.50341796875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1047.4256591796875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 743.4452514648438 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1901.990234375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1538.7491455078125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1272.4927978515625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 785.8048095703125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 484.72930908203125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 2474.58740234375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: -8929.57421875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.939, loss_val: nan, pos_over_neg: 1600.67041015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 2041.6475830078125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1055.3607177734375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 575.9491577148438 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 963.7943725585938 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 694.1337280273438 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3922.369873046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 897.3397216796875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 1735.661376953125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 24910206.0 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 1639.2281494140625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 941.0905151367188 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1869.0845947265625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: -4777.20947265625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 2171.738525390625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 840.197265625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.946, loss_val: nan, pos_over_neg: 526.7701416015625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 1047.2969970703125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 4012.25537109375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1468.751953125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 31312.5 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: -12832.8544921875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: -4662.17041015625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 3046.057861328125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 32609.453125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 4654.29150390625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 4236.95947265625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1923.805419921875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 2445.348876953125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: -4168.9794921875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1755.2762451171875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 2173.43212890625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1216.5904541015625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1498.4664306640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 12972.6884765625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 855.5279541015625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1704.7060546875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1521.0877685546875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 2030.934814453125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1392.5489501953125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 2509.867919921875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 1487.1483154296875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 911.6470947265625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1950.9520263671875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1617.331787109375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1861.4979248046875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 2925.8486328125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1823.425537109375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 1088.912841796875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 1123.15478515625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2138.67041015625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1025.2073974609375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 866.4144897460938 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 782.0892333984375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1409.4130859375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1369.0506591796875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 2294.191162109375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 3141.100341796875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1281.659423828125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 3031.1611328125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 13814.1455078125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 857.859130859375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 741.2982788085938 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 2065.24462890625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1386.6796875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 6843.37841796875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 1002.5469360351562 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 2626.812255859375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 574.3857421875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 657.3652954101562 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1079.1339111328125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1091.5770263671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1542.2608642578125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 604.36474609375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 647.0189208984375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1077.320556640625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 2299.60595703125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 921.6267700195312 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1235.4420166015625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 997.0086669921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1239.065673828125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 7336.451171875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 15234.9189453125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1598.2774658203125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2082.564208984375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 2033.182861328125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 3051.753662109375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 5196.38232421875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 3317.11474609375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 2815.31787109375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 3750.60791015625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 4789.41845703125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2567.82763671875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: -12801.6162109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 4212.68359375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1859.9986572265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 3984.5224609375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 2666.058349609375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 849.395751953125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 650.6571655273438 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: -5277.21142578125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 23070.208984375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 2635.02197265625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 893.3275756835938 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 803.827392578125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 792.568115234375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 9667.36328125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1596.315185546875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1136.3970947265625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 1364.7880859375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 4033.996826171875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 925.4956665039062 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: -3421.960205078125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 944.90283203125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 792.3135986328125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1075.1224365234375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 4687.64208984375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1389.709228515625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 952.7484130859375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 8352.46484375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 2214.56201171875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 668.5968627929688 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: 1063.0926513671875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 800.4834594726562 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1637.704345703125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 654.6027221679688 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 804.8668212890625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: -423621.6875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 3865.00390625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1567.407470703125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 716.1768188476562 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 391.85064697265625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2260.30908203125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 3926.967041015625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 2601.17431640625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 724.188720703125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 3322.719482421875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 2700.450927734375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 1904.125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1739.6068115234375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 698.7560424804688 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1348.9114990234375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 9345.3525390625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1783.546875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 3276.172607421875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1211.8658447265625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 1027.69580078125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1046.1943359375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 6201.0087890625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 3902.05126953125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 14960.4267578125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1106.498291015625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 4746.83056640625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1251.3590087890625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 3807.91015625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 2191.6962890625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1242.6708984375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 2490.88427734375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1751.5784912109375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1335.853759765625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 2698.37451171875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1153.256591796875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 1456.8458251953125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: -5539.7724609375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4549.3427734375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 933.2743530273438 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 3217.4736328125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: -5025.32373046875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2885.8583984375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 2605.797119140625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 3103.9208984375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2225.093505859375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 2950.040771484375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 3964.186279296875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1020.078857421875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 18716.30859375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 12999.9677734375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1717.344482421875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2223.25537109375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1054.1529541015625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1319.4014892578125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3504.05712890625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 771.1920166015625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 2428.400634765625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1421.925048828125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 2169.177001953125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 914.8570556640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1590.05029296875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 775.0942993164062 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 807.3688354492188 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1394.318359375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 552.8455200195312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 519.7066040039062 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 535.0386962890625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 2129.062744140625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 3651.335205078125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 1119.6392822265625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 442.94732666015625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 790.5437622070312 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1470.763916015625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 987.8992919921875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 817.1135864257812 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 773.7079467773438 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 1320.468017578125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 786.7185668945312 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 547.3032836914062 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1053.7523193359375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2270.457763671875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1459.603515625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 7701.009765625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 1990.5045166015625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 678.343505859375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 3104.794189453125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 44050.95703125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: -6541.99658203125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 2208.727783203125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1866.9869384765625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 3239.134521484375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 10118.8896484375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1961.099853515625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1140.77490234375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 740.2525024414062 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 686.4291381835938 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1721.2646484375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 6437.28271484375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1683.4610595703125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 1272.68603515625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 1816.7017822265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: -29751.7734375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 1115.8544921875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2417.785888671875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 1781.838623046875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 1220.028076171875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 1086.70751953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 734.6453857421875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 3247.6669921875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 2437.340576171875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 924.2584228515625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1000.8640747070312 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 3102.49658203125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 8649.38671875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1272.2852783203125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 616.186279296875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 655.4506225585938 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 5421.55712890625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 2015.730712890625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1059.12109375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1209.8052978515625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 726.3782348632812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2067.888916015625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1717.4732666015625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 4327.765625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1049.2994384765625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 997.7797241210938 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 8885.421875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: -8164.3046875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 4838.46142578125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 980.4926147460938 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1049.1197509765625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1475.686767578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 3259.759521484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 1131.81396484375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 1377.436279296875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -37462.03125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 665.8837280273438 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 1213.925048828125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 766.4501953125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1356.6456298828125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 981.3973388671875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 2818.05126953125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 832.1109008789062 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 582.9229125976562 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 2537.19580078125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 1393.1251220703125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1139.917236328125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: -19268.53125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 777.8526611328125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 842.3336181640625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 1018.747314453125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 1000.1024169921875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1137.242431640625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.925, loss_val: nan, pos_over_neg: 1483.7474365234375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: -2411.908935546875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 2964.68505859375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 2982.201416015625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 6650.12744140625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:39:57<100015:47:28, 1200.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 2776.0302734375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 1851.7734375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 5073.083984375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 914.9872436523438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1439.75390625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 2562.899658203125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 3986.3583984375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1590.9510498046875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 938.4153442382812 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: -4810.630859375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 2606.7080078125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1892.455810546875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1420.7562255859375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1175.7529296875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 648.43701171875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 2060.46630859375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1384.7479248046875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.93, loss_val: nan, pos_over_neg: -3777.008056640625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 1394.45263671875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.922, loss_val: nan, pos_over_neg: -29905.30859375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 1367.61962890625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 1043.47412109375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 2521.769287109375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 7024.8037109375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1770.9638671875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1091.4542236328125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 31133.1640625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1960.3267822265625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 3602.5400390625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 8067.27294921875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 4643.3720703125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1688.933837890625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 1922.6328125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1394.9022216796875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: -6062.38720703125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 2208.546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 16652.24609375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1273.1708984375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: -4836.62841796875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1443.066162109375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 2308.2734375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 6609.037109375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 1576.6298828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2332.611328125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 32051.9140625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: -63582.10546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 584.581787109375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: -7010.6884765625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 2529.537109375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 1833.679443359375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 2152.7080078125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 1685.814453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1389.2166748046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 284393.4375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: -13121.599609375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: -5694.3232421875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 1414.555908203125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1060.96533203125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.922, loss_val: nan, pos_over_neg: -13105.69140625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 15440.796875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1182.3492431640625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: -4466.580078125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 3314.990234375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 2729.72265625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4109.462890625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 3343.7685546875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: -2458.70166015625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 143851.0625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1928.560791015625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 3665.50927734375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 4757.14306640625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 5506.24072265625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 2424.142333984375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1657.49267578125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 2104.4365234375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 2549.803955078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1357.0653076171875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.91, loss_val: nan, pos_over_neg: -4558.78125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 883.1714477539062 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 4762.2041015625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1784.82666015625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 595.7508544921875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 1691.0399169921875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 955.9569091796875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 915.3439331054688 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 793.2153930664062 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1148.1087646484375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 55933.41015625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 712.7410278320312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 2884.62353515625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: -33862.4296875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 2745.173828125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 4283.43310546875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 1137.95263671875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 3252.9345703125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1503.0029296875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 1553.856201171875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 3275.4794921875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 514.3841552734375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 2323.810546875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3156.2109375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1754.978271484375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 1070.92724609375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2215.968017578125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 33306.07421875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1174.3106689453125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 2212.11572265625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 976.9185791015625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 747.5553588867188 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 867.4927368164062 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 2302.036865234375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 1271.8341064453125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1139.240234375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2984.593994140625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 4207.3203125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.914, loss_val: nan, pos_over_neg: -5896.76318359375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.921, loss_val: nan, pos_over_neg: -5349.1630859375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1402.188232421875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 2072.63720703125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 2267.71826171875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 29803.693359375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3506.833984375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1295.21533203125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 920.2169799804688 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 4486.05078125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1005.2816772460938 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1803.7327880859375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1260.625732421875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1735.757568359375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1897.09765625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 2235.836669921875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 1726.016845703125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 3743.483642578125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1490.126708984375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1233.993408203125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 658.580322265625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 1482.0115966796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1552.0902099609375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 2561.22021484375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 649.8106079101562 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 1463.8228759765625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 2330.173095703125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 898.3704223632812 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1429.2745361328125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1149.43310546875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1488.7239990234375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 2509.595703125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 869.0999755859375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 4573.84423828125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 980.1603393554688 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 1597.9263916015625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1026.72509765625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 2144.4697265625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1813.51806640625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 3401.42333984375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 815.4529418945312 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 24059.4921875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 948.21142578125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1347.399169921875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.914, loss_val: nan, pos_over_neg: -6758.0302734375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 1742.529541015625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 5680.587890625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1044.41552734375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 2299.10693359375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 394.83038330078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 398.8850402832031 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1158.72607421875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3905.697509765625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 6686.6728515625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 8482.18359375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 13509.794921875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 2539.84521484375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 2166.411865234375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1683.5001220703125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2040.9404296875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1390.3189697265625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 594.362548828125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1628.8638916015625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 727.6414794921875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 447.33221435546875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 7724.51025390625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1621.1348876953125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1796.689697265625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1595.531005859375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 6211.55615234375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 1068.6614990234375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 658.5917358398438 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 3411.533447265625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 2055.234619140625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: -4926.97216796875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 4535.99267578125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1749.8531494140625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: -16162.65234375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 1984.5284423828125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 1250.3538818359375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 39325.52734375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 9797.595703125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1752.1248779296875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1790.514892578125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 935.4705200195312 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2802.374267578125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 623.8469848632812 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: -7741.6748046875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1071.2489013671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 513.1222534179688 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3720.440185546875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1898.2911376953125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 1370.8685302734375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 671.8619384765625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 685.0542602539062 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 2708.3056640625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 5928.37158203125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1307.04736328125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1074.91162109375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2248.546142578125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 947.1129150390625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1287.817138671875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 2228.78955078125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1947.761962890625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1298.548583984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1450.6485595703125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 5590.5712890625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: -11324.7998046875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 2542.656982421875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 3041.324462890625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1603.2738037109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1168.648193359375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2456.077392578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 6611.5166015625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 2452.875732421875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1923.033203125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -11857.1533203125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1655.9859619140625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1114.5562744140625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 4564.44775390625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: -3591.837158203125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 920.4867553710938 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 857.3189697265625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 35509.5 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 7077.5908203125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 5790.60791015625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 49370.265625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 845.3496704101562 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1205.43017578125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 10429.5693359375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 4193.8173828125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1094.0423583984375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 1101.117431640625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1214.835205078125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 5369.1669921875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 2172.921630859375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 1420.26611328125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 9569.939453125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.925, loss_val: nan, pos_over_neg: 1037.0911865234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 30402.09375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 6055.85400390625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 28318.767578125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1553.453857421875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1968.3056640625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1209.978271484375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1459.26171875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 2188.2509765625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 5082.38916015625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 807.2922973632812 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1641.120849609375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1338.85302734375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 3635.136962890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 3230.1044921875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.925, loss_val: nan, pos_over_neg: -4604.78759765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 37612.01171875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1151.6417236328125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2848.66552734375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 2057.6767578125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1262.91845703125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1038.1392822265625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1102.067138671875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1430.990234375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 4772.21044921875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 2517.73583984375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1243.435791015625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1739.04833984375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1894.721923828125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 792.6815185546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 6518.498046875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 1704.81640625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 2824.49072265625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1712.8192138671875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1476.8565673828125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1712.5098876953125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1177.1795654296875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 2498.146240234375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.925, loss_val: nan, pos_over_neg: 12314.5361328125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1426.0474853515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1669.4942626953125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 2482.032470703125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2467.397216796875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -3861.405517578125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 7704.55517578125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 884.7332763671875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 775.93798828125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 25999.986328125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 4770.52880859375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 2052.269775390625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1253.3798828125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 27552.990234375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2803.83349609375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1593.650390625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 1399.779296875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1491.963623046875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1068.3887939453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 4282.97412109375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2791.479248046875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 1003.596923828125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1437.1322021484375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 2031.3411865234375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 583.6765747070312 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 695.1117553710938 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1113.984130859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 2567.938232421875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 7547.71923828125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 33517.12109375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1477.3538818359375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1324.2562255859375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 3772.94384765625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 16449.119140625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1655.8604736328125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 609.8065185546875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 781.5662841796875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 51751.09375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: -6030.7099609375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 7274.23779296875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 1378.1710205078125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 3002.158203125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 4864.02880859375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 2218.025390625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 2973.016845703125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1280.779052734375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1294.16796875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 1883.0262451171875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 913.7892456054688 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4374.537109375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 997.3294067382812 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: -30385.1640625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1781.48779296875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 3641.294677734375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: -15022.310546875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 902.02490234375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 737.4840087890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1240.11376953125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1197.462158203125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 26738.291015625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1124.5626220703125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1796.0576171875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 1523.23193359375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2041.704345703125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 8152.5654296875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.914, loss_val: nan, pos_over_neg: -9012.087890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1001.6215209960938 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1058.325927734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 5817.65087890625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 4016.723388671875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 1873.02197265625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 3799.92919921875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1030.5560302734375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 1589.239013671875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 22680.3046875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 2109.097900390625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1944.9844970703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 2226.327392578125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 3585.323974609375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: -76594.203125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 6403.234375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1209.7591552734375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1092.09033203125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3792.57666015625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8987, loss_val: nan, pos_over_neg: 2565.287841796875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 17725.95703125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 11780.162109375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 13330.77734375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 2130.94970703125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1533.2149658203125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2269.224609375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 758.1945190429688 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 8351.314453125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1663.5496826171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 500.3431091308594 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 3039.08056640625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 6181.73974609375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 972.29638671875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 4326.1533203125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 1050.77880859375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1132.9644775390625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1970.367431640625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1545.52294921875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 2020.933349609375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 5174.28173828125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 3291.482666015625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 3114.94580078125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 10078.8740234375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2114.76123046875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 1592.6029052734375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1201.5216064453125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 594.057373046875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1709.15576171875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 782.169921875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1737.1322021484375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2304.421630859375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 7530.72021484375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1868.0755615234375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: -11486.921875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 2512.089111328125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 2103.428466796875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1169.4654541015625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1461.7205810546875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 2558.055908203125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1230.9349365234375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 1200.8582763671875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 2408.95361328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1904.7708740234375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: -25957.744140625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: -19894.99609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 5395.5205078125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: -4871.2587890625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 2228.8056640625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 4446.19921875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 42552.1953125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 6519.19970703125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 2369.002685546875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1416.81884765625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 1321.56201171875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 976.3785400390625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2029.9696044921875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 1740.7298583984375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 223862.265625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1862.827880859375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 6149.43994140625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: -6692.2001953125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 1898.2691650390625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 1772.8380126953125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 6810.07421875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 70330.9609375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 4044.204345703125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9321, loss_val: nan, pos_over_neg: 1376.6600341796875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1103.4508056640625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 910.0184936523438 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 25776.873046875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 2843.82177734375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 693.1392211914062 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1106.8671875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 619.9368286132812 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 527.0968627929688 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 658.1196899414062 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 634.4827270507812 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 1580.7578125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1983.4991455078125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 771.3433227539062 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1094.2867431640625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: -97637.640625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 782.6519775390625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 10811.10546875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 686.609130859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1847.99462890625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: -13945.720703125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 2737.043212890625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1234.28369140625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 3358.79296875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2808.6494140625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 3654.068603515625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 6315.064453125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 3236.935546875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 4743.68505859375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1610.43896484375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 3572.563720703125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 2652.713134765625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 2624.84423828125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 4618.4765625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 4048.330810546875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 1733.679443359375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 6690.595703125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 1473.0521240234375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1469.2138671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: -6462.5078125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 3625.138916015625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1826.9664306640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1671.1207275390625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.912, loss_val: nan, pos_over_neg: -8857.4267578125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 3145.560791015625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 12102.859375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1946.7861328125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 5570.3984375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2311.6767578125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1883.8489990234375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 932.4706420898438 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1638.407958984375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: -7017.72900390625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 12375.5654296875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 2451.239501953125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: -69455.3984375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 2713.873046875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 1064.890380859375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 1644.660400390625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 4698288.0 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1085.3656005859375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 2067.384033203125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1010.4219360351562 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2981.951171875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 7615.3720703125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2269.5634765625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 1909.10498046875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 7257.62744140625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 2509.712158203125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 4518.37939453125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 1929.8634033203125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1024.482421875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1984.822021484375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1605.897216796875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: -16181.787109375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 6666.82470703125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1530.1561279296875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1800.85888671875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1756.2784423828125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 1214.1256103515625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: 1453.849609375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 1299.949462890625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 628.8189697265625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 8304.9189453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1631.981201171875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1669.130859375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 2877.56787109375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 11076.8681640625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 1963.6680908203125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 3620.09765625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1562.040283203125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 2489.271484375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1493.738525390625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 12266.537109375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 2878.30908203125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1077.1240234375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 3271.242919921875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 2990.249267578125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 1320.3721923828125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 720.6920776367188 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1982.1513671875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 2968.2783203125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 3208.653564453125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1128.8980712890625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 749.4777221679688 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1084.7734375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 2261.526611328125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9343, loss_val: nan, pos_over_neg: 2554.51220703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 719.2345581054688 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1095.9853515625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 5020.212890625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -18023.42578125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 32305.134765625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1511.84375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1884.1920166015625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1594.5904541015625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1684.5887451171875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: -26864.71484375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1431.425537109375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1043.4019775390625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: -7245.08251953125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 5125.2294921875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 1655.281982421875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1701.2032470703125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 1832.947998046875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1485.5091552734375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2130.880615234375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 2287.94921875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1155.4248046875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1774.1025390625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 3310.25439453125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 4426.62744140625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 4464.791015625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 5678.8544921875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1659.6956787109375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1520.273193359375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 7099.95263671875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 2767.73974609375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 3452.2587890625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 6049.087890625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 2554.523193359375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 1519.78173828125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1220.4677734375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1836.9306640625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 4198.43359375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 6895.14208984375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 2266.600341796875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 882.5283813476562 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 1114.1060791015625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1344.3912353515625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1393.87939453125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1652.54052734375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 11872.9326171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1948.272705078125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 847.4385986328125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1255.2498779296875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1050.131591796875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 918.2046508789062 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 892.9920043945312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 3998.1484375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 3662.3603515625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 2495.98876953125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1012.7833862304688 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 35811.60546875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 24842.12109375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 2046.1973876953125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1192.362060546875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 11356.0400390625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1654.394287109375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 1148.4176025390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1380.516845703125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 291028.625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1837.990478515625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1543.4205322265625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 1247.5986328125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: -3730.239990234375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 2171.042724609375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1168.2110595703125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2134.091552734375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 2267.85205078125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2813.239501953125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 5040.78125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 19100.8046875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1147.09814453125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 8460.0517578125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 878.6343994140625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 4015.284912109375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 885.0553588867188 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 3891.833251953125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: -842751.625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 24595.9140625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 3083.489013671875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1369.4854736328125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1029.5947265625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 2855.08544921875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 2211.600830078125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 2919.945556640625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 983.709228515625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1000.2305297851562 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1708.657958984375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 12547.3173828125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 806.4009399414062 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 797.735595703125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2161.577880859375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 11591.3203125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 19710.869140625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 940.0065307617188 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 849.1981811523438 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: -12204.7470703125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 552484.5 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1236.696044921875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 957.748046875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1339.6551513671875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1563.805419921875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 5132.98779296875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 3110.40625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 985.3955688476562 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 1381.0634765625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: -26061.564453125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 3566.28173828125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 2098.823974609375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 2934.504638671875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 2167.744140625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8946, loss_val: nan, pos_over_neg: 3989.90869140625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1001.5339965820312 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1075.217529296875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 1672.4764404296875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 1731.2918701171875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 3249.38525390625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 989.2283935546875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1540.43701171875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: -3709.119384765625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 3913.4970703125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1391.50244140625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1381.3978271484375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1736.90380859375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2030.8935546875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 2217.10205078125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.895, loss_val: nan, pos_over_neg: -2474.236083984375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 16812.69140625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.924, loss_val: nan, pos_over_neg: -338005.65625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: -1219289.375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 1456.5831298828125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 5274.005859375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 12609.9248046875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 2265.947265625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1184.5029296875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 11255.6337890625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1627.2081298828125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 3176.62451171875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 2595.142578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1261.859375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 32559.42578125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:00:01<100122:16:23, 1201.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 915.0073852539062 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1190.4296875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: -9027.373046875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1812.66357421875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 1824.3536376953125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 3802.115234375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 2484.25537109375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 7818.12646484375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 4718.265625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: -23409.009765625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 3008.78564453125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 998.4008178710938 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: -4114.201171875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 3330.475341796875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 16388.4140625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 486.24273681640625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 220997.171875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1481627.75 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2551.81884765625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 2900.4853515625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1282.052490234375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 951.1390991210938 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2160.34619140625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 554.3411254882812 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 1272.7413330078125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1517.784912109375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1569.71923828125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 9128.7236328125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 5858.33203125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: -9143.6796875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1302.4000244140625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1660.477783203125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3331.606689453125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 520.1856079101562 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1966.9112548828125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2025.714111328125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: -17105.19140625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1006.8687133789062 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1232.1717529296875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 916.5042724609375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 2112.87353515625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 5076.67529296875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -7552.48974609375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 3005.43896484375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 3795.646728515625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 3536.660888671875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 2013.8748779296875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: -10106.060546875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2905.103271484375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 14701.849609375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 4899.591796875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 4798.67431640625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 2476.834228515625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 7715.22216796875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 4843.24609375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 2005.243896484375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: -14571.6328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: -7648.1650390625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 4961.4189453125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 6564.65869140625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 3905.639892578125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 5150.9931640625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.911, loss_val: nan, pos_over_neg: -4055.68212890625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 4374.31103515625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 6588.64111328125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 11619.26953125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 7785.81201171875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 9513.75390625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2572.632568359375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 2358.921142578125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 2123.1533203125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1810.4964599609375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1521.190673828125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 1357.888427734375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1479.69677734375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 2711.494140625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 1061.2030029296875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 747.7968139648438 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: 1812.70166015625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1397.64697265625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 2401.808349609375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 1672.3056640625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1726.414306640625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1020.232177734375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 1328.261474609375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 4069.52978515625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 816.07666015625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 824.155029296875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1315.24853515625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: -11698.716796875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1158.8614501953125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 1657.39013671875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 2410.0380859375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 3833.122314453125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1535.2833251953125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 785.0856323242188 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 1866.0447998046875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 815.5452880859375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 3558.170166015625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 3668.13623046875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 2280.628662109375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: -13272.8642578125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 5087.51220703125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1567.1693115234375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 1064.772216796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1533.8614501953125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 3126.39501953125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2261.40625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 1694.50341796875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 51288.99609375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1460.998779296875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 8236.7158203125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.901, loss_val: nan, pos_over_neg: -3469.379150390625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1838.49853515625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 2273.531982421875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 1505.7181396484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 1812.5517578125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 4914.20556640625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: -39158.5859375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 30588.181640625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 2342.617919921875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1762.157470703125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 3405.590576171875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 5375.54248046875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 6841.5654296875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 949.8619384765625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 5667.6748046875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 7420.5341796875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 810.1507568359375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1555.25390625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 1215.211669921875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1387.5325927734375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 1358.8685302734375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 4141.50732421875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1693.389892578125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 507.6087951660156 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 759.5479736328125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1882.5594482421875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 883.6702880859375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 750.10986328125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 713.5235595703125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: -39577.390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 8539.3583984375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1346.3331298828125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1492.89208984375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 1649.2196044921875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 2142.850341796875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 3279.670166015625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 1573.257080078125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 959.7039794921875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 763.3081665039062 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 1539.8226318359375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 928.580078125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1834.255615234375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1190.510498046875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 1135.5494384765625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1986.39794921875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: -4880.8828125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 1915.8487548828125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1477.5162353515625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1728.68505859375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1468.1497802734375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1515.2969970703125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2050.26318359375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1449.381591796875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2280.0166015625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 1258.0872802734375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1407.0926513671875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1483.5245361328125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1017.80859375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 725.9194946289062 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 702.8294067382812 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1833.4493408203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 960.890625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 720.3832397460938 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1250.1458740234375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 3100.402587890625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 2849.52294921875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1569.6768798828125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 906.4702758789062 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2307.7958984375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1094.1572265625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1452.1820068359375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1648.074951171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1183.9913330078125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 2625.1669921875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.902, loss_val: nan, pos_over_neg: 4245.169921875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1842.0673828125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1230.97314453125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 756.2969970703125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 4528.9248046875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 1258.4454345703125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 4865.39111328125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 9801.2490234375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 2827.097412109375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 4610.2236328125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 5497.34423828125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 4080.6201171875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 2154.521240234375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: -5466.12841796875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 25856.328125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 6408.31689453125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2446.625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 2996.492919921875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 881.7698974609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: -105863.1171875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: -6735.32177734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1731.5303955078125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 4675.37744140625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1110.62744140625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 1295.1483154296875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1157.8043212890625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 911.7153930664062 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 4109.341796875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 1660.618896484375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 1130.83935546875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1732.9013671875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1676.95654296875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 170258.0625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 5880.27197265625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1192.95361328125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: -60045.3203125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1938.935302734375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2987.28662109375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 2540.7041015625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1306.118408203125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 2034.7996826171875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 2193.929443359375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 3145.224609375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 3296.08203125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 646.7845458984375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1171.888671875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 1422.412109375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 9083.4833984375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1001.4627685546875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1654.1102294921875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1537.278076171875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 18916.177734375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 663.402587890625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1683.5152587890625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 735.0845947265625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 1382.91650390625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 2943.329345703125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1860.77490234375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1191.3856201171875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 4624.0048828125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1204.39794921875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1720.263427734375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1195.0625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1811.259521484375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1261.89013671875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 1095.937255859375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 2302.83837890625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 2403.129150390625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 8402.6982421875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 6849.44775390625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 1437.849365234375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 14765.3603515625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 2466.085693359375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 14604.8173828125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 2184.681640625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 1800.5330810546875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1202.9720458984375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 2257.7041015625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1389.5150146484375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1062.028076171875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 631.2564086914062 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 2440.521728515625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: -8136.7294921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.908, loss_val: nan, pos_over_neg: -15209.8271484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 895.9627685546875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 1550.2076416015625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 1967.87890625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1136.907470703125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 2656.593505859375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 916.0968017578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1527.7476806640625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: -77816.859375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: 3285.695556640625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 1363.32421875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1701.6092529296875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1082.8316650390625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: -17971.076171875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 2685.18017578125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2845.462646484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 3943.134521484375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2255.9921875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 3396.46728515625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1449.782958984375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 2571.857421875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 1931.0089111328125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 2001.2554931640625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2391.868408203125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 2787.588623046875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 3626.376708984375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 2442.442626953125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 3524.2587890625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 8682.7998046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 25561.33984375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1401.7166748046875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 714.6112060546875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 3006.981689453125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 6101.0234375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 3246.2001953125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1689.923583984375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 2696.237548828125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 961.740966796875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1613.0594482421875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1130.3409423828125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 955.56298828125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 554.6195678710938 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1138.2947998046875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1256.1685791015625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1358.884765625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 2107.66796875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1482.3663330078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 8899.615234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1174.0804443359375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 2975.958984375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1285.661865234375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 4279.455078125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 16619.439453125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1394.9486083984375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 5665.435546875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 1601.0 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2754.055908203125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 4124.4560546875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 1023.2027587890625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2194.37353515625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 708.3528442382812 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 3468.5703125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 969.615234375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 1062.292236328125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1498.709716796875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 3283.10009765625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 760.0953369140625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1883.380615234375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 1482.675048828125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1245.537353515625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 659.2952270507812 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 2903.152587890625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1003.2384643554688 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1424.4488525390625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 5717.294921875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 2043.1654052734375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2672.783203125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1741.5516357421875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 925.0018310546875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1909.5068359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 2129.097900390625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 2014.4532470703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1330.1641845703125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1121.14404296875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: -16638.64453125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 4804.56689453125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1761.931396484375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1474.7379150390625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 2520.89208984375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 635.0744018554688 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1933.4798583984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1397.0367431640625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1576.932861328125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1527.0799560546875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1122.5159912109375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 894.201416015625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1120.5340576171875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 2659.113037109375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 1161.2518310546875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 792.1221313476562 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 1194.2857666015625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 4576.29345703125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 897.4149780273438 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 12600.1513671875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 2116.385986328125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 5641.18359375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 3102.47119140625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 3591.437255859375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1351.4173583984375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1351.1273193359375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1226.1392822265625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 5773.1220703125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 2529.63720703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1535.00244140625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 2762.7890625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 738.2337036132812 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 3407.609130859375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1182.4713134765625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2030.945068359375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 680.1197509765625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2275.569091796875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 3249.350830078125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1049.147216796875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1707.6141357421875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1056.72216796875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: 615.9171752929688 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1796.50634765625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1881.3494873046875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1987.4696044921875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 3594.36474609375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: -201273.671875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 4456.09912109375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 5343.84521484375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 2481.97021484375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2044.7001953125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 3097.94580078125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1830.526123046875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 950.4401245117188 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 2590.06884765625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 2152.722900390625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: -49595.64453125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 1966.655517578125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 13132.294921875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 3276.591796875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: -11310.279296875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: -4246.38720703125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 2307.921875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2203.870361328125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 6295.986328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -21087.03125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1317.5311279296875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 724.4412841796875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 2818.140869140625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 8143.11328125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 2221.32861328125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1668.60791015625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 5471.07763671875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: -581562.125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 2998.663330078125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 939.3170166015625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 5629.60302734375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 2675.604248046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 4147.82421875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3859.947265625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 1656.3812255859375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 1589.5640869140625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 1551.9063720703125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 4412.962890625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1802.0233154296875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 1518.7283935546875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 891.9575805664062 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1657.576904296875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1880.0333251953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 1898.671875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 5037.14306640625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 1762.09033203125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 11857.8603515625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 38896.30859375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 6030.96435546875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 3407.9560546875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 4358.3759765625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 1715.0394287109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 2566.30517578125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 1085.97705078125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 1659.8248291015625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 3584.138671875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 1627.3740234375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1080.4749755859375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1629.3812255859375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3530.769287109375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 4617.7880859375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 2918.508056640625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 723.8969116210938 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2139.143798828125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 14239.1064453125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 799.10009765625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1867.2880859375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 682.2769775390625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 933.9096069335938 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: -308983.21875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1444.311767578125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 897.6822509765625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: -153311.453125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1773.477294921875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 3202.45654296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1486.60546875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 2302.331787109375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 2634.616943359375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1152.208984375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 3233.0947265625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 2475.61279296875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1483.2366943359375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1711.7349853515625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1955.4603271484375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1189.361572265625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2326.912353515625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: -7287.1689453125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 1735.3123779296875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1233.095458984375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 4723.94921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 4202.88037109375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 2571.88525390625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1476.180908203125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 3291.294189453125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -4127.4609375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 850.4931030273438 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1310.039794921875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 7857.337890625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 12023.6005859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: -8269.267578125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 5080.7001953125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 2816.30712890625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 10495.330078125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1627.1156005859375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 1317.6767578125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 789.0916137695312 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 3116.6220703125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 3034.035400390625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 789.9901123046875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 2046.4178466796875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 2686.720458984375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 861.2899169921875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1028.8773193359375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 948.6109619140625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 784.83154296875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 6356.93994140625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 1234.232666015625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8948, loss_val: nan, pos_over_neg: 2519.45166015625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: -14078.4765625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 1769.02978515625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 1427.1129150390625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: -9770.625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1480.54345703125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1702.468994140625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1728.220458984375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2704.40234375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 3345.138671875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 6676.97705078125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 2005.6043701171875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1038.1734619140625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2435.8095703125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: -2024.05029296875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 4071.16845703125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 574.9392700195312 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 824.1446533203125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.901, loss_val: nan, pos_over_neg: -2784.161865234375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.906, loss_val: nan, pos_over_neg: -6246.92431640625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1879.5418701171875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: -42080.64453125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 1529.107177734375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: -62339.86328125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 3449.255615234375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 6251.8525390625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 12829.392578125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 3904.043212890625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1878.8983154296875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: -20342.43359375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1649.7470703125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: -89171.59375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 4217.42724609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1237.432861328125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 2068.83203125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: -139794.828125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 2164.36767578125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1826.186279296875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1879.2227783203125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 13016.583984375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 3013.767578125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1163.45361328125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 2433.5791015625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: 3224.553955078125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 2002.0452880859375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 2262.13818359375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1649.5052490234375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 1248.47265625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 2950.054931640625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1547.506103515625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 1867.6705322265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1678.8436279296875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1931.366943359375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1539.1494140625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 782.1441040039062 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 6479.794921875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 1741.8248291015625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 2429.682861328125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 1210.217529296875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: -14579.376953125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 14984.8564453125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 4934.83203125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 2031.340087890625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1142.9010009765625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1373.936767578125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 2208.063232421875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: -25734.07421875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8987, loss_val: nan, pos_over_neg: 2390.69677734375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 794.244873046875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1315.880615234375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 27327.435546875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 10899.189453125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 8703.0546875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1214.167724609375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 1739.7015380859375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 849.9476318359375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.891, loss_val: nan, pos_over_neg: -14551.740234375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 12713.60546875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1186.075927734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: -4393.806640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 21374.806640625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: -10461.75390625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1235.6529541015625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 2015.07861328125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 1764.0272216796875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 6573.19091796875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1876.9620361328125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1537.7392578125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 1426.570556640625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1761.493408203125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 118066.375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1207.720703125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 12510.3515625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 3574.673095703125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 1399.3341064453125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1666.4249267578125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: -24867.314453125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1717.54638671875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 6721.17822265625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 5266.73974609375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2572.07470703125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 809.098388671875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 2549.835205078125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: -45692.4609375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 827.2994995117188 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1959.52099609375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 1389.140380859375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 831.855224609375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 789.5621337890625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 407.4544372558594 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 392.701904296875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1055.6279296875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 1774.8272705078125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 1183.7176513671875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 1528.52099609375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 763.4108276367188 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 961.294189453125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1011.26611328125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1832.22314453125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1829.4505615234375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1549.296142578125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 858.443603515625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1126.559814453125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1885.474853515625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 2372.068359375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 2142.20166015625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 1038.7933349609375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 1186.3387451171875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 1330.646484375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1303.593994140625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 3013.750732421875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 2347.5166015625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: -8340.8837890625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 366808.0 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 13190.8515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 3979.416015625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1838.5792236328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 701.3015747070312 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 965.3842163085938 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 1460.6219482421875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1226.919677734375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 7391.16845703125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 766.322021484375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1570.879150390625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1976.4903564453125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1672.171142578125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1524.4791259765625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1123.5982666015625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 793.5996704101562 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 1751.3271484375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 2399.244384765625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1093.57666015625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 689.6441040039062 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 866.33203125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1400.6966552734375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 676.330078125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1814.9246826171875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1106.14208984375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 973.310546875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 1265.9476318359375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1058.87939453125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2834.609619140625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 1158.918701171875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1046.8221435546875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 2921.814453125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 769.4741821289062 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 2266.807861328125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1792.5736083984375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 771.370849609375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1315.169189453125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1229.031005859375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: -8193.9287109375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.915, loss_val: nan, pos_over_neg: -18351.458984375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 9433.8974609375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 5429.8115234375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1182.464599609375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: -8964.6435546875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 4459.79150390625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8875, loss_val: nan, pos_over_neg: 2223.722412109375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:20:04<100166:45:04, 1202.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 1829.0745849609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 2274.955322265625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 7070.23876953125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 837.2586669921875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 798.3116455078125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: -5539.25439453125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1090.5521240234375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 2152.70947265625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 2133.79248046875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: -2241926.0 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 12118.892578125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: -7432.04150390625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1228.2996826171875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1310.681640625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 3703.114013671875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 998.6375732421875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 4354.9443359375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 2696.554443359375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: -20356.728515625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 5732.634765625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1289.53271484375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2941.18017578125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 1030.8260498046875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 6617.72509765625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 4701.4541015625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1350.5989990234375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 10642.7060546875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1280.7938232421875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 2477.018798828125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 722.9835205078125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 5886.3447265625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 780.2947998046875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 3600.713623046875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 1642.55419921875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 2310.952392578125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: -3553.8173828125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 15703.4130859375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1594.479736328125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 3042.795166015625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 10979.353515625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 3844.591796875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 8988.8779296875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.905, loss_val: nan, pos_over_neg: -10175.71484375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.905, loss_val: nan, pos_over_neg: -12904.099609375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 1641.989501953125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1072.5328369140625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: -11169.76953125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1495.90771484375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 880.910400390625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3169.80126953125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 3973.525146484375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 2181.6689453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1971.2557373046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 1223.3814697265625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: -90917.7109375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.909, loss_val: nan, pos_over_neg: -2947.03125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 623.484375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1719.0821533203125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 5934.43017578125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: -4154.12744140625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 956.6080322265625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 962.9780883789062 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 1094.7969970703125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 6300.64990234375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 5895.39404296875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 2540.925048828125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 994.087646484375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1457.456298828125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8994, loss_val: nan, pos_over_neg: 3701.739013671875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 444546.84375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 1459.8935546875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 1945.1630859375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 916.4938354492188 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1499.2568359375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 588.5422973632812 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 889.6013793945312 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 1188.0955810546875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8928, loss_val: nan, pos_over_neg: 1232.968994140625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1492.7545166015625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 2959.230712890625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 1140.5091552734375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8921, loss_val: nan, pos_over_neg: 2433.98828125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 1479.8763427734375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 1252.067626953125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 703.6441040039062 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 1655.9373779296875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 1265.0870361328125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 1218.7420654296875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 1573.17626953125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 2021.027099609375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 5983.51123046875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8938, loss_val: nan, pos_over_neg: 1704.7244873046875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 2443.696533203125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1691.7052001953125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 824.821533203125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1462.1783447265625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 6531.30517578125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 601.3416748046875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 1566.56201171875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1614.0927734375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 802.0294799804688 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1260.7393798828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 3468.67138671875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 1860.177734375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2146.74560546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 1222.25732421875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2415.472412109375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 2688.54638671875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: -120185.0859375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 49092.37109375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 5638.978515625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1480.395751953125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 20966.513671875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 3266.273681640625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 3141.49365234375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2998.107177734375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 16961.24609375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 5273.900390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1288.663330078125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8949, loss_val: nan, pos_over_neg: 4766.44580078125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 1401.4053955078125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2621.2734375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 1050.8519287109375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1113.21875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1581.966064453125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: -38032.9140625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 2797.451904296875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 2181.33740234375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1786.4058837890625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 966.8658447265625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 4420.5263671875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1010.4531860351562 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1300.4879150390625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 912.2586059570312 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 1175.274169921875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: -6326.50732421875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: -4303.89990234375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 3486.89453125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 32324.109375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 5077.904296875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 2076.90234375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: -5256.38427734375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 4703.728515625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1641.3419189453125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1561.166748046875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 955.16748046875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 6962.2333984375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: -7255.318359375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2530.75830078125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8973, loss_val: nan, pos_over_neg: 1351.7359619140625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 5134.95751953125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 1691.50439453125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -17816.6796875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 4470.2353515625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 1043.1307373046875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 9206.6796875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 2434.349609375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 6154.52783203125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 12692.6474609375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 5196.2109375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: -17007.30078125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -17483.955078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2640.1962890625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 1993.657470703125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8986, loss_val: nan, pos_over_neg: 4625.35205078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 830.9146728515625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: 4206.044921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 607.3087768554688 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: 2737.283935546875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 2994.76123046875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: 2667.708740234375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 3075.170166015625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 2702.85400390625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1313.4613037109375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 9989.4619140625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 2662.2783203125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 1285.4627685546875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1998.81396484375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1050.2557373046875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: -301509.34375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: -5204.22119140625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2688.9462890625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 2170.69189453125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 3074.291015625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 1310.0023193359375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 1402.5421142578125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 1826.4462890625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 2091.248779296875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 1696.361083984375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 4360.01171875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 998.5374145507812 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 2300.67626953125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 1130.6275634765625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 2892.377197265625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 4880.39599609375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 5197.76806640625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8894, loss_val: nan, pos_over_neg: 3994.51318359375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 2578.08837890625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 685.353515625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 2742.16650390625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 7222.85205078125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1417.299072265625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 2685.5380859375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 1077.119873046875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 1134.8037109375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1145.6705322265625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 1024.0682373046875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1808.808837890625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 3608.037353515625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1222.744384765625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 664.8082885742188 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 723.8626098632812 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1301.8499755859375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 8951.228515625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1425.7889404296875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 2172.7998046875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1666.0423583984375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 3186.3642578125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 3827.60009765625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 2927.65478515625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 3986.337646484375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2945.0087890625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 4762.17578125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 1601.706787109375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 4146.12451171875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.906, loss_val: nan, pos_over_neg: -13637.376953125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 2518.634521484375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1881.2120361328125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 1065.7301025390625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 3309.47119140625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1963.19580078125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 1140.4261474609375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1124.552734375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 2003.8277587890625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 803.0118408203125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1345.0577392578125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 2013.3612060546875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2281.922607421875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 3448.50927734375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: -5996.49755859375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 25058.1953125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 7356.06396484375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 3818.845703125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 2337.901123046875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1319.003662109375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 979.7753295898438 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 3106.47607421875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 2295.43115234375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1346.7379150390625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 2180.306640625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1511.3994140625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 7868.3505859375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 2340.271240234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 12037.5634765625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 14195.4833984375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1586.173583984375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 3319.374755859375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 33096.2265625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1183.9749755859375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1977.546142578125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 13055.0244140625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 1657.7701416015625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8925, loss_val: nan, pos_over_neg: 38411.0703125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.905, loss_val: nan, pos_over_neg: -41497.4375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 2149.077392578125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1053.066650390625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 962.606689453125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 5618.59716796875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: -9658.294921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 2754.353271484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 4320.103515625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 47963.9375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: -2896.21240234375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1223.661865234375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 3596.88427734375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: -4100.35546875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 2420.0546875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1422.5596923828125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 1137.3560791015625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 62328.1953125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: -4084.115966796875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 4267.6279296875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 2396.060302734375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1586.778564453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 3369.172119140625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 4185.62548828125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 2537.980224609375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 2293.6015625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1265.90380859375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 831.3299560546875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 1723.264404296875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 788.6373291015625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1930.6820068359375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 4572.95556640625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1910.4698486328125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 1316.3905029296875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 744.4866333007812 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 28602.767578125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 576.4666137695312 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 1231.009033203125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1261.15380859375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1751.274169921875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 1560.288818359375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 2557.719970703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 1606.217529296875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 910.0381469726562 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 1228.1141357421875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1902.921142578125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: -31324.18359375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 3349.43896484375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1044.2650146484375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 1091.1080322265625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: -8580.3203125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 4316.95703125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 1384.29248046875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 732.9551391601562 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 2009.2750244140625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: -7283.5068359375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 2397.7470703125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2073.937255859375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.893, loss_val: nan, pos_over_neg: 980.4022827148438 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 3413.871337890625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 9296.8330078125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 933.98828125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 1941.2484130859375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 3048.802734375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 823.0652465820312 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 974.4615478515625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 652.7904663085938 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 2102.68603515625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 2374.78369140625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 2911.48828125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 2136.22998046875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 3080.408935546875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1997.2703857421875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 2158.3388671875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1297.18505859375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 2861.80029296875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 1547.536865234375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 2027.253662109375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2424.608154296875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 2358.610107421875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 580.601318359375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 900.2257690429688 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 5218.59130859375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 2725.3720703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1045.97900390625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 1220.9404296875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1357.8839111328125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: 1901.4727783203125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 2633.018310546875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: -10428.3662109375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 2316.362060546875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 1711.9365234375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1280.79736328125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1181.1412353515625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 630.5474243164062 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 5433.60791015625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 1534.91064453125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 1483.240478515625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2025.6116943359375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1632.1177978515625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 2510.23974609375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2582.26318359375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 6244.94140625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 2819.10595703125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8945, loss_val: nan, pos_over_neg: 5163.84033203125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 9284.1005859375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: -13061.6767578125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 4145.525390625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 3704.031005859375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1040.4554443359375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 618.0585327148438 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 1119.1065673828125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 485.8759765625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 42694.60546875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 1026.388916015625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1395.490478515625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 3396.866455078125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: -32050.4140625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 960.2413940429688 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1519.0537109375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1130.86083984375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1577.3135986328125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: -147837.15625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 2809.174072265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1957.1396484375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 1554.6370849609375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 3058.36328125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 2674.6865234375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 2404.4423828125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 1320.8927001953125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 1815.3753662109375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 1567.3807373046875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: 15577.9677734375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 3102.849365234375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: 1896.771240234375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1201.7501220703125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: -10750.845703125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 5043.0654296875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 14240.1884765625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 2510.313720703125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1744.2177734375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2173.33544921875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: -46758.3515625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 4105.5322265625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 12245.2958984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 1520.7198486328125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1774.542236328125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: -23437.40625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: -6090.36572265625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1107.1036376953125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 609.2711791992188 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1006.8663330078125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 3132.701171875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1486.4779052734375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 2300.815673828125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 6598.21435546875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 3694.2919921875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: -7621.373046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 2305.741455078125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 3086.840576171875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 1219.978759765625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 971.8916015625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 1754.7177734375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8966, loss_val: nan, pos_over_neg: -49501.203125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 5870.5400390625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 1384.454833984375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 432.4060974121094 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 1941.3681640625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 92632.625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 3237.334716796875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 3790.282470703125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 19071.67578125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: 2428.383056640625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 8560.81640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 8687.7548828125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: -23514.9921875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 7056.54248046875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1139.7779541015625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 1647.1878662109375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 1988.0880126953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 1669.3848876953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 4433.0205078125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 4555.703125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 2436.800048828125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8846, loss_val: nan, pos_over_neg: 4268.3486328125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1265.01318359375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: -13514.326171875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 5965.3603515625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 1221.73779296875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 731.33154296875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1419.516357421875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 998.2224731445312 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1046.5643310546875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 759.4305419921875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 871.1375732421875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 682.8768920898438 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 1356.0108642578125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.899, loss_val: nan, pos_over_neg: 4983.3427734375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 721.065673828125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1103.8382568359375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 2063.28125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 2321.513427734375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 639.378662109375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 888.8326416015625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: -11712.25 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -32570.173828125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 15981.2919921875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 943.3187255859375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 905.8831176757812 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 2109.689453125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: -26468.39453125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 499.6938171386719 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1626.7943115234375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 842.7264404296875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 37237.75 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 7182.5205078125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 1972.0101318359375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 3179.27880859375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 4858.40966796875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 14141.5419921875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 1483.6929931640625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: -7492.048828125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 7873.5283203125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: -29918.3671875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1393.949462890625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 807.4339599609375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 892.732666015625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 3390.253662109375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 2284.156982421875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 1264.8046875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 16776.126953125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 24337.734375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 2850.341796875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 1318.778076171875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.913, loss_val: nan, pos_over_neg: -5061.52197265625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.911, loss_val: nan, pos_over_neg: -14386.8232421875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: -10363.416015625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 723.2781372070312 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 8381.765625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 2940.298583984375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: -16703.390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 3658.786376953125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1658.3524169921875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 1768.5506591796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 4454.3486328125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: -19708.962890625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2373.463623046875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 4200.06982421875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 3065.068115234375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 7514.5380859375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: -5461.1748046875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1648.9591064453125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 3095.691162109375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 2835.5390625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8955, loss_val: nan, pos_over_neg: -6216.68505859375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 2362.765625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 2345.215087890625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1294.083740234375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 7500.37841796875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 3000.775390625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 649.0760498046875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 2299.705810546875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 34760.24609375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: -169938.6875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1171.19140625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 8023.68359375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1451.544677734375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1737.7578125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4162.09375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 1113.0172119140625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 2044.9039306640625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 1386.1759033203125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 969.1260986328125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2971.572509765625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 3427.33349609375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1547.8333740234375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 1060.318115234375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 1526.124755859375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 928.4850463867188 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 2198.93994140625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 1256.2354736328125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 5813.734375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2052.5986328125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 4615.74951171875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1595.248291015625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 8740.4873046875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: -9085.8935546875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 27782.275390625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 3091.688232421875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 2740.810302734375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8906, loss_val: nan, pos_over_neg: 7107.1357421875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 2153.54052734375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1482.4149169921875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1488.3323974609375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 3296.687255859375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 1707.6544189453125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1664.9010009765625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1955.52978515625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 2698.57568359375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 3635.1962890625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 13943.083984375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2478.134521484375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 4275.68408203125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 930.3877563476562 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 4432.73291015625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 2037.5269775390625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 1187.89794921875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: -24847.787109375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3922.40673828125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 125680.7890625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 2870.916748046875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2815.11376953125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1788.7569580078125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 1072.9503173828125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3757.05810546875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1286.8797607421875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 31156.677734375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 1032.8726806640625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 7210.87841796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 516.450439453125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -5082.478515625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 11319.4482421875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1782.9066162109375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 2403.43505859375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 9293.568359375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 6646.7451171875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 3481.203369140625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 988.7660522460938 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 3593.3154296875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 2546.225341796875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 1285.5159912109375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 269676.34375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 1580.1258544921875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 4152.501953125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 651.7778930664062 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.902, loss_val: nan, pos_over_neg: 1412.74365234375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: -6334.6845703125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 9217.5361328125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 1221.857177734375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1227.78515625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8973, loss_val: nan, pos_over_neg: 2007.1673583984375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: -3256.7255859375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 4025.320068359375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1008.9325561523438 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1007.8242797851562 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 18483.326171875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 2339.833251953125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 2620.644775390625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 3895.9404296875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 2134.922607421875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 1405.8492431640625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1428.8968505859375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 2715.97216796875 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 866.18359375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 2675.541015625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1930.1451416015625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1585.2760009765625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 5792.65478515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 5004.9140625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 2563.080322265625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 818.8170166015625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2231.483642578125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8901, loss_val: nan, pos_over_neg: 2718.2314453125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 2021.72119140625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 608.2199096679688 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 822.8181762695312 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1245.3289794921875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 5373.31201171875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 1787.406005859375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1532.38134765625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 1155.320556640625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 17406.0703125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.894, loss_val: nan, pos_over_neg: 3472.010498046875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1433.2066650390625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 792.0361328125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 1653.8433837890625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 3857.8740234375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 1066.5875244140625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 2181.35546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 926.4160766601562 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 2277.83544921875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 7504.22900390625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 1523.66162109375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 1565.6844482421875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 638.09033203125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1012.7573852539062 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1003.9639282226562 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1726.3172607421875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 928.2578125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1241.337158203125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9, loss_val: nan, pos_over_neg: -2937.84521484375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 4721.73388671875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 3343.62548828125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 1119.185302734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 2041.640625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: -10300.951171875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 1119.589111328125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 684.5072021484375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 1597.03759765625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3995.55712890625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2638.803955078125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: -7835.1240234375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: -96444.4375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1819.9873046875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 1119.2874755859375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 798.720458984375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1947.9324951171875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2136.998046875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 2247.41064453125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 30030.658203125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 6392.87744140625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 1603.5262451171875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1439.4794921875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1296.5792236328125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 2799.020263671875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 1288.15576171875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 2051.585205078125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2006.6102294921875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1341.2620849609375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 3099.148681640625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 3476.0048828125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: -4752.03564453125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8956, loss_val: nan, pos_over_neg: 2723.375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 17281.302734375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 11010.71875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8888, loss_val: nan, pos_over_neg: 1856.9368896484375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: -6719.74169921875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: -61401.05859375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 2451.612548828125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 1062.1214599609375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 4024.02392578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 1719.674560546875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: -10867.4619140625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [2:40:08<100234:09:17, 1202.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2494.420166015625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: -2554.18408203125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 3621.4375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: -16339.17578125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 2508.8515625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: -4552.7861328125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 1530.356689453125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1073.409423828125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: 2549.42919921875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 963.4039306640625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 1379.15625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1444.1549072265625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 1606.4625244140625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 1904.59619140625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 1190.239990234375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 2413.6162109375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1426.511962890625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: -4833.99169921875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 9697.0361328125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1665.00439453125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: 1294.8514404296875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 1287.264892578125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1223.15576171875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 931.9474487304688 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 9888.83984375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1417.6820068359375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 1831.97412109375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 2196.830078125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 15624.998046875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: 3147.9130859375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2518.1015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8935, loss_val: nan, pos_over_neg: 2303.462890625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 2219.201904296875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 3836.653564453125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 4866.25341796875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 3333.3154296875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 859.0841674804688 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8892, loss_val: nan, pos_over_neg: 3451.740478515625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 2753.2109375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8972, loss_val: nan, pos_over_neg: 4971.9931640625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1754.9525146484375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 1542.3182373046875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 2007.079345703125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 2629.10791015625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 5156.150390625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: -3497.338134765625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 1247.93701171875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 1525.8057861328125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 1925.5186767578125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 2096.80859375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8866, loss_val: nan, pos_over_neg: 2907.915771484375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 8808.5849609375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: -10309.4013671875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 1482.234130859375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2449.1982421875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 3159.3994140625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 28040.8046875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 1850.8076171875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 939.6622314453125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 1724.8116455078125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 3310.609619140625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 637.0846557617188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 4861.330078125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 1488.6351318359375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 11048.3916015625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 3389.1025390625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 1008.5672607421875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 3954.4404296875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 3549.99951171875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 1838.4879150390625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1252.8895263671875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 2331.3603515625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 1231.36474609375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8932, loss_val: nan, pos_over_neg: 2233.435791015625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 2432.920166015625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1771.6361083984375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 2062.713623046875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 1455.1878662109375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1929.422119140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 1275.748046875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.887, loss_val: nan, pos_over_neg: -6039.21484375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 889.8907470703125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 1168.4991455078125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2187.322509765625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1440.47119140625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 8896.7998046875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 1370.45166015625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8938, loss_val: nan, pos_over_neg: 6658.9501953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 1874.5118408203125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1701.006103515625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 4284.7099609375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 3538.225341796875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 2689.694091796875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 2813.08447265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8942, loss_val: nan, pos_over_neg: 3238.827392578125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1746.610107421875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 576.9569702148438 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: -5175.8408203125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 4661.98876953125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 1593.5146484375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 4299.22607421875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8881, loss_val: nan, pos_over_neg: -147395.203125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 1108.2503662109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1027.6490478515625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1175.6280517578125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1135.953125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: 3578.76904296875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2616.381591796875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 2006.483154296875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 9459.7060546875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 67604.4453125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: -7917.48779296875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1217.0472412109375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 2863.54150390625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8949, loss_val: nan, pos_over_neg: 2109.959716796875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 4374.28076171875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: -31576.26953125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 3099.301025390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 1450.8917236328125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 10065.70703125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.909, loss_val: nan, pos_over_neg: -3621.130859375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1553.1309814453125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 3947.624755859375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 649.5860595703125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 3883.866943359375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: -29919.09765625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.899, loss_val: nan, pos_over_neg: -10392.7861328125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: -2042.8740234375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: -21831.466796875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 6097.3173828125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 4352.43701171875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: -15722.4541015625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 8114.41015625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 2982.81005859375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 6677.00244140625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 4716.9208984375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: -5417.689453125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2380.159912109375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: -22068.9765625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1628.353271484375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1901.181884765625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 1081.77001953125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: 1391.701171875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 8399.3125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 3746.99853515625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8951, loss_val: nan, pos_over_neg: 6552.06201171875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 17185.603515625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 53939.44921875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 4150.4306640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: -3840.572509765625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -21179.275390625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: -34795.22265625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 22026.111328125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 7431.712890625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.899, loss_val: nan, pos_over_neg: 3583.53955078125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 8522.7265625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 4756.6552734375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 4835.5458984375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1312.3377685546875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 3094.482421875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 1312.5198974609375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: -12303.091796875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 2477.642578125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 65229.40234375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 8282.1962890625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 4581.1171875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 3018.10595703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1465.74365234375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 7739.65087890625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 22048.103515625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 4696.6796875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 13197.029296875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1359.642822265625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 5144.2783203125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 972.3153686523438 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 3491.007080078125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 2355.248046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 2160.614501953125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 7819.41552734375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3325.429443359375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: -4035.280029296875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 1610.9371337890625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8947, loss_val: nan, pos_over_neg: 1148.17724609375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 2236.683837890625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 4228.9052734375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.896, loss_val: nan, pos_over_neg: -51857.85546875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 2312.157958984375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 2647.567626953125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 4164.6953125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8952, loss_val: nan, pos_over_neg: 2812.973876953125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2442.568115234375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8952, loss_val: nan, pos_over_neg: 13863.6689453125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 2078.467041015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 8017.7607421875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 3734.816162109375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 2833.859130859375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 13886.7998046875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.916, loss_val: nan, pos_over_neg: -491242.1875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 1296.59423828125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1658.1279296875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 3122.0849609375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 4663.87890625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1620.8995361328125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 1728.732177734375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 5506.7763671875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: -15776.6572265625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 10108.0087890625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 3835.7314453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: -72729.765625 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=transl045'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
