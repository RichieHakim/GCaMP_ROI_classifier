{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/josh/opt/anaconda3/bin/python'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "# base_dir = '/n/data1/hms/neurobio/sabatini/josh'\n",
    "base_dir = '/Users/josh/Documents/'\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition, torch_helpers\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [],
   "source": [
    "# DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    # augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "    augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "                             stds=[0.229, 0.224, 0.225]),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=clmp'\n",
    "model.forward = model.forward_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5587, loss_val: nan, pos_over_neg: 1.0463637113571167 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.0802, loss_val: nan, pos_over_neg: 3.766254186630249 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.8559, loss_val: nan, pos_over_neg: 3.8215863704681396 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.6793, loss_val: nan, pos_over_neg: 18.27652931213379 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.5719, loss_val: nan, pos_over_neg: 19.421201705932617 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.5198, loss_val: nan, pos_over_neg: 28.400211334228516 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.4645, loss_val: nan, pos_over_neg: 50.231056213378906 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.4335, loss_val: nan, pos_over_neg: 53.602088928222656 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.3797, loss_val: nan, pos_over_neg: 51.45674133300781 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.3531, loss_val: nan, pos_over_neg: 60.74815368652344 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.3504, loss_val: nan, pos_over_neg: 88.90898132324219 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.3222, loss_val: nan, pos_over_neg: 141.582763671875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.3064, loss_val: nan, pos_over_neg: 184.57456970214844 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.2865, loss_val: nan, pos_over_neg: 128.48211669921875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.2843, loss_val: nan, pos_over_neg: 148.0657958984375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.2782, loss_val: nan, pos_over_neg: 137.0606689453125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.2577, loss_val: nan, pos_over_neg: 167.84774780273438 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.243, loss_val: nan, pos_over_neg: 263.75634765625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.2362, loss_val: nan, pos_over_neg: 301.8110046386719 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.2352, loss_val: nan, pos_over_neg: 280.9101867675781 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.2299, loss_val: nan, pos_over_neg: 228.8472900390625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.2184, loss_val: nan, pos_over_neg: 232.0416259765625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.2033, loss_val: nan, pos_over_neg: 379.7417297363281 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.2043, loss_val: nan, pos_over_neg: 601.3388671875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.1713, loss_val: nan, pos_over_neg: 718.37451171875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.1771, loss_val: nan, pos_over_neg: 567.7335815429688 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.1668, loss_val: nan, pos_over_neg: 240.3358154296875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.1658, loss_val: nan, pos_over_neg: 470.6559753417969 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.1599, loss_val: nan, pos_over_neg: 414.66558837890625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.1441, loss_val: nan, pos_over_neg: 590.4993286132812 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.1496, loss_val: nan, pos_over_neg: 830.8899536132812 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.1387, loss_val: nan, pos_over_neg: 703.2197875976562 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.1271, loss_val: nan, pos_over_neg: 2162.826416015625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.1213, loss_val: nan, pos_over_neg: 316.80987548828125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.1246, loss_val: nan, pos_over_neg: 468.9641418457031 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.1173, loss_val: nan, pos_over_neg: 450.40484619140625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.1252, loss_val: nan, pos_over_neg: 685.3872680664062 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.1123, loss_val: nan, pos_over_neg: 1902.573486328125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.1187, loss_val: nan, pos_over_neg: 507.97723388671875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.0922, loss_val: nan, pos_over_neg: 599.371826171875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.0907, loss_val: nan, pos_over_neg: 458.7165832519531 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.0915, loss_val: nan, pos_over_neg: 917.9236450195312 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.0764, loss_val: nan, pos_over_neg: 672.4711303710938 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.0793, loss_val: nan, pos_over_neg: 511.6710510253906 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.0734, loss_val: nan, pos_over_neg: 513.6574096679688 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.056, loss_val: nan, pos_over_neg: 319.19903564453125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.0516, loss_val: nan, pos_over_neg: 798.717529296875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.0505, loss_val: nan, pos_over_neg: 1529.83447265625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.0395, loss_val: nan, pos_over_neg: 654.0528564453125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 577.9224243164062 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.0421, loss_val: nan, pos_over_neg: 573.9695434570312 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.0436, loss_val: nan, pos_over_neg: 196.1876678466797 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.0382, loss_val: nan, pos_over_neg: 783.4732666015625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.0345, loss_val: nan, pos_over_neg: 132.97593688964844 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.0284, loss_val: nan, pos_over_neg: 388.3067321777344 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.0365, loss_val: nan, pos_over_neg: 155.18527221679688 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.0259, loss_val: nan, pos_over_neg: 442.8148498535156 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.0304, loss_val: nan, pos_over_neg: 146.77459716796875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.0308, loss_val: nan, pos_over_neg: 272.4349670410156 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.0216, loss_val: nan, pos_over_neg: 170.69888305664062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.0134, loss_val: nan, pos_over_neg: 158.09056091308594 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.0248, loss_val: nan, pos_over_neg: 201.91346740722656 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.0267, loss_val: nan, pos_over_neg: 230.28097534179688 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 557.8272705078125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.0223, loss_val: nan, pos_over_neg: 109.51348876953125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.0044, loss_val: nan, pos_over_neg: 284.98974609375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 244.60745239257812 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.011, loss_val: nan, pos_over_neg: 207.0958709716797 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 474.7755126953125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9992, loss_val: nan, pos_over_neg: 215.7072296142578 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.0019, loss_val: nan, pos_over_neg: 384.2242126464844 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.0065, loss_val: nan, pos_over_neg: 159.2694854736328 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.0076, loss_val: nan, pos_over_neg: 251.764404296875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 219.37060546875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9988, loss_val: nan, pos_over_neg: 330.0223083496094 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 328.7473449707031 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9871, loss_val: nan, pos_over_neg: 204.400146484375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9796, loss_val: nan, pos_over_neg: 185.1700897216797 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9942, loss_val: nan, pos_over_neg: 336.32489013671875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.0015, loss_val: nan, pos_over_neg: 198.37008666992188 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.985, loss_val: nan, pos_over_neg: 414.7425537109375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9817, loss_val: nan, pos_over_neg: 205.12574768066406 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 309.9580078125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 532.4124755859375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 411.0006103515625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9741, loss_val: nan, pos_over_neg: 268.92431640625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9792, loss_val: nan, pos_over_neg: 185.21609497070312 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9668, loss_val: nan, pos_over_neg: 288.6422424316406 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9633, loss_val: nan, pos_over_neg: 573.853271484375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9603, loss_val: nan, pos_over_neg: 2062.26806640625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9658, loss_val: nan, pos_over_neg: 367.1512451171875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 927.89208984375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 403.6012268066406 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9702, loss_val: nan, pos_over_neg: 416.2385559082031 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9635, loss_val: nan, pos_over_neg: 381.346435546875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 897.2767944335938 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9608, loss_val: nan, pos_over_neg: 258.8701171875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 1136.50927734375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 593.9765014648438 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 427.6163024902344 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9448, loss_val: nan, pos_over_neg: 628.40283203125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 592.9531860351562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 580.338134765625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 707.0685424804688 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 309.79248046875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 520.0994262695312 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 1321.1307373046875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 359.4255065917969 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 771.293212890625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 536.6649169921875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 554.0269775390625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 364.5650939941406 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 374.2427062988281 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 534.358154296875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 910.2036743164062 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 243.93710327148438 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 337.9479064941406 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 200.83497619628906 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 408.031982421875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 252.2806396484375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 607.8696899414062 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 220.00033569335938 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 255.3684539794922 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 429.1139831542969 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 224.468017578125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 529.5594482421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 224.78851318359375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 301.001220703125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 520.7054443359375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 355.5070495605469 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 891.8355102539062 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 385.8611145019531 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 210.62547302246094 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 393.4499206542969 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 387.8304138183594 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 900.2819213867188 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 720.6959228515625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 299.4462890625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 439.13812255859375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 188.36679077148438 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 267.7032165527344 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 251.90908813476562 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 230.99342346191406 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 318.05364990234375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 229.5149383544922 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 285.38763427734375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 374.97613525390625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 188.9615936279297 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 1106.4700927734375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 286.3263854980469 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 616.089599609375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 447.30133056640625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 541.9341430664062 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 529.971923828125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 493.1505432128906 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 321.2541198730469 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 460.03424072265625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 266.0008239746094 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 510.66119384765625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 258.0645446777344 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 594.8255004882812 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: 695.0076293945312 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 365.0043640136719 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8923, loss_val: nan, pos_over_neg: 1427.3646240234375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 423.2065124511719 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 219.2989501953125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 981.21044921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 380.7654724121094 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8919, loss_val: nan, pos_over_neg: 568.5880737304688 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 319.4693908691406 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 299.0747985839844 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 274.6278381347656 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 275.566162109375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 293.4501037597656 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 296.9486389160156 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 1419.8712158203125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 302.3809814453125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 320.62255859375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8898, loss_val: nan, pos_over_neg: 493.8181457519531 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8913, loss_val: nan, pos_over_neg: 227.92269897460938 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 507.92559814453125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8887, loss_val: nan, pos_over_neg: 325.7248229980469 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8906, loss_val: nan, pos_over_neg: 357.38287353515625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 681.069091796875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 371.794921875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 960.6661376953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 330.8414306640625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 666.32763671875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 475.0352478027344 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8908, loss_val: nan, pos_over_neg: 351.2890930175781 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 315.6412658691406 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8783, loss_val: nan, pos_over_neg: 793.1367797851562 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8855, loss_val: nan, pos_over_neg: 418.2935791015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 434.1011657714844 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8815, loss_val: nan, pos_over_neg: 516.7066040039062 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 564.9000854492188 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: 302.2835693359375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8799, loss_val: nan, pos_over_neg: 422.9893493652344 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 351.2707824707031 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8981, loss_val: nan, pos_over_neg: 202.97132873535156 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8838, loss_val: nan, pos_over_neg: 627.498046875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8855, loss_val: nan, pos_over_neg: 352.3435363769531 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 407.2021179199219 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 426.79010009765625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8928, loss_val: nan, pos_over_neg: 180.3863525390625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 941.682861328125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8799, loss_val: nan, pos_over_neg: 345.8048095703125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8797, loss_val: nan, pos_over_neg: 545.1105346679688 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8791, loss_val: nan, pos_over_neg: 351.453857421875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 456.2667236328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8848, loss_val: nan, pos_over_neg: 394.2069396972656 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8758, loss_val: nan, pos_over_neg: 497.9176025390625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8738, loss_val: nan, pos_over_neg: 315.7369079589844 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.873, loss_val: nan, pos_over_neg: 488.23968505859375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 342.97027587890625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8767, loss_val: nan, pos_over_neg: 345.46905517578125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 285.0691833496094 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8863, loss_val: nan, pos_over_neg: 363.2892150878906 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8748, loss_val: nan, pos_over_neg: 274.9122314453125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 458.87890625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 420.79931640625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 453.5384826660156 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8688, loss_val: nan, pos_over_neg: 330.1257019042969 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8772, loss_val: nan, pos_over_neg: 427.61505126953125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.878, loss_val: nan, pos_over_neg: 415.0859680175781 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 475.59393310546875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 624.3652954101562 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 429.2597961425781 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 329.6488342285156 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8869, loss_val: nan, pos_over_neg: 238.2509307861328 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8739, loss_val: nan, pos_over_neg: 548.131591796875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 231.61514282226562 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8811, loss_val: nan, pos_over_neg: 295.8414306640625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 279.816162109375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8825, loss_val: nan, pos_over_neg: 278.2301940917969 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8802, loss_val: nan, pos_over_neg: 289.8297119140625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8823, loss_val: nan, pos_over_neg: 396.01080322265625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 495.1772155761719 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 204.40882873535156 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 490.0487060546875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 485.6562194824219 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 314.8815002441406 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 450.3526916503906 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 340.7039489746094 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8812, loss_val: nan, pos_over_neg: 192.91583251953125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 296.31610107421875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 367.8131408691406 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 297.58990478515625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8753, loss_val: nan, pos_over_neg: 280.2462463378906 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 285.903564453125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 233.55728149414062 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 360.46087646484375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 318.4286804199219 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8707, loss_val: nan, pos_over_neg: 257.1835021972656 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 427.1397705078125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 463.3018798828125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 497.46649169921875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 263.7445068359375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 323.6800842285156 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8682, loss_val: nan, pos_over_neg: 337.4404296875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 319.6629333496094 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 335.96661376953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8877, loss_val: nan, pos_over_neg: 231.92994689941406 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 270.4831848144531 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 352.75732421875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8728, loss_val: nan, pos_over_neg: 295.5950012207031 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 170.36351013183594 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 535.2041015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 288.4883117675781 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 182.3743896484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 642.6149291992188 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.868, loss_val: nan, pos_over_neg: 185.99501037597656 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 175.13070678710938 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 428.4310302734375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: 319.0596923828125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 213.3763885498047 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 260.0855407714844 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 245.4774627685547 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 265.6065979003906 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8645, loss_val: nan, pos_over_neg: 784.564453125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 324.1209411621094 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 400.55889892578125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 1762.755126953125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 297.5491943359375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 409.0596923828125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 686.1463623046875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 503.3856201171875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 311.5482482910156 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 527.108154296875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 330.3941345214844 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 410.7255554199219 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 367.3937683105469 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 570.0186157226562 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 503.461669921875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 355.94781494140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 354.681884765625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 595.1351318359375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.873, loss_val: nan, pos_over_neg: 199.05442810058594 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 262.82855224609375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 725.8319091796875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 332.44091796875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 379.22760009765625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 591.5203247070312 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 604.9830932617188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1212.1837158203125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 362.3002014160156 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 480.5890197753906 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 414.1405334472656 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 423.5791931152344 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 454.2806091308594 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 687.822998046875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 400.9823913574219 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 460.0264892578125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 327.8464050292969 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 400.49456787109375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 449.6253967285156 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 552.1898803710938 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 692.9371337890625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1101.2042236328125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 451.90814208984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 399.3565368652344 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 440.57244873046875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 455.2120361328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 586.0719604492188 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 786.8392944335938 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 408.2918701171875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 577.222412109375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 484.48614501953125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 302.2103271484375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 432.61822509765625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 1146.10888671875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 377.51910400390625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 560.0536499023438 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 1393.7645263671875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 350.58868408203125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 334.5410461425781 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 595.2517700195312 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 351.64117431640625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 479.59332275390625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 508.0940246582031 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 546.7769165039062 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 685.9225463867188 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 775.3059692382812 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 369.6273498535156 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1475.7508544921875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 452.9361267089844 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 441.6918640136719 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 483.4727478027344 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 375.044189453125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 578.0592651367188 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 492.5455627441406 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 386.68670654296875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 779.8329467773438 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 860.7576293945312 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 995.6942749023438 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 519.6853637695312 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 917.5982666015625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 354.70074462890625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 541.5726318359375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 572.2471923828125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 1278.202392578125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 354.9791564941406 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 749.1587524414062 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 763.5342407226562 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 700.523681640625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 207.39999389648438 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 737.5433349609375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 385.1487731933594 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 468.4450988769531 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 545.5149536132812 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 474.03466796875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 258.57708740234375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 343.4541931152344 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 433.6487121582031 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 409.09344482421875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 376.6317443847656 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 412.5563659667969 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 462.0089416503906 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 245.95028686523438 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 668.6800537109375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 291.08905029296875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 411.1493835449219 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 475.1204528808594 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 466.3460998535156 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 362.31878662109375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 354.4947509765625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 459.26580810546875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 528.0426635742188 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 907.6571044921875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 598.1431884765625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 581.715087890625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 612.9237060546875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 478.8102111816406 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 589.9039306640625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 760.7168579101562 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 805.1981811523438 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 669.6482543945312 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 557.2269897460938 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 466.0688781738281 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 1279.0684814453125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 284.9652404785156 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 363.2684326171875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 458.84619140625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 332.2691345214844 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 261.1371154785156 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 543.2796020507812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 560.46923828125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 410.9308166503906 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 439.9167175292969 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 429.1470642089844 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8467, loss_val: nan, pos_over_neg: 726.2797241210938 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 554.9170532226562 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 527.9559936523438 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 590.66748046875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 643.6299438476562 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 488.77703857421875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 270.50531005859375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 519.5454711914062 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 217.72714233398438 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 347.51275634765625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 750.0604858398438 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 423.6301574707031 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 272.2481689453125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 965.3362426757812 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 215.5577850341797 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 271.2730407714844 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 1289.0401611328125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 774.3412475585938 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 229.8717041015625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 479.3948974609375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 414.2652587890625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 824.3021850585938 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 275.65771484375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 464.0544128417969 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 685.6320190429688 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 364.7319641113281 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 412.0262145996094 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 606.3860473632812 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 741.123046875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 319.65484619140625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 378.00128173828125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 356.5713195800781 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 477.1324157714844 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 594.4136352539062 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 335.91790771484375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8273, loss_val: nan, pos_over_neg: 469.435302734375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 526.4893188476562 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 386.5045166015625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 519.4515991210938 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 260.17816162109375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 403.4833679199219 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 404.8020324707031 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 298.83050537109375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 218.27069091796875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 508.4967956542969 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 442.03131103515625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 318.2485656738281 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 641.5552368164062 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 403.9620056152344 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 635.455078125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 306.8438720703125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 463.8038635253906 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 582.7650756835938 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 412.0228576660156 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 791.0733032226562 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 463.3399353027344 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 280.4111633300781 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 517.6639404296875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 441.952880859375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 310.62396240234375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 307.505126953125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 596.3780517578125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 315.4479675292969 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 427.81170654296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 721.0731811523438 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8211, loss_val: nan, pos_over_neg: 629.1632080078125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 291.3645324707031 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 397.23931884765625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 581.0455322265625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 433.16998291015625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 438.0317687988281 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 502.1142883300781 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 788.3214111328125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 657.2462768554688 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 588.4503173828125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 670.8612060546875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 446.9910888671875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 830.8610229492188 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 1103.7906494140625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1028.9052734375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 895.4664916992188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 817.0609130859375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 1012.0709838867188 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 752.0635986328125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 760.9461059570312 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 853.9740600585938 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 594.8385009765625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 686.7787475585938 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 445.7933654785156 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1011.6453857421875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 2564.872802734375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 689.3756713867188 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 677.391357421875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1622.1322021484375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 763.5117797851562 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 326.59814453125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8221, loss_val: nan, pos_over_neg: 1557.121826171875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 1203.39794921875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 527.8980102539062 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 486.19219970703125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 566.1845703125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1307.1092529296875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 400.8914489746094 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8188, loss_val: nan, pos_over_neg: 664.4779663085938 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 539.2205810546875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8222, loss_val: nan, pos_over_neg: 629.8131713867188 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8248, loss_val: nan, pos_over_neg: 689.234619140625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 372.8727722167969 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8306, loss_val: nan, pos_over_neg: 523.424560546875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 379.9832763671875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 610.31787109375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 389.7930908203125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 423.3454284667969 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 396.8931884765625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 498.8281555175781 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 736.9615478515625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 864.4725341796875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 379.0234375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 662.6111450195312 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 408.01763916015625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 241.17196655273438 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 461.343505859375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 405.9095458984375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 567.2240600585938 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 374.5350646972656 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 724.7662353515625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 1000.7446899414062 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 428.331787109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 826.1251220703125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8256, loss_val: nan, pos_over_neg: 1435.029541015625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 772.9208374023438 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 419.9820861816406 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 512.2479248046875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 685.5014038085938 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 405.7402038574219 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 957.3154296875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 1564.1080322265625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 522.9310913085938 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 512.7092895507812 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 1323.224365234375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 783.0018310546875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8241, loss_val: nan, pos_over_neg: 656.4979858398438 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 432.6850891113281 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 501.5152893066406 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 607.3228759765625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 609.7243041992188 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 553.445556640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1720.2119140625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 551.6347045898438 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 422.83453369140625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8194, loss_val: nan, pos_over_neg: 921.6117553710938 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 443.7527160644531 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 851.2319946289062 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 355.140625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 421.8660583496094 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 583.7886352539062 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 514.9639892578125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 608.9036254882812 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1518.79443359375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 550.4590454101562 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8217, loss_val: nan, pos_over_neg: 412.74066162109375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 766.8250122070312 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 544.5199584960938 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 294.1707763671875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.826, loss_val: nan, pos_over_neg: 769.6375732421875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 500.17877197265625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 360.9644775390625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1083.346435546875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8159, loss_val: nan, pos_over_neg: 767.8530883789062 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 795.8289794921875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 329.6371765136719 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 653.72509765625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 457.7151184082031 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8205, loss_val: nan, pos_over_neg: 338.3625793457031 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 721.4608764648438 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 445.2054748535156 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8231, loss_val: nan, pos_over_neg: 299.68359375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 403.680419921875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 428.34930419921875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8252, loss_val: nan, pos_over_neg: 310.9274597167969 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 393.5570983886719 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 299.6945495605469 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 278.5340576171875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 413.827392578125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8118, loss_val: nan, pos_over_neg: 426.9234313964844 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 749.1969604492188 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8199, loss_val: nan, pos_over_neg: 246.59854125976562 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 507.73626708984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1029.3585205078125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 462.5622253417969 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 492.7926940917969 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 529.7352905273438 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 701.8826904296875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 420.0191345214844 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 732.1431884765625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 380.6136474609375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 698.95703125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 593.4246215820312 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 654.4664916992188 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 625.2017211914062 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 539.3986206054688 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 624.0978393554688 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8171, loss_val: nan, pos_over_neg: 554.0811157226562 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 529.7869262695312 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.819, loss_val: nan, pos_over_neg: 506.1485595703125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 556.2318115234375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 488.1476135253906 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 594.5242919921875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 663.83642578125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 790.2598876953125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 458.42999267578125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 844.9437866210938 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 668.62255859375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 561.488525390625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 517.4143676757812 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 656.6162109375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1193.3607177734375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 926.0494995117188 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 574.7750854492188 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 570.8011474609375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1010.6051025390625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8233, loss_val: nan, pos_over_neg: 1140.2890625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 561.174072265625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 1320.9908447265625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 861.7894287109375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 668.742919921875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1082.064697265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 571.1317138671875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 660.3724365234375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 412.2113952636719 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8236, loss_val: nan, pos_over_neg: 628.3108520507812 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 481.5339660644531 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 447.5116271972656 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 710.0650024414062 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 738.68359375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8248, loss_val: nan, pos_over_neg: 530.9866943359375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 561.9724731445312 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 1074.3192138671875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 942.4482421875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 543.7356567382812 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 593.4419555664062 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 607.5390014648438 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 791.7787475585938 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 463.0740966796875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 876.0960083007812 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 595.8547973632812 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 366.8993835449219 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 516.6937255859375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 822.0288696289062 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 430.744140625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 590.7935180664062 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 419.6275634765625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1561.085205078125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 526.8374633789062 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8233, loss_val: nan, pos_over_neg: 329.1357727050781 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 769.378662109375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 358.9131164550781 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 309.7991943359375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8212, loss_val: nan, pos_over_neg: 496.2594909667969 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 588.8369750976562 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 581.5484619140625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 621.6235961914062 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 662.4765014648438 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 737.6992797851562 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 364.9369812011719 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 737.138427734375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 919.7787475585938 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 884.111328125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1807.579833984375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 824.24951171875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 1459.3251953125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 1588.5047607421875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1165.0386962890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 1156.145263671875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 717.0997314453125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1161.654052734375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1346.2603759765625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 616.0308837890625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 900.9297485351562 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 1017.314453125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8221, loss_val: nan, pos_over_neg: 521.3380737304688 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 555.2377319335938 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 838.7802734375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 942.4846801757812 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 1518.5374755859375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [21:13<106119:04:49, 1273.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 609.788330078125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 509.2325439453125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 571.3419799804688 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 713.7899780273438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8198, loss_val: nan, pos_over_neg: 610.425537109375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 707.8052978515625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1390.362548828125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 631.4072265625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 747.34375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 819.7352294921875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 604.325927734375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 691.0167846679688 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 600.0771484375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 653.1404418945312 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 1014.4102172851562 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 535.5944213867188 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 745.2576293945312 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8181, loss_val: nan, pos_over_neg: 1722.126220703125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 969.6682739257812 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8153, loss_val: nan, pos_over_neg: 463.71923828125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 742.2012329101562 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 666.6190795898438 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8112, loss_val: nan, pos_over_neg: 481.2845153808594 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 644.1983032226562 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 511.98095703125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 685.322265625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 576.4588623046875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 694.2780151367188 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 638.6641235351562 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 568.1671752929688 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 1084.57080078125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 498.6284484863281 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: 827.5286254882812 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 623.6656494140625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 422.8110046386719 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 666.0903930664062 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8171, loss_val: nan, pos_over_neg: 562.9207153320312 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 670.9736328125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 943.8040161132812 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 467.1346435546875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 494.99200439453125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 907.58837890625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 562.5975952148438 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 531.5568237304688 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 542.21142578125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 769.6659545898438 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 432.8582763671875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 690.6578979492188 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 878.135009765625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 940.4818725585938 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8126, loss_val: nan, pos_over_neg: 774.1168212890625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8114, loss_val: nan, pos_over_neg: 826.3255615234375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 680.29150390625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 885.7904052734375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 825.802001953125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 708.6417236328125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 479.94775390625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1026.6038818359375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 990.5628051757812 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1010.064697265625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 576.6776733398438 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 851.3173217773438 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 1029.97216796875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1971.18896484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 863.3262329101562 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1443.4677734375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8145, loss_val: nan, pos_over_neg: 1957.4205322265625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 885.399658203125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1134.18994140625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 1234.9578857421875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 546.58935546875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 675.9906005859375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 762.2147827148438 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1147.9114990234375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 907.6568603515625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 820.5736694335938 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 618.7177124023438 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 476.9209289550781 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 413.63153076171875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 582.0996704101562 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 401.52685546875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 584.7890625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 423.71697998046875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 431.01409912109375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1144.64306640625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 506.0931396484375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 439.46258544921875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 785.8652954101562 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 1242.9561767578125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 463.10003662109375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 454.2681579589844 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8157, loss_val: nan, pos_over_neg: 940.432373046875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 627.5760498046875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.815, loss_val: nan, pos_over_neg: 308.8321838378906 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1195.6778564453125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 4581.9716796875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 596.410400390625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 584.9486694335938 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8114, loss_val: nan, pos_over_neg: 707.2216186523438 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 661.6134643554688 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 820.24560546875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 434.7679443359375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 527.6776123046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 485.6877136230469 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 747.509521484375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 728.3947143554688 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 509.7876281738281 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 735.481689453125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 749.9902954101562 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 898.0697631835938 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 641.7564086914062 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 891.26123046875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 552.63330078125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 484.2884826660156 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 809.3671875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 573.3519287109375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 554.6560668945312 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 321.7773742675781 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 702.1954956054688 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 785.2557983398438 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 496.39031982421875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 451.9810791015625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1602.530517578125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 729.9273681640625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 431.6014404296875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 513.8038330078125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 714.591796875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 771.0302734375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 759.6730346679688 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2407.098876953125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 446.28466796875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 552.1209106445312 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1319.862060546875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 597.5422973632812 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 966.4207153320312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 514.8787841796875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 854.1151123046875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 504.4775390625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 736.4938354492188 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 584.9454956054688 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 626.4812622070312 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 1488.58203125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 572.2113647460938 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 744.0607299804688 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 882.5347290039062 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 725.0050659179688 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 377.5912170410156 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8074, loss_val: nan, pos_over_neg: 1496.46337890625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1271.29931640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 1908.870849609375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 398.51385498046875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 949.2787475585938 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 599.6538696289062 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 529.1272583007812 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 925.6937255859375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 706.4840698242188 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 915.3634643554688 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 931.9791870117188 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1119.42236328125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 894.66259765625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 661.1975708007812 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1121.930908203125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 551.2096557617188 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 443.03887939453125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1088.3707275390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 661.6842651367188 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 411.745361328125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 746.3009033203125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 554.677734375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8104, loss_val: nan, pos_over_neg: 487.26507568359375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 540.498779296875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 659.4892578125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 488.52984619140625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 688.9338989257812 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 723.1837768554688 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 481.41888427734375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1123.5809326171875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 629.9379272460938 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 778.5667114257812 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 529.8375244140625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 572.974365234375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 511.6713562011719 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 806.3685302734375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 394.4627380371094 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 619.4620361328125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 642.3604736328125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1745.692138671875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 657.024169921875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 467.34368896484375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 555.5103149414062 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 1220.5621337890625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 495.8842468261719 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1107.5015869140625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 511.59814453125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 953.2023315429688 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 605.1292724609375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 453.5714111328125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 724.1434936523438 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 471.9178466796875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 810.8889770507812 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 475.1126403808594 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1113.6470947265625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 454.8326110839844 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 729.7368774414062 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 689.522705078125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 526.4653930664062 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 964.810302734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 733.65478515625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 544.0858764648438 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 538.47900390625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1028.1812744140625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 528.2865600585938 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 500.15118408203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1357.3035888671875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1285.616455078125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 563.271728515625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 953.0883178710938 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1706.328857421875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 555.3450317382812 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 482.1121826171875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 662.2210083007812 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 1269.681640625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 834.6310424804688 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 583.8138427734375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 766.8526611328125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1329.427734375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 379.1914978027344 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 407.4989929199219 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 775.1843872070312 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 564.5889892578125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 437.3827209472656 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 694.1936645507812 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 589.5076293945312 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 350.09210205078125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 650.8584594726562 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 809.1246337890625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 364.93157958984375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 476.8314208984375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1191.5677490234375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 462.663330078125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 438.79052734375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 422.326171875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 976.7871704101562 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 630.05419921875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 422.9144592285156 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 1107.265625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 685.553466796875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 2037.153564453125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1369.7939453125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 441.77630615234375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 621.0774536132812 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1027.6153564453125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1377.176513671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 716.849365234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 709.7009887695312 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 991.427490234375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1140.4207763671875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1030.417724609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 708.7573852539062 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 388.0227966308594 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 536.25927734375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1380.181396484375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 345.29119873046875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 589.754150390625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1148.2320556640625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1240.2593994140625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1203.75 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 608.3988647460938 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 464.4737548828125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 414.74261474609375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 539.1277465820312 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 633.2932739257812 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 517.162353515625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1399.8665771484375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 915.3469848632812 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 643.8460083007812 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1072.1856689453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 352.9810485839844 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 543.9840698242188 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 669.2637329101562 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 515.2654418945312 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1003.751708984375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 819.2607421875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 581.6222534179688 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 754.38916015625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 626.8073120117188 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 848.4281616210938 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 408.3953857421875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 459.1434631347656 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 353.16937255859375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 706.7361450195312 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 441.9649658203125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 399.5171813964844 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 702.255859375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 428.00677490234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 624.9161987304688 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 2035.51513671875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 858.6246337890625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 814.7723999023438 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1299.0052490234375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 1411.606689453125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 791.1026000976562 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1112.9937744140625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 964.1653442382812 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 983.2815551757812 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1271.3326416015625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1232.736572265625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 2522.7373046875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 839.71484375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1240.4862060546875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 850.1572265625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 3219.88427734375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 820.1143798828125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 663.8670654296875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2460.914794921875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1438.55419921875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 797.7960815429688 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1900.38720703125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 847.8467407226562 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 1281.2822265625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1304.142822265625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 586.3984985351562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 525.8903198242188 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 5131.72265625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 725.3682861328125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 757.4830322265625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 406.6026916503906 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 692.778564453125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 752.1546630859375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 831.1854858398438 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1360.3663330078125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1838.1871337890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 827.9458618164062 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 600.8353271484375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 580.6343383789062 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 883.4246826171875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 552.4899291992188 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 586.94677734375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 791.4200439453125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1339.3870849609375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 453.0027160644531 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 7469.38818359375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 778.3904418945312 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 810.344970703125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 476.1302490234375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 589.365234375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 1041.2965087890625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 754.71826171875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 571.1768798828125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 682.581787109375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 957.7449951171875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 552.4754028320312 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 494.1650390625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 653.9799194335938 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 1162.9896240234375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 684.8333129882812 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 936.2760620117188 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 517.8302612304688 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 953.0446166992188 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 837.2146606445312 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1688.33740234375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 1053.4822998046875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1392.42041015625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1006.1851806640625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 1310.73486328125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1173.567138671875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 502.34979248046875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 918.6875610351562 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 702.9430541992188 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 507.5233154296875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 647.7637329101562 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 937.2047729492188 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1223.985107421875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 461.7417297363281 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 537.5547485351562 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 2691.161376953125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 791.2208862304688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 854.632568359375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 861.8467407226562 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 768.6116333007812 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 2291.241943359375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 908.302490234375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 965.912353515625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 946.7808227539062 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1576.6702880859375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 774.6422729492188 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 393.9298095703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 717.65576171875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 832.7353515625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 695.0101928710938 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1697.939453125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 449.69012451171875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 849.6024780273438 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 531.2716064453125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 513.5401611328125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 704.326171875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 835.9163818359375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 784.92724609375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 885.9083251953125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1322.1885986328125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1303.4716796875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 562.3860473632812 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 350.2685241699219 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 2611.54638671875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 836.3932495117188 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 675.9807739257812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1222.7318115234375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 576.6085815429688 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 789.4427490234375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 895.451416015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 432.7040710449219 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 565.373291015625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 628.244873046875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 482.5628662109375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 711.507080078125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1030.456298828125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1761.7659912109375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 376.78656005859375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 419.1022033691406 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 496.47088623046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1935.79443359375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 650.1573486328125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 490.7609558105469 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 975.90380859375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 763.24658203125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 780.9609375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 459.1907653808594 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 468.73114013671875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1064.760986328125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 647.1942749023438 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 591.1923217773438 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 751.3092651367188 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 974.28173828125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 688.9762573242188 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 751.4356079101562 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 711.6565551757812 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 844.9815673828125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 600.30419921875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 2479.969970703125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1119.589111328125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 842.3906860351562 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 945.3848266601562 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 823.5054931640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 439.2279052734375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 709.7883911132812 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 566.8765869140625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 364.74359130859375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 481.0936279296875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 781.9390869140625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 483.9427185058594 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 458.0205993652344 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 855.1494750976562 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 412.72564697265625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 588.1674194335938 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 508.3714294433594 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 915.8094482421875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 450.5770568847656 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 485.8924255371094 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 565.0718383789062 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1076.81982421875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 795.916259765625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 538.5718383789062 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 562.3417358398438 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 918.419677734375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 922.339599609375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 663.3353881835938 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 567.4327392578125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 855.4146118164062 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 761.3186645507812 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 628.0177612304688 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 524.0032958984375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 737.82861328125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 727.2979736328125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 514.7799682617188 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 616.5540771484375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 475.2095031738281 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 574.3087768554688 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 384.5555725097656 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1054.5135498046875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 617.9312744140625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1116.3341064453125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 908.8895263671875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 399.04876708984375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 575.3433227539062 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 547.5440673828125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 642.891357421875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 381.09844970703125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 534.5838012695312 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 641.5181884765625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1046.93701171875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 659.5950317382812 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 652.0220336914062 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 651.2857666015625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 736.7905883789062 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 556.5177001953125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 525.8138427734375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1615.0989990234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 355.8753356933594 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 799.5130615234375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 538.9649047851562 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 853.4640502929688 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 904.007080078125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 452.8295593261719 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 797.6915283203125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 715.9428100585938 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 950.9528198242188 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 542.99365234375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 607.363525390625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 411.1937255859375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 644.01611328125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1132.204833984375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1238.2603759765625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 673.9238891601562 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 476.177978515625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 988.4356689453125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 714.416748046875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 545.6596069335938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 233.50277709960938 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 919.6904296875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 721.7051391601562 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1191.03125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 832.886474609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 826.900146484375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 747.0029296875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1464.94921875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 916.5631713867188 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 769.144287109375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 2048.2392578125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 850.9757080078125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 472.21405029296875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 5045.017578125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 608.2400512695312 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1123.7274169921875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 918.072998046875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 952.9575805664062 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 755.6011352539062 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 867.091064453125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1464.201904296875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 814.6896362304688 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1119.98974609375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 498.4050598144531 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 918.123779296875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1824.794921875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 419.4953308105469 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 468.1632080078125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 930.8477783203125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 554.0404052734375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1125.630859375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 669.3864135742188 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 524.4058837890625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 981.7482299804688 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1140.761962890625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 509.9118957519531 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 567.1012573242188 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 967.0340576171875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1221.97705078125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1382.273681640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 489.5352478027344 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 864.6249389648438 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 5173.6708984375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 1424.85400390625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1628.54736328125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 662.173095703125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 966.2130126953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 568.497314453125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 4394.38427734375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1579.305908203125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 979.0402221679688 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1042.0577392578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 634.2376098632812 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1314.979736328125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 680.8152465820312 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 687.4452514648438 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 636.9102172851562 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 591.02783203125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 2295.084228515625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1565.9630126953125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 633.1351318359375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1795.688720703125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 2536.34521484375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 729.6556396484375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 485.41571044921875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1011.8912353515625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1977.854248046875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 713.9539794921875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 769.4133911132812 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 713.9878540039062 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 2963.69580078125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2065.366943359375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1191.22998046875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1526.9940185546875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 2038.7755126953125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 2456.77294921875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1023.7263793945312 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 783.47216796875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 787.4248046875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 828.8035278320312 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 446.13055419921875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1036.0113525390625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 447.2882080078125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 464.4985046386719 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 399.6627197265625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 846.5247802734375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1553.1514892578125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1043.149658203125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 924.8990478515625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 801.9793701171875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 993.3822021484375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 2790.947021484375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1209.208251953125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 940.8840942382812 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1567.3616943359375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1248.099853515625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 839.3660278320312 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 2580.269775390625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1370.547119140625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1107.3336181640625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1725.7354736328125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 758.4631958007812 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 522.9207153320312 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 541.68505859375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 607.6920166015625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 538.9068603515625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 474.7742004394531 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1287.1876220703125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 475.5709533691406 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1319.4661865234375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 821.7785034179688 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1006.2268676757812 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 567.4542846679688 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 728.931640625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1196.9398193359375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 683.0709838867188 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 468.2001953125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 544.9006958007812 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 1415.428466796875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1068.4754638671875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 577.3253173828125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1298.4959716796875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 596.0494995117188 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 805.581787109375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 949.6702880859375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 489.67626953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 633.4331665039062 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 655.5736694335938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 794.2447509765625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 873.3569946289062 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 691.7037963867188 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 934.403564453125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 475.15869140625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 2724.187744140625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1442.178955078125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1197.5582275390625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1074.17138671875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1508.63671875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 835.5635375976562 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 591.2799072265625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 791.0457153320312 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 868.0714111328125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 949.0062866210938 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1208.9310302734375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 718.233154296875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 653.6909790039062 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 671.6427001953125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 780.1256713867188 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 668.460693359375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1510.4005126953125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 714.033203125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1105.24462890625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 3927.43408203125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 746.927001953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 937.7252807617188 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1695.0382080078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1270.69287109375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 569.8968505859375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1372.7442626953125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 791.3941040039062 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1509.4503173828125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 829.218017578125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1506.9580078125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1965.43115234375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1013.5216064453125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1843.3212890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 1186.5989990234375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 863.2474365234375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 804.469482421875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1836.9237060546875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 582.4686279296875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 924.47900390625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 783.1080322265625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 794.6357421875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1150.5272216796875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1365.043701171875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1038.77783203125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 934.3685302734375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [42:09<105272:56:31, 1263.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 811.5289916992188 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1319.1134033203125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1285.5338134765625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1342.169677734375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 590.3429565429688 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1870.6231689453125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 2697.01806640625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1080.3848876953125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1383.94140625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 787.5067749023438 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 4712.591796875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1092.5882568359375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1211.0230712890625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 506.8359069824219 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1085.86474609375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1895.6241455078125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1662.430908203125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 866.0725708007812 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 941.968994140625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1251.0587158203125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1523.4246826171875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1608.8037109375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 661.2158813476562 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1006.6263427734375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 701.7608642578125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 432.5838317871094 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 470.2866516113281 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1315.8472900390625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1175.60302734375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 619.05224609375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1256.8115234375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1780.4666748046875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 647.605224609375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 617.0983276367188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 563.6981811523438 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 689.55615234375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 809.8182373046875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 493.247314453125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1206.285888671875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 740.568115234375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1347.3675537109375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1269.5350341796875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 654.7665405273438 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 645.41943359375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1310.4649658203125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 552.1231689453125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2775.614990234375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 469.70977783203125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1230.452880859375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1208.31982421875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 861.2926025390625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 544.105224609375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 560.1873779296875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1179.2291259765625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 580.5245971679688 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2220.66552734375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 598.4481811523438 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1368.1766357421875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1198.6201171875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 675.9517211914062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 756.2680053710938 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1129.9287109375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 443.25250244140625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 551.9642333984375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 811.1227416992188 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 431.22393798828125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 521.6239624023438 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 508.14862060546875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1541.8717041015625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1126.813720703125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 552.2098388671875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 534.930908203125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 800.2608642578125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 724.613037109375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 687.4182739257812 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 597.4465942382812 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 533.8441162109375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 794.6596069335938 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 598.93212890625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 542.1268310546875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1760.49169921875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1698.8759765625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1306.9205322265625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 647.3949584960938 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 639.3165893554688 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 654.6743774414062 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 2102.08349609375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 763.9010620117188 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 690.3474731445312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 573.317138671875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 926.2449340820312 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 494.3186950683594 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1569.3162841796875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 662.79345703125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 803.0112915039062 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 4352.79150390625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1055.9896240234375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 613.782470703125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 514.775634765625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 994.7977294921875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 739.5504760742188 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 551.4151000976562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 791.031005859375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 897.0352783203125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1129.2357177734375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1010.0 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 956.6363525390625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 564.5003662109375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 500.13134765625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 762.6665649414062 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 682.622802734375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 456.1044616699219 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1079.652099609375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 551.5320434570312 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1189.60791015625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 22821.869140625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 816.8944702148438 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1367.728271484375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 907.3109130859375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1143.4127197265625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 563.90087890625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 5240.79443359375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 680.2759399414062 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 939.8267822265625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2534.4853515625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: -5357.8203125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1147.2578125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 857.8399047851562 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 817.7559204101562 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 966.7450561523438 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 646.5316772460938 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 655.0104370117188 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 394.4474182128906 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 835.2550659179688 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 864.421142578125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 913.7135009765625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 4155.345703125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 951.167236328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 704.9521484375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 892.8331298828125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 965.2677001953125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 657.1910400390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1116.6806640625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1008.0513305664062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 497.802978515625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 565.9790649414062 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 872.2449340820312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 695.1626586914062 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 978.773193359375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 541.96728515625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 801.1641845703125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1255.69873046875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1557.6248779296875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 523.3513793945312 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 516.354736328125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 738.3433837890625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 931.9771118164062 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1067.238037109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 902.0706176757812 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 648.2310180664062 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 997.1251831054688 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1602.2322998046875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 683.5198364257812 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1049.7509765625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1004.8988037109375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 667.6837158203125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 509.6638488769531 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 904.2144775390625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1330.4989013671875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 586.113525390625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 973.5732421875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 481.71234130859375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 790.5338745117188 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1947.63671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 774.9302978515625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1182.5731201171875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 721.0890502929688 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 6367.490234375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 629.0234375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1038.5675048828125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1060.2059326171875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1179.5362548828125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 497.626708984375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 532.6080322265625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 538.45166015625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 613.1852416992188 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 493.6982421875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 847.0479736328125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 900.7723999023438 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 720.294189453125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 489.608154296875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 790.576416015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 524.88427734375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 676.9751586914062 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1295.5517578125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 648.3065185546875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 926.8937377929688 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 720.6679077148438 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1132.1502685546875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1268.9202880859375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 507.5629577636719 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 751.5645751953125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 545.3063354492188 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2016.0457763671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1493.79296875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 585.0651245117188 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 512.1046752929688 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 651.8179931640625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1273.2501220703125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1870.615234375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 820.592529296875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 432.7087097167969 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 846.0430297851562 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 667.9352416992188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1230.3138427734375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 680.8118286132812 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 744.5721435546875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 727.456298828125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1417.8201904296875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1259.663818359375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 676.3740234375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 626.810546875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1077.0343017578125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 509.69781494140625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1514.5247802734375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1092.2203369140625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1002.02880859375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1875.2606201171875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 1116.17578125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1277.6519775390625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2420.080078125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1019.64990234375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 730.1832275390625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1314.8836669921875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 962.7462158203125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 819.47314453125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1133.552490234375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1043.2672119140625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1055.765625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 834.51611328125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1148.5093994140625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 564.0223999023438 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 816.5730590820312 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 503.17913818359375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 603.5535278320312 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 557.4090576171875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 648.9515380859375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1795.637451171875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1173.027587890625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1344.8118896484375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 849.9795532226562 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 763.5664672851562 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 607.9702758789062 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 550.6439819335938 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 536.9638061523438 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 759.2957153320312 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1145.3463134765625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1142.0255126953125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 675.8765869140625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 888.8177490234375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 808.103271484375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 911.0999145507812 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 852.4762573242188 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 663.416259765625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 404.98077392578125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 736.6746215820312 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1018.3807983398438 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1228.84423828125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 654.2127075195312 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 543.1530151367188 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 613.054931640625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 904.3578491210938 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1010.4837036132812 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 849.9281005859375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1195.333251953125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1148.4874267578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 566.8056640625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1688.1646728515625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1375.2349853515625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 711.6692504882812 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 725.5162963867188 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 653.1211547851562 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1195.8662109375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1203.5809326171875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 702.734375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 600.42236328125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1139.59033203125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1017.1299438476562 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 427.1745910644531 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 774.4566040039062 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1036.779052734375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 753.01416015625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 724.7022705078125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 523.6077880859375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1283.5201416015625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 838.8558349609375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1265.079833984375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1581.742919921875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1527.370361328125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 571.0125732421875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1795.186279296875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 786.2492065429688 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 721.20361328125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 562.314453125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1122.281982421875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1189.778076171875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 695.6051635742188 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 704.2625122070312 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 828.4472045898438 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 591.3353271484375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 829.8283081054688 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1044.738037109375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 601.1853637695312 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 611.4470825195312 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 579.0167236328125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1118.459716796875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 862.0068359375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 592.2293090820312 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 743.0355834960938 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 559.7942504882812 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 621.375244140625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 523.5669555664062 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 552.2941284179688 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 574.7185668945312 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 471.9261169433594 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 849.7578735351562 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1013.2236938476562 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 918.5974731445312 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 518.5700073242188 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 572.5247192382812 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1086.845703125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 914.308837890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 531.730712890625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 677.0490112304688 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 821.7542724609375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 745.277099609375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 973.478271484375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 548.2317504882812 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 560.5201416015625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1767.767333984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 583.4246826171875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 851.9226684570312 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 396.4991760253906 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 571.5182495117188 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 955.52490234375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 770.4091186523438 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 567.2261352539062 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 788.5796508789062 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1355.216796875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 684.1162109375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 936.6747436523438 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 721.0297241210938 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 805.4459838867188 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 611.3428955078125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 920.058837890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1330.7489013671875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 687.191162109375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 784.1804809570312 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 902.1431274414062 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 379.2627258300781 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1839.4296875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1215.5367431640625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 459.1734619140625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 737.475830078125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 553.65283203125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1194.33447265625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 776.1730346679688 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 755.6879272460938 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1672.404541015625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1548.4111328125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1308.909423828125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 893.255859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 639.783447265625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1136.7210693359375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 647.0089721679688 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1340.2301025390625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 830.7685546875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 628.2515258789062 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1602.4356689453125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 3236.17529296875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1092.962158203125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1107.7911376953125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 652.4555053710938 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 726.0314331054688 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1258.1976318359375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 667.7543334960938 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 552.2780151367188 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 774.8259887695312 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1208.0313720703125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 2315.0810546875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 2360.331298828125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1576.2947998046875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1005.912109375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1194.46240234375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2299.115966796875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1263.3189697265625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 721.798583984375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 764.9136352539062 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1210.7235107421875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 866.011474609375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1081.819580078125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 841.126953125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1044.9757080078125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 2775.81005859375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 921.604248046875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 612.1952514648438 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 688.7406005859375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1098.52099609375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 519.8298950195312 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1670.468017578125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 809.538330078125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1031.7698974609375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 977.446044921875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 674.2670288085938 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 710.6322021484375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 505.0550231933594 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 705.3370361328125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1413.1495361328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1326.931884765625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1085.4085693359375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1364.75 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 640.1588134765625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1593.16162109375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 773.4702758789062 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1067.29052734375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 700.2268676757812 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 906.9965209960938 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1133.5498046875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 933.9119262695312 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 503.75079345703125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1064.2098388671875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1234.4068603515625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1424.7979736328125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 912.7285766601562 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 759.6300659179688 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 761.9395141601562 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 531.29931640625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1714.6240234375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 833.0526123046875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 718.5899047851562 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 641.8343505859375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 618.098876953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 612.6192626953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 426.59033203125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 523.4669799804688 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1059.1622314453125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 754.8872680664062 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 507.3439636230469 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 487.99560546875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 986.7271118164062 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1748.6910400390625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 717.3541870117188 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1320.48095703125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 771.6140747070312 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 874.2329711914062 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 528.41845703125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 821.9847412109375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 674.664306640625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 518.6862182617188 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1592.0367431640625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1076.26318359375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 854.2095947265625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 453.3673400878906 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 935.71435546875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 956.6838989257812 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1148.08251953125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 965.1085815429688 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1773.2020263671875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 833.417724609375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 834.1328735351562 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 2268.813232421875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 797.016845703125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 516.9354248046875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 506.3650817871094 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 640.7673950195312 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 842.2896728515625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1020.0264282226562 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 633.130126953125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 566.1823120117188 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1074.1810302734375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 849.501953125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 797.0001831054688 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 752.3306274414062 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 400.04608154296875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 556.3753051757812 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 418.1819763183594 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 781.6906127929688 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1463.6900634765625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 915.2453002929688 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 593.3728637695312 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 629.5087890625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1645.015380859375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 936.0004272460938 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 852.4406127929688 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1075.6724853515625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 981.3570556640625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2080.522216796875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1487.3594970703125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1223.5352783203125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 559.2947387695312 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1045.810791015625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 630.7171630859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1089.697998046875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1048.7440185546875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 747.8216552734375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1030.7064208984375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1121.104736328125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 822.3804321289062 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1715.178466796875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 809.8180541992188 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 602.94384765625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1009.0614013671875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 469.3238830566406 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 687.3101806640625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 639.9110107421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 961.9983520507812 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1268.5595703125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1334.0478515625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 544.9404296875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1388.4168701171875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1118.3382568359375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 676.9843139648438 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 699.4033203125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 3247.06640625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 828.5148315429688 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 742.4534912109375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1242.222900390625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 5477.650390625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1342.670654296875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1045.5123291015625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1339.521484375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 2962.775634765625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 792.907958984375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 546.786376953125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1502.67431640625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1404.0142822265625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1278.6580810546875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 599.16943359375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 828.7637939453125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1424.1268310546875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1342.7376708984375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 2262.539794921875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1391.0948486328125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 954.5677490234375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: -8072.74365234375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 638.1166381835938 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2837.23388671875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1068.719482421875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 2013.33447265625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1836.317138671875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1260.5770263671875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 690.76025390625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 2460.011474609375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 732.3938598632812 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1019.7047729492188 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1265.986083984375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1550.46533203125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1850.4532470703125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 892.1719360351562 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 677.2322387695312 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 2508.126708984375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1088.6805419921875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 752.0841064453125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1410.8577880859375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 723.06884765625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1845.125244140625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 628.0984497070312 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 950.1923217773438 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1491.59033203125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1809.120849609375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1333.300048828125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1305.474609375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 890.5625610351562 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 617.2571411132812 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1060.1488037109375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 507.1881408691406 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 815.1095581054688 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1869.413330078125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 712.7205200195312 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1068.90380859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1313.4490966796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1156.4083251953125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 919.6776123046875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1251.3480224609375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1045.1947021484375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 910.010986328125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1939.5821533203125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1317.4232177734375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 876.8552856445312 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1699.3590087890625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1047.6307373046875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 2832.053955078125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 765.1513671875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 942.474853515625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1190.5091552734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 678.5650634765625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1099.3818359375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 803.06005859375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1999.197021484375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 787.9405517578125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 2039.007568359375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 830.5236206054688 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 668.3228759765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 823.9531860351562 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 487.9786376953125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 608.4600830078125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 896.698486328125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 873.3352661132812 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 3748.11083984375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 843.3680419921875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1746.7777099609375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 5939.1396484375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2373.116943359375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 597.5460815429688 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 476.99847412109375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 621.0660400390625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1259.8013916015625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 551.1543579101562 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 538.862060546875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 872.1337280273438 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 965.5403442382812 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 546.5408935546875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 587.0694580078125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1048.5621337890625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1458.0208740234375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 913.6638793945312 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 773.7337036132812 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2332.80126953125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 904.7278442382812 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 3901.305419921875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 801.1605224609375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 441.4413757324219 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1221.2781982421875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 10553.66796875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1433.5953369140625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 763.8890991210938 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1862.6978759765625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 2847.5009765625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 2739.54931640625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 982.8090209960938 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 446.4375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 960.0236206054688 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1118.9669189453125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 870.3377685546875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 940.76171875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 851.2229614257812 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1016.5455932617188 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1613.9071044921875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1207.5826416015625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 633.5304565429688 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 496.748046875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 913.1279907226562 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1657.9134521484375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 781.427978515625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1333.699951171875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1407.5196533203125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 904.0305786132812 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1488.3359375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1182.527099609375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1028.4659423828125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 733.3531494140625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 2438.6552734375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 981.9487915039062 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 743.1066284179688 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1022.3301391601562 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 13400.541015625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 642.0006713867188 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 2206.434326171875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1756.2147216796875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1214.226806640625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1213.5948486328125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 658.8509521484375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 762.5130615234375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1145.8924560546875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1441.4114990234375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1350.1402587890625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 863.8484497070312 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 761.3350219726562 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1615.3057861328125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 559.0734252929688 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1185.3199462890625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 552.7955322265625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 586.0205078125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1106.15380859375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 590.6190185546875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 752.07421875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 626.1209106445312 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1254.66259765625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 730.9546508789062 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 624.0726318359375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 889.3692626953125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 564.5620727539062 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 361.1278076171875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1083.3765869140625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 871.2781372070312 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:03:20<105580:15:23, 1266.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 875.8759765625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1111.1373291015625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 902.0773315429688 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 892.1907958984375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 769.300048828125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 616.8064575195312 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 865.061279296875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1197.7471923828125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 991.906494140625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 434.4156799316406 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 875.6856689453125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 864.9790649414062 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 554.1574096679688 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 684.4320678710938 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 897.5399169921875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1312.587646484375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 709.6805419921875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 377.3351745605469 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 755.4008178710938 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 810.8510131835938 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1087.3525390625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 659.7057495117188 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 667.4532470703125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1166.27783203125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1270.7291259765625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 801.8967895507812 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 907.20556640625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 955.8851928710938 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1228.214111328125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 798.53125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 911.3174438476562 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1505.8055419921875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 909.4244384765625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1721.395751953125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1036.2125244140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1824.3837890625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 901.4558715820312 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1318.9368896484375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 960.0773315429688 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 692.4721069335938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 2976.954345703125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1122.09912109375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1986.068359375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1412.865966796875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1313.912353515625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 606.4940185546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1873.2701416015625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 861.7344970703125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1432.0985107421875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 734.3942260742188 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1293.200927734375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1251.0159912109375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1447.2105712890625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 846.3413696289062 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 958.4971923828125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1256.224609375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1020.8329467773438 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1189.8330078125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 914.8994750976562 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 667.6170654296875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 948.0462036132812 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1373.2113037109375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1136.6158447265625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1742.009765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 4011.604248046875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1435.0985107421875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 926.1871948242188 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 802.0321655273438 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1086.2459716796875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 768.1984252929688 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 767.5415649414062 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 631.8369750976562 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 899.755859375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 745.8598022460938 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 740.8951416015625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 680.1044311523438 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 751.8527221679688 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1005.7022705078125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 809.362548828125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 874.856201171875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 696.5739135742188 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 397.1595153808594 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1701.5198974609375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1182.97900390625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 2361.085205078125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1212.0567626953125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 831.6763305664062 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 901.0339965820312 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 857.4807739257812 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1399.5789794921875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 896.9077758789062 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 644.1498413085938 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 500.7955627441406 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1214.924072265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 562.5505981445312 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 537.3587036132812 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 473.78173828125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 586.0119018554688 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 596.286865234375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 775.0023803710938 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 692.0517578125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 627.5361328125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 588.015869140625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 529.9180908203125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1000.48681640625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 854.0386962890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 669.1203002929688 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 679.3797607421875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1348.990966796875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1320.4873046875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1366.089111328125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 867.2508544921875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 856.6878051757812 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 9618.333984375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1365.3172607421875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 2443.1181640625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 798.0803833007812 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 835.9549560546875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 844.3269653320312 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 851.9821166992188 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 5080.96728515625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 815.787109375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 812.9288940429688 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 822.966552734375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1294.2513427734375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 3032.39404296875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 622.8580932617188 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 859.1392211914062 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 990.0372924804688 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 2694.66650390625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1330.3453369140625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1114.308349609375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 726.2156372070312 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 756.540771484375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 673.79736328125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 843.95751953125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1440.3240966796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1708.418212890625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1278.8941650390625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 646.2225341796875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 709.296875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1393.926025390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1230.9085693359375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 794.6607055664062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 763.21630859375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 901.3426513671875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 617.2284545898438 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1223.3685302734375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1041.7288818359375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 881.1823120117188 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1294.0916748046875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1435.3035888671875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 963.361572265625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 6248.8271484375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1458.654052734375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1383.6434326171875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 968.478271484375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1189.7332763671875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 885.1382446289062 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 713.5057983398438 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1022.7745971679688 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1528.282470703125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 602.4728393554688 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 979.4019775390625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1110.934326171875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 417.6190490722656 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 545.57275390625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1504.67236328125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 533.4021606445312 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 472.5212097167969 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2104.514892578125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1305.677490234375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 951.23681640625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1015.4517211914062 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 845.4816284179688 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 451.0108947753906 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1211.907958984375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 633.0623779296875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 618.7477416992188 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 932.0673217773438 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 877.218994140625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 972.064208984375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 578.2598266601562 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 882.402587890625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1005.7171630859375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 4701.8359375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 759.5706176757812 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2206.488525390625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1214.552978515625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1930.7135009765625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1299.619140625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 973.5474853515625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1775.1661376953125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1229.95947265625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1028.3814697265625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1082.8427734375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 814.0960083007812 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1289.3671875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 662.4937133789062 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 664.2606811523438 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1081.567138671875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 3010.958984375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 885.1428833007812 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 863.6531982421875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1357.6407470703125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1070.0576171875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 957.294677734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1269.957275390625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 2678.96240234375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1818.931640625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1062.1141357421875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1994.8873291015625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1206.48681640625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 557.8161010742188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1533.9595947265625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 3443.65283203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 825.5445556640625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1301.082275390625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1749.87548828125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 853.9752807617188 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1713.0245361328125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1330.8880615234375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 573.3327026367188 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1191.6070556640625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1301.7176513671875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 724.5172729492188 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 796.7622680664062 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1423.8233642578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1754.295166015625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1429.6580810546875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1098.3956298828125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1309.4697265625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 719.4903564453125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 983.0704345703125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1562.9864501953125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 674.3723754882812 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 885.3108520507812 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1006.8851318359375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 732.40673828125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 955.3851928710938 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 921.116455078125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 467.6567687988281 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 623.3914184570312 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1330.6007080078125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 727.173828125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 448.7243347167969 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 554.7251586914062 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 809.8515625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 808.654541015625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 789.9290161132812 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 851.7061767578125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1281.722412109375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1063.0443115234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1161.73291015625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 988.8668212890625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 997.8893432617188 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 2203.22265625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 520.710693359375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 914.189453125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 2349.18896484375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 2874.303955078125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1024.520751953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1937.638916015625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1740.2716064453125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1530.8612060546875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1216.909912109375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 4350.650390625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1869.833251953125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 614.249267578125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 908.1127319335938 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 792.0078735351562 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 2572.135009765625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 808.9523315429688 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 813.5228881835938 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2518.474853515625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1555.8321533203125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 595.4767456054688 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 861.6939086914062 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1417.1715087890625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 991.9840087890625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 777.4060668945312 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 854.7581176757812 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 713.017578125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 921.3768310546875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1626.318359375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 835.770751953125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1035.0863037109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1100.765625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1269.166748046875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 762.7055053710938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 937.3164672851562 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1073.9840087890625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1265.9228515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 816.8826904296875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1696.093505859375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 894.8734741210938 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 588.4359130859375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 881.7174682617188 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1797.7366943359375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1055.7781982421875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1241.545654296875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 977.299560546875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1200.7191162109375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 697.9472045898438 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1179.334228515625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 3758.952880859375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1067.1575927734375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1602.175048828125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2232.82177734375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1579.6929931640625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1117.677734375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 5148.494140625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 530.5411987304688 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1717.5947265625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1070.0438232421875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 465.0778503417969 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1273.3822021484375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1549.2589111328125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1659.4261474609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 3137.048583984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 4007.960693359375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1356.3662109375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 810.97509765625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1935.844482421875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 559.4116821289062 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1338.0694580078125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 442.9588317871094 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 475.6374206542969 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 633.2365112304688 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1026.38818359375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1545.1864013671875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1728.5787353515625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1076.9261474609375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1288.862548828125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1194.4732666015625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 681.0050048828125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1690.404296875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1492.7325439453125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1405.3426513671875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1171.676025390625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1781.4427490234375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1554.867431640625 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1721.77392578125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1733.2823486328125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1317.2093505859375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2843.045166015625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 911.5860595703125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 907.613525390625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1857.1458740234375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1783.9686279296875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1199.5626220703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1077.983154296875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 2015.5604248046875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 7685.70068359375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1645.591552734375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1055.2783203125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 2064.252685546875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1823.064453125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1192.30029296875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 920.2473754882812 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 4808.46826171875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 612.0671997070312 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1124.22900390625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 893.90283203125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 7118.23583984375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 5589.36328125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 2036.697265625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 2251.45703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1629.8074951171875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 877.4320068359375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1601.2222900390625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1985.2945556640625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 7590.55517578125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 520.1220092773438 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1515.7447509765625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 730.49951171875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 673.7111206054688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 626.8427734375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1023.0239868164062 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 525.7044677734375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 859.5079956054688 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1800.27099609375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2161.731201171875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 715.215576171875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 559.5409545898438 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 842.2689819335938 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 3723.687255859375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1142.2794189453125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 851.28466796875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 940.32666015625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 624.4379272460938 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 825.4423217773438 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 900.0306396484375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1123.3563232421875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 901.0264892578125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 756.37353515625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 827.228759765625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1076.2535400390625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 966.0105590820312 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 687.8034057617188 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1045.355224609375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2380.58056640625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1009.742919921875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 690.7675170898438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 901.0463256835938 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 2138.927734375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 950.5836791992188 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 914.9635009765625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 636.2492065429688 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1050.364501953125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 929.5498046875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1668.7130126953125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 562.9613037109375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1255.1573486328125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1562.7723388671875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 763.2340087890625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 754.70703125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1424.4796142578125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 970.7003173828125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 837.1991577148438 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 990.8659057617188 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1038.1610107421875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 827.44677734375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1455.46337890625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1115.199951171875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 689.108154296875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 919.7216796875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1215.7347412109375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 883.7864379882812 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1590.387451171875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 975.713623046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 754.0386962890625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 777.0106811523438 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1220.891357421875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1564.1385498046875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 768.8386840820312 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 431.454833984375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 626.4464111328125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1661.0010986328125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1750.6612548828125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 552.6290283203125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 343.2318115234375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1259.991455078125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 695.9993286132812 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1617.263671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 721.25146484375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 928.2537231445312 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1258.494873046875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1685.7799072265625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1298.618408203125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1470.797119140625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1571.80859375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 694.5272827148438 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1554.283203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1980.149169921875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1140.45361328125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1428.3717041015625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2150.208251953125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1085.177978515625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 2498.016845703125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1242.7850341796875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 3144.623291015625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 2289.38916015625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 828.5309448242188 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 902.0775756835938 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 2464.255126953125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1095.5419921875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 783.1513671875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 844.5599365234375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1114.8262939453125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 6609.35791015625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 796.1605834960938 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 860.71630859375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1943.6036376953125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1211.8448486328125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 774.0402221679688 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 2543.692626953125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 916.4344482421875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 741.546875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1197.876220703125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1312.33837890625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 905.5189819335938 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 881.3546752929688 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1005.7398681640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1301.5816650390625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 3483.591552734375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1061.4825439453125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 899.2941284179688 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1013.0599365234375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1370.4110107421875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 3444.219970703125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1051.471923828125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1095.3389892578125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1427.8580322265625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 747.4966430664062 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1349.7808837890625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1591.20703125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1043.03173828125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1120.932373046875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1290.9281005859375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 635.8790893554688 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1159.1859130859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 430.00665283203125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1514.5518798828125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 816.35595703125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 574.7169189453125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 2196.4619140625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 2778.7568359375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 951.1835327148438 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 805.09033203125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 911.15576171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 907.81787109375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1691.2303466796875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1233.587890625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1576.2449951171875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1553.950927734375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1280.9256591796875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 740.8853149414062 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 953.240966796875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1243.443359375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1276.9071044921875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1278.51953125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1369.4439697265625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 932.0330810546875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1496.093505859375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1218.72412109375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1679.8292236328125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1989.92333984375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1828.785400390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 986.4232177734375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 956.227783203125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1556.9244384765625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 959.5869140625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1313.398681640625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 769.08740234375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 535.19580078125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1032.0655517578125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 662.1483764648438 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1005.1268920898438 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1139.4881591796875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 648.7373657226562 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1894.9237060546875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1256.396728515625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1549.24169921875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1475.5550537109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1448.227294921875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1712.5003662109375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 972.8926391601562 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1856.673095703125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1054.7332763671875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 521.1294555664062 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1842.7606201171875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 2170.380126953125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 2549.54833984375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 912.0626220703125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 985.9403076171875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1515.171142578125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 4202.8720703125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 925.779296875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 932.7745361328125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 2016.4093017578125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 2415.704345703125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1210.8756103515625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1058.5595703125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 775.7683715820312 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1400.670654296875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1847.5479736328125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1566.4599609375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1014.164794921875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1553.82275390625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 2966.599853515625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1099.724853515625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 981.9800415039062 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1336.7078857421875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 822.0546264648438 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 929.2881469726562 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 2089.36474609375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1283.435791015625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 518.1825561523438 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 747.437255859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 723.72900390625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 635.4810791015625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 906.3977661132812 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1093.9417724609375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 588.265380859375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 922.9605712890625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1384.38671875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 936.451171875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1778.0013427734375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1805.8707275390625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1380.218505859375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1227.9373779296875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1821.7044677734375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1166.136474609375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1111.0631103515625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1362.67236328125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 616.346923828125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 2020.4036865234375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1597.522216796875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 910.1906127929688 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 983.8414916992188 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2192.689697265625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1025.2093505859375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: -8676.3056640625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1042.9779052734375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 528.7291259765625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1303.5509033203125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1744.4395751953125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 745.1964111328125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 781.037353515625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1827.6307373046875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2331.42578125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1530.1484375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1416.302978515625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 561.80126953125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 778.1784057617188 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1766.7274169921875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1138.923828125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 711.3275756835938 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1198.428466796875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 784.0174560546875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 819.2708129882812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1290.7857666015625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 767.237060546875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 502.32733154296875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 664.6464233398438 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 793.7097778320312 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1193.614501953125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 702.2447509765625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 815.159912109375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 441.1123046875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1136.4105224609375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 773.7708129882812 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 615.3951416015625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 670.9225463867188 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 663.3413696289062 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1723.8970947265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 2636.470458984375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1007.6746215820312 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 639.674072265625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 935.1238403320312 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1600.39404296875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 760.9802856445312 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1219.56982421875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 543.462890625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 700.5641479492188 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1526.3153076171875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 909.4017333984375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 735.857666015625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 718.9783935546875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1152.921875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1033.366455078125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1546.8560791015625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 687.9132690429688 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1117.4097900390625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1317.4271240234375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1155.2496337890625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 938.5608520507812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 699.078857421875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 577.845458984375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 717.967529296875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1263.4620361328125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1177.8321533203125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 818.8583374023438 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1148.181396484375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 952.5118408203125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 893.885498046875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 2244.386962890625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1042.37060546875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1427.74267578125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1241.659912109375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 733.7024536132812 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1704.5234375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1124.8717041015625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1312.31494140625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 632.4257202148438 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1650.470947265625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1505.228271484375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1253.889892578125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1599.795166015625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 656.176025390625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 659.6989135742188 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1395.5789794921875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 8321.5458984375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1070.4085693359375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 888.4688720703125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 759.6229858398438 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 554.8099365234375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 669.4611206054688 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 988.0426635742188 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 740.20263671875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1226.7237548828125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 411.89703369140625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 953.9907836914062 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 831.529541015625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:24:23<105419:26:41, 1265.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 739.2103271484375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 653.6482543945312 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 733.9569091796875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1222.999755859375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 432.6592102050781 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 455.9900817871094 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 669.7667846679688 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 984.7442016601562 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 581.0574340820312 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 576.3468627929688 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1042.283935546875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 786.6644287109375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1320.3170166015625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1723.017333984375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 784.761474609375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 811.0428466796875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1252.3028564453125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 896.1829833984375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 2758.9892578125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1205.2742919921875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 689.637451171875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1554.3089599609375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 635.3490600585938 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1174.7509765625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1628.9847412109375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 2303.9462890625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 867.7361450195312 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 3368.2099609375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1400.375732421875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1008.9710693359375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1194.185791015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1874.4163818359375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1381.147216796875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 627.224609375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1114.4503173828125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1017.1575927734375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 867.6575927734375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 548.1049194335938 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1114.7030029296875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 789.1068725585938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 846.96728515625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1197.50830078125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 880.1564331054688 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1265.478515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 846.9321899414062 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 913.6541748046875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 878.03662109375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1310.6572265625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1196.52001953125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1422.870361328125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 10616.1904296875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1952.5526123046875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1264.98291015625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 2815.353271484375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1097.8704833984375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 808.993408203125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 698.3487548828125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1018.2943725585938 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1062.5806884765625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1313.48193359375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1606.23876953125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 704.6647338867188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 2199.84423828125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1106.68359375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 941.8491821289062 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 780.7640380859375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1171.479736328125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1645.19189453125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1183.266845703125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 943.1302490234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 804.0697021484375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 2876.525634765625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1920.850830078125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 823.2750244140625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 706.7429809570312 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1043.6187744140625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 3482.614990234375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 891.8259887695312 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 575.9196166992188 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1444.9248046875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 746.1796875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1139.045654296875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1418.8990478515625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 669.8822631835938 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 575.4752197265625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1259.111328125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1070.71337890625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1280.7899169921875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 736.8523559570312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 865.42529296875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2274.309814453125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1080.69140625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 675.1807250976562 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 914.5625610351562 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1443.1878662109375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 6811.57861328125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1135.6810302734375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 732.6176147460938 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 2058.4775390625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 5334.38916015625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 845.1780395507812 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 914.893798828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 790.604248046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 779.0288696289062 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1228.158935546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1447.19287109375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 861.0712890625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1129.6392822265625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1314.797607421875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1210.2305908203125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 945.8917846679688 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1988.8515625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1028.991455078125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 2703.793212890625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1396.2467041015625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1436.9306640625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1832.05322265625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1301.548583984375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 3490.174560546875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 6722.736328125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 6259.77685546875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1357.3834228515625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1742.9097900390625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1436.1090087890625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 2306.300048828125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 731.7178955078125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1000.1068725585938 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1498.6375732421875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1189.1119384765625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1107.82568359375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 749.536865234375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 2247.5693359375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 850.7742309570312 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1421.857666015625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 659.8025512695312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 805.69921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 919.9776611328125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 919.0015258789062 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 779.8140869140625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1079.22607421875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 783.9048461914062 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 767.280517578125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1568.3094482421875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1151.6322021484375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 610.5445556640625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 631.1323852539062 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1337.724365234375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 840.8463134765625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1079.7125244140625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 657.0681762695312 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1127.2899169921875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1602.81201171875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1388.7435302734375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 454.00665283203125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 859.6278076171875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1164.5848388671875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 931.2548828125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 928.0655517578125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 853.1580810546875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1051.3658447265625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1578.426513671875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1546.0616455078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1103.570556640625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 973.1841430664062 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 759.6442260742188 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1073.7904052734375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1194.73095703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 819.7293090820312 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 873.3331298828125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 793.0276489257812 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 701.0062255859375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 908.7509765625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1149.5699462890625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 759.2002563476562 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 891.234375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 583.7171020507812 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1518.0404052734375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1220.7864990234375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 805.5827026367188 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 849.4434204101562 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1606.712890625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 996.7468872070312 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 907.15625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1131.9949951171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 705.5772094726562 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 426.2516174316406 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 753.3343505859375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 864.2223510742188 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1802.305908203125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1305.5836181640625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 560.0990600585938 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 884.8578491210938 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1028.1070556640625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 988.4002685546875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 767.1783447265625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 803.7115478515625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1722.1256103515625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 716.485595703125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1820.809326171875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 856.3565673828125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1207.708984375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1077.1590576171875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 891.3316650390625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1414.480712890625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 927.1212158203125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1452.702392578125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1284.62646484375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1498.038330078125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 916.2696533203125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1032.762939453125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1307.286376953125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1607.620849609375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 712.2434692382812 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 916.8198852539062 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1663.914306640625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1328.5823974609375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1343.9991455078125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2012.4219970703125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1265.752197265625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 950.4304809570312 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2618.241943359375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 3761.025634765625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 903.8775634765625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1914.0413818359375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1478.46337890625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1238.59521484375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1119.7789306640625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1069.9739990234375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1188.5943603515625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 792.6185913085938 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1086.6641845703125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1189.6644287109375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 758.0115966796875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 931.9368896484375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 806.570068359375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1630.4412841796875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1453.4493408203125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1077.2850341796875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1008.5042724609375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1552.2742919921875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 880.1238403320312 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1459.435791015625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1000.39794921875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1108.09765625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1099.6673583984375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 675.4115600585938 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 956.3351440429688 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1364.142578125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1480.01171875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 891.85107421875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1166.543212890625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 974.474853515625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1567.70849609375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1518.217529296875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 724.0492553710938 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1820.5185546875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1244.7347412109375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1308.6385498046875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 979.8072509765625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1271.0980224609375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 535.5138549804688 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 608.6935424804688 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 2065.177734375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1270.8489990234375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 703.5148315429688 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1125.65283203125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 690.4059448242188 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 875.88916015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1448.873291015625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 992.8057861328125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 728.6087036132812 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1819.6597900390625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 699.2628173828125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1014.5679931640625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 498.8116455078125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1493.8392333984375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 3360.6162109375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 968.9417724609375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1121.07421875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 890.239501953125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1145.5 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 2211.0439453125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 659.1508178710938 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1099.2066650390625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1472.8155517578125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1293.4576416015625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 921.982177734375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1072.10888671875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 938.2683715820312 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1096.239990234375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1313.0594482421875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 953.408447265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 608.63818359375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 747.2216796875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 7371.9248046875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1125.8995361328125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 852.8277587890625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 650.8336791992188 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 708.89697265625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 707.4921264648438 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 791.5115356445312 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 714.2711791992188 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 871.9446411132812 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 941.3820190429688 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 988.1822509765625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1280.0845947265625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1584.9029541015625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 672.966796875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 917.1923828125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1049.6346435546875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 621.7926635742188 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 891.5676879882812 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1604.360595703125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 631.1134643554688 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 758.5957641601562 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 955.646728515625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 962.768798828125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1015.7821044921875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 829.9217529296875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 860.1004638671875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 853.983154296875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 946.4639282226562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1197.370361328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 745.060302734375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 586.83056640625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 658.0252075195312 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1247.9381103515625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 608.5654907226562 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 860.0842895507812 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1058.7723388671875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1278.250244140625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 3815.579833984375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 741.8178100585938 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 682.6390991210938 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 843.1524047851562 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1162.681884765625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1136.188232421875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1386.6436767578125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 994.078369140625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 936.1908569335938 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1710.7276611328125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 3111.546630859375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 2003.6492919921875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 2014.8935546875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1477.1710205078125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1159.6065673828125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1540.131103515625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1918.408935546875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 993.7976684570312 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1974.2149658203125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 741.557861328125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 2058.127685546875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2020.4188232421875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1313.7928466796875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1166.9840087890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 843.3223266601562 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2012.9405517578125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 614.0288696289062 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2676.602294921875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1493.516357421875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1195.3377685546875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 780.1767578125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 788.6640625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 3212.33056640625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 2802.32177734375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 955.9087524414062 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1505.3721923828125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1022.5676879882812 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 856.09814453125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1796.71240234375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 539.5779418945312 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 468.5759582519531 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 634.0430908203125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 650.7655029296875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 643.0665283203125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1656.3172607421875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 781.5767822265625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1541.2044677734375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 870.1532592773438 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 858.8665771484375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 717.2130126953125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 531.5191650390625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 978.4916381835938 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1708.71728515625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1006.3705444335938 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1207.6478271484375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1709.0313720703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1056.2755126953125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 942.15673828125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1365.05419921875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 869.0162353515625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1437.714599609375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1387.0908203125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 618.7041625976562 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 2623.593017578125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1738.8253173828125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1649.060546875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 901.07666015625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1045.639892578125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1447.0679931640625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1300.626708984375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1892.1484375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 850.1821899414062 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1211.96826171875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1561.3028564453125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1423.86669921875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 4989.26904296875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1183.73828125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2594.06298828125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1283.216064453125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 2933.91796875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 4950.6572265625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1682.080078125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1011.6818237304688 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 2158.74853515625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1481.3638916015625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1942.9503173828125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1751.4718017578125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1527.96533203125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 74049.2578125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: -30735.22265625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1935.8797607421875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 2853.629638671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1174.2603759765625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 3866.09423828125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 3026.928466796875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 945.5773315429688 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 653.4384155273438 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 2379.09130859375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2218.83349609375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1621.7152099609375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 919.9471435546875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1452.0875244140625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: -23558.970703125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1727.4608154296875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1587.6995849609375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1353.3857421875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1914.8690185546875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1249.9488525390625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1321.8477783203125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1104.56005859375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1608.606689453125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2420.327392578125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1545.8450927734375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 922.305419921875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1853.689208984375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 825.6982421875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 2031.2635498046875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 765.3320922851562 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 739.3297119140625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1354.6636962890625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 715.3701171875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1170.7083740234375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1087.9832763671875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1514.37353515625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1345.09033203125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1684.185791015625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1129.62548828125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1251.9970703125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1747.7412109375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 991.1546630859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1262.3126220703125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1165.067138671875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 746.9939575195312 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1540.021728515625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1292.284912109375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1268.33203125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 555.88623046875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 897.8881225585938 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1414.303955078125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1486.7799072265625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 748.7276000976562 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 912.1939697265625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 615.0098266601562 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 851.25634765625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 718.5049438476562 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1260.805419921875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1096.4366455078125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1206.245849609375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 819.9215087890625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1155.5540771484375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 754.5986938476562 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 630.2002563476562 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1129.080078125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1110.25 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1407.5032958984375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 756.5089721679688 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 834.3546752929688 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1037.283935546875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 698.0961303710938 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1962.5494384765625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 755.3184204101562 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1355.8331298828125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1190.2303466796875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1415.8900146484375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 771.78271484375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1634.882080078125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1025.2724609375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 820.7655029296875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1281.8250732421875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1514.9969482421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 935.281005859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1187.2724609375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1079.10302734375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 3018.2265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2093.834228515625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1056.4012451171875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1262.0657958984375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 864.482666015625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1328.653564453125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1568.9775390625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1535.8829345703125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 2717.487548828125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1204.8184814453125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 4554.56640625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1049.678466796875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 965.4154663085938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1296.9412841796875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1091.2830810546875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1114.626953125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 777.1013793945312 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1393.0430908203125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1342.586181640625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 567.9963989257812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 814.9647827148438 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 531.593017578125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 827.45458984375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 656.803955078125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 601.7432250976562 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 741.6565551757812 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 772.0709838867188 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 910.3421020507812 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 491.7245788574219 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 852.6278686523438 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 887.1227416992188 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 598.160888671875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 951.3737182617188 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 680.4718627929688 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 681.5557250976562 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 712.1205444335938 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 727.959228515625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1324.4674072265625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1071.1605224609375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 435.0902099609375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 426.70709228515625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 871.0026245117188 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 722.1591796875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 614.1203002929688 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 463.62713623046875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 535.6912231445312 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 739.6409301757812 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1682.865966796875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1420.40869140625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1577.4278564453125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1882.407470703125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 992.1644287109375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1438.3798828125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2650.578857421875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 885.5425415039062 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 599.0596313476562 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 657.8471069335938 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 651.7818603515625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1964.2486572265625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 855.4102172851562 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1812.61865234375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1120.330322265625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1627.637451171875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1035.9246826171875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 5247.046875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 3526.2421875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1281.372802734375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 753.3464965820312 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 5100.29345703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 3198.451416015625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 781.0512084960938 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1674.2406005859375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 2324.29541015625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1463.1796875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1134.119384765625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 844.3712768554688 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 912.5237426757812 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1349.271728515625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1122.42041015625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1020.7159423828125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 4249.2109375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1431.7969970703125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1416.1163330078125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 3429.809326171875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 581.9138793945312 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1839.2667236328125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 789.8084106445312 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 2633.849853515625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1646.54345703125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1099.0244140625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 907.4080810546875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1689.110107421875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 2007.3577880859375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 940.6649780273438 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1127.1246337890625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1364.8353271484375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1218.6563720703125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 4358.4365234375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 2369.646240234375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1570.061279296875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1454.000732421875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1067.974853515625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1521.958740234375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2195.04833984375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1492.2413330078125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1214.0128173828125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 2900.89404296875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1484.3179931640625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 998.9642944335938 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1274.989501953125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1403.7457275390625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 2616.1279296875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1198.981201171875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1523.01318359375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 800.0447387695312 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1140.11376953125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1869.3436279296875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 891.6506958007812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2136.30322265625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2086.5439453125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1132.8448486328125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1142.482421875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 878.2811889648438 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1148.297119140625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 912.7667236328125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1556.3016357421875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 583.3359985351562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 652.0126342773438 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 849.6691284179688 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 639.3930053710938 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1379.629638671875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 899.3931884765625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1079.63232421875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 811.8025512695312 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 971.4014282226562 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1372.4324951171875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1881.8101806640625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1950.8272705078125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1515.1494140625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1430.2515869140625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1248.1253662109375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1901.7574462890625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1121.403564453125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 894.7743530273438 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 774.159912109375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 899.2329711914062 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 980.1412353515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 846.2885131835938 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1430.4073486328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1224.49755859375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1017.5725708007812 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 855.8482055664062 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 10270.5341796875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 828.6943969726562 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1034.2344970703125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 548.4096069335938 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1348.6351318359375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 2193.608642578125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1632.573974609375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 796.6903076171875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 705.431884765625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 999.8800659179688 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1046.736328125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1246.8619384765625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 867.6129760742188 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 790.46142578125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1337.7952880859375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1049.7054443359375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1450.9478759765625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 817.31298828125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 670.2935180664062 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 925.9559326171875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1152.85791015625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1019.3055419921875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 913.9572143554688 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 2299.677490234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1281.1690673828125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 677.148193359375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1390.3702392578125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1993.959716796875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 721.916259765625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1511.3232421875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 716.2125854492188 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1376.6380615234375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 2532.205810546875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1168.5555419921875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1463.791748046875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 979.1475830078125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 2056.40771484375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 991.5755615234375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1154.7198486328125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:45:32<105558:54:24, 1266.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1431.099853515625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 765.1487426757812 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 3187.507568359375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 907.0603637695312 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 944.46142578125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1765.876953125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2467.568359375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1053.24658203125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 806.7343139648438 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 792.5269165039062 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1122.659423828125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 3522.732177734375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1847.4876708984375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 478.34844970703125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 701.996337890625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1215.0362548828125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1103.294189453125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 868.8233642578125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1414.211669921875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1236.8541259765625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 886.9662475585938 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1230.3868408203125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1006.0126342773438 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1958.4591064453125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 912.8534545898438 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 850.9699096679688 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1257.1368408203125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 682.154296875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 2747.373291015625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 930.5517578125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1808.5731201171875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 2081.96875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1240.284423828125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1554.430419921875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 766.888916015625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 653.0223999023438 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 709.73095703125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 989.4148559570312 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 888.6320190429688 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1295.22705078125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 531.7694091796875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 847.0502319335938 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 639.1192016601562 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1390.1256103515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1241.4495849609375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1567.1978759765625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 854.712890625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1154.994140625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1243.9111328125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1554.0333251953125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 592.1624145507812 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 6100.73046875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1749.614501953125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1247.5963134765625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1590.52880859375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1284.5599365234375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 618.089111328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1841.4659423828125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 977.2457885742188 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2511.455810546875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1983.1141357421875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1341.1717529296875 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 870.1400756835938 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1249.43896484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1251.9033203125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 699.159912109375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 831.39697265625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1194.3720703125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 866.8963012695312 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1200.003173828125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 627.0542602539062 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1291.2821044921875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 949.9718627929688 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1424.46240234375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 4097.80126953125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 948.1602783203125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1247.0540771484375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 2164.97607421875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1525.2882080078125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1853.7923583984375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1004.551025390625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1376.271484375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 944.6898193359375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1343.113037109375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 2082.805419921875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1522.2642822265625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1410.6336669921875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2768.43701171875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 2954.612060546875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1455.4259033203125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1471.346923828125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1040.084228515625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 681.3568725585938 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 992.626708984375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 3872.1455078125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1556.8232421875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1292.692626953125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1215.6026611328125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1443.3497314453125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1646.0599365234375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 2831.805419921875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1647.173828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 729.3735961914062 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1563.41943359375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 2246.575439453125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 3234.509765625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1722.4976806640625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 2290.24072265625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 3031.09326171875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1128.67578125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 918.8659057617188 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 947.411376953125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1440.597900390625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1054.0 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1063.569091796875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1864.896484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 822.0985107421875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 947.472900390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1041.0108642578125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1048.7010498046875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1751.2894287109375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 800.5678100585938 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1071.943603515625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 856.8858032226562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 757.875244140625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 575.1177368164062 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 757.2597045898438 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 798.05615234375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1047.1588134765625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 634.6265869140625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1009.4898071289062 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1105.81591796875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1005.1727905273438 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 500.24688720703125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 603.1914672851562 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 886.2742919921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1093.11279296875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1194.0499267578125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1102.5794677734375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 756.6301879882812 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 718.56689453125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 864.982666015625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1114.227783203125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1091.6715087890625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 898.3137817382812 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 868.2401123046875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1415.3756103515625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1287.972412109375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 964.529296875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 4564.06396484375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 917.7081298828125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 29401.1328125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 5525.685546875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1918.8465576171875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1413.8525390625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 823.626708984375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1479.4561767578125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1163.6893310546875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 933.7523193359375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 3435.38671875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1466.93017578125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1787.4263916015625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1111.1741943359375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1490.935302734375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 772.623291015625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2541.4794921875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 729.6707763671875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 890.4600830078125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1243.40234375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 988.8317260742188 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 711.93310546875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1065.54052734375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1620.4298095703125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1799.8641357421875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1009.1035766601562 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2607.866943359375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1795.7000732421875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1600.6827392578125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 4632.0126953125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1364.395263671875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 3188.705322265625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1104.1199951171875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1488.186767578125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 847.5072021484375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 3744.999755859375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 2086.077392578125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1175.8033447265625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 666.831787109375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 652.781982421875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1587.8521728515625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 583.6616821289062 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 871.7614135742188 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 758.7548217773438 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1244.6209716796875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 716.7825927734375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 706.7671508789062 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 841.3895874023438 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 711.0267333984375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1506.77490234375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1157.297119140625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 722.6903686523438 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1257.419677734375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1579.3665771484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 949.6996459960938 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 2001.2464599609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 561.608154296875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1258.936279296875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 567.2807006835938 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 991.9616088867188 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1074.029296875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1140.7132568359375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 900.1754760742188 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 931.77783203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 980.6072998046875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1640.2589111328125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 2700.02001953125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1406.403076171875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1110.4228515625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 943.4525146484375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1483.60986328125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1019.3118286132812 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1099.986328125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 2085.022705078125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 702.06103515625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1469.646728515625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 2755.001220703125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1112.2911376953125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1578.2686767578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1510.8001708984375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1253.9091796875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1488.9490966796875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1091.5625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1631.5728759765625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 791.4423828125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 907.1473388671875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 948.5595092773438 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 685.4542846679688 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 991.7203979492188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1313.9962158203125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 603.8212280273438 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 961.5651245117188 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1018.9124755859375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1715.5301513671875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 579.1441040039062 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 833.940185546875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 2468.11279296875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1904.436279296875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 876.5847778320312 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2150.907958984375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 899.334716796875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 4337.3779296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 2621.16748046875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1585.50146484375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 940.616943359375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 919.2127685546875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 871.7415771484375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1376.335693359375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 3288.96875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 664.3607788085938 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 441.3763427734375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 784.8989868164062 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 379.34716796875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1095.2789306640625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1100.2216796875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 649.8869018554688 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 974.2296142578125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 469.6352844238281 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 765.5015258789062 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1490.7891845703125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1491.1927490234375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 710.3046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1361.9991455078125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 949.5472412109375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 795.8778076171875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 970.8223266601562 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1185.8477783203125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 885.7318725585938 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1391.8414306640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1701.3299560546875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 933.830322265625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1923.1612548828125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1289.2918701171875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 983.22314453125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2163.7158203125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 926.5184326171875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 943.460205078125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 323.6719665527344 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1156.56494140625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1590.1370849609375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1838.137939453125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1458.9464111328125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1049.0054931640625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 2019.991943359375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1180.1822509765625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1094.549072265625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2217.189208984375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 681.77099609375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1996.3651123046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1576.2860107421875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 824.2678833007812 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1461.5509033203125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1975.0966796875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 920.1268920898438 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1737.2977294921875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1385.9451904296875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 773.6060791015625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 877.1624145507812 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1500.883544921875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1428.268310546875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 859.9152221679688 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1605.3033447265625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1104.93359375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1216.981689453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1277.0755615234375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1471.9791259765625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1351.615478515625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1288.958740234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1884.60302734375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1004.9933471679688 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1434.67138671875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1499.82421875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1214.16015625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1639.2608642578125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1622.469482421875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 915.3995361328125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 793.849365234375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1117.2332763671875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 935.203857421875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 649.0477294921875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1227.1866455078125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1140.367431640625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1435.876708984375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 689.88330078125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 904.2672729492188 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1040.5126953125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1037.1759033203125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 982.22216796875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1511.79638671875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 739.4881591796875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 621.5208740234375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1695.6497802734375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1082.38671875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 955.0508422851562 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2226.813720703125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 986.51025390625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1202.6854248046875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1449.66015625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 937.7058715820312 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 639.0354614257812 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 992.1649780273438 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1046.792236328125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1275.640625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 833.9255981445312 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 715.9558715820312 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 637.0595092773438 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 860.480712890625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1069.0054931640625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1351.970703125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 2020.052734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1164.0906982421875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1246.2193603515625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1090.1837158203125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 2221.34228515625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1689.79296875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 913.876220703125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1538.944580078125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1322.2081298828125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2543.686767578125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1367.1253662109375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1908.436279296875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1662.6846923828125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1176.4483642578125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2833.861572265625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1533.633544921875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 782.244873046875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1785.204833984375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1474.4915771484375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2013.4783935546875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2819.70849609375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 926.1250610351562 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1124.79443359375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1160.5262451171875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 996.5917358398438 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 2927.098388671875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1250.7183837890625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1571.0185546875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 769.7867431640625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 830.7095336914062 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1307.7535400390625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 850.01123046875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 888.8060302734375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1212.949951171875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 997.2500610351562 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 2029.1510009765625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2206.55029296875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 903.7720336914062 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 43649.78125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1087.356201171875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1131.5003662109375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 985.1676635742188 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2047.006103515625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 3016.5615234375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1025.4923095703125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 2338.64013671875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1016.0208129882812 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 2037.0450439453125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1835.655029296875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1371.6951904296875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1128.349853515625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1960.28076171875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 847.9480590820312 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 11063.9296875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1542.09228515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3733.642822265625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 815.8862915039062 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 695.2493286132812 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1420.1522216796875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1649.3529052734375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3832.251953125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 161075.84375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1324.25537109375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1253.549072265625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1596.6015625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 976.8724365234375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1094.55224609375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 2218.699951171875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 2111.575439453125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 856.875732421875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1373.04052734375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1111.864990234375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1520.9384765625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 861.659912109375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 893.225341796875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 3084.43017578125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1191.2198486328125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.758, loss_val: nan, pos_over_neg: -16088.646484375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1284.332275390625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 3711.01416015625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1040.8206787109375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1357.0677490234375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1417.52197265625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1284.1705322265625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 682.2977294921875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 570.9345092773438 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 940.895263671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 3222.39404296875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 5081.81298828125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1302.0816650390625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 507.9853515625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 934.8577880859375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1017.9141235351562 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 2021.0487060546875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 535.77587890625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 702.432861328125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1238.1597900390625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 957.2254028320312 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 2078.6748046875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1146.58251953125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 4565.84765625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1217.8321533203125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1471.8577880859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 773.48974609375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 6926.43408203125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 899.3017578125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1377.65576171875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1008.6676635742188 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 841.0965576171875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 776.3513793945312 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 922.9591674804688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 4326.74169921875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 849.6723022460938 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1117.1595458984375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 849.1199340820312 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 838.294921875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1181.7052001953125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 675.7449340820312 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1896.853759765625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1118.9635009765625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 622.1959838867188 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 812.4579467773438 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1276.0263671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 750.8877563476562 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 583.729248046875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 848.7839965820312 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 803.9567260742188 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1035.560791015625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 946.8121948242188 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1078.0836181640625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1437.564208984375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 851.5879516601562 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1577.5054931640625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1182.0833740234375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1507.806640625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 869.0042724609375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1977.4090576171875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 970.1253662109375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 832.0559692382812 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3884.84814453125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 4525.703125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1019.6663818359375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 562.3847045898438 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1133.1461181640625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 850.2865600585938 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 976.7584228515625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 863.1079711914062 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 987.8658447265625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 936.1034545898438 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1183.2978515625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 845.4620361328125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1185.162353515625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1107.951171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 908.76318359375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1187.75537109375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 924.6522216796875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 675.590576171875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1588.077880859375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 912.008056640625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1105.1044921875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 829.465576171875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1087.5081787109375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1196.5244140625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1156.86962890625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1065.439697265625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 919.6829223632812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 839.7154541015625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1805.5706787109375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1318.5545654296875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2571.22802734375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1189.6207275390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 3335.03076171875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1442.6761474609375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1284.60888671875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2381.3427734375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 889.05859375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 2312.14111328125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2639.904296875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2767.013916015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1608.7822265625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1538.6414794921875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1065.4388427734375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1177.50048828125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1603.5528564453125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1611.6590576171875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1230.2763671875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 952.0868530273438 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1748.4222412109375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1451.3409423828125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1359.6025390625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 923.283203125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1284.881103515625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1254.8692626953125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1114.67822265625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1977.7054443359375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1435.1826171875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 645.4007568359375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 992.3388061523438 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1462.769287109375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1324.8983154296875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 962.6423950195312 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1310.448486328125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1501.4730224609375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 980.4105224609375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 3171.62841796875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1547.1932373046875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 701.3136596679688 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 2787.397216796875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1201.9830322265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1162.95166015625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1241.735107421875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 908.557861328125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1442.979736328125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1229.6202392578125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 867.5157470703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1416.7789306640625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1824.8670654296875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1416.5865478515625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 772.592529296875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1282.0433349609375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2495.26611328125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1383.885986328125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1135.9647216796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1420.4886474609375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 855.3645629882812 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 767.550537109375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1202.760009765625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1379.359375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1016.395263671875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1058.586181640625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1036.1507568359375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1400.7147216796875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 814.455810546875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 3002.612060546875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1060.2740478515625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2820.98291015625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1241.1593017578125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 937.8500366210938 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 176364.453125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1064.083984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1640.057861328125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 3370.0244140625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1108.399169921875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 4705.68212890625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1118.0723876953125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 721.248779296875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1675.2398681640625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 541.1571044921875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 987.0861206054688 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1971.472900390625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1132.156494140625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 973.0776977539062 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1919.3380126953125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1477.465087890625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1165.581787109375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1056.0283203125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1588.428955078125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 712.0059204101562 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 939.1487426757812 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 3659.324951171875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 940.1064453125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1058.6212158203125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 2450.515869140625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 3701.789306640625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1322.2874755859375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 988.3677368164062 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1196.955810546875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2181.706787109375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2321.43505859375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 2299.170166015625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 4215.80126953125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1121.0606689453125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1517.717529296875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1426.843017578125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 756.25830078125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 940.0293579101562 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1141.0411376953125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 616.2516479492188 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 708.123291015625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 702.4962158203125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1179.7169189453125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 576.292236328125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 484.5174560546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 546.6392211914062 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 412.63067626953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 860.1575927734375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1807.796630859375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1064.350341796875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1118.27294921875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 586.3687133789062 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 896.6860961914062 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1566.5157470703125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1023.3674926757812 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 890.77294921875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 617.8565063476562 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 544.6550903320312 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 882.0662231445312 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1106.3863525390625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 828.445556640625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1050.81201171875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1241.8941650390625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 841.8419799804688 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 2083.548095703125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 995.701416015625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 895.822509765625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1391.3238525390625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1612.7906494140625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 846.8960571289062 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 584.65283203125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 717.2196655273438 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 967.7354125976562 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 983.9880981445312 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1027.9508056640625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 2537.8046875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 689.322021484375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1741.9482421875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 615.5110473632812 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1132.884033203125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1635.046630859375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 723.256591796875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1704.0238037109375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1387.30126953125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 919.5557250976562 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1899.76708984375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1217.0704345703125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 707.7980346679688 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1494.915283203125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 2441.123779296875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1300.2860107421875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1441.2845458984375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 941.6084594726562 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1020.2988891601562 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1342.5234375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1848.482666015625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1145.753662109375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1736.1363525390625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1069.6026611328125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:06:33<105385:42:29, 1264.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2585.496337890625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1069.0421142578125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 3873.705322265625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1273.3228759765625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2976.782958984375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 2552.41064453125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1025.0238037109375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1144.614501953125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 2395.68505859375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1902.3817138671875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2181.73388671875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1408.593994140625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 4371.87158203125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1689.47021484375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 693.5455322265625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 2264.97216796875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1672.6748046875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1218.9613037109375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1535.2431640625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1096.283447265625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1653.425537109375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 953.505126953125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1442.039306640625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1630.656494140625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2006.346435546875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 4546.533203125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2169.468505859375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1306.175048828125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3197.21728515625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 2428.896728515625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 852.4268188476562 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1036.021240234375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 931.712646484375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 2277.9609375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1498.9996337890625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1077.721435546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 11262.0859375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 3147.7275390625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1340.1038818359375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1082.962158203125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1562.2265625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1511.603515625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 2057.065673828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 14380.22265625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 2538.801025390625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2432.2333984375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 4226.787109375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1860.36865234375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 950.7725219726562 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 668.6873168945312 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1022.76220703125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 2146.384521484375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1797.201416015625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 3247.27197265625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 843.9199829101562 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1447.2467041015625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1530.3839111328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1671.0498046875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1524.1719970703125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1303.5146484375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1068.6546630859375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1371.7689208984375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1141.3082275390625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 7278.1923828125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1615.0050048828125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 12803.0234375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2540.474609375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1595.5260009765625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 608.3079833984375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1172.6767578125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3912.472900390625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1709.0506591796875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1095.01171875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 893.02880859375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 2306.783203125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1045.0069580078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1095.9769287109375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 939.376953125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1870.9283447265625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 848.3788452148438 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 606.4318237304688 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2941.5048828125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1349.4072265625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 4054.243408203125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2286.6123046875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1150.065673828125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1429.5439453125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1063.1524658203125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1566.634765625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 7096.8193359375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 948.2565307617188 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 4179.99072265625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1356.8607177734375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 846.83251953125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1148.848388671875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 2440.779052734375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1112.37109375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 3153.60693359375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 812.2496948242188 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: -10285.158203125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 2654.09130859375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1720.527587890625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 2683.528076171875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 940.594970703125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1042.9415283203125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1083.9989013671875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 3182.634765625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1252.031982421875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 715.728271484375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1003.620361328125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2284.5029296875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1076.69482421875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 683.2523193359375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 661.4202880859375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 3816.35400390625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 895.0419921875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1206.6478271484375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1293.1451416015625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1244.9991455078125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1352.754638671875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 904.3787841796875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1145.3275146484375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 824.4722900390625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2276.150146484375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 5470.337890625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1264.12646484375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1162.023193359375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1819.863525390625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 8362.66015625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 676.6637573242188 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 889.0394897460938 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 796.1898193359375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1292.83935546875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2124.710205078125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1549.443359375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1376.556640625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 2991.931640625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1457.8095703125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1649.1046142578125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 3207.212646484375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1883.68603515625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 3603.18408203125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1461.7342529296875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 637.7913208007812 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 996.0093383789062 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1090.3157958984375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 913.7430419921875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1319.1826171875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1451.75390625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2349.705810546875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1497.74755859375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1354.7940673828125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1066.125244140625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1664.67724609375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 2275.8505859375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 985.7883911132812 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1758.529052734375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1188.0511474609375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1506.916748046875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1236.1087646484375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1972.5479736328125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3569.33544921875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 3183.4267578125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 879.9063110351562 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 724.3157958984375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 4387.5888671875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1077.4830322265625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 698.0292358398438 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1453.457275390625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 873.4169311523438 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 835.6842041015625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 938.3556518554688 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 728.9712524414062 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1009.020263671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 850.3628540039062 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 997.0931396484375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 707.2303466796875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 543.3615112304688 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 657.8228149414062 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 730.6151123046875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1578.8184814453125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 624.825927734375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 405.051513671875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1003.1002807617188 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 940.716552734375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 591.2001953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 653.8900146484375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2324.0537109375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1563.9925537109375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1148.136474609375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 553.9855346679688 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1591.295654296875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 2588.08642578125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1457.7060546875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 804.309326171875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 4592.7373046875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 2194.408935546875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1258.0963134765625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1003.1661987304688 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1576.5623779296875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 718.00048828125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1818.046142578125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1283.1004638671875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2433.500732421875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 890.2391357421875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3304.5263671875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1067.632568359375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1311.9344482421875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1359.013427734375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 909.5530395507812 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1014.6167602539062 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 3927.873046875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1271.8734130859375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1142.5284423828125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 803.1341552734375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 993.4262084960938 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2537.76611328125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1279.7662353515625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 822.5660400390625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1603.21044921875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1306.74462890625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 755.4737548828125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 965.1015625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 907.4097290039062 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1036.4642333984375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 747.7901611328125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1291.7213134765625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 859.7237548828125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1468.254150390625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 3135.0546875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1191.8345947265625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 626.7084350585938 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1248.45068359375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1551.336181640625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1653.9593505859375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1775.5330810546875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1217.919921875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 2201.521484375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 514.4207153320312 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1673.2501220703125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 11764.908203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1305.401123046875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1342.0089111328125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1352.3253173828125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: -104884.625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 2187.641845703125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2696.658935546875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 3629.901611328125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 6944.6044921875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2665.0439453125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1936.6162109375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 536.4564208984375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1978.1168212890625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1090.992919921875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1421.216064453125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1926.196044921875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 847.6610107421875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 1285.218505859375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 3083.09033203125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1014.667724609375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 891.0624389648438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 751.671630859375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1721.7664794921875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1114.4755859375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 727.3795776367188 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1254.9205322265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1196.867919921875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1707.4185791015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1544.256103515625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1328.632080078125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1207.784912109375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1381.234619140625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 2805.9912109375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1578.65087890625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 856.39013671875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1315.4276123046875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 874.9989624023438 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 657.078125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 3131.77197265625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2691.984619140625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1576.0758056640625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1226.2266845703125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 896.4902954101562 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 2491.501953125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1329.3197021484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1731.5343017578125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1510.914794921875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 992.6923828125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 581.9490966796875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1091.060546875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 5682.2529296875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1031.5638427734375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1678.39111328125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 801.70361328125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1235.360595703125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1731.196044921875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1428.38427734375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 2955.213134765625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 2703.459228515625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2387.864501953125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1873.730224609375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1014.0980834960938 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1278.100341796875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 903.3161010742188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 2476.115234375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1618.74267578125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1171.9150390625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 874.3596801757812 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1496.3466796875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 969.8408203125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1493.10595703125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.753, loss_val: nan, pos_over_neg: -10761.21484375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1517.622314453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1026.254638671875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 680.3370971679688 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1230.364501953125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2704.544921875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 992.5192260742188 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 763.915771484375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1417.380859375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1325.1260986328125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 2033.8421630859375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1664.343505859375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1374.8551025390625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1258.237060546875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1101.1898193359375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1873.1240234375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 981.2356567382812 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1067.9949951171875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 855.6306762695312 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 966.96533203125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 806.169921875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1059.8931884765625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1176.8245849609375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1823.954345703125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1193.141845703125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1138.8551025390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2288.436279296875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1512.635009765625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 982.0906982421875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 981.0663452148438 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1142.005615234375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1885.6973876953125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1384.443359375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1035.6075439453125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 573.6614379882812 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 913.1776123046875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1144.1793212890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1613.1754150390625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 668.2745971679688 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 593.2440185546875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1624.6217041015625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 778.3511352539062 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 876.1387329101562 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1185.0318603515625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1144.6883544921875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1778.2315673828125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 876.1315307617188 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1043.5372314453125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1306.6168212890625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2037.2210693359375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1008.054443359375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1674.689697265625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 871.01904296875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 946.6915283203125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 955.0111694335938 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 587.6868896484375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1063.6656494140625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 3018.470947265625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1790.2054443359375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1516.7998046875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1818.944580078125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 4633.828125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 2070.072509765625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1124.10693359375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1637.9569091796875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1584.9010009765625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1853.940673828125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 16028.1435546875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 931.41748046875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1417.3421630859375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1646.0362548828125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2746.529296875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 8689.9794921875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1168.88037109375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 3039.861572265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 5630.90234375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1043.707763671875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1075.6290283203125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 783.1508178710938 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1981.8201904296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 3312.621337890625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1350.182373046875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1223.79931640625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1097.4525146484375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1103.4722900390625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1065.781494140625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2646.47802734375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 881.8468627929688 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1290.592529296875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 2304.728759765625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1157.1248779296875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1429.0299072265625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1820.958740234375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 681.4248046875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1029.9915771484375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 2011.946044921875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1335.703125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1111.810791015625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1483.74658203125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1526.7510986328125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 3412.549072265625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: -4536113.5 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1357.7265625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 965.1082763671875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1783.158935546875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1242.750732421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1327.4339599609375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1732.247802734375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2202.3935546875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1359.18603515625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1537.57763671875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 600.2593994140625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 892.4696655273438 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1899.0263671875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 4412.109375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 3084.132568359375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1042.357666015625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1240.2567138671875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 781.7025756835938 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1310.7752685546875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 738.2425537109375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 808.9735717773438 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1230.01904296875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 932.0064086914062 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 850.4781494140625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 814.9833374023438 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1134.4925537109375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1063.3475341796875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1068.4288330078125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 742.0169677734375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 684.4539184570312 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1614.9317626953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1103.2222900390625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1050.78369140625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 941.4404907226562 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 698.75244140625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1773.0208740234375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1142.1546630859375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1065.9708251953125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 963.43603515625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 952.1370239257812 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 3025.38916015625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1948.67529296875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 858.7582397460938 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 571.5906982421875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1276.71240234375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1012.7308349609375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 993.6289672851562 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 733.5995483398438 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1236.2677001953125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1662.887939453125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 2110.62548828125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 2049.843017578125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 627.1026611328125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1315.103515625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 887.201416015625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 114717.703125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1129.694580078125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 813.767333984375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2799.429931640625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1277.6533203125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 915.006103515625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 804.91552734375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1452.669921875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 3456.745849609375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 860.556640625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 992.4623413085938 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 668.1467895507812 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1046.3297119140625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2916.4326171875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 762.7451171875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1089.673095703125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1345.036865234375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1107.5543212890625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1375.21533203125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 635.5238037109375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1021.740478515625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1187.5787353515625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1026.4871826171875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 622.3198852539062 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 868.9507446289062 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 942.9065551757812 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 736.741455078125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 945.4344482421875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1124.3262939453125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 26420.8984375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1183.912841796875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1125.3287353515625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1607.3099365234375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1571.5264892578125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1077.0140380859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1837.537353515625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2474.708984375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1684.791259765625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1801.9637451171875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 863.3595581054688 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1028.642822265625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 2033.122314453125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 16160.19921875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 2384.787353515625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1217.36328125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1096.298828125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 902.339111328125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 4549.01611328125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7398, loss_val: nan, pos_over_neg: 16130.6201171875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 6050.59033203125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1606.0865478515625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 652.2363891601562 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1152.310791015625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1647.51708984375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1227.5980224609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 913.4843139648438 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2642.83642578125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 832.3350830078125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1258.0113525390625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1188.956298828125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1702.2894287109375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1340.0537109375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 770.1312866210938 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1191.4454345703125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1688.7618408203125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1198.60009765625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 807.4884643554688 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1157.6295166015625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1841.080078125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1060.3428955078125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1018.0818481445312 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 559.9519653320312 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 663.7196044921875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2072.576171875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1629.1776123046875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1483.9049072265625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1887.330810546875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 679.581787109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1216.9578857421875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 968.3878784179688 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1694.293701171875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 916.7786865234375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 902.1306762695312 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 745.0396118164062 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1181.9293212890625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2073.4873046875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 3419.384765625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 982.1222534179688 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1008.7018432617188 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 2154.14892578125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1005.0509033203125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 720.1404418945312 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 939.4847412109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1705.7188720703125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 2096.353271484375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1560.0360107421875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 849.5489501953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 651.3394775390625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 8719.89453125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 2452.658203125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 994.8143310546875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1641.202392578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 815.1903076171875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 4714.490234375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1191.69677734375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1351.77001953125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1333.985595703125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1346.1282958984375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1010.8556518554688 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1153.11767578125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1511.3387451171875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1733.158935546875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1310.1981201171875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1992.013916015625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1054.4207763671875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 3509.34033203125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 3245.5166015625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1516.1055908203125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 874.6221313476562 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 840.3653564453125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1421.23583984375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1569.513427734375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 901.6498413085938 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 821.5924682617188 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1755.9610595703125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 798.3407592773438 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 920.3735961914062 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 921.6552124023438 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 911.341064453125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1051.31689453125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1392.5693359375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 931.4902954101562 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1017.3001098632812 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 860.480224609375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1159.525390625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1091.384033203125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 976.6549682617188 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 772.6597900390625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2499.554931640625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 719.1637573242188 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 738.049560546875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 796.7279663085938 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1178.3966064453125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1427.4725341796875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1407.9359130859375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 790.206298828125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1402.9141845703125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 904.3031005859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1222.393798828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1296.7464599609375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1166.828125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 900.2122192382812 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1038.3756103515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 2865.3720703125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1429.4178466796875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1152.9183349609375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1250.40673828125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1549.8162841796875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1235.96630859375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 658.937255859375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 796.2315063476562 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 963.498779296875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 2347.598388671875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1798.853271484375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 3360.9970703125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 965.1270141601562 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 956.9140625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1468.0020751953125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1497.7264404296875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1930.772705078125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1419.2119140625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 889.6922607421875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1384.2261962890625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1483.6658935546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 866.9267578125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1323.0418701171875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2915.326904296875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 795.1110229492188 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1381.96484375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1330.9853515625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1471.953125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1494.7315673828125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1265.827392578125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 912.69287109375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 981.1854858398438 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 862.8771362304688 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1110.5919189453125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1711.3995361328125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 907.50927734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1072.157958984375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 704.9178466796875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1269.7607421875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 779.9134521484375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 1477.6153564453125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1683.262451171875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 555.4287719726562 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 741.7930908203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 2204.427490234375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1103.9957275390625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 2855.40673828125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 903.722900390625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1342.108642578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 987.1395263671875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1392.8763427734375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1022.717529296875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3274.975830078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1388.319580078125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1202.4031982421875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 2279.238037109375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1466.61962890625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 1422.246337890625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1066.0670166015625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1832.736328125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1053.311767578125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1032.3984375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 982.32080078125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1695.879638671875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1498.1732177734375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2400.11962890625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1739.657470703125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1726.7481689453125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1049.1812744140625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1418.713134765625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1920.322265625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1887.6536865234375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1196.1505126953125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 954.4096069335938 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1441.733154296875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:27:45<105590:34:26, 1267.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 967.9889526367188 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1657.6190185546875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 2460.54638671875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1486.33203125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1297.356201171875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1888.7569580078125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 8103.541015625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 741.6808471679688 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 714.9130249023438 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1597.3013916015625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1741.596923828125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 597.6911010742188 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 578.7215576171875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 992.544677734375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 996.8966674804688 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1797.9044189453125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 877.791259765625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 698.4531860351562 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 959.2302856445312 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1496.1044921875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1114.1593017578125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 895.1055297851562 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1036.7261962890625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 889.4019165039062 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1071.5877685546875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1044.6563720703125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2423.054443359375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 3212.09814453125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1085.783203125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 1301.9322509765625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2026.42724609375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1759.5848388671875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1720.6513671875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1374.453125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2308.307861328125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 785.135498046875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 496.57025146484375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 830.2232055664062 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1118.839599609375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 937.1206665039062 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 989.2242431640625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1148.6109619140625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 961.9535522460938 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 973.5025634765625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1487.1416015625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 14256.0869140625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 4077.81103515625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 764.2704467773438 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1117.3570556640625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2633.64013671875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1192.651611328125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 2751.528564453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1597.88232421875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1387.147216796875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1166.2269287109375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 611.1776123046875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2462.16064453125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 734.7921752929688 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1737.58154296875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 2162.019287109375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1324.840087890625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 605.6820068359375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 851.0018310546875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 728.25146484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1232.7884521484375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1208.4996337890625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 782.637451171875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 986.4630126953125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1465.9267578125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 5027.865234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1177.079345703125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1244.23828125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 966.0055541992188 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 2868.348876953125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1074.5875244140625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 991.4041137695312 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1204.3115234375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1323.915283203125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1555.374755859375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 2335.8359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1727.47802734375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1365.7041015625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1881.700439453125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 11178.521484375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 846.7894287109375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 783.7461547851562 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1116.3209228515625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1600.0281982421875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1707.0509033203125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1835.5550537109375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1190.1951904296875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3360.30908203125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1184.1510009765625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1652.6116943359375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1423.669677734375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 854.2640991210938 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 776.3240966796875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1043.4906005859375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 929.57958984375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1921.7916259765625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 910.9201049804688 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1531.663818359375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 1333.220458984375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1096.5279541015625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 2051.040771484375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1670.6636962890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 679.1582641601562 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1062.1708984375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1436.3765869140625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 728.7296142578125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1672.4022216796875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1055.397705078125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1457.65234375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1802.1064453125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1005.6231689453125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1037.0941162109375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1270.4161376953125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1051.6148681640625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1972.1588134765625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 734.998291015625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 886.8467407226562 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1414.8546142578125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1614.8602294921875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 4782.29638671875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1725.868896484375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 868.8917846679688 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 2213.05859375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1240.3470458984375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1911.99609375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 943.5665893554688 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2068.970458984375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1935.07177734375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 825.6193237304688 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1564.7012939453125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 3651.44677734375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 2414.74072265625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 942.5271606445312 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 926.69970703125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 904.3182983398438 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1453.317626953125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1502.5872802734375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1315.7462158203125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1252.116943359375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1048.3892822265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 936.5245971679688 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 939.2576293945312 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 2915.02685546875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: -67266.7109375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1588.2352294921875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2467.793212890625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1330.7752685546875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1805.156982421875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1397.772705078125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1143.6953125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1505.8150634765625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1075.040283203125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1157.4267578125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 713.364501953125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 546.5512084960938 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1616.9197998046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2812.645751953125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1262.8162841796875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 655.594482421875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 854.9681396484375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1296.0538330078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 796.8525390625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 898.5757446289062 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 922.0801391601562 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1557.5003662109375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1510.4569091796875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1053.916015625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1517.861083984375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2052.807373046875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1187.6551513671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1230.58203125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 2605.42626953125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 610.2487182617188 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1568.20458984375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1326.1131591796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1218.453369140625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 956.3607177734375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1337.3682861328125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1402.7432861328125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1162.8414306640625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1334.132568359375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 817.7276000976562 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 3260.435546875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 10076.49609375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1350.6846923828125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 2338.542724609375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1277.8720703125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1199.591552734375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 636.9894409179688 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 894.1445922851562 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1035.6690673828125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1120.63134765625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1459.68798828125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 782.0480346679688 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 956.3076782226562 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 967.5388793945312 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 518.1884765625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1571.5032958984375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1453.5098876953125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 933.7825317382812 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 944.2190551757812 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 971.9327392578125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 825.734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 853.95068359375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 760.8399047851562 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1286.4296875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 2340.066650390625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 865.058349609375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 628.7074584960938 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1785.5865478515625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1515.0946044921875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 2909.591064453125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 746.1345825195312 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 702.987548828125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1536.74267578125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 2312.95458984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 2111.904296875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1572.1982421875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 977.8093872070312 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1523.4759521484375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1061.7806396484375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 718.0179443359375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 2244.201904296875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1178.401123046875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1142.658935546875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 651.680419921875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 1152.1973876953125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1025.173095703125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 3520.396728515625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1409.87841796875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1480.039306640625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1703.135986328125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1366.3729248046875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1757.1439208984375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1704.6495361328125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1027.866943359375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 569.1434936523438 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1617.6534423828125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1090.8131103515625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1633.5390625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1947.06982421875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1772.82568359375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 3059.31396484375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 722.659423828125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 724.0801391601562 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1928.4127197265625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1759.0855712890625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1169.125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1237.0657958984375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1667.124267578125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 946.4391479492188 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 796.5834350585938 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 601.288818359375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 974.7509765625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1192.988037109375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 924.8580322265625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1142.51416015625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1607.61767578125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1046.3072509765625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1723.177001953125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 965.6718139648438 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1050.5396728515625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1365.4849853515625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1189.0772705078125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 2212.72509765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 5686.71435546875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 971.3748168945312 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 798.262451171875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 930.3511352539062 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 924.2298583984375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 968.3650512695312 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 2189.837890625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 4313.486328125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1804.6241455078125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 957.9290161132812 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 717.78076171875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 996.5848388671875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 987.473876953125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1533.4884033203125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1060.095947265625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1063.396484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1676.629150390625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 665.8355102539062 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1054.7044677734375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 2948.84375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 2940.359375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1094.8048095703125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 782.5986938476562 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2237.895263671875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1376.3272705078125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 892.661865234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 979.207763671875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 999.526611328125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 789.6654052734375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1737.8499755859375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 4112.96728515625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1595.4805908203125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 810.6373291015625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1695.353515625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1193.13232421875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1203.196044921875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 763.661865234375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1951.55712890625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1563.5919189453125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 3010.715576171875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1565.719970703125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1396.99951171875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1071.3214111328125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1602.843017578125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1215.7275390625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 871.1153564453125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1128.1614990234375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1090.6319580078125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1896.4237060546875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1387.2584228515625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1350.72265625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1366.660888671875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1042.1033935546875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 755.1279296875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 982.0447387695312 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7419, loss_val: nan, pos_over_neg: 1671.5582275390625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1319.0609130859375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 23932.693359375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1879.0540771484375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 942.6226806640625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1036.207275390625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1098.2103271484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1528.7684326171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1067.318603515625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1576.816650390625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1617.323974609375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1305.5242919921875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1508.80712890625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1039.7332763671875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 2217.180908203125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1318.0740966796875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 978.777099609375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1209.5936279296875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 5679.4677734375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 4733.36962890625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1405.0601806640625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1032.8258056640625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1284.90869140625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1181.0543212890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 2208.29296875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 877.397216796875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 977.7403564453125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 982.9894409179688 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 2299.545166015625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 2278.509765625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1379.498291015625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1673.4112548828125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 2970.76171875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2107.1328125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1925.1087646484375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 2574.254150390625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 2210.759033203125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1858.929443359375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1648.2120361328125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1672.095458984375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 2341.79052734375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1111.463623046875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1423.5126953125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1397.1336669921875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 3788.34521484375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1290.9573974609375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1062.309814453125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1249.5574951171875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1924.8289794921875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 765.71875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1064.512451171875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 733.65625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 3660.015625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1707.462158203125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 514.9143676757812 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2012.6705322265625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1548.2789306640625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 504.9223327636719 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1150.019287109375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 825.342529296875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2109.089599609375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1097.529296875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 433.7658996582031 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 931.588623046875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1307.1163330078125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2444.578369140625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1190.1424560546875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1435.936279296875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1869.4661865234375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1048.1156005859375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1405.1015625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1908.80859375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1521.57666015625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1839.5880126953125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 963.7244873046875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1256.3424072265625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2467.31201171875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 10503.9140625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7434, loss_val: nan, pos_over_neg: 7949.44482421875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 957.4435424804688 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2102.126220703125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 12921.025390625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1425.1102294921875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 3185.957275390625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 2289.865966796875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1305.400390625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1787.61279296875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1783.36279296875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 2449.099853515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1206.3477783203125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1597.34326171875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1167.403564453125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1857.8851318359375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1761.4884033203125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 3281.494873046875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1158.1241455078125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7377, loss_val: nan, pos_over_neg: 2107.805419921875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 2855.52099609375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1232.0177001953125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7409, loss_val: nan, pos_over_neg: 5290.86962890625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 948.934814453125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1809.8367919921875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1470.002685546875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1009.8067626953125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 992.1978759765625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1160.1656494140625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 2424.80224609375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 22343.849609375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1712.6484375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 969.4779052734375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1093.1005859375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1637.1328125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1247.54541015625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1080.5616455078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 667.2026977539062 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1242.598388671875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1687.926025390625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1160.5616455078125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1241.7789306640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 2337.80908203125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1033.117431640625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1697.8192138671875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1142.848876953125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 957.0098266601562 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 10442.294921875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 4815.2802734375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1566.0919189453125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1788.0245361328125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7412, loss_val: nan, pos_over_neg: 9963.0830078125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1157.8343505859375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 2377.981201171875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 21773.728515625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 3996.17236328125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 704.5606079101562 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1316.0052490234375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1250.04345703125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1359.30224609375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1616.0413818359375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 980.631103515625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 726.831787109375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 3216.251220703125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1161.1834716796875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 2000.7452392578125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1842.9300537109375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1292.2020263671875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 882.5769653320312 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1178.6680908203125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1214.600341796875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1217.2200927734375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1936.9530029296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 6134.14404296875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 1432.10986328125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 548.3812255859375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1669.178466796875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: -9499.94140625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1569.0439453125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 936.4603271484375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 974.3214721679688 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1054.138916015625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 2413.247314453125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1098.5740966796875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 994.41455078125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1527.3671875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1182.543701171875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1904.2745361328125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1136.861572265625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1402.4376220703125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 935.0045776367188 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1107.814697265625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1285.494384765625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 3812.542724609375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1776.57470703125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1158.8255615234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 843.29248046875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1062.6153564453125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 803.4630126953125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1213.0906982421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 3445.44970703125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1274.71630859375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1630.46142578125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1311.6572265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1503.0892333984375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 2011.5322265625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1178.6451416015625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1280.590576171875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1239.2841796875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 750.5223388671875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 964.1969604492188 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1370.39013671875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 954.51953125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1206.5791015625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1228.281982421875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 906.5855102539062 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 701.4729614257812 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 813.8690795898438 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1115.3843994140625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1269.2169189453125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1228.123291015625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1005.4605712890625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 903.7487182617188 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 767.4508666992188 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1370.70068359375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 852.248779296875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 766.9859008789062 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 644.9713134765625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1352.46826171875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1415.444091796875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 2006.7720947265625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 849.7510986328125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 832.57861328125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1007.173095703125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1175.032470703125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 937.1153564453125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1750.1253662109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2240.600830078125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 3814.162841796875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1559.3944091796875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 828.7543334960938 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 656.4244384765625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1605.486328125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 2536.178955078125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 7013.419921875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 988.2349243164062 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1042.5367431640625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 721.9896240234375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 974.2037963867188 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1805.9937744140625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1681.1053466796875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 2282.3203125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 835.278564453125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1057.047119140625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 4815.31201171875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 866.6065673828125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1022.1168823242188 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 2031.032958984375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 2952.94189453125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1794.9244384765625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1922.2354736328125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 2815.790771484375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1583.504638671875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1432.6435546875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1955.52099609375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1985.14697265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 7343.49072265625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1034.2073974609375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 736.397705078125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1505.8624267578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 3082.027587890625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1001.3018188476562 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 612.6395263671875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 817.7188720703125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 845.9335327148438 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1083.1693115234375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 915.936767578125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1165.084228515625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1192.8048095703125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1310.9534912109375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1115.7364501953125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1200.3236083984375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1062.249755859375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1670.335693359375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1128.2471923828125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 672.021484375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1389.3765869140625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1901.376220703125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 739.84375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1218.6553955078125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 865.142578125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1743.59228515625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 3419.69287109375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2822.12451171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 2049.764892578125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 2060.1044921875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 791.7069702148438 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 2043.8590087890625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 2247.971923828125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 933.7190551757812 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1424.8944091796875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1186.998779296875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1030.3472900390625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 2019.0003662109375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 970.445068359375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1024.5665283203125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2547.621337890625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1773.3006591796875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 22958.0546875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1009.2500610351562 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1491.259521484375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1916.2664794921875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1929.8203125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1774.364990234375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1101.726806640625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1037.8118896484375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 921.4912719726562 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 4070.13134765625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 847.2189331054688 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1354.7840576171875 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "#     augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "    \n",
    "    augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "                             stds=[0.229, 0.224, 0.225]),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "#                                                     pin_memory=True,\n",
    "#                                                     num_workers=32,\n",
    "#                                                     persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "#                                                     pin_memory=True,\n",
    "#                                                     num_workers=32,\n",
    "#                                                     persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(0)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
