{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUEklEQVR4nO3df6xf9X3f8ecrhhArgYaIC3JsJ0aZ080g1RTLo0LasiQrLqlm0g7JbAWkMpkxkMjWboL80+QPb6nWJB3aYCMDYbYsrqUkw0pCWpcmy9AI5po5GGNYrOBgxx52ShFGmlht3vvjflC/u/na9/r++N5wP8+HdPQ9530+n3M+R5Zf9+jzPd/vN1WFJKkP71joAUiSRsfQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKGvRS/JwSQfn2bbSvLXZnieGfeVRsXQlxZQkmVJdiQ50v5orFroMWlxM/SlhfUm8G3gNxd6IOqDoa+uJFmf5IkkryY5muTfJnnnpGbXJvlRkp8m+ddJ3jHQ/7eT7E/yF0n+OMkHT3Oea5M8l+REkp8k+d1h7arq5aq6F3hq7q5SOj1DX705BfxT4CLgV4CPAf9kUptPAuuAXwY2Ar8NkOQ64NPAbwBjwH8HvnKa8zwA3FpV5wOXA382lxchzZShr65U1e6q+n5Vnayqg8B/AP72pGa/X1WvVNVLwB8CN7T6rcC/qqr9VXUS+JfA2tPc7f8lsCbJBVX1F1X19LxckHSWDH11JcmHk3wjyf9O8hoTwX3RpGaHBtZ/DLy/rX8Q+DdtauhV4BUgwPIhp/pN4Frgx0n+W5JfmcvrkGbK0Fdv7gOeB1ZX1QVMTNdkUpuVA+sfAI609UNMTNm8d2BZWlX/Y/JJquqpqtoIXAz8V2D7HF+HNCOGvnpzPvAa8HqSvw7cNqTNP09yYZKVwJ3AH7X6vwfuTnIZQJJfSHL95M5J3pnkHyb5har6y3a+U6cbUJJ3Aee1zfPatjQvDH315neBfwCcAL7EXwX6oEeA3cAe4JtMvClLVX0d+H1gW5saehb4tdOc50bgYGv3j4HfOsOY/g/welt/vm1L8yL+iIok9cM7fUnqiKEvSR0x9CWpI4a+JHXknIUewFQuuuiiWrVq1UIPQ5LeVnbv3v3TqhqbXP+5D/1Vq1YxPj6+0MOQpLeVJD8eVnd6R5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOvJz/4lcSXq7WnXXN2fc9+DnPjGHI/kr3ulLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNThn6SdyXZleQHSfYl+WyrfybJT5Lsacu1A33uTnIgyQtJrhmoX5lkb9t3T5LMz2VJkoaZzrdsvgF8tKpeT3Iu8HiSR9u+L1bVHww2TrIG2ARcBrwf+NMkH66qU8B9wGbg+8C3gA3Ao0iSRmLKO/2a8HrbPLctdYYuG4FtVfVGVb0IHADWJ1kGXFBVT1RVAQ8D181q9JKkszKtOf0kS5LsAY4BO6vqybbrjiTPJHkwyYWtthw4NND9cKstb+uT68POtznJeJLx48ePT/9qJElnNK3Qr6pTVbUWWMHEXfvlTEzVfAhYCxwFPt+aD5unrzPUh53v/qpaV1XrxsbGpjNESdI0nNXTO1X1KvBdYENVvdz+GLwJfAlY35odBlYOdFsBHGn1FUPqkqQRmc7TO2NJ3tvWlwIfB55vc/Rv+STwbFvfAWxKcl6SS4HVwK6qOgqcSHJVe2rnJuCRubsUSdJUpvP0zjJga5IlTPyR2F5V30jyn5KsZWKK5iBwK0BV7UuyHXgOOAnc3p7cAbgNeAhYysRTOz65I0kjNGXoV9UzwBVD6jeeoc8WYMuQ+jhw+VmOUZI0R/xEriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjkznh9HflWRXkh8k2Zfks63+viQ7k/ywvV440OfuJAeSvJDkmoH6lUn2tn33tB9IlySNyHTu9N8APlpVvwSsBTYkuQq4C3isqlYDj7VtkqwBNgGXARuAe9uPqgPcB2wGVrdlw9xdiiRpKlOGfk14vW2e25YCNgJbW30rcF1b3whsq6o3qupF4ACwPsky4IKqeqKqCnh4oI8kaQSmNaefZEmSPcAxYGdVPQlcUlVHAdrrxa35cuDQQPfDrba8rU+uS5JGZFqhX1WnqmotsIKJu/bLz9B82Dx9naH+swdINicZTzJ+/Pjx6QxRkjQNZ/X0TlW9CnyXibn4l9uUDe31WGt2GFg50G0FcKTVVwypDzvP/VW1rqrWjY2Nnc0QJUlnMJ2nd8aSvLetLwU+DjwP7ABubs1uBh5p6zuATUnOS3IpE2/Y7mpTQCeSXNWe2rlpoI8kaQTOmUabZcDW9gTOO4DtVfWNJE8A25PcArwEXA9QVfuSbAeeA04Ct1fVqXas24CHgKXAo22RJI3IlKFfVc8AVwyp/znwsdP02QJsGVIfB870foAkaR75iVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI1OGfpKVSb6TZH+SfUnubPXPJPlJkj1tuXagz91JDiR5Ick1A/Urk+xt++5Jkvm5LEnSMFP+MDpwEvidqno6yfnA7iQ7274vVtUfDDZOsgbYBFwGvB/40yQfrqpTwH3AZuD7wLeADcCjc3MpkqSpTHmnX1VHq+rptn4C2A8sP0OXjcC2qnqjql4EDgDrkywDLqiqJ6qqgIeB62Z7AZKk6TurOf0kq4ArgCdb6Y4kzyR5MMmFrbYcODTQ7XCrLW/rk+vDzrM5yXiS8ePHj5/NECVJZzDt0E/yHuCrwKeq6jUmpmo+BKwFjgKff6vpkO51hvrPFqvur6p1VbVubGxsukOUJE1hWqGf5FwmAv/LVfU1gKp6uapOVdWbwJeA9a35YWDlQPcVwJFWXzGkLkkakek8vRPgAWB/VX1hoL5soNkngWfb+g5gU5LzklwKrAZ2VdVR4ESSq9oxbwIemaPrkCRNw3Se3rkauBHYm2RPq30auCHJWiamaA4CtwJU1b4k24HnmHjy5/b25A7AbcBDwFImntrxyR1JGqEpQ7+qHmf4fPy3ztBnC7BlSH0cuPxsBihJmjt+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkem88PoK5N8J8n+JPuS3Nnq70uyM8kP2+uFA33uTnIgyQtJrhmoX5lkb9t3T/uBdEnSiEznTv8k8DtV9TeAq4Dbk6wB7gIeq6rVwGNtm7ZvE3AZsAG4N8mSdqz7gM3A6rZsmMNrkSRNYcrQr6qjVfV0Wz8B7AeWAxuBra3ZVuC6tr4R2FZVb1TVi8ABYH2SZcAFVfVEVRXw8EAfSdIInNWcfpJVwBXAk8AlVXUUJv4wABe3ZsuBQwPdDrfa8rY+uT7sPJuTjCcZP378+NkMUZJ0BtMO/STvAb4KfKqqXjtT0yG1OkP9Z4tV91fVuqpaNzY2Nt0hSpKmMK3QT3IuE4H/5ar6Wiu/3KZsaK/HWv0wsHKg+wrgSKuvGFKXJI3IdJ7eCfAAsL+qvjCwawdwc1u/GXhkoL4pyXlJLmXiDdtdbQroRJKr2jFvGugjSRqBc6bR5mrgRmBvkj2t9mngc8D2JLcALwHXA1TVviTbgeeYePLn9qo61frdBjwELAUebYskaUSmDP2qepzh8/EAHztNny3AliH1ceDysxmgJGnu+IlcSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdmc4Poz+Y5FiSZwdqn0nykyR72nLtwL67kxxI8kKSawbqVybZ2/bd034cXZI0QtO5038I2DCk/sWqWtuWbwEkWQNsAi5rfe5NsqS1vw/YDKxuy7BjSpLm0ZShX1XfA16Z5vE2Atuq6o2qehE4AKxPsgy4oKqeqKoCHgaum+GYJUkzNJs5/TuSPNOmfy5steXAoYE2h1tteVufXB8qyeYk40nGjx8/PoshSpIGzTT07wM+BKwFjgKfb/Vh8/R1hvpQVXV/Va2rqnVjY2MzHKIkabIZhX5VvVxVp6rqTeBLwPq26zCwcqDpCuBIq68YUpckjdCMQr/N0b/lk8BbT/bsADYlOS/JpUy8Yburqo4CJ5Jc1Z7auQl4ZBbjliTNwDlTNUjyFeAjwEVJDgO/B3wkyVompmgOArcCVNW+JNuB54CTwO1Vdaod6jYmngRaCjzaFknSCE0Z+lV1w5DyA2dovwXYMqQ+Dlx+VqOTJM0pP5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjU4Z+kgeTHEvy7EDtfUl2Jvlhe71wYN/dSQ4keSHJNQP1K5PsbfvuaT+QLkkaoenc6T8EbJhUuwt4rKpWA4+1bZKsATYBl7U+9yZZ0vrcB2wGVrdl8jElSfNsytCvqu8Br0wqbwS2tvWtwHUD9W1V9UZVvQgcANYnWQZcUFVPVFUBDw/0kSSNyEzn9C+pqqMA7fXiVl8OHBpod7jVlrf1yfWhkmxOMp5k/Pjx4zMcoiRpsrl+I3fYPH2doT5UVd1fVeuqat3Y2NicDU6SejfT0H+5TdnQXo+1+mFg5UC7FcCRVl8xpC5JGqGZhv4O4Oa2fjPwyEB9U5LzklzKxBu2u9oU0IkkV7Wndm4a6CNJGpFzpmqQ5CvAR4CLkhwGfg/4HLA9yS3AS8D1AFW1L8l24DngJHB7VZ1qh7qNiSeBlgKPtkWSNEJThn5V3XCaXR87TfstwJYh9XHg8rManSRpTvmJXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHZlV6Cc5mGRvkj1JxlvtfUl2Jvlhe71woP3dSQ4keSHJNbMdvCTp7MzFnf7fqaq1VbWubd8FPFZVq4HH2jZJ1gCbgMuADcC9SZbMwfklSdM0H9M7G4GtbX0rcN1AfVtVvVFVLwIHgPXzcH5J0mnMNvQL+JMku5NsbrVLquooQHu9uNWXA4cG+h5uNUnSiJwzy/5XV9WRJBcDO5M8f4a2GVKroQ0n/oBsBvjABz4wyyFKkt4yqzv9qjrSXo8BX2diuublJMsA2uux1vwwsHKg+wrgyGmOe39VrauqdWNjY7MZoiRpwIxDP8m7k5z/1jrwq8CzwA7g5tbsZuCRtr4D2JTkvCSXAquBXTM9vyTp7M1meucS4OtJ3jrOf6mqbyd5Ctie5BbgJeB6gKral2Q78BxwEri9qk7NavSSpLMy49Cvqh8BvzSk/ufAx07TZwuwZabnlNSnVXd9c1b9D37uE3M0krc/P5ErSR0x9CWpI4a+JHXE0Jekjsz2w1n6OTObN7x8s0ta/LzTl6SOLOo7fe96Jen/552+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjoy8tBPsiHJC0kOJLlr1OeXpJ6NNPSTLAH+HfBrwBrghiRrRjkGSerZqO/01wMHqupHVfV/gW3AxhGPQZK6laoa3cmSvw9sqKp/1LZvBP5mVd0xqd1mYHPb/EXghRme8iLgpzPs+3blNfeht2vu7Xph9tf8waoam1wc9Y+oZEjtZ/7qVNX9wP2zPlkyXlXrZnuctxOvuQ+9XXNv1wvzd82jnt45DKwc2F4BHBnxGCSpW6MO/aeA1UkuTfJOYBOwY8RjkKRujXR6p6pOJrkD+GNgCfBgVe2bx1POeorobchr7kNv19zb9cI8XfNI38iVJC0sP5ErSR0x9CWpI4sy9Hv8qockDyY5luTZhR7LKCRZmeQ7SfYn2ZfkzoUe03xL8q4ku5L8oF3zZxd6TKOSZEmS/5nkGws9llFIcjDJ3iR7kozP6bEX25x++6qH/wX8XSYeEX0KuKGqnlvQgc2zJH8LeB14uKouX+jxzLcky4BlVfV0kvOB3cB1i/nfOUmAd1fV60nOBR4H7qyq7y/w0OZdkn8GrAMuqKpfX+jxzLckB4F1VTXnH0hbjHf6XX7VQ1V9D3hloccxKlV1tKqebusngP3A8oUd1fyqCa+3zXPbsrju2oZIsgL4BPAfF3osi8FiDP3lwKGB7cMs8jDoXZJVwBXAkws8lHnXpjn2AMeAnVW16K8Z+EPgXwBvLvA4RqmAP0myu30tzZxZjKE/ra960OKQ5D3AV4FPVdVrCz2e+VZVp6pqLROfZl+fZFFP5SX5deBYVe1e6LGM2NVV9ctMfCPx7W36dk4sxtD3qx460ea1vwp8uaq+ttDjGaWqehX4LrBhYUcy764G/l6b494GfDTJf17YIc2/qjrSXo8BX2di2npOLMbQ96seOtDe1HwA2F9VX1jo8YxCkrEk723rS4GPA88v6KDmWVXdXVUrqmoVE/+X/6yqfmuBhzWvkry7PZxAkncDvwrM2VN5iy70q+ok8NZXPewHts/zVz38XEjyFeAJ4BeTHE5yy0KPaZ5dDdzIxJ3fnrZcu9CDmmfLgO8keYaJm5udVdXFI4yduQR4PMkPgF3AN6vq23N18EX3yKYk6fQW3Z2+JOn0DH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkf8HaHzVE381SwwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAADnCAYAAACjZ7WjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABZvElEQVR4nO29aZAc533f/+me7rnvY+97sbu470PgARK8SZGOJeosWYot20qcSpVjv8gbVexKOalU5UVS+SuJK6mKneiILEuMSJEUKR7gARA3QAALYLEL7H3Mzu7M7NzHzvV/AU+blHDtcqYXAp5PFaooEZz5TvfT3+fs31eqVCoIBAKB4LMhr7UAgUAguBcQZioQCAQ1QJipQCAQ1ABhpgKBQFADhJkKBAJBDVBu9S8lSarZVn+lUpFW+98KHUKH0PHbqeV+0iFGpgKBQFADhJkKBAJBDRBmKhAIBDVAmKlAIBDUgHvGTCVJQpZlJOkzrdt/Zg2KoqAoyprquFtQFAWTyYQsr30zk2UZg8GwphpUVcXtdmOxWO6K9iFJ0prrcDgc2O32NdXwST5LW73lbv4df4iiUCqV0Ps9f1mWsdlseDwe7HY7RqORTCbDxMQEy8vLumoxGo2YzWYURaFcLlMoFMhms5TLZV113C34fD56e3vxer1Eo1HGxsYIh8O661BVlUAgQH9/P3a7naNHjxKNRnXX4XQ6OXjwIM3NzaTTaQYHB7ly5Qq5XE53LbIsY7VacbvdVCoVYrEY6XRadx0DAwM8++yzmEwmXnrpJcbHxymVSrrrgOvXxO124/f7SafThMNh8vn8ij5j1WZqNBpxOBy43W4CgQCZTIbh4eEVC/gsmM1m9u3bR29vL4lEglwuh8lkoqmpibNnz5JKpXTRYTKZaG9vp7u7m+XlZebn50kkEqiqSiqVolgs6qIDwOv10tTUhKqqjI6O6nYNPonFYmHPnj18/vOfZ/v27cRiMX75y1/ywx/+kGQyqZsOm83GAw88wIYNG9i6dSsulwuPx8OhQ4eYm5vTtfPfs2cPe/fuxWg0srCwQEdHBx6Phw8++EDXDldVVVwuF01NTbS0tGCz2ZienmZoaEg3QzUajWzcuJEXX3yRz33uc/T09PDggw/yox/9iPfee4+FhQVddBgMBjZs2IDdbqdSqaAoivbPIyMjTE5OrqiNrMpM/X4/27Ztw+/343K5NBP56U9/yuXLl3VrHG63m23bthEIBLh8+TKxWIxSqcTGjRsplUqcPHmSQqFQVw2SJBEIBHj22Wf51re+xbFjx3jttdeYmJigUqlQKBR0HbUrikJ3dzdOpxOLxcLMzAzBYFDXHt9ut7NhwwZaWlpQFIUtW7bQ1NTE4cOHuXjxom46PB4Pe/fuJZPJcOXKFZqamujv76dYLPLuu+/q9tACtLe3I8sylUoFo9GI1WrFarXq9v1w3cQ8Hg8ej4dAIEBnZyddXV00NjaSyWRYWFggk8mQyWTqqkOWZfx+P4lEgmPHjjEyMsLAwADf+ta3CAQCvPLKK0xPT9dVA1yfPe3cuZPl5WVCoRCFQoF8Po/b7WbDhg3k83kWFhbu+Nm5rZmqqoosy5TLZWRZxmw2s337dnp7ezGbzfj9fvr7+2lubiYcDjM1NUUikfjMP/R2SJKEx+Nhw4YNNDQ0EI1GiUQiJBIJLBYLgUAAm81GLBaru5ZSqUQoFOL8+fMEg0HMZjMWi4VUKqX7mpTT6eTBBx/kkUce4cSJE5w9e5bBwUEGBwd16+RMJhMmk4mRkREuX75MV1cX69atw+/3I0mSLh2LJEl0dXXR0tLCzMwMAJVKhUAgwMaNG/n44491NVNJknA6nRSLRaxWK9FolHw+r2v7UFUVRVEwGAw4HA5aWlo4ePAgXq8XSZJ4++23626kAIVCgWg0yvj4OKqq4nQ6icVi2Gw2/H4/69evJxwOk81m66rDaDTicrmoVCrk83ntt9tsNjo7O8nlciwvLxOJRO7o825rpr8+RS2VSszMzODxeHC73RQKBYxGI21tbXR0dOByuXQx06oWWZbZu3cvhUKBZDLJ1NQU2WyW+fl53abX2WyW8+fPE4lEMBgM2kNSqVR0X0tubW3FbreTyWTw+Xx4vV7cbrfWIepBPp/XRsTZbJaFhQWmpqZ0X3JYXFxkeXmZffv2kUgkcDgceDweksmkrssNAAsLC2zatIlCocD7779PPB7XfcaQy+Uol8uUSiWy2SzRaJRUKsXy8jKzs7PMz8/rYqalUon5+Xk6OjoolUoYDAbtXsViMQwGA36/n9nZ2bq22cXFRebn53niiSfYtm2bdk2KxaI2Kl3J+vptzfTXjaBYLDI9Pc26des09w6FQly6dIkTJ04Qj8dX/qtWQaVSYW5ujldeeQVVVens7OTAgQMcP36cI0eOcOnSJV0W9yuVCrlcjkQiweTkJIpy/ZJWpwx6rpcCnD17litXruByubDb7czPz5NKpXR/aKemprDZbBgMBhYWFkilUszPz+vWsVQqFYaHh/nZz37Gv/yX/5J9+/ZhNpu5fPkyH3zwAaFQSBcdVYaGhojH47S3t2M2m4lEIszNzemqoVQqEYvFMBqNpNNp5ufn+fnPf87c3BzHjx/XtbOLxWJMT0/T3t6OxWIhn8+TTqfJZrNkMhnNZOtppvl8nuPHj9PU1MSLL75IT08PV65cYXR0lKNHjxIOh1fUXqVb/eVbvc+6fv16uru7KRaLeL1eisUi77///k2HxPV4r1aSJMxmMy6Xix07dtDX18eZM2c4derUTXfz66GjeqrAbDZrF79UKpHL5cjn8zdsEHf7e8afRYeiKPT39+P3+7HZbNjtdmKxGB988IGu9wX+cZPhW9/6Fk1NTfziF7/g9ddfv+kUsl46VFXld3/3d3n++eeZmZnhBz/4AVeuXLnpZ9Xz3XyTyaSdgKnO6OLx+E073Hrem+oOevXIWCqVYm5ujlAo9Bt66qXDZDKxZcsWHn74YbLZLENDQ5w5c+amncvNdKzaTD95prN6Xq1Sqdx0JFZv8zAYDJjN5tseR6qXDlmWUVWVSqWiTe/XQsdKqZcOp9OJ2WzGYDAgSRL5fP6Wa0/1vh7V866FQuGWs4V66ujt7eWBBx4gGAxy5MiRW86c6l3opHomWpIkCoXCLUdgerTVT/pJuVy+oZ5661BVFUmSKBaLq3p2V22mK+VeNw+hQ+i4nQ5FUTCbzRSLxdsuQYmqUb99OmpyaF8gENyeYrG4Jud+Bfqw9u/5CQQCwT2AMFOBQCCoAcJMBQKBoAbccgNKIBAIBHeGGJkKBAJBDRBmKhAIBDVApJMKHULHPabjbtJyP+kQI1OBQCCoAcJMBQKBoAYIMxUIBPc1kiRhMpk+8+fcU2Z6NwSECT6NuB+Cu51KpbL2ZlrNf1rr1Ee4XjWqubkZt9u9ZhokScJgMNwVaZx3Aw6HA5fLJa7HJ6hW0QL9OxqDwaDVDxWd3Kepxeh01YVOPB4PTzzxBP39/bz99ttcuHBhTZIWPxnB0NnZydzcHMlkUreizJIkaUWQm5ubaW9vJx6PMzMzw+LiYt0zqG6kR1VVAK0k4lq8mOHxeNi9ezd2u52xsTGuXr2qSxX3GyHLspY0sFZpsR6Ph4aGBjo7O4lGoywuLlIsFgkGg7ppMhgMPProo7S2tnL27FmmpqaYm5vTLRnj1zGZTNjtdkwmE+VymaWlJV0DOavY7XY6OztRVZWxsTHi8fiq7smqzLSaA/X1r3+dxx57jI6ODv76r/+a4eHhuue2fBKLxUJfXx8+nw+3243D4aBYLDI3N8fi4qJuOvx+P08//TQ7duygsbERk8nE3Nwcr7/+Om+++aZu16TaOKt1RCuVCplMhnQ6rWtH5/f7+dznPqfVl92yZQtdXV28++67ulZNqna0FotFCzfMZrNafIdeqKrKgQMH6OnpQVEU8vk8lUqFM2fOMD8/r5sOSZLw+Xz84R/+Id/5zncYHx9ncnKSN998k3feeUfXCGxJkuju7ub5559n//79zMzM8Ld/+7dcunRJ9wGIx+OhubmZ5uZment7uXjxIiMjIysekK3KTB0OB+vXryebzXLu3Dnsdjs7duwgkUgwMTGhS0O12Wxs27aN3bt3azn1JpMJg8FAIpEglUrpYmKSJNHS0sL69evJ5/OMjIzQ29vLI488QmNjI5FIhFOnTpHL5eo+QjQajbjdbnw+Hw6Hg3K5TDQaJR6PE41GyWazdW+oVquVzZs343a7tcgUVVXZuHEj4+PjDA4O6jJSlmUZj8dDT08PTU1NzM7OagF6BoOBbDar2+zF4/HQ3t5OLpcjEomQzWbp6OigsbERWZZ1i5QpFouMj48TiURQFIVUKkVPTw//7t/9O3p7e/lP/+k/6TYyNJlMrFu3jl27dtHT00NHRwcXL15kenr6jgPsaoHRaMRkMpHP50mlUjQ2NvLUU0/hcDj4+OOPb5oMcSNWbKaSJOH1eqlUKhw7dgy4fpOWl5dpbm4mEonokgNVLBaRZZne3l78fj+RSIR8Pk8sFiMajXLt2jVdzNRisWC32zl//rw2xVYUhc2bN3PgwAGi0SjlclnLpLpZjMlnpZryuHHjRjZv3kyxWGR+fp5yuUyxWMRgMFAqlZiYmKj5d1cxGAw0NDTQ1tbG8vIyJpOJUqmk5fu0t7dz+fJlXUxMlmXsdjsvvPACzzzzDN/73vdIJpMsLy9TLBZRVVW3sEOPx8PGjRuJRCKEw2EKhQK5XA6r1arr2mWpVOLq1avMzMyQzWYJBoMYDAbS6TRbtmzB5XLplthaXUcPh8McO3YMu92Oy+XC7XYTjUZ1W5qqxg0Vi0VKpRJGo5HHHnuMnTt38t/+23/j0qVLdzybWrGZVqeO8/Pz2gNaNQebzYbH49ElwC2fz3P16lUWFhZ4+OGH8Xg8ZDIZLZtdj00xSZJobGzEYrEQj8dxuVwYjUaMRiOFQoGPP/6YN998kwsXLpBKpTAYDKiqWpfe32q10tnZyde//nUefvhhfvaznzEyMkIsFtM6uHqPOkqlEslkElVVaW9vp1AoEIvFKJVKWtvw+XxEIpG6G1mxWCQej3Pt2jWGhoa0EYYsyxgMBm0jSK/Y6Q0bNpBIJJiZmdFihZeWlnRfww0Gg5w7dw63262Z6szMjJZiq5eZlstlwuEws7Oz2Gw2xsbGmJmZ0TX4Ef4xfbm6LFYdfOTz+RU/M6ua5kciES3uuRodqygKsViMRCKhWwMJh8McOXKE9evX89BDD2mL2LOzs8Risbp/v9Fo1DLHP5lbYzKZGBoa4qWXXuK1114jnU4D1PW6FItFMpkM77//PleuXGFkZITJyUmi0aiW+qgHS0tLjI6O8tRTT7F582ZCoRAXL15kcnKSy5cvk0wmkSQJo9Go6a6XocXjcQ4dOsTw8LA2eyqVShSLRQqFgm7ttBotvX37dqamprBarSQSCYaGhnQ306WlJc6fP89XvvIV5ufntVGgw+HAarXqpiOfzzM9Pc2RI0dwu91UKhVCoRCRSETXDdPq9a+a59zcHK+99hpjY2NMTEysaFlsVWaayWQYHh4mEAhoD0UulyMYDOq6G1cqlThz5gySJHHx4kVUVeX48eNcunRpRWsdq8FgMGC1WgmHw2SzWWw2G5lMBpvNRjKZJBaLceTIEc1I6006nWZkZISpqSnsdjuKomgmquemYLlc5uOPP+bll1+mXC5jsVi4fPkyb731FlNTU58yD1mWMRqNdZtFlMtlgsEgiUQCk8mkGWomk9HVxCKRCIcPH6a1tZWBgQEcDgeHDx9mbm5uTU4XnDhxgq6uLpxOJy0tLRiNRqLRqK6bg5VKhWAwSC6Xw263k8vliMfjup8IMhqNLC0tkUqlsNlsLC4ukkwmCYfDK9bymQL1qlOmtU44NJvN+Hw+fD4fY2Njt2wUtdJhNpu10U31ZYFPnqdUFOWWm071vB6fTIv9h++66d+tlw6j0aidMV1aWrpl5/YPI/u6FrGork3ebtRTr+uxbt06Hn/8cVRV5dq1a1y4cIFgMFiX9nE7LdW1fbfbTUNDAwAzMzM3nVXWu8DInS631FqHJElYLBbtdMXt0lFvp+OeSietruGutY47Qei4v3TIsozX6yWTyZDNZutq6rfTslLu9XtTKx33VDqp3ovXAsGdUt1wEdy7iPf8BAKBoAYIMxUIBIIaIMxUIBAIaoBIJxUIBIIaIEamAoFAUAOEmQoEAkENEGYqEAgENUBEPQsdQsc9puNu0nI/6RAjU4FAIKgBwkwFAoGgBggzFdQdkRp796Io99Qb5WvKZzJTo9FIS0sLbW1tWCyWWmlaFdXCy2uZhOn3++ns7BSJnP9ANa21mtG11lSrJd0N9+Zu6VzW+r6oqorH48Hr9a6pjk9WwFstq+qWDAYDXq+Xnp4e1q1bh9vtZm5ujnfeeYdkMrlqMavV4nQ6aW1tJRAIMDs7y8jIiK4a4HqjrNaGdLlcpFIp5ubmdK/PqCgKfr8fs9kMXC9MHI/Hdcs7gusPSDWCohrwFw6HWVpa0k1DlUAggMvlwmaz4fV6aWho4Ny5c4yMjOie2irLMi6XS4tz0TP08UZYLJaa5MWvltbWVvbu3Yvb7SYYDHLmzJk1uSZ2ux2/34/dbtcKRK+maNKqzNRmszEwMEBLS4vW2z/00EO4XC5+8Ytf6JZy6Ha78fv92oPS3t6Ow+EgGAzqauqKotDQ0ICqqhQKBYxGI93d3TQ3N/Pxxx/rFnGsKArbt29n//79eDwe1q1bhyRJHDp0iEOHDjE1NaWLgVitVnbt2oXL5SKZTJLJZHC5XCiKopupy7LM1q1b+b3f+z18Ph92u522tjYSiQQ//OEPmZyc1K2jMxqNtLe309raqkXaxONx3UIffx2TyURzczOtra3k83ld46artLa28swzz9DW1sbCwgIOh4Pe3l5isZiu6aRGo5Gmpia6urro6ekhnU7z8ccf65NOKkkSLpeL1tZWLBYL2WyWdDqN3W5n165dvP/++7qYqdlspq2tjZ6eHpxOJ4qikM1mtTjbVCqlayiXw+FAVVVt2tLV1UVjYyPxeJyLFy/qoqOjo4M9e/ZQLBaZnZ3Fbrfz/PPPs3XrVjKZDKFQSBcDURRFS9+cmZlhaWkJi8WCx+PRbeQhyzJdXV1IkkQwGKS9vR2Px0NLSwsdHR26TvWbm5v5yle+QiKR4OLFi2SzWex2O16vl9nZWd10VPF6vXR2dvLUU09RKBQIh8N1DVr8daxWKzt27GD9+vXE43EWFxeJRCI0NTXR09Oj26zBYrHQ3NxMQ0MDW7Zs4Stf+QrhcBiLxUImk2FycnJFOlZlpjabTZsiSJKE0+mkv79f13qNkiQRCAR4/PHHGRgYYGxsjFOnTpHNZmlqaiIUCunW6yuKoq3FWSwWmpqa2LlzJzt37mR4eJiRkZG6x6ioqsrmzZsxmUzMz8+TzWbJZDIUi0W2bNnC8vKybqMPo9HIxo0b2bhxI8eOHaNcLrO8vKxrxlC1ino8HiebzdLQ0IDD4cBsNiNJkq7LHna7nSeffFLLwqq2S7/fvyajQpvNRk9PDwcOHCAcDjMwMMD8/LxuI/VisUggEMDn85HL5TCbzVQqFfL5vK5Bh6qqYrVacTqd2O12VFXVOlu/37/ia7KqdNLqD60ahNvtprW1lYmJCV1inqvfXSgU2LZtG83NzVy9ehVVVbWHqDpq1gNJkjRDtVgs2pSloaEBs9msS1KqyWTCbDZrM4VisUi5XGZ4eJjp6WlOnjxZd0OvYrFY6O3txWq14vP5tJwsv9/P9PS0buZRKBRIJpOUSiXi8TjpdJp0Os3MzIyuBlZNsVVVlaGhIWKxmJaeuhaFhhYXF5mfn+fVV18lmUxSLpexWq26mWmlUqFYLOL1egkEAvT29vLWW29pSbp63ZtUKkU8Hsfv9zM8PMyPfvQjzGazltq60ud2VWa6tLREqVTC6/ViMpno6OhAURRGRkZ0C5ArlUpMTk7y0UcfsX//fkwmE1arVYtS1vNhyWazWg56VUcoFOLs2bMcO3ZMl0aayWQYGxvTRqd2ux2fz0c+n+e9994jGAzWXUOVUqmE1WrF5XJpi/upVIpAIIAsy7rdm2oWerlcJp1OMz4+zsLCAseOHdNtZFodBR8+fJhNmzbR3t6upYImEok1MdNkMsng4CDBYBBFUTQtelEqlTh16hSNjY3s27cPl8tFX18f4+Pjuj631fQDk8lELpdjYWEBRVHIZDKk0+kV35tVZUApisLOnTt59NFH6e3tpa2tjdnZWb7//e9z7NixG+6E1eNVMFVV6e3tZceOHbjdbmKxGJOTk8zPzzMzM3PDkVi9Xklrb29n06ZNtLa24na7iUQiHDlyhLGxMd1CyqxWKx0dHRiNRoxGI1arlampqVuuh9VDh8lk4tFHH2X37t1kMhnC4TBTU1OMjIwwPz9/w0Zaax2yLNPf38+BAwcIBAI0NTXR1tbG97//fX7+85/f9LPqEdpmtVqx2Wxs27YNr9fLxMQEExMTRCKRm5q6Xq+T2mw2crncLXev6/XMtLS08OCDD7J//35UVeX111/n0KFDN51B1UuHxWKhoaFBO95ZKBSIRqMkk8kb3p+aB+oZjUY6OjrYsmULZrOZCxcuMDY2dtOpdT2Dymw2G1arlXK5TCKRuGUmer10VHf0A4EA6XSaSCRCLBbTPZ30k+fk/iH185afVc/7oiiKtv5VXXbQU0d1M/KZZ55hw4YNXL16lZdeeumWJz3q+f539SxjuVyua/DjnWhZCfW8JiaTifb2dtra2hgbG2NqampNdMiy/KkE29W01c+cTmo2m5Fl+baJi3d7kYJa6biThFQ9dNwp94OO6siwUqncdpP0Xrged5OWO9VhNBqpVCq3PBZ1t9+beyrqWegQOoSOu0vL/aRj7d+rEwgEgnsAYaYCgUBQA4SZCgQCQQ0Q6aQCgUBQA8TIVCAQCGqAMFOBQCCoAcJMBQKBoAaIdFKho+4vMaiqetv6BPfL9dBDx92k5W7VUX3jaTVFoMU5U8GaITY5BXcbgUAAp9NZ088UZiqoK3dD3pJA8OtIkkRHR0dNs7hENOE9jJ7l7m6GwWAQhiq46wiFQgBavd1a8JlauSzLmM1mHA6HLgWQ73aq4X5+vx+v17um16SaiLDWfLIaz63+Tq359epZwtB/E0VRMBqN9+W1qVQqhEIhyuUyHo+nJp+5qpGp0WjUco/sdjuyLLOwsMDCwkJNRP02YrFY6O7uZtu2bfh8PsxmM8ePH79pfdd6YrVa6evrQ1VVzp8/r2tA2a9jNptvW4i51p2OLMu43W7tujc2NmK1WllcXCQajeoaYmexWPD7/TQ0NGCz2bSCxOPj4+Tzed10fBJZlnE4HBiNRi2xIp/P695OAZxOJw0NDeTzeUKhkG5pEPCPhe537drF4uLiZw6cXJWZWq1Went7tdFXNSAsEAiwuLhIIpHQLQLB6/WyYcMG4vE4Y2NjdU0CNZlMN6yV6nK52Lt3L21tbaxbtw6r1UpraytWq5WLFy8Si8XqpumTOJ1OHnzwQXbt2sXWrVtZXFzk7//+7zl+/PiaPLjVgsjpdJrBwcEbmlg98uPL5TK5XI5169axadMm+vv7KRaLRCIRLQFBr6RWSZL4xje+wWOPPcbg4KBW2f3q1aucOXOGkZERXZdiFEXB7Xbjdru10pmpVApJkshms7pq2bx5M//iX/wLent7OX78OKdOneLUqVPaFFwPKpUKmzZtYseOHRw7dozx8XHGx8dXFfy4YjNVVZWGhgb6+vqw2+2kUingutFURwN6xiwfPHiQ7373u4yPj/M3f/M3vP/++3WNTqnmLMH1G6EoChs2bGD//v1YLBacTidWq5VKpUJzczO9vb18/PHHdW+ksiyzYcMG/vAP/xCn06kVIn7hhRdQFIX3339f15FHQ0MDn//85/niF79IJBLhJz/5CYcPH9baiyRJWnZWPSJE8vk8VquVL3zhC1gsFj788EPS6TRNTU0cOHCA119/XZcU3eXlZYLBIJcuXeLEiRMkk0kt+8jlctX9+6tUR+ttbW00NjYiyzLhcJhyuUyhUNDW1/UYtauqyt69e/nLv/xLWltbOXfunBaEuXHjRpaWlnQbodrtdq1AtdPp5MqVK+zatYuPPvqIiYkJbDYbJpOJ2dlZLQLnZqwqndRqteL1evH7/cRiMS1dMJvNUiwWdR2qJ5NJzTxbWlro6+tjaGioLiOxfD6PoiiYTCZtatra2srTTz/Nhg0baG9vx2q1Eo/HtZH59u3bGR8fr/uDK0kSnZ2dbNq0iVgsxpUrV0gmk3g8Hj73uc9x+vRp3cIOHQ4HX/3qV/mn//Sfap3sH//xH+N0Ojly5Ig2ui+VShSLxbqNEC0WCxs2bGBhYYFUKkU0GiUcDmO1WmlubiYej9e9gykWi5w5c4aWlhZMJhOZTEb7oyj67f/KskxPTw/f/e53KZVKHD58WGsT1Y5Nr/y0xsZGfv/3f5/du3dz6dIlpqamtAFYa2srLS0tukVPOxwO8vk8V65cwefzYTQaeeqpp/ja177Gz3/+c06fPs3k5CQWi4VisXjLme+K72a5XKZcLtPS0sLevXtZXFzk2rVrJBIJJiYmSKVSuk4VPv74Y37605/i8Xi0B8Vut9dtWlssFikWi1oKavX7/H4/TU1N+Hw+5ufntaTWRCKh20ZUuVwmFoshSRLRaJRQKITNZiMajerWwRkMBvbu3csXvvAFGhsbKZVKTE1NYTAYePTRR5menmZkZIRMJkMul6NSqdRtqj87O8uvfvUrXC4XpVJJi1CRZRm73X7HqQiflVgspi2FVbO5mpqamJ2drctvvxHVtdp3330Xi8WixRgvLy+zvLystWs9lj7C4TCXL19mx44dhEIhJEnSlh0qlQoXL16su4Yqy8vLxGIxpqenSaVSGI1G4vE4hUKBkydPcuLEiTtuIys201KpRCQSIZVKsW7dOnp7eymVSly+fJmZmZlb5h7Vg0gkwi9/+Ut6eno009DjAalUKmQyGYaGhvjJT36CyWTC6XSiKAqFQoHFxUUOHz7Mhx9+SCQS0UXP9PQ07733HpIksbCwQCwWY2lpiWvXrulippIk4XQ6eeqpp1i3bh3BYJBEIkEoFCKbzTI2Nsa1a9d+4yhKPdpLpVJhYWGBoaEh1q1bh8FgwGazkU6nSaVShEIh3TbmQqEQk5OTPPbYY8iyjN/vJ5FIrGpdbrVUO5ef//znNDc3YzKZyOfzmokWCgUKhYIuz24ul+Pv/u7vANiyZQuNjY10dHRgMBi06bVeZLNZEokETqeTUqmE0+nk0qVLzMzMMDg4uCIvWVXU8/z8PG+99RYmk4nW1lYmJiYYHBxckx3KcrnM6Oio1sMVCgVdd2vz+TynT5+mUqlQKpXYuHEj586d45VXXuHChQvaGmG9KZfLjI+P8/HHH+P3+8lkMsTjcVKpFCMjI3XtYKpHj1RVxWazYbFYKJVKpFIpIpEIyWSSaDTK4cOHa3am706IxWLaOmU1X2h+fp7R0VFdO/1CocDg4CBf+9rX2L9/P+l0mv/wH/4D586d021kWtWxuLhIpVLR9jdKpRL5fF63DeMqwWCQv/7rv2bTpk089thjbNy4kUwmw6FDh3RbjoLrEemhUAiHw0G5XMblcmEymRgeHl7xs7vqDCiHw4HT6USSJPL5POl0+pahevfi+72fpLom9eSTTzI+Ps6HH354y/WVeqVxWiwWvF4viqKwvLzM0tKSNp3WQ4fFYuGb3/wmf/qnf0oikWBqaorJyUmOHDnCu+++e9PNwXrdF1VVcTgc2v9OpVK3HKXXS4fVauXJJ5/k2WefZXh4mB/84Ae37Fjq+W6+2WzG4/FgMBhIJBK3XZqr97NrMpkwGo3aBpjeycJms5mmpib8fj/Nzc34fD7eeecdZmZmVqRDBOrVUIeiKOzbtw+z2cypU6dIJBJromMl1COvfv/+/fz5n/85Xq+X48eP89Zbb3HmzJn78np8EpPJRFtbG5FIhEQiUTcDuxMt1Rc6MpnMbUfo98O9gesdryzLWK1WAJaWllakQ5hpjXU4nU56enoIBoO3PC93r14PRVHw+Xxs2LCBQqHAyMgI0Wi0rjnxd/P1WAsdd6Kl+tbTnWwW3wvXRA8d4t38GpNIJJicnNQ2o+pxhvJuplwus7CwQC6XI5PJrOnbV4Kbs9Y1G+5FhJnWgVgsRqlUwm63k06n7ytDqT6k1eM2AsH9gjDTOlCpVEgkEiiKcl/W8qye37wff7vg/kWkkwoEAkENuP9qbwkEAkEdEGYqEAgENUCYqUAgENQAkU4qdAgd95iOu0nL/aRDjEwFAoGgBggzFQgEghogzFQguI8wGo26FqW+n/hMV7Va9LhSqazZ62mSJGE0GqlUKhSLxbvmNblqIeL7EVmWsdlsWjzG5OTkmoXHwfUCI3a7nUKhcMtiK/XEaDTi9XqRJIlwOLxmb8UZDAa8Xi+xWEy7J/drO601qzJTSZIwmUzYbDYURSGXy5HNZnWNK6lSLTlntVq10LRkMrkmpmq323G73ZjNZhRFYWJiQrc6kZIkYbfbtSiQtbgXcN1I+/r66OjowGQy0djYSF9fH+fPn79pSbN6oqoqfr8fj8eD2WxmampK9xRdo9HI/v37+frXv47f7+fVV1/ljTfeWLM037a2Nvbv38/CwgKLi4sEg0Fdc9s+iaqquFwu8vn8mmmoRtZXnxu73U6lUllxltyqzNRsNtPR0cG+ffsIBAKcPXuWS5cuEY1GdX0f22Aw4HK58Hq9tLa20tbWRiaT4aOPPmJ6elq3ADmTyURvby979+6lqakJj8dDJpPhRz/6EaOjo7r0/Iqi8KUvfYmNGzfy4YcfMj09rT0seo6CrFYrPT09WkZYT08PfX199Pf38+677zI8PKzbKFWWZbxeLw6HA6vVis/nw+Vycfz48bqGLv46DoeDL37xi3z5y19mfn4egKamJl5//XUuX76sezrp5s2b+bM/+zPK5TJHjhzh//2//3fbEom1pjogCwQCPPvss7S0tPCzn/2Ma9eu6V6o2mAw4PF4sNlseL1ezGYzwWCQycnJFWlZsZlW83Meeughvv3tb9PU1MQvfvELCoUC58+f17V3MRqNuFwunE4nZrOZlpYWdu/ejd/v5wc/+IEu6ZMGg4F169bxwgsvEAgECIVCzM/PYzKZ2L9/P5lMhvn5+bo/MMVikWAwyJe+9CXS6TR2u53Ozk7Gx8cZGhrSzVC9Xi92ux2Hw8GWLVvo6elhamqKfD5Pb28v8XicyclJXbRU48eNRqO2Vuj1evH5fLqaabFYRJZlRkdHeffdd5mfn8dsNtPX18fIyIiuswij0aglMZjNZnp6evjc5z7H0tISw8PDuqVU2Gw2bDYbTqeT9vZ2HnzwQS5duqTFcevZwRiNRgKBAOvWrcPr9ZJMJonFYivObluVmZrNZnw+H4VCgZmZGdxuNwMDA0xOTpLJZHQZEUqShMPhwO1243A4MJvNWhql2Wzm7bffJplM1tVEqnk+/f392vqg1WrVYnRNJhNbt25leXm57nEdlUqF48ePazlMkUgEt9vN5z//efr6+jh27BihUKju90ZVVaxWK4FAgO3btxMIBJAkSetk9Zy52O12mpubkWVZM5GmpiZaWlqYmZnR7YEtFovMzs5y+PBhLl26RDqdxu1209HRQVNTE1NTU7rogOvGoaoq58+f13KfDAYDra2tLCws6JJOCmjPZbFY5MMPP+TixYsEg0GMRiM2m41UKlX3GV11qbKvr4+DBw+yZcsWwuEwFy9eZHl5ecXXYVWBesVikcXFRYaHh5FlmVwuR3NzMy6Xi4WFBV2n1xaLhUqlokV0zM7OEo/HUVVVC7erB2azGVmWMZlMOBwOGhoa6OrqYnJykunpaeD6lLcavKcH8XicCxcuaOuDZrOZxsZGnn76aR5++GFef/11Pvzww7pOszOZDMViUcuGd7vdbNq0iWQyic1m0y2pVZZlGhoa2L59Ow0NDSQSCYrFIoqi6PKgfpJCocDc3BzxeJzZ2VkKhQKqqtLQ0IDFYtFNh6qqOJ1OYrEYsixrI+ZsNkupVEJVVYxGoy7T7GpmWqFQYGFhgUgkQi6Xo1QqYTAY6r6B+8nIeqfTSWdnJw899BCnT5/m2LFjJBKJFT8nqwrUSyQSDA8P43a78fv92tRO7119g8GAoigYDAYMBoO2wXDu3DktrrVeVBvc7Owsx48fp6enhz179miRtZlMhsnJSUZHR3WbOlUqFaLRKLIsa2Xwzp8/z/Hjx7lw4QKhUKju65Xz8/OcOHECh8PBlStX6OjoIJ1Ok0wmCQQC2lS/3qFp5XKZVCpFQ0MDX/ziF7WI6aGhIS1UTi+Wl5eZnp6mv7+fcrlMsVgkl8sRi8V0C1yE69dkamqKt99+m/b2dtxutxaLvrCwQCqV0m29sto+qzNZSZIol8vkcjldlqQqlQrJZFL73r//+79naGiIRCLB1atXicVi9R+ZwvXRx/DwMOVyme7ublwuF6FQSPcNqHQ6TSgU0gysuj44PDysm5ZSqcTIyAj/9b/+V4aGhnjqqafI5/OcO3eO06dPs7CwoOuDm81mWVxc/FScsR5R01UqlQrXrl1jaWmJs2fPsmvXLgwGgxYj3N/fD8Dx48fr3slUp9ZtbW34fD4tk/1m2T71ZHx8nK1bt2oxzwaDgVAopGvcc6lUIpvNMj4+TiwWw+12Y7VateNaeu535PN5bWRaKBSQJEnrZPQy9GKxSDQaJZVKEQwGOX36NAaDAVmWV7WOveoMqOpunMlk0s543upITj3eq1UUBUmSUFVVM05ZlrVp/41+Wz3f71VVlZaWFmw2G/Pz8ywtLd23aa1VquuUbrdbmz0sLy9z4cKFT5l8vXRUj2dVj89dunTplqPieiZgPv/882zbto10Ok04HObIkSMMDw/XvJ3eTss//HvtGa7+s97JsZ/U8onvuunf0+uZud0SgwjUEzrWTIfBYMBkMlX/G62z++TMQQ8dNpsNVVWJxWI3/Tv11OF2u+nt7aVQKDA/P08kErnp/oIodPLbp0OYqdBxX+m4XcjhvXA97iYt95MO8W6+4L5ChPwJ6oUwU4FAIKgBwkwFAoGgBoh0UoFAIKgBYmQqEAgENUCYqUAgENQAYaYCgUBQA+6pdNLW1lYWFxdv+SrY3X5WTei4d3SoqookSat6NVGcM/3t03HPjExVVUWW75mfI7gHKJVKIhLkPuKecR9VVTEYDKLxCm6Jw+HQ7bsqlcqn3jsX3NvcM2ZaraYuuHuolgK8W6gG6+lJtTyk4N7nnsh8lSQJl8uFqqprOjJVVRWz2UyhUNA9xwauXwdFUbTSasFgcM2C9VRVxefzoSgKiURCKxq9VsiyTFtbGwaDgWAwqMt3VioVrRB0uVy+a2ZN1WpRsixTKpVuWmFNb03V2qJrhaqqlEqlVWv4zGaqqirAmkXXAjidTnp6eshms2uyblqNWvD5fFitVnK5HKFQiEQioVsjlSSJnp4eDhw4wLZt23C5XHz00Ue8/vrruplHFZvNRm9vL+3t7VpibDgcJhqN6h61bDAYaGhoYOvWrezfv59Tp04xMjKi2/cXi0UMBgMWi4VsNnvT9vBZH+SV0N/fz/PPP4/D4WB2dpZjx45x8eLFun/vzagmVZjNZiYnJ3UtmA3Xi990dnayfft2ZmdnGRkZWVV+3KrN1OfzsWfPHtrb27l27Rrnzp0jFovp2sPJsozb7Wbz5s10dXWRSCS4du1a3fOWPvn9TqcTn8+H2+3Wenu3200gEGBycpJgMKhLjIvT6eSZZ56ho6NDM/EvfOELbNy4kf/yX/6LbiF2Ho+HJ554gp07dxKJRJiamkKWZWw2G7IsUygUdA1t2717N/v372fLli3s2bOH3t5ezp07x+zsrC4a4HohZIvFgs1mI51OU6lUUBQFWZYpl8tIkqSbkTY0NPDcc88xMDBAKpXS0jKqkT96Y7PZ2LhxIx0dHSiKQkdHB4ODg7rGgvf29vLCCy+wbds2JiYmaG1t5ejRo8zPz6/Iz1Zlpk6nk29/+9t84xvfIJfL8d577wHoGhcryzLt7e309fXhdru1XJuOjg4ikYgupl4102r8gyzLKIqiJaW2tLRw6NAhXSrdBwIBnE4nExMTZLNZHA4HFouFAwcOEAqF+M//+T/Xfcqvqirbt2/nn/2zf4bRaOSDDz741Hqhw+HQpvt6zGRaW1t5/PHHMZvNJJNJ4vE4u3fvZv369czNzena8edyOS3gLxAI0NLSQrlc1jocPSruGwwGtm3bRnd3t1bIPZfLaWm2elJ9Vjdt2kRvby8+n49cLsfo6KhuU31Jkmhvb+fgwYPs2rWLYrGI2WymubmZBx54gBMnTqzI1FdspkajkZ6eHp577jna29v58MMPSSQSGI1GLBaLbmbqcrnYvHkz7e3t5HI5lpaWKBaLeDweLBaLbiF25XKZUqlEqVRCURQtpKs6Wg4Ggxw7dqzuo9NEIsHS0pJWfDmXy7G4uEipVOKRRx7hBz/4AXNzc3XVoCgKXV1dDAwMaNOkaiHoUqn0qZyweDxe1zwqWZYJBALE43GCwSA+nw9Zltm9ezeBQEBbL9QLSZIwm8088sgjfPWrX8XhcDA3N8fIyAjvvfcemUym7tNbRVFYt24dLS0tOBwOJiYmGB0dJZlM0t3dzdjYmC6dnCzLbN++nccff5zW1lYaGhqwWq1MTU1x9uxZFhYW6q4Brt8Tj8dDS0sLxWKRdDqNqqr4/X5UVWV8fLy+ZlooFJicnOSXv/wlMzMzXL58mYmJCVKplK6N0+l00tvbS3Nzs/ZwptNppqentXXcelMqlYjFYpjNZu1YlizLuFwuHA4He/bsYefOnVy6dKnuSyDxeJxIJMLAwAANDQ2YTCZcLpdm8OvWrSMajdZ1Y6xUKjE3N8fp06exWCzkcjkqlYp2eL1QKGjhevUeJVcqFVKpFDMzMyQSCaanp0mlUgQCAYxGo+5Hlqr3fs+ePTz88MNMTk5qkeDVyPLqEkA9MZvNtLe34/P5gOthf9Uk30AgUPcOF67PYB544AH27duH0WjE5/MhSRKjo6OMjo7qtlFZHXRU26WiKNqzUygU8Pv9mEymO+70V5VOurS0xP/+3/+bzZs343a7SSQSBINBXYPKJEnC5/OxadMmPB4PANPT07zxxhtaREa9qT6wc3NzFItFnE4npVIJq9XK1atXmZ+f58iRI7oF+y0tLWG1Wlm/fj1ms5lAIIDf7+e9994jnU5rm2P11HDhwgX+v//v/2PXrl1ks1kURcHlcpFKpZifnycYDOqyZlqpVAgGg3R2dlIul0mn01po2uLiou4bldUHd3p6mqmpKdLpNNFolLm5OdLptC7Hp0qlEkNDQ5w7d45t27Zp+WnVuHa9YqeXl5c5ffo0TzzxBFu3bsXpdHLs2DHeeust3db24fo9icfjjI+PUywW6erqYu/evQwODjI1NaWFQN4pq96AWlxc5IMPPsDr9VIqlUgkErqOTKs9SkdHB729vaiqSj6fp1Ao6KoDrqe1hkIhAC13/Ny5cwwNDTE+Pq7L2lzVyBYWFti3bx8PPvgguVyOv/mbv+Hll18mEonUfTRYKpUIBoMsLi4SDAbp7u7G4XCQz+eZnZ1lampKt+UXgGg0SjKZpLOzk2QySTgc5ty5c7pNI29EdY20ml8/MzNDLperewQ3oEV/V6eulUqFfD5PLpcjm82yvLysbYrVk0qlwvnz53nttddwu92cP3+ev/3bv+XSpUu6nwpaWlriyJEjdHV1MTExQTgcJpFIMDQ0xKVLl1b0zPzWZkBZrVYef/xxXnjhBfr7+0kmk7z11lu8/fbbjI6O3vSm1PP9XoPBgNVqxel0ArCwsLAmOlRVxel0YrFYCIfDtxyN1kuHqqp4vV4sFgv5fJ5EInHT9Mt66mhoaKC5uVlbArqdadW7fWzbto0XX3wRm81GNBrV4smvXLnyqTXTer2bX13eWEkHX69r4vf72blzJ2fOnLmjTdp63htFUbBYLJhMJm3JcqXP7m+tmUqSRGdnJ21tbdpwvRqvfKuRqR7FEu7kAPLdXrRB6KiPDrvdTmdnJw6HA7vdTi6XY3x8nGAw+Kn2cj8UOjGbzZhMpltGb+uhY6Xcc2YK13t6VVVZXl6+46nJ3X5DhI57W0d186u6MVcul8nn87/Rfu8HM73XdPxWv05aPZIkEPy2YDAYKBQK2jlPwb3DPVPoRCC426nWTljL988F9UOYqUCgI8Vicc2Ligjqg0gnFQgEghogRqYCgUBQA4SZCgQCQQ0QZioQCAQ14J5KJxU6hA6h4+7Scj/pECNTgUAgqAE1MVOHw7HmKYzVSu4CgUCwFtTEfYxG45omMFajQu4WM13rjkUgEOjPZ3afalEPRVm7N1MtFgs7d+7E7/evmQa4burV4rttbW33vanKsozVasXn8+FyudbselQTW6vRMoJPc79fE6PRyMDAAO3t7Z8pLv4zOaDNZqO7u5vNmzczPT3Nxx9/rGu9SrheheeBBx7gS1/6Eh0dHbzyyiu6hnFVMZlMDAwM4PV6sdls5PN5MpnMqlIOV0vVzCuVCpVKhWKxuGbxylarlZaWFrq7u/F6vcRiMU6dOqXr9VAUBY/Hw8DAALt27UKSJE6ePMnp06fX7L34ahX3u+WVUovFQmtrK9euXVtrKWuCzWbjxRdf5Hd+53fI5/McOnSIX/3qV6vykFWbaWdnJy+88AIPPPAA/f39GAwGXn75ZV5++WUtKqLehV6tViv9/f1s2bIFWZbp7+/nySef1OJ89Xxg2traeO6557RYY4PBQFNTE7FYTLcHx2g0apEpsixrSQCxWIxsNqubDqfTSX9/v5ba6vF4cLvdlEoljh49qkuHW01i2LZtG9/85jd5+umnURSFt956i7/6q7/i8uXLur3WKcsyDQ0N7N+/n4cffphQKMTx48e5cuUKS0tLK67oXgscDge9vb1s2bKFqampNTHTzs5Otm3bRqlU4uLFiywtLekaBa6qKo8//jgbNmzg2rVrxONx2tra+NrXvsabb77J1atXV1S0e0VmWp3St7S08MUvfpGenh4ymQzLy8t0dnbyyCOPMDU1xfj4uC4Vs6uRGEajkcXFRSKRCKqq0tnZqVUx1wOz2czjjz/Orl27OHr0qJa15HQ6sdvtujSQamCbzWbD4/FoyY/5fJ433niDq1ev3rI4c61QFIWNGzeyefNmKpUKpVKJZDJJPp/H4XDQ3NzM6Oho3XVUtRgMBsLhMMPDwzgcDrxeL3v27GFiYkKX66GqKvv27eORRx5hz5499PT0YDAY2LFjB6+88grHjh1jZmZG1xlEU1MTf/Inf8KePXtIJBL88pe/xGAw6FqBLRAI8NRTT9HS0oLRaKS/v5+5uTmOHj3K4uIi+Xy+7h3MwMAABw8e1KKHqoF6bW1tvPjii/zqV7/izJkzd+xltzRTp9Op/SBZlpEkCYfDwfPPP09XVxfhcBij0Ui5XGZiYgKr1YrRaNRtRFjVls1mSaVSmM1mvF4vi4uLNDU13bbKfK3w+/0cPHiQlpYWzGYziqJQLBYxmUy6JrZWp/eVSgWn08lDDz3Epk2bmJ+fZ3x8XBcNfr+fAwcO8MgjjzA5Ocn09LSWmrq0tITH49G1jSSTSU6ePMnU1BQejwer1UqhUMBisehipoFAgIcffphAIKDNWPx+P1arVQs81HvK/+yzz/JHf/RHDA0NMTw8jNPppKGhgWAwqJsGh8NBU1MTfr8fh8PB1q1bSSaTtLS0MDU1xfnz5+s+Wrbb7VgsFmRZZnl5GbfbjcFgoKGhAZ/Px+zsLJOTk3d8XW5ppp9sbFXjqoaC9fb2Isuylu9TjZF1u92YTCZdTKxYLBKPx4nH4/h8PjZv3ozD4aBYLBIMBnWbOhmNRjweD62trTQ3N5NIJEilUrrlw8M/RipX/4RCIV5//XWOHj3K5cuXdckYgusjZJvNpkUqG41GFhYWtA4lEonoshFVTV/IZrMEg0Gi0SgNDQ04nU5CoZBuI8FyuaxFYmSzWdLpNCaTiVAoRCQSIZfL6W6mlUqFyclJLl26xPj4ONlsVvdNqKph/tmf/RldXV3Mz8+Tz+eJxWIEg0Fd7s/c3ByFQoHm5mZtryMQCBAIBBgfH0dVVaxW6x1/3i3N9EbD/lwux4kTJ+jq6qKzs5NwOEwmk0FRFC5cuMDJkyd1M5BcLkc0GiUWi1EsFlEUhZ6eHqxWq3Zz9CCTyXDkyBFtnc7v95PJZLQUSr1YXl4mk8lgMpkIBoOagU1OTup2LRYXF3nppZfYtm0bW7Zswe12E4/HsVqtmEwmlpaWdBuV5vN54vE4ZrOZQqGA0WikWCySSqV0M7BYLMbZs2cxm82USiWy2SxGo5FwOMzs7OwdZR/VmlOnTvHcc89po+JYLPap/Ck9KBaL/OpXv2Lfvn3Mzc3xve99j+npaa2D0YNQKMTly5e1QZDb7cbtdmO1WhkaGmJ+fn5FG6arji3p6OjgmWeewel0ksvlMBgMnDlzhmPHjt3QhOv1KpjVaqW9vZ2Ojg7a2trw+/189NFHnDlz5oYGUg8dJpOJxsZGtm3bhtvtZnl5mUgkwtDQEHNzczccIdfrelQ3oaxWq1bVfWFh4aZmWi8dO3fu5Ctf+QpNTU3Mz89z9epVTpw4wejo6A2jnuulw+l00tHRgdPp1EYZi4uLjI2NkUwmddPR3NxMb28vmzZtorW1lcuXL/P+++8TCoVq3j5up8VkMvHFL36Rvr4+Tp06xccff8z8/PxNP6uer3E+8MADmEwmjhw5cttBWD1y5BobG9m0aRO9vb0EAgFt8/bq1au89957jI+P/4af1TwDymg00t3drYlSFIXx8fGbjsTqeUNkWcZoNGpnxNLp9E0X0+udcFilXC6vWaCeqqooiqIlLN7qHtdLhyRJWCwWWlpacLvdhEIhZmdnb3pN6nk9PB4PnZ2dGI1GUqkU0WiUhYWFG2qpdztVFAVVVbHb7UQikZtOZ+v9bn5Vx50sRdU7sbVSqdzRTKFeOmRZxmAwaD5is9kACIfDN7w/NTVTSZIwmUyUy2Vtyna7vO27vUiB0HFv6zCbzVr44q3Oeep1PQwGA+Vy+aYdnSh08tunY1XnTKtrUJ907bvlELJAcCNyuZxua3F3ggiCvPdYlZnmcjmRYyMQCASfYFXnIYSRCgQCwae5vyscCAQCQY0Q6aQCgUBQA8TIVCAQCGqAMFOBQCCoAcJMBQKBoAbUJJ20+ibFWrxps1KEDqHjXtdxN2m5n3TUZGQqDuzfmvs9vkQguB+oiZlKknTf58jcCrPZvNYSBILfwGKxrGl2271GTRzQYDBgsVhq8VH3HEajEbPZLDobwV1HY2Mj69atW2sZ9ww1ecKrVfgFn0aWZWw2m671M+9G7oaORFXVu6KNVisU3S3oHYB5L/OZxviyLGOxWHC73bqmTt5Ih9lsJpvNrumrrrIsax2LJEl4PB4ymYxuxbI/iclk0uJT1ioNU1EU/H4/drudxcVF4vG47hpkWaaxsZHGxkYKhQKTk5O6F0KuYjQa2bJlCwAXLlxYk3bxSS1+v59wOLxmGu5WJElalY+s2EyrwVsul4t169bR29tLuVzm6NGja1IAxWAw0NLSgsfjIRQKEQ6Hda/Io6oqra2tdHV10dTUhN1uJ5fLMTs7y8mTJ3XVoigK69ev54tf/CIdHR1cuHCBl19+mampKV11+P1+du7cSV9fH6VSidHRUY4dO6a7ke3du5cDBw4QiURIJBKsW7eO06dPMzs7q6sOr9fLk08+yXe+8x0A/vIv/5IjR47oqqGKwWBgy5YtHDx4UIt00TMV9G7DbrfT39+PLMuYTCYkSeLcuXMrbqsrNlOz2cyWLVt47rnn2L17Nw0NDQwNDWlFd/Usc2az2diyZQvr1q3D5XIBMDY2xoULF3SLN5Ykie7ubh5//HGampro6Ohg/fr1qKrKO++8w8jIiG7RJbIss3v3bv71v/7X7Nu3j1gsBsD777+vm5lKksTAwABf+tKX8Pv9DA0NsbCwgMFgwOFw6Gqm7e3t/NEf/REAv/jFL4hGo2zYsIE///M/59/8m3+j2xTXaDTy7LPP8tRTTzE9PY3P5+M73/kOiUSCwcFB3QcgLpeL/v5+vF4vdrudxx9/nLNnzxIKhe6qMoV6sXv3bv7gD/6Acrmsxcq88cYbvPLKKyvKolqVmb744os888wzFAoFAoEA2WyWjo4OrFZr3W9GNXrCYrHwzDPP0NfXx9jYGNFoFK/Xy9NPP8327dv56U9/ytjYWF21wPUHZf369ZTLZa5evUoymcRqtfLII4/w6KOPcujQIRYXF3XJPWpsbOTLX/4y7e3tfPTRR0xOThKJRLSYDD1CytxuN1/4whd48cUXOXnyJAsLCywuLlIoFPD5fCwsLOgyczAYDGzduhWr1cqPf/xjPvjgA5qbm9m1axf9/f34/X7dOpiGhga6u7s5ffo04XAYt9tNT08PX/jCF0in04yNjelqqAaDgWQySTgcpqWlhYGBAdrb2xkeHubixYt1H7Xb7XaamppwuVzYbDbcbjdNTU0sLS1x4cIFxsfHdcsJ8/l8vPDCC6xfv56TJ08yOztLa2srjz76KEePHl1RYuuKzdTn82EymRgfHyeVSnHp0iVyuRwWiwWn08nS0lJdG0a5XMZisWiZ1wDxeJxkMonRaKRUKqGqqi6bPpIk4ff7aWxsZG5ujlQqpaWStrS00N/fz8MPP8zo6Cizs7N17WgURWHz5s0Ui0Veeukl4vE45XIZq9WK2+3WbfPFbrfT09NDe3s7Z86coVwuUywWyefzGAwGLU6l3lQqFUKhEC+99BJHjhwhmUzywAMP8NWvflULbtOLSqVCOp0mGAySTCYpFAosLS3R3t7Oxo0bCYfDuq4nV3PbvF4vn//853nzzTeJRqM0NTVx9uzZun9/uVzG7XbT3NyMz+fj0Ucf5YEHHuCdd965YeZSvZAkiaamJhoaGrTU5ap/7dy5k/Xr16+o81+RmSqKgtPpJBqNoigK6XSaQqGAqqpaBni9qVZMv3TpEoODg3zzm99k27ZtnD59mmg0SrlcJhqN6jKFq1QqpFIpwuGwZhILCwtUKhVGRka0aIpqxEs9UVWVbDbLuXPnCAaDmM1mDAYDHR0dxGIx3TagqtHfExMTvxEhbDAYsNvtuiSllstlZmZmsNvt9PX1USwWaW9vJxKJcOjQIV03fxKJhDbaq1QqqKpKLpfj2rVrDA4O3jDYr560tbXR2NjIlStXeOONN5AkiUKhwPj4uLY0VE+y2SwLCwsEAgFMJhOVSoWZmRmGh4dZXFzUdbNUkiSmpqaYm5tjenqaxcVFwuEwkiSRTqfx+Xwkk8kbhkD+OnfsfgaDAZvNRi6XY2hoiGg0qo12FEVheXkZRVGQZVmXniUej/Pyyy+zdetWnnjiCdra2sjn8+TzeRYXF+/ox9eCZDJJKBRi3759VCoVotEopVKJq1evcvHiRQ4fPszo6Gjdpy3ZbJahoSH8fj8Wi4VkMonFYkFVVYLBoG69/dLSEu+88w52ux1VVfH7/eRyObLZLJlMBlVVb5sXViuqJ0waGhqIxWJcuHCBa9euMTk5qauZplIpLl++TGdnJwaDQQt+HBoaYmpqSlfzUFWVtrY2WltbmZ6e5uTJkzQ2NpLL5bhy5You16VSqTA7O4ssyySTSYLBICaTiatXrzI/P6/bkkelUiGTyRAIBEin08iyTKFQYHZ2llOnTjE7O7siLXdspqVSSVvfiUQidHZ20t3djaqqRCIRrl69Sjgc1vVc5eTkJP/9v/93VFVl69atABw6dIgTJ07osj4I10dA165dY8eOHaxfv55gMMj8/DyvvfaadrpAr8YRiUQIBoPs3buXVCqF1WrF4/HoMtqoUigUOHnyJOFwmA0bNuB0OjGbzSwtLTE6Oko0GtXteiwvLzM/P09fXx+Li4vMzs7i9/uJxWK6rlFWKhXGxsbw+/34/X6KxSLT09MMDw/r8pxIkoTNZsNkMmGz2RgeHv7U7CAYDHL06FEWFhbqrqVKqVRiamqKUCikJZRWl4P0ZHZ2lh//+MesX7+eRCJBLBZjamqKpaWlFX/WqtNJrVYrjY2N5PN5YrEYmUyGSqWiRcguLy9/Kn2xnjGtPT09PPbYYxgMBt5++21GR0frkvp4Mx2KorBu3Tq6urqYmZlhdHT0tsfE6nU9zGYzmzdvpr29HbfbTSaT4e23377pOeB6Rj1X43MBbe30ZtRLx/bt23G5XMzMzNDR0UFTUxMLCwtMTk4yMTHxG5rqWUyjvb2dQCBANBplfn7+lmvotSp04nA4cLlc5HI5bb22+v+3trbidDoJh8O6PzOroV46qmv55XKZUql02w6uplHPVYxG4w1z2auH1iuVSt3NtIrH48FoNN52zaVeOlRVRVXVO16r1aOBGo1GTCYThULhpg/uvf6gVDPQ0+m0tq5fLBYpl8uk0+nfaCv1zoiHO0smrZWZ3uo7qyayvLwsKr7VQMdnMtNaCLgTfht0VB/StdaxUoSOe0/H3aTlftKx9i9NCwQCwT2AMNMaodeGl0AguDsRZioQCAQ1QEQ9CwQCQQ0QI1OBQCCoAcJMBQKBoAbUJJ30TrjbjzUIHULHvaLjbtJyP+kQI1OBQCCoAcJMBQKBoAYIMxUIBIIaIMxUINCRalTI3ZCUKqgtn8lMq2mca4EkSbhcLlRVXZPvv1upPqSSJIkI7puwVtdEVVVaWlrYs2cPXq9X3Bvuvujrz8KqS+N7vV56enpwu90Eg0GmpqZ0rxju8/l44IEHiMfjZDIZpqam1ixyWpZlHA4HxWJRK0eoB9XKP5VKBY/Hg8vl0qJbDAYD6XSaWCxGPp/XNT3W5XLh8Xi0guHFYpHl5WUtYkYv7HY7LpcLs9mMxWLB6/USi8W4ePGirkWZDQYDra2tuN1uAoEAzc3NlEolLeZGT2RZxuVyaRX2M5mM7om+cD0vbPPmzbS0tGhlCa9du/ZbG+q3KjO12Wxs27aNhx56iIceegi3282rr77KD3/4QyYnJ3V5YCuVCl6vl29961vYbDYaGxsJBoP8j//xP3jnnXd0zyT3eDz8wR/8AT09Pbz33nu89957umSSl8tlZFmmubmZzZs343K5iMfj5PN5lpeXkWUZRVEIBoNcuXJFl6Ayi8XC888/T3t7O6FQiOXlZcxmM5Ikcfz4cS5evFh3DXA9YPBP//RP2bx5MzMzMwAMDAxw+fJlvvvd7+oab+zxeGhoaMDj8dDT00M8HieVSmGxWEin07pq8fv99PX10dDQgNvtJhqNcuTIEV1zsRobG3nyySd55JFHtHSISqXC//yf/5NXX31V1w7GYDDQ29vLpk2bcDgcTE1NMTg4uOJC5iueoxsMBrq7u2lqaiIejxMKhXC73XzjG99g69atuk37ZVnWilOPjo4yODjIwMAA//7f/3sOHDig+/JDX18fTzzxBA888ADf+MY3+NrXvkZDQ0Pdv7dSqZDNZolEIlQqFS0jS5IkTCYTRqORQCBAIBDQ5ZpUo54DgQDFYhGbzYbdbkdRFCwWi67tY8OGDezbtw+bzYYkSeTzeUKhEM3NzezYsUOXzLIqdrud9vZ2bTbX0tKC1+vFZrPpOipUFAW3243VasVqtbJjxw7+7b/9t3z5y1/GZDLposHj8bB37166u7sJh8Ncu3aN6elpnE4njzzyCG63Wxcd8I/t5MUXX+Rf/at/xV/8xV/wve99j6985Ssrbh8rbtlOp5O+vj6ampqQZZlgMMilS5fI5/N4PB7d1oEkScJoNDI3N8fc3BxXrlzh0KFDpNNpfvd3fxefz6eLjioej4epqSneeustrl27xqZNm3j88cd1W9NNJpOMj49rvakkSZjNZtxuNw6HA0mSdJnWKopCe3s70WiUhYUFLQeq2l5SqVTdNXxSx+nTpzl//jzLy8vk83mCwSDlcpkNGzbovt7e0dHBM888Q3d3t5ZqazabdR2FKYqC0WgkHo8zPz/P9PQ0hUKBPXv20NjYqIsGo9GI2WwmGAwyPj5OOBwmm81SLBZZv349AwMDuq2jejwetm/fTm9vL8VikUgkooVkrvR5WXHXXP2R1UiKWCympRrqOVWB62XvGhoaUBSFfD6vZes4nU66u7tZXFzUTUsymcRgMJDP57XY6eofPZYcyuUy4XCYQCCgrRO2trbS0tJCOp3mwoULuo2AyuUyBoMBs9lMa2srHR0deDweTp06pVsbKZVKTE9P09vbi6IoWK1WZFnG7XZrJlbvjr+6XlwNsdu5cyfbtm2jVCphMplIpVKMj4/raqb5fJ5KpYLZbKZQKHD16lV+9rOf3TZGpZbkcjlSqRSFQgG73U53dzcDAwN0dXWxtLSEz+fTJZhTkiQURSGVShEMBkkkEuTzea5du8aHH3644u9fsZnG43GuXr2Ky+XCYrGQzWZJJBIYjUYmJyd1W9QvlUoMDg7y0EMPsX37dm0TyuFwaDlUenL58mUuXrxIf38/mUyGyclJhoaGdF1Ml2WZpqYmenp66OnpYcOGDTgcDl599VXdInSLxSKzs7Ps3r0bn89HX18f27ZtI5VK8eqrr+pqpidOnKC1tZW9e/dit9vp7+/Hbrdz9uxZjh49WvPwNoPBgMViwWazaZ17Pp+nUCgwOTnJD3/4Q86cOcPnPvc5ZFkmEokwPT2t6zS/UqlQKpXw+XzaaPDChQta4rAepNNp0um0NmtyuVw0NTVhNps5fPgw165d06WDqVQqhEIhTp48iaqqdHV1sbi4yDvvvEMoFFrx563YTAuFAleuXCGTyTAwMEBDQwOSJDExMcHo6KiuO6QTExP8r//1v/jjP/5jNm/ejNfrZWJigp/97GcMDg7qpgOuxwr/+Mc/5sCBAySTScbGxnR/UEqlEplMBlmW8fl8BAIBZmZmOHr0KHNzc7ptDF69epW5uTl6e3tJp9McP36cS5cu8f777+u6MZjNZvnwww/xer24XC5mZ2c5fPgwP/nJT+piHDabDYfDQSqVIh6Pf+rfTU9PMzMzw5EjR3jrrbfo7e3lwoULpNPpmuu4HdVBj9frZXl5mdnZWcLhsG5ttVQqMTMzQ09PD42NjaiqysjICN///vf5v//3/+oa9wwQCoV45513tIj01Z42WXUGVHXNsqGhAYvFQiwWu2VUbD1TMK1WK01NTSiKQigUIpFI3NTU610sobpoXSqVdA8pqx7P6u7upru7m0AgwPj4OKdOnSIej99QT72uR39/Pw6Hg2w2y/LyMgsLC7eMAK/nfbFarQQCATKZDLFY7JaG/ll0WCyWislk+g0j/XUkScLpdFIul2/64Na70En1HDLcPuCvHvfG6XTS1dWF0WikXC6TSqWYmZm5ZSDl3V7oRATq3YM6qsscRqORXC53y+lsPdNa7yQ2t946Vspn0WE2myvlcvmORt/VLPt0Ol3zTu4fPv+uuCa36vglSVrRaPhubyPCTIUOoeMe03E3abmfdIh38wUCgaAGCDMVCASCGiDMVCAQCGqASCcVCASCGiBGpgKBQFADhJkKBAJBDRDppEKH0HGP6bibtNxPOsTIVCAQCGqAMFOBQCCoAcJMBQKBoAYIMxUIBIIasKrcBkVRKJfLWoGGtTqrWq0YpSgKiqIQi8XWJBjsRkiStGbXpZoaq3dQ292IwWDAaDRqRTWWl5fX7L5U9ciyrBWHXiuqz8xva3hdragWUs/n8yQSCQqFwqrbx4rNVJIk7HY7TqcTQKssv7y8rPvDa7Va8fv9OBwO7HY74XCYYDC4JjUiP4nFYqG5uZlkMqlrtX8Ak8nEnj17aGlp4cSJE8zMzKxJByPLMmazmXK5rFV31xur1cqDDz7IwYMHkWWZ6elpTp06xeDgINlsVlctqqrS3t7OunXrcDgcxGIxPvroozUxs/7+fg4cOEAgEODdd9/lwoULa2qqkiTh8XgAWFpa0q2tyLJMa2srXV1dmM1mlpeXmZyc5OrVq6uqu7viqlGqqhIIBGhvb9eiIKrV1aenp29a7q3WxxqqiZzVeGO/348kSczPz3PlyhVisVjNddhstkqlUqFSqWgpn5IkaXHLdrsdr9fLvn37eO655/i7v/s73njjjRs21Hoc8zAajbz44os8/vjjWgLC4cOHefvtt29qqPU6btLV1cXTTz+N0+lkenqa48ePMzExcdPPqrUOVVV59tlnefDBB3E6nSwuLlIsFvF6vZw5c4Zf/vKXN0zjrFftzoMHDzIwMIDD4cBmsxGNRnnttde4cOHCDcsU1utoVGNjI3/1V39FU1MTs7OzTE1NceXKFU6fPs3MzIyuNW/b29vZvXs3PT09tLa2Ui6XOX36NG+//bYu98Zms7F3714CgQDZbJZKpYLD4SAcDvPhhx+u2MtWPDKt5tqUSiWMRiPt7e3s2bOHcDjM//k//4dr167pVm3fYrHgdDpxOp1YrVbMZjMej4disciVK1dqPo369YtrMBhwuVxahfVq/MLAwIBWFNlkMunW62/fvp0/+ZM/YW5ujvHxcWw2G//kn/wTZmZmdItXrrJhwwaeeuopUqkUPT09bNq0iZ/85CdcunRJl5GH3W6npaWFK1eukM1mMZvNmM1mXC4XBw8eZHBwULdo45aWFp5//nnMZjPJZBKHw0FzczOjo6NcunRJt+fFZrPx7W9/m4aGBsbHx5mYmCAWi9HQ0MDTTz/NO++8o1tUe2dnJy+88ALbt2+nv7+fzs5OJEkiFovxzjvv1P37VVWlu7sbr9eLwWCgWCySy+XweDw88cQTZDIZTpw4saLZ9oo3oMrlMsVikWw2q0UKd3d388wzz7Bp0yZt+l9vKpUKxWIRl8tFS0sLjY2N+Hw+fD4fdrsdi8VS88C0Uqmk/SmXy5RKJYrFojYlqFYNHxoa4ic/+QnDw8O6TbFVVeXBBx+ku7tbCzdMJpP4/X6ee+45jEajLjo+qadQKGipoF1dXfze7/2ebjG+1eTcxcVF8vk8iqJgs9moVCr4fD76+/t1ScCUJImGhgba2tpobW3F5/PR29vLY489RktLi25pvgCbNm3ihRdeoFAoEIvFSKfT5HI5SqUSvb29DAwM6JKd1tDQwMGDB+no6MBoNBIKhTh27BivvPIKs7OzuiwXGo1Gmpqa8Hg8BAIB/H4/LpcLt9vNxo0beeKJJ1bcVlc8Ml1eXiabzZJOp7FYLITDYU6cOIGiKGQyGcxmM7Is1723rVQqJBIJXC4Xu3btorW1lXw+z9WrVykWixgMBq3HqRflcplsNoskSaRSKRRF0QLj0uk0yWRS1/XKTCZDsVjEZDJpozCHw8GOHTvw+/3Mzc3ppmVqaop0Oo3T6cThcGjTOL0ifCuVCna7XYs2bmtro6+vj/7+flRV1c3EZFnG4/GQSqWIRqNEo1HC4TDvvvsu7733nm77DA6HA4fDQaVSYf369YyMjGgbuK2traiqyvT0tLZ8VS+MRiN79+5l165d2O12FhcXsVgsDAwMEIvFyGQyt4wuqRXFYpFYLIbdbmfDhg2kUilCoRCBQEAL+lupjhWbaaVS+VRujcFgYHl5mXQ6zfz8vK7mEY/HCYVCtLS08OCDD3L+/Hk+/PBDksmkbg9LtfGpqqqdciiXy+RyOS06Vg8KhQJvvPEGzz77LOvWrSORSNDT00NXVxeZTAaLxaKLjipjY2OcPn2a3//936ejo4OFhQVee+01lpaWdPl+m81Gd3c3nZ2dmM1mWlpaaG1tpbm5mStXrhAMBnVpq5VKhdnZWc6fP4/NZiMQCBCJRBgcHNQ1zTeZTHLs2DH+4i/+gi9/+cs4HA7a2towmUzYbDbefPNNxsbG6q6jWCwSjUYpFou43W6mpqaYnJwkl8tpz44e5PN5BgcHyWQy2O129uzZw/79+3G73UQiEY4dO7bijuUzBeqpqorNZsNkMmkGksvlbiiiXovYjY2NPP/88/T29jI4OMjg4CDhcJhMJkMymfyN9Z966bDZbDidTkwmE4VCgXQ6TSqVuunIo146du/eze/8zu/g8XgwGo2kUik++OADjh49Sjgc1k0HgM/n44UXXmDnzp3aNO5mvX0tdRgMBtra2mhvb9eyhvx+Pw0NDRgMBpaWlvjggw+YmZmpq44qFouFjRs30tXVRX9/PyaTiTfeeIMTJ07c9LPq+W6+1+ulubmZtrY2uru7mZyc5IMPPtDl3sD10XpHRwcbNmzQ9hai0SiFQoFsNkswGNR1I8zv9/PQQw/xz//5P6enp4df/OIX/Mf/+B9vGhBatwwoWZYxGAy3DU+r14VQFAW3261N6QuFwk0NvZ46ZFnGZrNhs9mA61PudDqt+y56tZNTVVXLSC8Wi586F6yHjirV84yFQuGWI8F6PLCA1iaraZzVGcvN0mPr2T4MBgNOpxOPx3PbI3x6FDqpHl8rlUprErr4ySWfO5kl1LOtqqrKQw89xKOPPqp1dDfzRhGop4OOauM0m80Ui0VtDVNvHStB6Lj3dNxNWn6bdHg8Hvbt28fJkyeJRqMr1rGqN6AEN6ZcLmsGWh2tCwSC3w5SqRTz8/Or3oQTZloH6r0jKhAIak+hUCCVSq3q7ScQhU4EAoEAuH5sq3rudjUIMxUIBAKuzygTicSqz/6KdFKBQCCoAWJkKhAIBDVAmKlAIBDUAGGmAoFAUAOEmQoEAkENEGYqEAgENUCYqUAgENSA/x8Tz7hF7X6kLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [],
   "source": [
    "# DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)\n",
    "DEVICE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "#             if classifier_fc_sizes is not None:\n",
    "#                 self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "            \n",
    "            # self.base_model = torch.nn.DataParallel(self.base_model)\n",
    "            \n",
    "            \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            \n",
    "            self.base_model.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.base_model.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            \n",
    "            self.base_model.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.base_model.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            \n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.forward_latent(X)\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        # interim = self.get_head(interim)\n",
    "        # interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight 6 True\n",
      "base_model.6.0.bn1.weight 6 True\n",
      "base_model.6.0.bn1.bias 6 True\n",
      "base_model.6.0.conv2.weight 6 True\n",
      "base_model.6.0.bn2.weight 6 True\n",
      "base_model.6.0.bn2.bias 6 True\n",
      "base_model.6.0.downsample.0.weight 6 True\n",
      "base_model.6.0.downsample.1.weight 6 True\n",
      "base_model.6.0.downsample.1.bias 6 True\n",
      "base_model.6.1.conv1.weight 6 True\n",
      "base_model.6.1.bn1.weight 6 True\n",
      "base_model.6.1.bn1.bias 6 True\n",
      "base_model.6.1.conv2.weight 6 True\n",
      "base_model.6.1.bn2.weight 6 True\n",
      "base_model.6.1.bn2.bias 6 True\n",
      "base_model.7.0.conv1.weight 7 True\n",
      "base_model.7.0.bn1.weight 7 True\n",
      "base_model.7.0.bn1.bias 7 True\n",
      "base_model.7.0.conv2.weight 7 True\n",
      "base_model.7.0.bn2.weight 7 True\n",
      "base_model.7.0.bn2.bias 7 True\n",
      "base_model.7.0.downsample.0.weight 7 True\n",
      "base_model.7.0.downsample.1.weight 7 True\n",
      "base_model.7.0.downsample.1.bias 7 True\n",
      "base_model.7.1.conv1.weight 7 True\n",
      "base_model.7.1.bn1.weight 7 True\n",
      "base_model.7.1.bn1.bias 7 True\n",
      "base_model.7.1.conv2.weight 7 True\n",
      "base_model.7.1.bn2.weight 7 True\n",
      "base_model.7.1.bn2.bias 7 True\n",
      "base_model.PreHead_0.weight P False\n",
      "base_model.PreHead_0.bias P False\n",
      "base_model.PreHead_1.weight P False\n",
      "base_model.PreHead_1.bias P False\n",
      "base_model.PostHead_0.weight P False\n",
      "base_model.PostHead_0.bias P False\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if name[11].isnumeric():\n",
    "            if int(name[11]) < 6:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name, name[11], name[11].isnumeric())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2),\n",
    "                                        scale=(0.4, 1.3), \n",
    "                                        shear=(-25, 25, -25, 25), \n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002,\n",
    "                                    prob=1),\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.1,\n",
    "                                    prob=1),\n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    DEVICE='cpu',\n",
    "                                    # DEVICE=0,\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=20,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=2\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 2)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0].cpu())\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 device(s) found.\n",
      "0 Quadro RTX 8000\n",
      "1 Quadro RTX 8000\n",
      "2 Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "torch_helpers.show_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parallel = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "# model_parallel = torch.nn.DataParallel(model, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: masks_cat_raw,   device:cpu,   shape: torch.Size([711808, 36, 36]),   size: 3.690012672 GB,   requires_grad: False\n",
      "var: ROIs_without_NaNs,   device:cpu,   shape: torch.Size([711807]),   size: 0.005694456 GB,   requires_grad: False\n",
      "var: masks_cat,   device:cpu,   shape: torch.Size([711807, 36, 36]),   size: 3.690007488 GB,   requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "torch_helpers.show_all_tensors(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "# criterion = [_.to(DEVICE) for _ in criterion]\n",
    "# criterion = [_ for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parallel.to(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chopped_parallel = torch.nn.DataParallel(model_chopped, device_ids=[0,1,2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.to(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2404,  0.0128,  0.2456,  ...,  0.0054,  0.1058, -0.0937],\n",
       "        [ 0.2258,  0.0810,  0.1474,  ...,  0.0128, -0.0015,  0.0386],\n",
       "        [ 0.1850, -0.1030,  0.2427,  ..., -0.0209,  0.1576,  0.0269],\n",
       "        ...,\n",
       "        [ 0.1244, -0.0533,  0.2121,  ...,  0.0425,  0.1055, -0.0326],\n",
       "        [ 0.0988, -0.0051,  0.1536,  ...,  0.1117,  0.0398,  0.0576],\n",
       "        [ 0.1317,  0.0796,  0.1021,  ...,  0.0725,  0.0170,  0.0077]],\n",
       "       device='cuda:0', grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torch.rand(100,3,224,224, dtype=torch.float32, device=0)\n",
    "# model.forward(test_data)\n",
    "model_parallel.forward(test_data)\n",
    "# chopped_parallel.forward(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model_parallel.parameters():\n",
    "#     print(param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.6106, loss_val: nan, pos_over_neg: 1.0100092887878418 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.4306, loss_val: nan, pos_over_neg: 1.0811201333999634 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 7.3048, loss_val: nan, pos_over_neg: 1.6084449291229248 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 7.2868, loss_val: nan, pos_over_neg: 1.3654723167419434 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 7.058, loss_val: nan, pos_over_neg: 1.2637993097305298 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.9146, loss_val: nan, pos_over_neg: 1.451255202293396 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7889, loss_val: nan, pos_over_neg: 1.9823416471481323 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.6413, loss_val: nan, pos_over_neg: 2.8610496520996094 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.6275, loss_val: nan, pos_over_neg: 3.529315233230591 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.5229, loss_val: nan, pos_over_neg: 3.92543625831604 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.4852, loss_val: nan, pos_over_neg: 3.6489999294281006 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.4582, loss_val: nan, pos_over_neg: 3.394658327102661 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.3583, loss_val: nan, pos_over_neg: 3.660940170288086 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.2643, loss_val: nan, pos_over_neg: 4.729483604431152 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.1989, loss_val: nan, pos_over_neg: 8.11876392364502 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.1589, loss_val: nan, pos_over_neg: 15.538749694824219 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.0919, loss_val: nan, pos_over_neg: 20.589574813842773 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.0745, loss_val: nan, pos_over_neg: 19.653650283813477 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9896, loss_val: nan, pos_over_neg: 15.846778869628906 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.979, loss_val: nan, pos_over_neg: 12.495311737060547 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 13.392111778259277 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 18.149465560913086 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8661, loss_val: nan, pos_over_neg: 26.862796783447266 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 31.06601333618164 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 35.900753021240234 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 29.839229583740234 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7318, loss_val: nan, pos_over_neg: 34.23074722290039 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.6813, loss_val: nan, pos_over_neg: 38.39274597167969 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.6801, loss_val: nan, pos_over_neg: 36.64750671386719 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.6039, loss_val: nan, pos_over_neg: 50.955101013183594 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.5932, loss_val: nan, pos_over_neg: 51.26704406738281 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.5452, loss_val: nan, pos_over_neg: 40.1784782409668 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.4926, loss_val: nan, pos_over_neg: 40.78590393066406 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.5467, loss_val: nan, pos_over_neg: 49.33595657348633 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.541, loss_val: nan, pos_over_neg: 88.45403289794922 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.5117, loss_val: nan, pos_over_neg: 59.086727142333984 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.4281, loss_val: nan, pos_over_neg: 60.44749450683594 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.4247, loss_val: nan, pos_over_neg: 61.400856018066406 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.352, loss_val: nan, pos_over_neg: 47.290252685546875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.4012, loss_val: nan, pos_over_neg: 45.980751037597656 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.4402, loss_val: nan, pos_over_neg: 59.21748733520508 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.3228, loss_val: nan, pos_over_neg: 71.24930572509766 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.3524, loss_val: nan, pos_over_neg: 76.2256088256836 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.3625, loss_val: nan, pos_over_neg: 50.95716094970703 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.3611, loss_val: nan, pos_over_neg: 33.54733657836914 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.2859, loss_val: nan, pos_over_neg: 46.454429626464844 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.2655, loss_val: nan, pos_over_neg: 91.94129943847656 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.3197, loss_val: nan, pos_over_neg: 96.26688385009766 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.2902, loss_val: nan, pos_over_neg: 108.54254150390625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.3089, loss_val: nan, pos_over_neg: 69.74514770507812 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.223, loss_val: nan, pos_over_neg: 37.94501876831055 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.2135, loss_val: nan, pos_over_neg: 54.03105163574219 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.2562, loss_val: nan, pos_over_neg: 66.8348388671875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.2233, loss_val: nan, pos_over_neg: 62.93287658691406 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.1873, loss_val: nan, pos_over_neg: 70.97303009033203 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.0998, loss_val: nan, pos_over_neg: 78.3505859375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.1743, loss_val: nan, pos_over_neg: 49.891929626464844 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.1326, loss_val: nan, pos_over_neg: 59.34720993041992 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.1423, loss_val: nan, pos_over_neg: 71.21038818359375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.1906, loss_val: nan, pos_over_neg: 66.04652404785156 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.1247, loss_val: nan, pos_over_neg: 98.01042175292969 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.105, loss_val: nan, pos_over_neg: 77.56015014648438 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.081, loss_val: nan, pos_over_neg: 66.390625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.1219, loss_val: nan, pos_over_neg: 57.083404541015625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.1303, loss_val: nan, pos_over_neg: 81.03072357177734 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.0812, loss_val: nan, pos_over_neg: 98.02435302734375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.0554, loss_val: nan, pos_over_neg: 60.24885940551758 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.0639, loss_val: nan, pos_over_neg: 51.71074676513672 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.1158, loss_val: nan, pos_over_neg: 57.01509475708008 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.1026, loss_val: nan, pos_over_neg: 86.32465362548828 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.0768, loss_val: nan, pos_over_neg: 61.51651382446289 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.0268, loss_val: nan, pos_over_neg: 64.36505889892578 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.0141, loss_val: nan, pos_over_neg: 66.90707397460938 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.0287, loss_val: nan, pos_over_neg: 82.996826171875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.0265, loss_val: nan, pos_over_neg: 75.59764099121094 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 4.9894, loss_val: nan, pos_over_neg: 56.42458724975586 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.0334, loss_val: nan, pos_over_neg: 55.319480895996094 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 4.9584, loss_val: nan, pos_over_neg: 69.47603607177734 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 4.9459, loss_val: nan, pos_over_neg: 120.37332153320312 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 4.9871, loss_val: nan, pos_over_neg: 77.11437225341797 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 4.9874, loss_val: nan, pos_over_neg: 70.6766357421875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 4.9365, loss_val: nan, pos_over_neg: 61.194454193115234 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 4.9588, loss_val: nan, pos_over_neg: 53.66823196411133 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 4.9373, loss_val: nan, pos_over_neg: 88.9123764038086 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 4.9384, loss_val: nan, pos_over_neg: 99.35674285888672 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 4.9143, loss_val: nan, pos_over_neg: 76.77317810058594 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 4.9007, loss_val: nan, pos_over_neg: 86.6654052734375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 4.8709, loss_val: nan, pos_over_neg: 66.23966217041016 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 4.9365, loss_val: nan, pos_over_neg: 59.99204635620117 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 4.8693, loss_val: nan, pos_over_neg: 69.95247650146484 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 4.8649, loss_val: nan, pos_over_neg: 101.50975036621094 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 4.9135, loss_val: nan, pos_over_neg: 101.12616729736328 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 4.9104, loss_val: nan, pos_over_neg: 72.32825469970703 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 4.9005, loss_val: nan, pos_over_neg: 58.00322723388672 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 4.9231, loss_val: nan, pos_over_neg: 98.81159210205078 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 4.8631, loss_val: nan, pos_over_neg: 96.44501495361328 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 4.8075, loss_val: nan, pos_over_neg: 111.41935729980469 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 4.8764, loss_val: nan, pos_over_neg: 96.98907470703125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 4.7547, loss_val: nan, pos_over_neg: 101.27534484863281 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 4.822, loss_val: nan, pos_over_neg: 106.64264678955078 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 4.8993, loss_val: nan, pos_over_neg: 115.54216003417969 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 4.8418, loss_val: nan, pos_over_neg: 68.29241180419922 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 4.7873, loss_val: nan, pos_over_neg: 79.05561828613281 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 4.8388, loss_val: nan, pos_over_neg: 113.69963836669922 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 4.8373, loss_val: nan, pos_over_neg: 54.739749908447266 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 4.8082, loss_val: nan, pos_over_neg: 70.79679870605469 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 4.831, loss_val: nan, pos_over_neg: 126.80365753173828 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 4.7671, loss_val: nan, pos_over_neg: 102.84394073486328 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 4.7692, loss_val: nan, pos_over_neg: 123.92866516113281 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 4.7773, loss_val: nan, pos_over_neg: 72.71710968017578 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 4.806, loss_val: nan, pos_over_neg: 90.87579345703125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 4.8334, loss_val: nan, pos_over_neg: 106.88422393798828 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 4.7585, loss_val: nan, pos_over_neg: 79.67230987548828 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 4.7846, loss_val: nan, pos_over_neg: 83.11205291748047 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 4.7631, loss_val: nan, pos_over_neg: 100.80370330810547 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 4.7056, loss_val: nan, pos_over_neg: 99.13790130615234 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 4.7552, loss_val: nan, pos_over_neg: 78.02235412597656 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 4.7615, loss_val: nan, pos_over_neg: 79.73979949951172 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 4.7119, loss_val: nan, pos_over_neg: 125.16661834716797 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 4.743, loss_val: nan, pos_over_neg: 149.73672485351562 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 4.7533, loss_val: nan, pos_over_neg: 104.51974487304688 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 4.7707, loss_val: nan, pos_over_neg: 100.42884063720703 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 4.7407, loss_val: nan, pos_over_neg: 72.51827239990234 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 4.7194, loss_val: nan, pos_over_neg: 62.034908294677734 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 4.7553, loss_val: nan, pos_over_neg: 83.35787963867188 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 4.7087, loss_val: nan, pos_over_neg: 110.3826904296875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 4.7268, loss_val: nan, pos_over_neg: 78.24844360351562 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 4.6965, loss_val: nan, pos_over_neg: 116.50920867919922 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 4.7586, loss_val: nan, pos_over_neg: 63.390987396240234 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 4.7188, loss_val: nan, pos_over_neg: 98.57605743408203 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 4.7043, loss_val: nan, pos_over_neg: 71.65868377685547 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 4.6856, loss_val: nan, pos_over_neg: 95.74405670166016 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 4.7386, loss_val: nan, pos_over_neg: 108.42613983154297 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 4.746, loss_val: nan, pos_over_neg: 68.43022918701172 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 4.739, loss_val: nan, pos_over_neg: 79.54875946044922 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 4.7039, loss_val: nan, pos_over_neg: 97.94529724121094 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 4.6372, loss_val: nan, pos_over_neg: 116.17443084716797 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 4.7536, loss_val: nan, pos_over_neg: 59.00634002685547 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 4.6632, loss_val: nan, pos_over_neg: 104.79859161376953 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 4.7036, loss_val: nan, pos_over_neg: 80.53284454345703 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 4.7143, loss_val: nan, pos_over_neg: 85.99874877929688 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 4.7111, loss_val: nan, pos_over_neg: 146.62757873535156 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 4.6807, loss_val: nan, pos_over_neg: 104.06427001953125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 4.6732, loss_val: nan, pos_over_neg: 66.4682846069336 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 4.7151, loss_val: nan, pos_over_neg: 72.15486907958984 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 4.6573, loss_val: nan, pos_over_neg: 96.71090698242188 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 4.6796, loss_val: nan, pos_over_neg: 109.91934204101562 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 4.644, loss_val: nan, pos_over_neg: 80.74893951416016 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 4.6727, loss_val: nan, pos_over_neg: 87.56886291503906 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 4.689, loss_val: nan, pos_over_neg: 52.90232467651367 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 4.6202, loss_val: nan, pos_over_neg: 76.25741577148438 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 4.6406, loss_val: nan, pos_over_neg: 109.2862319946289 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 4.6436, loss_val: nan, pos_over_neg: 94.29341888427734 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 4.6279, loss_val: nan, pos_over_neg: 77.18363952636719 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 4.6122, loss_val: nan, pos_over_neg: 105.54435729980469 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 4.5775, loss_val: nan, pos_over_neg: 103.34194946289062 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 4.6115, loss_val: nan, pos_over_neg: 101.88607025146484 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 4.6043, loss_val: nan, pos_over_neg: 124.85124206542969 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 4.6347, loss_val: nan, pos_over_neg: 101.5020980834961 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 4.6012, loss_val: nan, pos_over_neg: 84.82207489013672 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 4.6659, loss_val: nan, pos_over_neg: 65.53297424316406 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 4.5546, loss_val: nan, pos_over_neg: 153.89369201660156 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 4.6086, loss_val: nan, pos_over_neg: 95.99696350097656 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 4.5562, loss_val: nan, pos_over_neg: 117.72724914550781 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 4.6021, loss_val: nan, pos_over_neg: 65.75831604003906 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 4.6243, loss_val: nan, pos_over_neg: 46.124732971191406 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 4.6165, loss_val: nan, pos_over_neg: 80.44316101074219 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 4.6226, loss_val: nan, pos_over_neg: 113.08394622802734 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 4.5922, loss_val: nan, pos_over_neg: 86.15156555175781 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 4.5868, loss_val: nan, pos_over_neg: 102.59397888183594 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 4.6337, loss_val: nan, pos_over_neg: 75.85256958007812 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 4.5976, loss_val: nan, pos_over_neg: 71.51020812988281 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 4.6449, loss_val: nan, pos_over_neg: 66.84339141845703 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 4.5738, loss_val: nan, pos_over_neg: 75.22587585449219 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 4.5557, loss_val: nan, pos_over_neg: 88.2454605102539 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 4.5558, loss_val: nan, pos_over_neg: 146.35464477539062 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 4.5891, loss_val: nan, pos_over_neg: 104.1111068725586 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 4.5516, loss_val: nan, pos_over_neg: 99.4378662109375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 4.5961, loss_val: nan, pos_over_neg: 83.56049346923828 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 4.5777, loss_val: nan, pos_over_neg: 84.25259399414062 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 4.6022, loss_val: nan, pos_over_neg: 79.78675842285156 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 4.5474, loss_val: nan, pos_over_neg: 65.38778686523438 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 4.5397, loss_val: nan, pos_over_neg: 75.60724639892578 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 4.5522, loss_val: nan, pos_over_neg: 93.79583740234375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 4.5549, loss_val: nan, pos_over_neg: 92.6357192993164 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 4.536, loss_val: nan, pos_over_neg: 107.47942352294922 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 4.5317, loss_val: nan, pos_over_neg: 101.42607879638672 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 4.5564, loss_val: nan, pos_over_neg: 113.0007553100586 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 4.5463, loss_val: nan, pos_over_neg: 80.4188232421875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 4.5553, loss_val: nan, pos_over_neg: 100.6851806640625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 4.5243, loss_val: nan, pos_over_neg: 78.6200942993164 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 4.5304, loss_val: nan, pos_over_neg: 159.4232177734375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 4.4919, loss_val: nan, pos_over_neg: 139.7393341064453 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 4.4869, loss_val: nan, pos_over_neg: 135.1275634765625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 4.5604, loss_val: nan, pos_over_neg: 115.0756607055664 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 4.5335, loss_val: nan, pos_over_neg: 171.720703125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 4.5664, loss_val: nan, pos_over_neg: 128.36891174316406 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 4.5066, loss_val: nan, pos_over_neg: 117.43611145019531 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 4.5309, loss_val: nan, pos_over_neg: 144.10446166992188 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 4.4979, loss_val: nan, pos_over_neg: 107.37210845947266 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 4.4651, loss_val: nan, pos_over_neg: 135.2125701904297 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 4.5097, loss_val: nan, pos_over_neg: 95.83651733398438 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 4.5128, loss_val: nan, pos_over_neg: 126.62980651855469 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 4.5229, loss_val: nan, pos_over_neg: 119.01602172851562 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 4.5428, loss_val: nan, pos_over_neg: 110.77928161621094 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 4.4654, loss_val: nan, pos_over_neg: 96.04358673095703 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 4.5086, loss_val: nan, pos_over_neg: 114.96697235107422 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 4.513, loss_val: nan, pos_over_neg: 119.14838409423828 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 4.4604, loss_val: nan, pos_over_neg: 131.78860473632812 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 4.4755, loss_val: nan, pos_over_neg: 148.25428771972656 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 4.5261, loss_val: nan, pos_over_neg: 93.204833984375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 4.518, loss_val: nan, pos_over_neg: 106.09412384033203 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 4.487, loss_val: nan, pos_over_neg: 109.11898803710938 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 4.4952, loss_val: nan, pos_over_neg: 106.20977020263672 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 4.5003, loss_val: nan, pos_over_neg: 100.60972595214844 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 4.5221, loss_val: nan, pos_over_neg: 86.56534576416016 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 4.5105, loss_val: nan, pos_over_neg: 79.55363464355469 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 4.4654, loss_val: nan, pos_over_neg: 130.0071258544922 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 4.531, loss_val: nan, pos_over_neg: 139.7374725341797 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 4.4957, loss_val: nan, pos_over_neg: 102.49424743652344 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 4.4334, loss_val: nan, pos_over_neg: 108.44002532958984 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 4.4731, loss_val: nan, pos_over_neg: 68.56725311279297 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 4.4356, loss_val: nan, pos_over_neg: 77.21109008789062 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 4.5075, loss_val: nan, pos_over_neg: 131.7688751220703 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 4.4341, loss_val: nan, pos_over_neg: 125.29129791259766 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 4.4999, loss_val: nan, pos_over_neg: 88.41297149658203 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 4.4634, loss_val: nan, pos_over_neg: 146.76524353027344 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 4.4598, loss_val: nan, pos_over_neg: 112.73296356201172 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 4.4479, loss_val: nan, pos_over_neg: 109.74137878417969 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 4.5199, loss_val: nan, pos_over_neg: 106.64706420898438 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 4.4436, loss_val: nan, pos_over_neg: 102.62055969238281 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 4.441, loss_val: nan, pos_over_neg: 99.63496398925781 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 4.4995, loss_val: nan, pos_over_neg: 110.83482360839844 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 4.431, loss_val: nan, pos_over_neg: 89.31523132324219 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 4.4547, loss_val: nan, pos_over_neg: 95.97828674316406 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 4.4352, loss_val: nan, pos_over_neg: 92.81977081298828 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 4.4359, loss_val: nan, pos_over_neg: 100.64485168457031 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 4.4733, loss_val: nan, pos_over_neg: 140.05709838867188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 4.4055, loss_val: nan, pos_over_neg: 157.94757080078125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 4.4955, loss_val: nan, pos_over_neg: 125.00022888183594 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 4.3918, loss_val: nan, pos_over_neg: 123.17376708984375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 4.4688, loss_val: nan, pos_over_neg: 88.76290130615234 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 4.4396, loss_val: nan, pos_over_neg: 97.7470474243164 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 4.4485, loss_val: nan, pos_over_neg: 110.94607543945312 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 4.4258, loss_val: nan, pos_over_neg: 127.87673950195312 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 4.3896, loss_val: nan, pos_over_neg: 154.02960205078125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 4.4199, loss_val: nan, pos_over_neg: 151.72344970703125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 4.399, loss_val: nan, pos_over_neg: 97.9468765258789 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 4.4004, loss_val: nan, pos_over_neg: 181.53436279296875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 4.386, loss_val: nan, pos_over_neg: 130.65573120117188 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 4.3724, loss_val: nan, pos_over_neg: 143.303466796875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 4.4323, loss_val: nan, pos_over_neg: 119.80046081542969 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 4.3796, loss_val: nan, pos_over_neg: 160.66978454589844 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 4.4407, loss_val: nan, pos_over_neg: 108.53348541259766 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 4.4407, loss_val: nan, pos_over_neg: 174.90411376953125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 4.4063, loss_val: nan, pos_over_neg: 186.36497497558594 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 4.4151, loss_val: nan, pos_over_neg: 142.3834686279297 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 4.4285, loss_val: nan, pos_over_neg: 123.8962631225586 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 4.4443, loss_val: nan, pos_over_neg: 114.80653381347656 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 4.4175, loss_val: nan, pos_over_neg: 98.82453918457031 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 4.3825, loss_val: nan, pos_over_neg: 146.3802032470703 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 4.3653, loss_val: nan, pos_over_neg: 134.79347229003906 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 4.4017, loss_val: nan, pos_over_neg: 170.29006958007812 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 4.397, loss_val: nan, pos_over_neg: 138.3511962890625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 4.3719, loss_val: nan, pos_over_neg: 153.54298400878906 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 4.3814, loss_val: nan, pos_over_neg: 113.14270782470703 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 4.4119, loss_val: nan, pos_over_neg: 100.92095947265625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 4.4053, loss_val: nan, pos_over_neg: 106.21723937988281 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 4.3893, loss_val: nan, pos_over_neg: 182.4680633544922 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 4.3902, loss_val: nan, pos_over_neg: 134.03811645507812 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 4.4548, loss_val: nan, pos_over_neg: 106.87659454345703 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 4.352, loss_val: nan, pos_over_neg: 117.77259826660156 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 4.3992, loss_val: nan, pos_over_neg: 102.00008392333984 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 4.3883, loss_val: nan, pos_over_neg: 84.33232116699219 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 4.3885, loss_val: nan, pos_over_neg: 96.61185455322266 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 4.3632, loss_val: nan, pos_over_neg: 113.86663055419922 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 4.3907, loss_val: nan, pos_over_neg: 133.35862731933594 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 4.3462, loss_val: nan, pos_over_neg: 146.83816528320312 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 4.3719, loss_val: nan, pos_over_neg: 195.0900421142578 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 4.371, loss_val: nan, pos_over_neg: 98.77704620361328 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 4.3085, loss_val: nan, pos_over_neg: 94.57488250732422 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 4.3701, loss_val: nan, pos_over_neg: 102.46903228759766 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 4.3571, loss_val: nan, pos_over_neg: 91.38505554199219 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 4.3756, loss_val: nan, pos_over_neg: 106.67595672607422 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 4.3359, loss_val: nan, pos_over_neg: 110.16141510009766 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 4.3414, loss_val: nan, pos_over_neg: 105.40555572509766 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 4.3049, loss_val: nan, pos_over_neg: 133.8924102783203 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 4.3717, loss_val: nan, pos_over_neg: 89.32312774658203 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 4.3445, loss_val: nan, pos_over_neg: 74.09676361083984 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 4.3738, loss_val: nan, pos_over_neg: 146.92074584960938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 4.3593, loss_val: nan, pos_over_neg: 104.28189849853516 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 4.3658, loss_val: nan, pos_over_neg: 77.22217559814453 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 4.3763, loss_val: nan, pos_over_neg: 129.21583557128906 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 4.3176, loss_val: nan, pos_over_neg: 98.7999496459961 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 4.318, loss_val: nan, pos_over_neg: 117.661865234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 4.3645, loss_val: nan, pos_over_neg: 169.31622314453125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 4.3045, loss_val: nan, pos_over_neg: 138.06747436523438 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 4.3714, loss_val: nan, pos_over_neg: 114.0635757446289 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 4.3515, loss_val: nan, pos_over_neg: 109.37447357177734 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 4.322, loss_val: nan, pos_over_neg: 89.11369323730469 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 4.3037, loss_val: nan, pos_over_neg: 153.57936096191406 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 4.3617, loss_val: nan, pos_over_neg: 108.97831726074219 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 4.3917, loss_val: nan, pos_over_neg: 109.90054321289062 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 4.3476, loss_val: nan, pos_over_neg: 82.24291229248047 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 4.3562, loss_val: nan, pos_over_neg: 91.54486083984375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 4.3874, loss_val: nan, pos_over_neg: 91.59260559082031 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 4.3096, loss_val: nan, pos_over_neg: 123.07073974609375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 4.3254, loss_val: nan, pos_over_neg: 213.07240295410156 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 4.3147, loss_val: nan, pos_over_neg: 200.65939331054688 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 4.3264, loss_val: nan, pos_over_neg: 130.49916076660156 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 4.3283, loss_val: nan, pos_over_neg: 72.18878173828125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 4.3722, loss_val: nan, pos_over_neg: 86.90546417236328 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 4.2895, loss_val: nan, pos_over_neg: 178.88035583496094 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 4.3013, loss_val: nan, pos_over_neg: 190.21710205078125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 4.3377, loss_val: nan, pos_over_neg: 171.7261505126953 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 4.2892, loss_val: nan, pos_over_neg: 144.91542053222656 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 4.3356, loss_val: nan, pos_over_neg: 118.41497039794922 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 4.3426, loss_val: nan, pos_over_neg: 104.05464935302734 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 4.2814, loss_val: nan, pos_over_neg: 104.99186706542969 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 4.2987, loss_val: nan, pos_over_neg: 156.0469970703125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 4.3626, loss_val: nan, pos_over_neg: 157.66171264648438 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 4.3527, loss_val: nan, pos_over_neg: 100.3263168334961 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 4.2835, loss_val: nan, pos_over_neg: 121.573486328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 4.2767, loss_val: nan, pos_over_neg: 168.35105895996094 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 4.3279, loss_val: nan, pos_over_neg: 123.1777114868164 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 4.3023, loss_val: nan, pos_over_neg: 121.15904998779297 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 4.2638, loss_val: nan, pos_over_neg: 197.1437530517578 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 4.3148, loss_val: nan, pos_over_neg: 125.38664245605469 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 4.2895, loss_val: nan, pos_over_neg: 220.6475067138672 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 4.2658, loss_val: nan, pos_over_neg: 166.6380615234375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 4.3388, loss_val: nan, pos_over_neg: 105.88420104980469 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 4.343, loss_val: nan, pos_over_neg: 135.14691162109375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 4.2744, loss_val: nan, pos_over_neg: 127.13609313964844 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 4.2806, loss_val: nan, pos_over_neg: 149.60382080078125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 4.2894, loss_val: nan, pos_over_neg: 218.7859344482422 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 4.2702, loss_val: nan, pos_over_neg: 187.1542205810547 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 4.323, loss_val: nan, pos_over_neg: 170.30027770996094 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 4.3059, loss_val: nan, pos_over_neg: 139.91726684570312 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 4.2662, loss_val: nan, pos_over_neg: 165.60055541992188 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 4.323, loss_val: nan, pos_over_neg: 127.9231185913086 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 4.284, loss_val: nan, pos_over_neg: 205.65464782714844 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 4.2782, loss_val: nan, pos_over_neg: 225.8633575439453 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 4.319, loss_val: nan, pos_over_neg: 116.29499816894531 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 4.2797, loss_val: nan, pos_over_neg: 143.9059295654297 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 4.2258, loss_val: nan, pos_over_neg: 219.43838500976562 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 4.2769, loss_val: nan, pos_over_neg: 165.91970825195312 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 4.3031, loss_val: nan, pos_over_neg: 160.58705139160156 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 4.3651, loss_val: nan, pos_over_neg: 129.57858276367188 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 4.2503, loss_val: nan, pos_over_neg: 139.1705322265625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 4.3073, loss_val: nan, pos_over_neg: 167.77789306640625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 4.2496, loss_val: nan, pos_over_neg: 159.8079071044922 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 4.2992, loss_val: nan, pos_over_neg: 191.08351135253906 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 4.3289, loss_val: nan, pos_over_neg: 133.26266479492188 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 4.3183, loss_val: nan, pos_over_neg: 174.82293701171875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 4.2931, loss_val: nan, pos_over_neg: 138.8896026611328 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 4.2834, loss_val: nan, pos_over_neg: 259.576904296875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 4.2726, loss_val: nan, pos_over_neg: 138.7542724609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 4.3108, loss_val: nan, pos_over_neg: 92.50743103027344 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 4.2998, loss_val: nan, pos_over_neg: 100.90611267089844 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 4.2814, loss_val: nan, pos_over_neg: 110.70594024658203 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 4.3065, loss_val: nan, pos_over_neg: 149.66348266601562 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 4.2839, loss_val: nan, pos_over_neg: 126.04762268066406 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 4.2337, loss_val: nan, pos_over_neg: 146.56036376953125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 4.3409, loss_val: nan, pos_over_neg: 112.65382385253906 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 4.3363, loss_val: nan, pos_over_neg: 94.49192810058594 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 4.256, loss_val: nan, pos_over_neg: 122.51921844482422 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 4.2647, loss_val: nan, pos_over_neg: 158.1798553466797 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 4.2401, loss_val: nan, pos_over_neg: 197.59970092773438 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 4.2338, loss_val: nan, pos_over_neg: 121.2422866821289 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 4.2725, loss_val: nan, pos_over_neg: 156.56893920898438 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 4.287, loss_val: nan, pos_over_neg: 151.33236694335938 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 4.226, loss_val: nan, pos_over_neg: 179.22207641601562 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 4.3017, loss_val: nan, pos_over_neg: 101.53435516357422 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 4.2602, loss_val: nan, pos_over_neg: 145.2141876220703 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 4.2855, loss_val: nan, pos_over_neg: 160.28233337402344 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 4.2646, loss_val: nan, pos_over_neg: 173.861328125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 4.3236, loss_val: nan, pos_over_neg: 154.08224487304688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 4.2151, loss_val: nan, pos_over_neg: 152.844970703125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 4.2806, loss_val: nan, pos_over_neg: 167.8987579345703 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 4.2505, loss_val: nan, pos_over_neg: 152.12879943847656 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 4.2066, loss_val: nan, pos_over_neg: 142.5716094970703 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 4.2555, loss_val: nan, pos_over_neg: 124.65496826171875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 4.241, loss_val: nan, pos_over_neg: 182.4636993408203 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 4.2305, loss_val: nan, pos_over_neg: 188.6475830078125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 4.2617, loss_val: nan, pos_over_neg: 175.0924835205078 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 4.3284, loss_val: nan, pos_over_neg: 106.0150375366211 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 4.1977, loss_val: nan, pos_over_neg: 130.7196044921875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 4.2448, loss_val: nan, pos_over_neg: 140.46824645996094 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 4.2989, loss_val: nan, pos_over_neg: 100.23178100585938 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 4.305, loss_val: nan, pos_over_neg: 135.06936645507812 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 4.2881, loss_val: nan, pos_over_neg: 158.85707092285156 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 4.2789, loss_val: nan, pos_over_neg: 142.09857177734375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 4.2328, loss_val: nan, pos_over_neg: 177.7854461669922 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 4.2284, loss_val: nan, pos_over_neg: 179.1440887451172 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 4.2949, loss_val: nan, pos_over_neg: 103.12368774414062 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 4.2889, loss_val: nan, pos_over_neg: 138.90516662597656 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 4.261, loss_val: nan, pos_over_neg: 198.45025634765625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 4.2514, loss_val: nan, pos_over_neg: 137.5988311767578 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 4.2548, loss_val: nan, pos_over_neg: 176.0218505859375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 4.2282, loss_val: nan, pos_over_neg: 142.06393432617188 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 4.1946, loss_val: nan, pos_over_neg: 163.63082885742188 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 4.2475, loss_val: nan, pos_over_neg: 156.4615478515625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 4.2053, loss_val: nan, pos_over_neg: 171.65000915527344 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 4.2359, loss_val: nan, pos_over_neg: 280.29345703125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 4.2015, loss_val: nan, pos_over_neg: 167.84934997558594 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 4.2591, loss_val: nan, pos_over_neg: 106.2336654663086 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 4.2237, loss_val: nan, pos_over_neg: 169.19778442382812 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 4.2266, loss_val: nan, pos_over_neg: 172.67889404296875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 4.2011, loss_val: nan, pos_over_neg: 118.99088287353516 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 4.2342, loss_val: nan, pos_over_neg: 98.09500885009766 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 4.2162, loss_val: nan, pos_over_neg: 180.8906707763672 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 4.204, loss_val: nan, pos_over_neg: 144.69432067871094 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 4.2246, loss_val: nan, pos_over_neg: 137.43775939941406 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 4.2219, loss_val: nan, pos_over_neg: 145.3663787841797 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 4.2386, loss_val: nan, pos_over_neg: 169.81787109375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 4.2238, loss_val: nan, pos_over_neg: 131.3186798095703 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 4.2243, loss_val: nan, pos_over_neg: 111.61493682861328 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 4.2521, loss_val: nan, pos_over_neg: 88.13189697265625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 4.2262, loss_val: nan, pos_over_neg: 146.21791076660156 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 4.2352, loss_val: nan, pos_over_neg: 131.8392333984375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 4.1959, loss_val: nan, pos_over_neg: 198.01043701171875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 4.2163, loss_val: nan, pos_over_neg: 254.20675659179688 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 4.1785, loss_val: nan, pos_over_neg: 166.1537322998047 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 4.2213, loss_val: nan, pos_over_neg: 141.3778533935547 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 4.2514, loss_val: nan, pos_over_neg: 137.60357666015625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 4.198, loss_val: nan, pos_over_neg: 99.13823699951172 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 4.2211, loss_val: nan, pos_over_neg: 130.0079803466797 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 4.1634, loss_val: nan, pos_over_neg: 182.76748657226562 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 4.2236, loss_val: nan, pos_over_neg: 126.2627182006836 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 4.2195, loss_val: nan, pos_over_neg: 132.7268829345703 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 4.2314, loss_val: nan, pos_over_neg: 151.19004821777344 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 4.2015, loss_val: nan, pos_over_neg: 189.33970642089844 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 4.1874, loss_val: nan, pos_over_neg: 134.81991577148438 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 4.1885, loss_val: nan, pos_over_neg: 131.02542114257812 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 4.2135, loss_val: nan, pos_over_neg: 121.95873260498047 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 4.1981, loss_val: nan, pos_over_neg: 138.7036590576172 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 4.2512, loss_val: nan, pos_over_neg: 124.08488464355469 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 4.2105, loss_val: nan, pos_over_neg: 118.28878021240234 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 4.2328, loss_val: nan, pos_over_neg: 142.07212829589844 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 4.179, loss_val: nan, pos_over_neg: 146.73826599121094 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 4.2283, loss_val: nan, pos_over_neg: 132.57797241210938 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 4.2247, loss_val: nan, pos_over_neg: 150.04415893554688 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 4.2191, loss_val: nan, pos_over_neg: 134.76202392578125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 4.2373, loss_val: nan, pos_over_neg: 127.42649841308594 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 4.2306, loss_val: nan, pos_over_neg: 124.49250793457031 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 4.1883, loss_val: nan, pos_over_neg: 206.635986328125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 4.1913, loss_val: nan, pos_over_neg: 182.1182861328125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 4.21, loss_val: nan, pos_over_neg: 186.56570434570312 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 4.2236, loss_val: nan, pos_over_neg: 167.86233520507812 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 4.1663, loss_val: nan, pos_over_neg: 174.4544677734375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 4.1965, loss_val: nan, pos_over_neg: 199.59654235839844 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 4.1874, loss_val: nan, pos_over_neg: 171.79678344726562 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 4.2222, loss_val: nan, pos_over_neg: 99.67432403564453 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 4.2259, loss_val: nan, pos_over_neg: 145.89837646484375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 4.2412, loss_val: nan, pos_over_neg: 111.56486511230469 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 4.2411, loss_val: nan, pos_over_neg: 139.81396484375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 4.1802, loss_val: nan, pos_over_neg: 211.9264678955078 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 4.196, loss_val: nan, pos_over_neg: 205.61080932617188 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 4.2006, loss_val: nan, pos_over_neg: 226.89984130859375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 4.2251, loss_val: nan, pos_over_neg: 192.9445037841797 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 4.2693, loss_val: nan, pos_over_neg: 123.685791015625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 4.1676, loss_val: nan, pos_over_neg: 160.332763671875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 4.2569, loss_val: nan, pos_over_neg: 132.09288024902344 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 4.1899, loss_val: nan, pos_over_neg: 149.4341583251953 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 4.1884, loss_val: nan, pos_over_neg: 141.88540649414062 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 4.1711, loss_val: nan, pos_over_neg: 153.1090087890625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 4.17, loss_val: nan, pos_over_neg: 208.69674682617188 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 4.1372, loss_val: nan, pos_over_neg: 256.9014892578125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 4.2027, loss_val: nan, pos_over_neg: 209.11709594726562 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 4.149, loss_val: nan, pos_over_neg: 146.1354522705078 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 4.2036, loss_val: nan, pos_over_neg: 178.2553253173828 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 4.1912, loss_val: nan, pos_over_neg: 153.7821807861328 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 4.1669, loss_val: nan, pos_over_neg: 104.22305297851562 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 4.2166, loss_val: nan, pos_over_neg: 165.79055786132812 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 4.1687, loss_val: nan, pos_over_neg: 245.15689086914062 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 4.1537, loss_val: nan, pos_over_neg: 132.80455017089844 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 4.2094, loss_val: nan, pos_over_neg: 152.22804260253906 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 4.14, loss_val: nan, pos_over_neg: 269.33837890625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 4.1212, loss_val: nan, pos_over_neg: 232.48866271972656 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 4.1364, loss_val: nan, pos_over_neg: 215.82736206054688 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 4.159, loss_val: nan, pos_over_neg: 162.6874237060547 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 4.1898, loss_val: nan, pos_over_neg: 124.58849334716797 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 4.2001, loss_val: nan, pos_over_neg: 157.0887908935547 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 4.1557, loss_val: nan, pos_over_neg: 198.20518493652344 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 4.1202, loss_val: nan, pos_over_neg: 257.6325988769531 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 4.1485, loss_val: nan, pos_over_neg: 239.93177795410156 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 4.1637, loss_val: nan, pos_over_neg: 203.8503875732422 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 4.1201, loss_val: nan, pos_over_neg: 128.4939727783203 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 4.1512, loss_val: nan, pos_over_neg: 155.16505432128906 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 4.1346, loss_val: nan, pos_over_neg: 182.4730682373047 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 4.2032, loss_val: nan, pos_over_neg: 137.57809448242188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 4.147, loss_val: nan, pos_over_neg: 170.42681884765625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 4.1733, loss_val: nan, pos_over_neg: 162.7513885498047 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 4.1686, loss_val: nan, pos_over_neg: 153.4692840576172 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 4.1718, loss_val: nan, pos_over_neg: 199.71778869628906 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 4.1282, loss_val: nan, pos_over_neg: 210.93482971191406 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 4.1499, loss_val: nan, pos_over_neg: 225.33680725097656 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 4.1984, loss_val: nan, pos_over_neg: 147.93495178222656 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 4.1489, loss_val: nan, pos_over_neg: 124.123779296875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 4.1502, loss_val: nan, pos_over_neg: 157.49777221679688 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 4.164, loss_val: nan, pos_over_neg: 144.5494384765625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 4.1633, loss_val: nan, pos_over_neg: 154.62623596191406 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 4.1235, loss_val: nan, pos_over_neg: 232.67079162597656 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 4.1444, loss_val: nan, pos_over_neg: 181.67893981933594 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 4.1497, loss_val: nan, pos_over_neg: 171.28915405273438 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 4.1266, loss_val: nan, pos_over_neg: 229.59864807128906 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 4.1689, loss_val: nan, pos_over_neg: 163.85971069335938 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 4.1436, loss_val: nan, pos_over_neg: 227.02626037597656 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 4.1215, loss_val: nan, pos_over_neg: 158.93313598632812 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 4.1661, loss_val: nan, pos_over_neg: 203.72763061523438 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 4.1553, loss_val: nan, pos_over_neg: 151.8385772705078 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 4.1988, loss_val: nan, pos_over_neg: 160.5801239013672 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 4.2133, loss_val: nan, pos_over_neg: 189.52130126953125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 4.1541, loss_val: nan, pos_over_neg: 200.8719482421875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 4.152, loss_val: nan, pos_over_neg: 216.53717041015625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 4.1605, loss_val: nan, pos_over_neg: 146.94808959960938 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 4.1787, loss_val: nan, pos_over_neg: 140.2659149169922 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 4.1759, loss_val: nan, pos_over_neg: 107.84630584716797 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 4.1242, loss_val: nan, pos_over_neg: 129.60585021972656 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 4.0905, loss_val: nan, pos_over_neg: 306.3993225097656 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 4.1284, loss_val: nan, pos_over_neg: 270.27691650390625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 4.1751, loss_val: nan, pos_over_neg: 184.11370849609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 4.1649, loss_val: nan, pos_over_neg: 208.76820373535156 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 4.171, loss_val: nan, pos_over_neg: 217.06434631347656 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 4.1706, loss_val: nan, pos_over_neg: 149.4275360107422 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 4.1312, loss_val: nan, pos_over_neg: 140.29086303710938 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 4.1619, loss_val: nan, pos_over_neg: 133.0029296875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 4.1477, loss_val: nan, pos_over_neg: 132.70762634277344 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 4.0758, loss_val: nan, pos_over_neg: 182.5474395751953 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 4.11, loss_val: nan, pos_over_neg: 199.0355987548828 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 4.1689, loss_val: nan, pos_over_neg: 148.81094360351562 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 4.1148, loss_val: nan, pos_over_neg: 194.5315399169922 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 4.1761, loss_val: nan, pos_over_neg: 172.0135040283203 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 4.186, loss_val: nan, pos_over_neg: 149.52845764160156 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 4.142, loss_val: nan, pos_over_neg: 104.56089782714844 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 4.0793, loss_val: nan, pos_over_neg: 169.89466857910156 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 4.1082, loss_val: nan, pos_over_neg: 157.68482971191406 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 4.1415, loss_val: nan, pos_over_neg: 117.89432525634766 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 4.1481, loss_val: nan, pos_over_neg: 119.28107452392578 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 4.1356, loss_val: nan, pos_over_neg: 237.5593719482422 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 4.1136, loss_val: nan, pos_over_neg: 165.4720001220703 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 4.1432, loss_val: nan, pos_over_neg: 129.6504669189453 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 4.1854, loss_val: nan, pos_over_neg: 131.64964294433594 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 4.1381, loss_val: nan, pos_over_neg: 125.59272766113281 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 4.1433, loss_val: nan, pos_over_neg: 145.04002380371094 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 4.1299, loss_val: nan, pos_over_neg: 161.08787536621094 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 4.1205, loss_val: nan, pos_over_neg: 159.60165405273438 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 4.1394, loss_val: nan, pos_over_neg: 228.31626892089844 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 4.159, loss_val: nan, pos_over_neg: 164.66021728515625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 4.0802, loss_val: nan, pos_over_neg: 185.143798828125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 4.1217, loss_val: nan, pos_over_neg: 174.10934448242188 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 4.1656, loss_val: nan, pos_over_neg: 146.2204132080078 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 4.0967, loss_val: nan, pos_over_neg: 211.04466247558594 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 4.1111, loss_val: nan, pos_over_neg: 232.84561157226562 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 4.1393, loss_val: nan, pos_over_neg: 199.9132843017578 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 4.1179, loss_val: nan, pos_over_neg: 159.25491333007812 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 4.103, loss_val: nan, pos_over_neg: 183.8504180908203 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 4.1407, loss_val: nan, pos_over_neg: 147.82923889160156 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 4.0958, loss_val: nan, pos_over_neg: 158.70184326171875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 4.1369, loss_val: nan, pos_over_neg: 140.99795532226562 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 4.1137, loss_val: nan, pos_over_neg: 130.5084991455078 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 4.1108, loss_val: nan, pos_over_neg: 297.9116516113281 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 4.1235, loss_val: nan, pos_over_neg: 198.70660400390625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 4.1451, loss_val: nan, pos_over_neg: 225.69955444335938 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 4.1239, loss_val: nan, pos_over_neg: 265.8674621582031 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 4.1343, loss_val: nan, pos_over_neg: 176.1457061767578 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 4.119, loss_val: nan, pos_over_neg: 125.1623764038086 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 4.0812, loss_val: nan, pos_over_neg: 210.93927001953125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 4.1408, loss_val: nan, pos_over_neg: 137.33729553222656 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 4.0947, loss_val: nan, pos_over_neg: 258.9129333496094 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 4.1299, loss_val: nan, pos_over_neg: 296.0765075683594 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 4.0956, loss_val: nan, pos_over_neg: 293.271728515625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 4.1005, loss_val: nan, pos_over_neg: 232.46560668945312 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 4.1506, loss_val: nan, pos_over_neg: 188.412353515625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 4.1441, loss_val: nan, pos_over_neg: 183.75729370117188 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 4.1048, loss_val: nan, pos_over_neg: 156.7905731201172 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 4.1068, loss_val: nan, pos_over_neg: 176.37481689453125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 4.1444, loss_val: nan, pos_over_neg: 212.04110717773438 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 4.089, loss_val: nan, pos_over_neg: 317.9455871582031 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 4.0932, loss_val: nan, pos_over_neg: 175.16741943359375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 4.1024, loss_val: nan, pos_over_neg: 246.1646728515625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 4.0807, loss_val: nan, pos_over_neg: 198.6979217529297 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 4.1252, loss_val: nan, pos_over_neg: 114.02122497558594 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 4.0573, loss_val: nan, pos_over_neg: 146.83212280273438 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 4.1438, loss_val: nan, pos_over_neg: 144.76101684570312 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 4.087, loss_val: nan, pos_over_neg: 200.0033721923828 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 4.0922, loss_val: nan, pos_over_neg: 243.249755859375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 4.0931, loss_val: nan, pos_over_neg: 217.0397491455078 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 4.0696, loss_val: nan, pos_over_neg: 265.30859375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 4.0923, loss_val: nan, pos_over_neg: 256.4494934082031 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 4.0704, loss_val: nan, pos_over_neg: 260.3043212890625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 4.1126, loss_val: nan, pos_over_neg: 228.56866455078125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 4.0659, loss_val: nan, pos_over_neg: 145.63427734375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 4.0703, loss_val: nan, pos_over_neg: 175.91094970703125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 4.1046, loss_val: nan, pos_over_neg: 184.14788818359375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 4.0402, loss_val: nan, pos_over_neg: 223.10638427734375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 4.1019, loss_val: nan, pos_over_neg: 203.82083129882812 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 4.0924, loss_val: nan, pos_over_neg: 145.38597106933594 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 4.1303, loss_val: nan, pos_over_neg: 184.97886657714844 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 4.095, loss_val: nan, pos_over_neg: 185.37989807128906 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 4.1325, loss_val: nan, pos_over_neg: 133.263916015625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 4.1286, loss_val: nan, pos_over_neg: 120.35292053222656 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 4.0382, loss_val: nan, pos_over_neg: 201.1746368408203 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 4.0527, loss_val: nan, pos_over_neg: 209.18182373046875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 4.0983, loss_val: nan, pos_over_neg: 128.9643096923828 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 4.0769, loss_val: nan, pos_over_neg: 160.86465454101562 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 4.0962, loss_val: nan, pos_over_neg: 154.975830078125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 4.1404, loss_val: nan, pos_over_neg: 158.86920166015625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 4.0888, loss_val: nan, pos_over_neg: 168.5359649658203 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 4.0457, loss_val: nan, pos_over_neg: 172.48883056640625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 4.1684, loss_val: nan, pos_over_neg: 109.71697998046875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 4.0918, loss_val: nan, pos_over_neg: 159.4154510498047 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 4.0595, loss_val: nan, pos_over_neg: 224.1882781982422 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 4.1237, loss_val: nan, pos_over_neg: 152.2755889892578 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 4.0597, loss_val: nan, pos_over_neg: 214.53562927246094 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 4.0749, loss_val: nan, pos_over_neg: 164.51431274414062 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 4.0883, loss_val: nan, pos_over_neg: 120.67511749267578 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 4.1509, loss_val: nan, pos_over_neg: 129.81967163085938 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 4.0767, loss_val: nan, pos_over_neg: 149.76234436035156 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 4.1095, loss_val: nan, pos_over_neg: 138.29574584960938 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 4.1292, loss_val: nan, pos_over_neg: 162.4740447998047 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 4.1006, loss_val: nan, pos_over_neg: 194.01698303222656 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 4.1102, loss_val: nan, pos_over_neg: 148.36221313476562 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 4.0811, loss_val: nan, pos_over_neg: 145.66148376464844 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 4.0852, loss_val: nan, pos_over_neg: 176.68096923828125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 4.1202, loss_val: nan, pos_over_neg: 151.6559600830078 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 4.0736, loss_val: nan, pos_over_neg: 145.9170684814453 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 4.052, loss_val: nan, pos_over_neg: 170.04934692382812 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 4.0748, loss_val: nan, pos_over_neg: 171.4230499267578 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 4.0163, loss_val: nan, pos_over_neg: 249.318603515625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 4.0521, loss_val: nan, pos_over_neg: 219.52188110351562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 4.1091, loss_val: nan, pos_over_neg: 152.3200225830078 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 4.0888, loss_val: nan, pos_over_neg: 187.5725860595703 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 4.0869, loss_val: nan, pos_over_neg: 198.84539794921875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 4.0335, loss_val: nan, pos_over_neg: 191.45228576660156 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 4.0952, loss_val: nan, pos_over_neg: 167.4955291748047 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 4.0819, loss_val: nan, pos_over_neg: 144.7252960205078 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 4.0757, loss_val: nan, pos_over_neg: 249.1234130859375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 4.0705, loss_val: nan, pos_over_neg: 181.17808532714844 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 4.0228, loss_val: nan, pos_over_neg: 291.9968566894531 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 4.0859, loss_val: nan, pos_over_neg: 389.66650390625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 4.0477, loss_val: nan, pos_over_neg: 212.17674255371094 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 4.0746, loss_val: nan, pos_over_neg: 155.94290161132812 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 4.06, loss_val: nan, pos_over_neg: 177.74606323242188 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 4.0654, loss_val: nan, pos_over_neg: 129.85385131835938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 4.0696, loss_val: nan, pos_over_neg: 192.67648315429688 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 4.1078, loss_val: nan, pos_over_neg: 182.8885040283203 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 4.0551, loss_val: nan, pos_over_neg: 294.04571533203125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 4.0859, loss_val: nan, pos_over_neg: 296.1553649902344 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 4.0996, loss_val: nan, pos_over_neg: 181.0212860107422 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 4.0603, loss_val: nan, pos_over_neg: 226.81190490722656 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 4.0729, loss_val: nan, pos_over_neg: 242.1851348876953 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 4.0595, loss_val: nan, pos_over_neg: 166.27304077148438 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 4.083, loss_val: nan, pos_over_neg: 209.15370178222656 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 4.0736, loss_val: nan, pos_over_neg: 216.58628845214844 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 4.0498, loss_val: nan, pos_over_neg: 221.27227783203125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 4.0701, loss_val: nan, pos_over_neg: 178.4580535888672 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 4.0544, loss_val: nan, pos_over_neg: 156.70843505859375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 4.0325, loss_val: nan, pos_over_neg: 181.55972290039062 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 4.0462, loss_val: nan, pos_over_neg: 181.22386169433594 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 4.0874, loss_val: nan, pos_over_neg: 166.486572265625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 4.0996, loss_val: nan, pos_over_neg: 164.21543884277344 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 4.0253, loss_val: nan, pos_over_neg: 345.8648681640625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 4.1122, loss_val: nan, pos_over_neg: 204.5574188232422 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 4.0881, loss_val: nan, pos_over_neg: 168.3410186767578 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 4.0632, loss_val: nan, pos_over_neg: 152.87017822265625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 4.062, loss_val: nan, pos_over_neg: 111.50467681884766 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 4.1295, loss_val: nan, pos_over_neg: 126.52845001220703 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 4.0941, loss_val: nan, pos_over_neg: 183.92384338378906 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 4.0537, loss_val: nan, pos_over_neg: 233.4520263671875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 4.0658, loss_val: nan, pos_over_neg: 273.3432922363281 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 4.1032, loss_val: nan, pos_over_neg: 174.29901123046875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 4.0657, loss_val: nan, pos_over_neg: 206.9175262451172 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 4.1049, loss_val: nan, pos_over_neg: 164.87835693359375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 4.0677, loss_val: nan, pos_over_neg: 158.2624969482422 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 4.0288, loss_val: nan, pos_over_neg: 127.85740661621094 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 4.1045, loss_val: nan, pos_over_neg: 119.19974517822266 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 4.0508, loss_val: nan, pos_over_neg: 138.51829528808594 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 4.0692, loss_val: nan, pos_over_neg: 210.15980529785156 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 4.0319, loss_val: nan, pos_over_neg: 339.45068359375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 4.0538, loss_val: nan, pos_over_neg: 202.1729736328125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 4.0427, loss_val: nan, pos_over_neg: 202.18844604492188 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 4.0521, loss_val: nan, pos_over_neg: 166.9287567138672 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 4.0644, loss_val: nan, pos_over_neg: 147.16932678222656 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 4.0865, loss_val: nan, pos_over_neg: 183.35182189941406 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 4.0808, loss_val: nan, pos_over_neg: 154.8529815673828 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 4.0141, loss_val: nan, pos_over_neg: 179.0621337890625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 4.0968, loss_val: nan, pos_over_neg: 148.37249755859375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 4.1109, loss_val: nan, pos_over_neg: 135.5157928466797 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 4.0619, loss_val: nan, pos_over_neg: 180.3437042236328 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 4.0243, loss_val: nan, pos_over_neg: 267.9679870605469 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 4.0428, loss_val: nan, pos_over_neg: 267.5064697265625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 4.0803, loss_val: nan, pos_over_neg: 175.869873046875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 4.0677, loss_val: nan, pos_over_neg: 156.0225067138672 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [15:28<77389:09:33, 928.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.9996, loss_val: nan, pos_over_neg: 221.7789764404297 lr: 0.00031623\n",
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 4.0963, loss_val: nan, pos_over_neg: 112.41771697998047 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 4.0751, loss_val: nan, pos_over_neg: 138.1513671875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 4.0033, loss_val: nan, pos_over_neg: 195.693603515625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 4.0682, loss_val: nan, pos_over_neg: 197.64073181152344 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 4.051, loss_val: nan, pos_over_neg: 216.97677612304688 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 4.079, loss_val: nan, pos_over_neg: 178.913330078125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 4.0528, loss_val: nan, pos_over_neg: 261.31243896484375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 4.0514, loss_val: nan, pos_over_neg: 199.71282958984375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 4.1074, loss_val: nan, pos_over_neg: 208.39236450195312 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.9882, loss_val: nan, pos_over_neg: 309.50616455078125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 4.0446, loss_val: nan, pos_over_neg: 230.28387451171875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 4.1204, loss_val: nan, pos_over_neg: 140.3318634033203 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 4.067, loss_val: nan, pos_over_neg: 199.54183959960938 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 4.0463, loss_val: nan, pos_over_neg: 153.79132080078125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 4.043, loss_val: nan, pos_over_neg: 128.05308532714844 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 4.0504, loss_val: nan, pos_over_neg: 174.55218505859375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 4.042, loss_val: nan, pos_over_neg: 252.39112854003906 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 4.0581, loss_val: nan, pos_over_neg: 198.71987915039062 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 4.0448, loss_val: nan, pos_over_neg: 208.5980224609375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 4.0134, loss_val: nan, pos_over_neg: 252.85665893554688 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 4.0507, loss_val: nan, pos_over_neg: 278.89031982421875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 4.0385, loss_val: nan, pos_over_neg: 206.884521484375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 4.0365, loss_val: nan, pos_over_neg: 147.33680725097656 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 4.0202, loss_val: nan, pos_over_neg: 176.80508422851562 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 4.0668, loss_val: nan, pos_over_neg: 180.1009063720703 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 4.0638, loss_val: nan, pos_over_neg: 175.14195251464844 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 4.0559, loss_val: nan, pos_over_neg: 189.75909423828125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 4.0459, loss_val: nan, pos_over_neg: 261.65753173828125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 4.056, loss_val: nan, pos_over_neg: 373.2056579589844 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 4.0678, loss_val: nan, pos_over_neg: 192.68650817871094 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 4.0211, loss_val: nan, pos_over_neg: 180.15028381347656 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 4.0243, loss_val: nan, pos_over_neg: 193.0492706298828 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 4.0659, loss_val: nan, pos_over_neg: 199.8202362060547 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 4.0194, loss_val: nan, pos_over_neg: 163.15660095214844 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 4.038, loss_val: nan, pos_over_neg: 181.6900634765625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 4.0808, loss_val: nan, pos_over_neg: 176.4973602294922 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 4.0271, loss_val: nan, pos_over_neg: 189.44015502929688 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 4.0257, loss_val: nan, pos_over_neg: 184.72439575195312 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 4.0371, loss_val: nan, pos_over_neg: 133.76365661621094 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 4.0822, loss_val: nan, pos_over_neg: 177.38912963867188 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 4.0503, loss_val: nan, pos_over_neg: 225.45550537109375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 4.0278, loss_val: nan, pos_over_neg: 240.87643432617188 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 4.0447, loss_val: nan, pos_over_neg: 200.41259765625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.9936, loss_val: nan, pos_over_neg: 183.0924835205078 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 4.0249, loss_val: nan, pos_over_neg: 166.57252502441406 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 4.0246, loss_val: nan, pos_over_neg: 222.59698486328125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 4.0288, loss_val: nan, pos_over_neg: 191.15115356445312 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 4.0448, loss_val: nan, pos_over_neg: 164.43917846679688 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 4.0779, loss_val: nan, pos_over_neg: 119.41305541992188 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 4.0345, loss_val: nan, pos_over_neg: 175.26458740234375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 4.0555, loss_val: nan, pos_over_neg: 193.99224853515625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 4.0056, loss_val: nan, pos_over_neg: 203.68263244628906 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 4.0431, loss_val: nan, pos_over_neg: 208.10678100585938 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 4.0117, loss_val: nan, pos_over_neg: 271.3167419433594 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 4.0091, loss_val: nan, pos_over_neg: 230.58074951171875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.9944, loss_val: nan, pos_over_neg: 208.7578125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.9843, loss_val: nan, pos_over_neg: 234.328857421875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 4.0995, loss_val: nan, pos_over_neg: 114.60820007324219 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.9588, loss_val: nan, pos_over_neg: 175.39097595214844 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 4.0542, loss_val: nan, pos_over_neg: 183.17710876464844 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 4.0762, loss_val: nan, pos_over_neg: 119.39871215820312 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 4.039, loss_val: nan, pos_over_neg: 161.2099609375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 4.0705, loss_val: nan, pos_over_neg: 188.7696990966797 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 4.0603, loss_val: nan, pos_over_neg: 261.2398376464844 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 4.0482, loss_val: nan, pos_over_neg: 172.6772003173828 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 4.0136, loss_val: nan, pos_over_neg: 203.7626495361328 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.975, loss_val: nan, pos_over_neg: 140.3064727783203 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 4.012, loss_val: nan, pos_over_neg: 192.4451446533203 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 4.0586, loss_val: nan, pos_over_neg: 121.14617919921875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 4.0643, loss_val: nan, pos_over_neg: 132.84677124023438 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 4.0204, loss_val: nan, pos_over_neg: 163.4199981689453 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 4.0202, loss_val: nan, pos_over_neg: 217.98721313476562 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 4.0178, loss_val: nan, pos_over_neg: 307.7709655761719 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 4.0737, loss_val: nan, pos_over_neg: 161.533447265625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 4.0584, loss_val: nan, pos_over_neg: 165.5275115966797 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 4.0394, loss_val: nan, pos_over_neg: 185.46426391601562 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 4.0565, loss_val: nan, pos_over_neg: 267.5020446777344 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 4.0011, loss_val: nan, pos_over_neg: 171.47271728515625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 4.064, loss_val: nan, pos_over_neg: 169.937255859375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 4.0529, loss_val: nan, pos_over_neg: 199.77317810058594 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.9582, loss_val: nan, pos_over_neg: 170.49586486816406 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 4.1006, loss_val: nan, pos_over_neg: 133.96961975097656 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 4.0187, loss_val: nan, pos_over_neg: 160.84664916992188 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 4.0053, loss_val: nan, pos_over_neg: 252.11875915527344 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 4.0343, loss_val: nan, pos_over_neg: 170.6884307861328 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.999, loss_val: nan, pos_over_neg: 242.40420532226562 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 4.0476, loss_val: nan, pos_over_neg: 214.46438598632812 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.9781, loss_val: nan, pos_over_neg: 241.25970458984375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 4.0018, loss_val: nan, pos_over_neg: 186.7926483154297 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 4.0123, loss_val: nan, pos_over_neg: 148.65916442871094 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.9878, loss_val: nan, pos_over_neg: 221.2959442138672 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.9755, loss_val: nan, pos_over_neg: 253.55686950683594 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 4.0159, loss_val: nan, pos_over_neg: 265.4414978027344 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 4.065, loss_val: nan, pos_over_neg: 187.30519104003906 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.9938, loss_val: nan, pos_over_neg: 246.34922790527344 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 4.0064, loss_val: nan, pos_over_neg: 224.84503173828125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.977, loss_val: nan, pos_over_neg: 294.35546875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.981, loss_val: nan, pos_over_neg: 238.90924072265625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 4.0707, loss_val: nan, pos_over_neg: 181.87681579589844 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 4.0348, loss_val: nan, pos_over_neg: 256.24169921875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.9645, loss_val: nan, pos_over_neg: 269.6471252441406 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 4.0265, loss_val: nan, pos_over_neg: 260.7807922363281 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.9776, loss_val: nan, pos_over_neg: 280.5641174316406 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 4.0255, loss_val: nan, pos_over_neg: 238.8651580810547 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 4.0766, loss_val: nan, pos_over_neg: 161.20980834960938 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.981, loss_val: nan, pos_over_neg: 210.65045166015625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 4.0136, loss_val: nan, pos_over_neg: 290.2890930175781 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 4.0643, loss_val: nan, pos_over_neg: 184.8959503173828 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 4.0228, loss_val: nan, pos_over_neg: 160.48748779296875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.9675, loss_val: nan, pos_over_neg: 192.26608276367188 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 4.0006, loss_val: nan, pos_over_neg: 189.7114715576172 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.9567, loss_val: nan, pos_over_neg: 267.4639587402344 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 4.0142, loss_val: nan, pos_over_neg: 166.17092895507812 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.9811, loss_val: nan, pos_over_neg: 178.5142822265625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 4.0404, loss_val: nan, pos_over_neg: 124.22213745117188 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.9689, loss_val: nan, pos_over_neg: 229.5302276611328 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.9956, loss_val: nan, pos_over_neg: 169.38388061523438 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 4.0596, loss_val: nan, pos_over_neg: 114.46159362792969 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 4.0405, loss_val: nan, pos_over_neg: 127.30522155761719 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 4.039, loss_val: nan, pos_over_neg: 130.6105499267578 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 4.0417, loss_val: nan, pos_over_neg: 125.56425476074219 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 4.0444, loss_val: nan, pos_over_neg: 162.94944763183594 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 4.0191, loss_val: nan, pos_over_neg: 189.1344451904297 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.9613, loss_val: nan, pos_over_neg: 196.9045867919922 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 4.0146, loss_val: nan, pos_over_neg: 208.56735229492188 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 4.0312, loss_val: nan, pos_over_neg: 238.29310607910156 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 4.0068, loss_val: nan, pos_over_neg: 222.5938262939453 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 4.0955, loss_val: nan, pos_over_neg: 144.24925231933594 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 4.0517, loss_val: nan, pos_over_neg: 188.66920471191406 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 4.0727, loss_val: nan, pos_over_neg: 125.97460174560547 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.9994, loss_val: nan, pos_over_neg: 211.35498046875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 4.0355, loss_val: nan, pos_over_neg: 261.88677978515625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 192.25991821289062 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 4.0098, loss_val: nan, pos_over_neg: 180.90505981445312 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.9512, loss_val: nan, pos_over_neg: 225.91030883789062 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 4.0483, loss_val: nan, pos_over_neg: 196.65798950195312 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.9816, loss_val: nan, pos_over_neg: 202.70416259765625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 4.0468, loss_val: nan, pos_over_neg: 169.7174072265625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 4.0377, loss_val: nan, pos_over_neg: 234.4770050048828 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 4.0141, loss_val: nan, pos_over_neg: 189.86520385742188 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.9898, loss_val: nan, pos_over_neg: 217.6759490966797 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.9878, loss_val: nan, pos_over_neg: 235.61451721191406 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.9865, loss_val: nan, pos_over_neg: 185.51046752929688 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 210.3914794921875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 4.0024, loss_val: nan, pos_over_neg: 229.41368103027344 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 4.0077, loss_val: nan, pos_over_neg: 176.83917236328125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 4.0373, loss_val: nan, pos_over_neg: 242.25209045410156 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 4.0182, loss_val: nan, pos_over_neg: 229.31692504882812 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 4.0162, loss_val: nan, pos_over_neg: 317.2164306640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 4.0039, loss_val: nan, pos_over_neg: 190.52122497558594 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 4.0113, loss_val: nan, pos_over_neg: 237.70997619628906 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 4.046, loss_val: nan, pos_over_neg: 129.36329650878906 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.9962, loss_val: nan, pos_over_neg: 168.93177795410156 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 4.0155, loss_val: nan, pos_over_neg: 148.0856170654297 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.962, loss_val: nan, pos_over_neg: 423.7794494628906 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 4.0257, loss_val: nan, pos_over_neg: 293.8595275878906 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.9505, loss_val: nan, pos_over_neg: 588.5679321289062 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 4.003, loss_val: nan, pos_over_neg: 364.4644775390625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 4.0153, loss_val: nan, pos_over_neg: 192.9986572265625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.9898, loss_val: nan, pos_over_neg: 150.1815948486328 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.9905, loss_val: nan, pos_over_neg: 203.9814453125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 4.038, loss_val: nan, pos_over_neg: 133.10629272460938 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 4.0099, loss_val: nan, pos_over_neg: 159.5006561279297 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.9714, loss_val: nan, pos_over_neg: 218.79849243164062 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 4.0749, loss_val: nan, pos_over_neg: 250.74501037597656 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 4.0292, loss_val: nan, pos_over_neg: 184.6592254638672 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 4.0072, loss_val: nan, pos_over_neg: 232.7879180908203 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.9936, loss_val: nan, pos_over_neg: 319.8846435546875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.985, loss_val: nan, pos_over_neg: 302.5916442871094 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.975, loss_val: nan, pos_over_neg: 161.9424285888672 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.9825, loss_val: nan, pos_over_neg: 257.3614196777344 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.9563, loss_val: nan, pos_over_neg: 287.06591796875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 4.0095, loss_val: nan, pos_over_neg: 147.1535186767578 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 4.0224, loss_val: nan, pos_over_neg: 141.18820190429688 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 4.0485, loss_val: nan, pos_over_neg: 156.7227783203125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.966, loss_val: nan, pos_over_neg: 228.31883239746094 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 4.0094, loss_val: nan, pos_over_neg: 180.6307830810547 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.9605, loss_val: nan, pos_over_neg: 200.87619018554688 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.9966, loss_val: nan, pos_over_neg: 237.48681640625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 4.0268, loss_val: nan, pos_over_neg: 253.6780242919922 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.9899, loss_val: nan, pos_over_neg: 252.90643310546875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.9638, loss_val: nan, pos_over_neg: 274.5105285644531 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.9823, loss_val: nan, pos_over_neg: 368.3680114746094 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 4.0437, loss_val: nan, pos_over_neg: 189.60174560546875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 4.0077, loss_val: nan, pos_over_neg: 256.1462707519531 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 4.008, loss_val: nan, pos_over_neg: 213.4305419921875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.9701, loss_val: nan, pos_over_neg: 123.47411346435547 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 4.0132, loss_val: nan, pos_over_neg: 109.6984634399414 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.9971, loss_val: nan, pos_over_neg: 266.47113037109375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 4.0161, loss_val: nan, pos_over_neg: 271.9169006347656 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.98, loss_val: nan, pos_over_neg: 205.30039978027344 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 4.0794, loss_val: nan, pos_over_neg: 184.53848266601562 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 4.0434, loss_val: nan, pos_over_neg: 263.0166931152344 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.9835, loss_val: nan, pos_over_neg: 297.3238220214844 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 4.005, loss_val: nan, pos_over_neg: 162.89865112304688 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 4.0433, loss_val: nan, pos_over_neg: 136.49278259277344 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.9789, loss_val: nan, pos_over_neg: 167.988037109375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 4.0385, loss_val: nan, pos_over_neg: 143.16778564453125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 4.043, loss_val: nan, pos_over_neg: 173.52780151367188 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.9465, loss_val: nan, pos_over_neg: 183.40640258789062 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 4.0005, loss_val: nan, pos_over_neg: 167.52870178222656 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.9877, loss_val: nan, pos_over_neg: 200.79351806640625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.9558, loss_val: nan, pos_over_neg: 224.76150512695312 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.9932, loss_val: nan, pos_over_neg: 228.6406707763672 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 4.0016, loss_val: nan, pos_over_neg: 167.92047119140625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 4.0075, loss_val: nan, pos_over_neg: 220.87564086914062 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 4.0307, loss_val: nan, pos_over_neg: 142.03126525878906 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.9906, loss_val: nan, pos_over_neg: 121.41543579101562 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.9633, loss_val: nan, pos_over_neg: 210.24375915527344 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.9778, loss_val: nan, pos_over_neg: 168.8290252685547 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 4.0322, loss_val: nan, pos_over_neg: 145.18429565429688 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 4.004, loss_val: nan, pos_over_neg: 249.0492706298828 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 4.0056, loss_val: nan, pos_over_neg: 224.74000549316406 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.9745, loss_val: nan, pos_over_neg: 220.72462463378906 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.9896, loss_val: nan, pos_over_neg: 195.78941345214844 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.9951, loss_val: nan, pos_over_neg: 215.09161376953125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.9804, loss_val: nan, pos_over_neg: 134.40560913085938 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 4.0049, loss_val: nan, pos_over_neg: 140.05905151367188 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.9532, loss_val: nan, pos_over_neg: 211.053955078125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 4.026, loss_val: nan, pos_over_neg: 171.02825927734375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.9998, loss_val: nan, pos_over_neg: 153.5211181640625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.975, loss_val: nan, pos_over_neg: 347.39691162109375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 321.3844299316406 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.994, loss_val: nan, pos_over_neg: 174.0184783935547 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.9947, loss_val: nan, pos_over_neg: 166.6689910888672 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.9994, loss_val: nan, pos_over_neg: 218.9142303466797 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.9834, loss_val: nan, pos_over_neg: 272.128173828125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 4.0193, loss_val: nan, pos_over_neg: 175.2191162109375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.9827, loss_val: nan, pos_over_neg: 282.2998962402344 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.9979, loss_val: nan, pos_over_neg: 227.6578369140625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 4.0233, loss_val: nan, pos_over_neg: 175.36988830566406 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.9618, loss_val: nan, pos_over_neg: 242.95436096191406 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.9706, loss_val: nan, pos_over_neg: 204.9944610595703 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 4.0033, loss_val: nan, pos_over_neg: 209.63172912597656 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.9838, loss_val: nan, pos_over_neg: 207.65382385253906 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.9355, loss_val: nan, pos_over_neg: 243.03692626953125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.9247, loss_val: nan, pos_over_neg: 338.11474609375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 4.0082, loss_val: nan, pos_over_neg: 231.89205932617188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.934, loss_val: nan, pos_over_neg: 262.4878234863281 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.9615, loss_val: nan, pos_over_neg: 233.302001953125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.942, loss_val: nan, pos_over_neg: 320.3873596191406 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.968, loss_val: nan, pos_over_neg: 170.65370178222656 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.9979, loss_val: nan, pos_over_neg: 192.84237670898438 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.9539, loss_val: nan, pos_over_neg: 244.11734008789062 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.9676, loss_val: nan, pos_over_neg: 214.4748992919922 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.953, loss_val: nan, pos_over_neg: 223.97479248046875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 4.0027, loss_val: nan, pos_over_neg: 264.52862548828125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.9357, loss_val: nan, pos_over_neg: 256.27386474609375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.9637, loss_val: nan, pos_over_neg: 224.43910217285156 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 4.014, loss_val: nan, pos_over_neg: 234.41441345214844 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.9284, loss_val: nan, pos_over_neg: 263.39801025390625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.982, loss_val: nan, pos_over_neg: 179.10601806640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.9353, loss_val: nan, pos_over_neg: 283.9739990234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.9714, loss_val: nan, pos_over_neg: 233.9114990234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.9415, loss_val: nan, pos_over_neg: 265.80548095703125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.9613, loss_val: nan, pos_over_neg: 291.8268737792969 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 4.0263, loss_val: nan, pos_over_neg: 210.47691345214844 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.9635, loss_val: nan, pos_over_neg: 204.33399963378906 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.9697, loss_val: nan, pos_over_neg: 223.47544860839844 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 4.0037, loss_val: nan, pos_over_neg: 155.40541076660156 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.9592, loss_val: nan, pos_over_neg: 209.62704467773438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.9885, loss_val: nan, pos_over_neg: 215.18624877929688 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.9899, loss_val: nan, pos_over_neg: 207.8566131591797 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.9832, loss_val: nan, pos_over_neg: 282.18988037109375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.9614, loss_val: nan, pos_over_neg: 212.20755004882812 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.9614, loss_val: nan, pos_over_neg: 194.88253784179688 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.9687, loss_val: nan, pos_over_neg: 191.62550354003906 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.9844, loss_val: nan, pos_over_neg: 168.4776153564453 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 4.0081, loss_val: nan, pos_over_neg: 140.70944213867188 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.9668, loss_val: nan, pos_over_neg: 115.16858673095703 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.9584, loss_val: nan, pos_over_neg: 253.80763244628906 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.9468, loss_val: nan, pos_over_neg: 212.9427490234375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 4.0097, loss_val: nan, pos_over_neg: 178.9651336669922 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.952, loss_val: nan, pos_over_neg: 223.28982543945312 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.949, loss_val: nan, pos_over_neg: 317.64154052734375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.9328, loss_val: nan, pos_over_neg: 207.7347869873047 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.9688, loss_val: nan, pos_over_neg: 233.83441162109375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.9412, loss_val: nan, pos_over_neg: 351.97216796875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.9571, loss_val: nan, pos_over_neg: 167.70401000976562 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.9319, loss_val: nan, pos_over_neg: 247.95523071289062 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.9427, loss_val: nan, pos_over_neg: 270.14581298828125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.9743, loss_val: nan, pos_over_neg: 239.9726104736328 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 4.0062, loss_val: nan, pos_over_neg: 172.127685546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.9897, loss_val: nan, pos_over_neg: 205.3607940673828 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.9552, loss_val: nan, pos_over_neg: 266.9256286621094 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 246.70462036132812 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.9948, loss_val: nan, pos_over_neg: 322.64227294921875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.953, loss_val: nan, pos_over_neg: 213.44216918945312 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.9798, loss_val: nan, pos_over_neg: 275.4090270996094 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.9228, loss_val: nan, pos_over_neg: 261.0829772949219 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.9834, loss_val: nan, pos_over_neg: 147.26126098632812 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.9696, loss_val: nan, pos_over_neg: 160.30137634277344 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.9843, loss_val: nan, pos_over_neg: 158.24388122558594 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.9649, loss_val: nan, pos_over_neg: 215.5576629638672 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.9692, loss_val: nan, pos_over_neg: 338.1499938964844 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.9774, loss_val: nan, pos_over_neg: 258.8385925292969 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.9547, loss_val: nan, pos_over_neg: 256.5045471191406 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.9709, loss_val: nan, pos_over_neg: 217.18243408203125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.9266, loss_val: nan, pos_over_neg: 363.7442321777344 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.9907, loss_val: nan, pos_over_neg: 234.66189575195312 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.9286, loss_val: nan, pos_over_neg: 388.7286682128906 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 4.0033, loss_val: nan, pos_over_neg: 214.04685974121094 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.9609, loss_val: nan, pos_over_neg: 246.14578247070312 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 4.0307, loss_val: nan, pos_over_neg: 176.7694549560547 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.9679, loss_val: nan, pos_over_neg: 164.87283325195312 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.9867, loss_val: nan, pos_over_neg: 176.0623016357422 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.9396, loss_val: nan, pos_over_neg: 249.606689453125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.9417, loss_val: nan, pos_over_neg: 286.9873962402344 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.9313, loss_val: nan, pos_over_neg: 305.8504333496094 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 4.0198, loss_val: nan, pos_over_neg: 138.60043334960938 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.9705, loss_val: nan, pos_over_neg: 196.8988494873047 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.9425, loss_val: nan, pos_over_neg: 278.0460205078125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.9343, loss_val: nan, pos_over_neg: 205.5790252685547 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.9571, loss_val: nan, pos_over_neg: 212.00621032714844 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.9767, loss_val: nan, pos_over_neg: 286.9431457519531 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.9874, loss_val: nan, pos_over_neg: 223.46791076660156 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 4.0141, loss_val: nan, pos_over_neg: 228.7568359375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.9403, loss_val: nan, pos_over_neg: 218.8631591796875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.9636, loss_val: nan, pos_over_neg: 289.7846374511719 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.9729, loss_val: nan, pos_over_neg: 220.00169372558594 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.9556, loss_val: nan, pos_over_neg: 196.40609741210938 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.9343, loss_val: nan, pos_over_neg: 215.4943389892578 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.9202, loss_val: nan, pos_over_neg: 230.02194213867188 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.9735, loss_val: nan, pos_over_neg: 263.185302734375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.9511, loss_val: nan, pos_over_neg: 302.34814453125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 243.93002319335938 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.9372, loss_val: nan, pos_over_neg: 323.1265869140625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 319.6649475097656 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 171.27392578125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.9556, loss_val: nan, pos_over_neg: 222.7398223876953 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.9462, loss_val: nan, pos_over_neg: 283.1199035644531 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.9134, loss_val: nan, pos_over_neg: 200.6659698486328 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.9254, loss_val: nan, pos_over_neg: 159.15945434570312 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.9748, loss_val: nan, pos_over_neg: 177.94639587402344 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.9833, loss_val: nan, pos_over_neg: 216.43304443359375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.9489, loss_val: nan, pos_over_neg: 237.931884765625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 4.001, loss_val: nan, pos_over_neg: 238.9929962158203 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.9556, loss_val: nan, pos_over_neg: 193.48333740234375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.9635, loss_val: nan, pos_over_neg: 186.44821166992188 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 4.0224, loss_val: nan, pos_over_neg: 130.62881469726562 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.9695, loss_val: nan, pos_over_neg: 233.20042419433594 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.9659, loss_val: nan, pos_over_neg: 224.93775939941406 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.9653, loss_val: nan, pos_over_neg: 284.46112060546875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.9397, loss_val: nan, pos_over_neg: 365.7318115234375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.9109, loss_val: nan, pos_over_neg: 349.929931640625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.9907, loss_val: nan, pos_over_neg: 232.71951293945312 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.9312, loss_val: nan, pos_over_neg: 175.53598022460938 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.9533, loss_val: nan, pos_over_neg: 256.3316345214844 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.9388, loss_val: nan, pos_over_neg: 277.31884765625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.9105, loss_val: nan, pos_over_neg: 308.85302734375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.9201, loss_val: nan, pos_over_neg: 428.268310546875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.9342, loss_val: nan, pos_over_neg: 237.2279052734375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.9596, loss_val: nan, pos_over_neg: 216.57073974609375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.9703, loss_val: nan, pos_over_neg: 208.10479736328125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.9169, loss_val: nan, pos_over_neg: 296.3356018066406 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.93, loss_val: nan, pos_over_neg: 243.0839080810547 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.945, loss_val: nan, pos_over_neg: 255.37567138671875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.9902, loss_val: nan, pos_over_neg: 160.72824096679688 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.9602, loss_val: nan, pos_over_neg: 230.1638641357422 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.9589, loss_val: nan, pos_over_neg: 221.93081665039062 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.9739, loss_val: nan, pos_over_neg: 219.05816650390625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.9532, loss_val: nan, pos_over_neg: 184.8765411376953 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.981, loss_val: nan, pos_over_neg: 191.56863403320312 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.9081, loss_val: nan, pos_over_neg: 248.81382751464844 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.9542, loss_val: nan, pos_over_neg: 146.93775939941406 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.9579, loss_val: nan, pos_over_neg: 202.53199768066406 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.9572, loss_val: nan, pos_over_neg: 211.41262817382812 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.9612, loss_val: nan, pos_over_neg: 181.17945861816406 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.9409, loss_val: nan, pos_over_neg: 218.02992248535156 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.9673, loss_val: nan, pos_over_neg: 274.91790771484375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.978, loss_val: nan, pos_over_neg: 261.21063232421875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.9498, loss_val: nan, pos_over_neg: 201.28829956054688 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.9711, loss_val: nan, pos_over_neg: 227.8343963623047 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.9146, loss_val: nan, pos_over_neg: 214.7361602783203 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.9476, loss_val: nan, pos_over_neg: 367.4652099609375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.9799, loss_val: nan, pos_over_neg: 216.53973388671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.996, loss_val: nan, pos_over_neg: 197.3218994140625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.9442, loss_val: nan, pos_over_neg: 154.2814483642578 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.9091, loss_val: nan, pos_over_neg: 150.24282836914062 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.8942, loss_val: nan, pos_over_neg: 338.7989501953125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.9472, loss_val: nan, pos_over_neg: 298.61651611328125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.9371, loss_val: nan, pos_over_neg: 266.05548095703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.9523, loss_val: nan, pos_over_neg: 375.62744140625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.9354, loss_val: nan, pos_over_neg: 398.2438659667969 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.923, loss_val: nan, pos_over_neg: 247.1939697265625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.9455, loss_val: nan, pos_over_neg: 252.2963104248047 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.8807, loss_val: nan, pos_over_neg: 247.57215881347656 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.945, loss_val: nan, pos_over_neg: 130.7951202392578 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 4.0354, loss_val: nan, pos_over_neg: 118.49072265625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.948, loss_val: nan, pos_over_neg: 287.661865234375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.9193, loss_val: nan, pos_over_neg: 298.4088134765625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.8874, loss_val: nan, pos_over_neg: 281.0148010253906 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.8881, loss_val: nan, pos_over_neg: 422.1561584472656 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.9187, loss_val: nan, pos_over_neg: 449.50665283203125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.9157, loss_val: nan, pos_over_neg: 279.0487365722656 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.9432, loss_val: nan, pos_over_neg: 223.47705078125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.913, loss_val: nan, pos_over_neg: 196.19053649902344 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.9377, loss_val: nan, pos_over_neg: 167.17840576171875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.9953, loss_val: nan, pos_over_neg: 161.81442260742188 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.9168, loss_val: nan, pos_over_neg: 223.92913818359375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.9256, loss_val: nan, pos_over_neg: 261.7574157714844 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.9506, loss_val: nan, pos_over_neg: 237.68772888183594 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.9555, loss_val: nan, pos_over_neg: 286.9146423339844 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.88, loss_val: nan, pos_over_neg: 305.9083557128906 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.9322, loss_val: nan, pos_over_neg: 174.6605987548828 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.9479, loss_val: nan, pos_over_neg: 180.38339233398438 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.8957, loss_val: nan, pos_over_neg: 269.39080810546875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.9494, loss_val: nan, pos_over_neg: 180.82098388671875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.9287, loss_val: nan, pos_over_neg: 246.68324279785156 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.9414, loss_val: nan, pos_over_neg: 260.9998474121094 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.958, loss_val: nan, pos_over_neg: 281.4775695800781 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.8752, loss_val: nan, pos_over_neg: 289.1718444824219 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.9471, loss_val: nan, pos_over_neg: 344.3370056152344 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.936, loss_val: nan, pos_over_neg: 265.1825866699219 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.9602, loss_val: nan, pos_over_neg: 181.94166564941406 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.8998, loss_val: nan, pos_over_neg: 204.52166748046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.9084, loss_val: nan, pos_over_neg: 212.38446044921875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.9746, loss_val: nan, pos_over_neg: 172.6927947998047 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.9157, loss_val: nan, pos_over_neg: 169.3082733154297 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.9429, loss_val: nan, pos_over_neg: 207.0204620361328 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.9057, loss_val: nan, pos_over_neg: 348.916748046875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.8854, loss_val: nan, pos_over_neg: 517.9896850585938 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.8775, loss_val: nan, pos_over_neg: 374.5400695800781 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.9497, loss_val: nan, pos_over_neg: 282.4171142578125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.9651, loss_val: nan, pos_over_neg: 246.33267211914062 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.8987, loss_val: nan, pos_over_neg: 303.4515075683594 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.9068, loss_val: nan, pos_over_neg: 186.35870361328125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.9488, loss_val: nan, pos_over_neg: 199.8321075439453 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.929, loss_val: nan, pos_over_neg: 215.93917846679688 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.9015, loss_val: nan, pos_over_neg: 222.74732971191406 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.8884, loss_val: nan, pos_over_neg: 381.7820739746094 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.9068, loss_val: nan, pos_over_neg: 509.5981750488281 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.8972, loss_val: nan, pos_over_neg: 280.56549072265625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.924, loss_val: nan, pos_over_neg: 188.4185028076172 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.9528, loss_val: nan, pos_over_neg: 212.01841735839844 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.9348, loss_val: nan, pos_over_neg: 213.55374145507812 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.9099, loss_val: nan, pos_over_neg: 281.744873046875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.9184, loss_val: nan, pos_over_neg: 273.79913330078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 312.6374206542969 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.9197, loss_val: nan, pos_over_neg: 223.40621948242188 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.9116, loss_val: nan, pos_over_neg: 186.47097778320312 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.9166, loss_val: nan, pos_over_neg: 264.03564453125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.9594, loss_val: nan, pos_over_neg: 241.0374755859375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.9165, loss_val: nan, pos_over_neg: 251.83743286132812 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.911, loss_val: nan, pos_over_neg: 195.92465209960938 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.927, loss_val: nan, pos_over_neg: 297.7460021972656 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.9335, loss_val: nan, pos_over_neg: 194.1823272705078 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.9441, loss_val: nan, pos_over_neg: 161.22373962402344 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 224.3306427001953 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.9736, loss_val: nan, pos_over_neg: 182.75013732910156 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.8894, loss_val: nan, pos_over_neg: 226.64102172851562 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.9268, loss_val: nan, pos_over_neg: 253.3168487548828 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.8961, loss_val: nan, pos_over_neg: 360.6479797363281 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.882, loss_val: nan, pos_over_neg: 367.38946533203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.9418, loss_val: nan, pos_over_neg: 229.30694580078125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.9572, loss_val: nan, pos_over_neg: 164.23858642578125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.9823, loss_val: nan, pos_over_neg: 215.50624084472656 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.8927, loss_val: nan, pos_over_neg: 218.40643310546875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.9519, loss_val: nan, pos_over_neg: 146.11349487304688 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.935, loss_val: nan, pos_over_neg: 179.1057586669922 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.9592, loss_val: nan, pos_over_neg: 230.9459991455078 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.9379, loss_val: nan, pos_over_neg: 216.68252563476562 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.9416, loss_val: nan, pos_over_neg: 281.2300109863281 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.9715, loss_val: nan, pos_over_neg: 333.36383056640625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.9444, loss_val: nan, pos_over_neg: 209.8025360107422 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.8811, loss_val: nan, pos_over_neg: 364.3860778808594 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.8869, loss_val: nan, pos_over_neg: 288.4967346191406 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 158.9062957763672 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.9458, loss_val: nan, pos_over_neg: 192.59347534179688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.9419, loss_val: nan, pos_over_neg: 150.7613525390625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.9369, loss_val: nan, pos_over_neg: 247.6509246826172 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.9078, loss_val: nan, pos_over_neg: 249.9729766845703 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.9918, loss_val: nan, pos_over_neg: 260.23101806640625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.9143, loss_val: nan, pos_over_neg: 263.0994873046875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.9385, loss_val: nan, pos_over_neg: 247.94639587402344 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.9615, loss_val: nan, pos_over_neg: 128.00880432128906 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.8857, loss_val: nan, pos_over_neg: 176.08657836914062 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.8895, loss_val: nan, pos_over_neg: 158.02236938476562 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.9047, loss_val: nan, pos_over_neg: 239.2915802001953 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.8979, loss_val: nan, pos_over_neg: 252.5166015625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.9196, loss_val: nan, pos_over_neg: 198.19039916992188 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.8964, loss_val: nan, pos_over_neg: 273.4361877441406 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.9009, loss_val: nan, pos_over_neg: 237.40931701660156 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.9503, loss_val: nan, pos_over_neg: 219.8643341064453 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.9091, loss_val: nan, pos_over_neg: 183.54083251953125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.8607, loss_val: nan, pos_over_neg: 195.59112548828125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.9115, loss_val: nan, pos_over_neg: 200.26815795898438 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.9121, loss_val: nan, pos_over_neg: 181.05996704101562 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.9531, loss_val: nan, pos_over_neg: 195.7936248779297 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.9154, loss_val: nan, pos_over_neg: 192.95465087890625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.9444, loss_val: nan, pos_over_neg: 300.9416198730469 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.9078, loss_val: nan, pos_over_neg: 201.18759155273438 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.9323, loss_val: nan, pos_over_neg: 214.57725524902344 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.9493, loss_val: nan, pos_over_neg: 206.91542053222656 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.9713, loss_val: nan, pos_over_neg: 213.4031982421875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.8835, loss_val: nan, pos_over_neg: 158.5213623046875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.9433, loss_val: nan, pos_over_neg: 191.0919952392578 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.9137, loss_val: nan, pos_over_neg: 186.82325744628906 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.9352, loss_val: nan, pos_over_neg: 170.718017578125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.915, loss_val: nan, pos_over_neg: 258.36920166015625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.9548, loss_val: nan, pos_over_neg: 210.2112579345703 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.8902, loss_val: nan, pos_over_neg: 212.1627960205078 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.876, loss_val: nan, pos_over_neg: 229.4886016845703 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.9036, loss_val: nan, pos_over_neg: 245.04090881347656 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.9142, loss_val: nan, pos_over_neg: 193.51588439941406 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.8824, loss_val: nan, pos_over_neg: 281.2635498046875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.9687, loss_val: nan, pos_over_neg: 149.94412231445312 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.933, loss_val: nan, pos_over_neg: 191.60169982910156 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.9364, loss_val: nan, pos_over_neg: 368.34619140625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.953, loss_val: nan, pos_over_neg: 326.00311279296875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.948, loss_val: nan, pos_over_neg: 209.0756378173828 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.9176, loss_val: nan, pos_over_neg: 210.937255859375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.9251, loss_val: nan, pos_over_neg: 213.68516540527344 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.9038, loss_val: nan, pos_over_neg: 228.24386596679688 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.9059, loss_val: nan, pos_over_neg: 215.27740478515625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.8695, loss_val: nan, pos_over_neg: 323.8419189453125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.9039, loss_val: nan, pos_over_neg: 568.5950927734375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 441.780029296875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.9415, loss_val: nan, pos_over_neg: 335.4244384765625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.9114, loss_val: nan, pos_over_neg: 241.11582946777344 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.9485, loss_val: nan, pos_over_neg: 160.173095703125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.923, loss_val: nan, pos_over_neg: 243.81382751464844 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.9299, loss_val: nan, pos_over_neg: 252.4474639892578 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.8746, loss_val: nan, pos_over_neg: 428.7364501953125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 218.3242645263672 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.8699, loss_val: nan, pos_over_neg: 341.5213623046875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.899, loss_val: nan, pos_over_neg: 353.9512939453125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.8843, loss_val: nan, pos_over_neg: 278.81719970703125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.8927, loss_val: nan, pos_over_neg: 263.1732177734375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.9027, loss_val: nan, pos_over_neg: 286.5960388183594 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.8781, loss_val: nan, pos_over_neg: 368.94757080078125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.9567, loss_val: nan, pos_over_neg: 252.05104064941406 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.8997, loss_val: nan, pos_over_neg: 359.98944091796875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.8723, loss_val: nan, pos_over_neg: 863.92724609375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.8683, loss_val: nan, pos_over_neg: 350.53277587890625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.9451, loss_val: nan, pos_over_neg: 227.48353576660156 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 219.4013671875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.8556, loss_val: nan, pos_over_neg: 672.804443359375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.8619, loss_val: nan, pos_over_neg: 402.506103515625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.887, loss_val: nan, pos_over_neg: 300.6282958984375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.8977, loss_val: nan, pos_over_neg: 387.9320373535156 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.904, loss_val: nan, pos_over_neg: 330.8018493652344 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.8579, loss_val: nan, pos_over_neg: 287.25469970703125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.9034, loss_val: nan, pos_over_neg: 287.6539611816406 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.9649, loss_val: nan, pos_over_neg: 180.08485412597656 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.9077, loss_val: nan, pos_over_neg: 264.7176818847656 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.9656, loss_val: nan, pos_over_neg: 180.3629150390625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.9113, loss_val: nan, pos_over_neg: 222.7172088623047 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.9235, loss_val: nan, pos_over_neg: 206.38894653320312 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.9294, loss_val: nan, pos_over_neg: 202.43600463867188 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.9538, loss_val: nan, pos_over_neg: 233.64308166503906 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.8954, loss_val: nan, pos_over_neg: 294.2734680175781 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.8741, loss_val: nan, pos_over_neg: 249.22442626953125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.9194, loss_val: nan, pos_over_neg: 224.74142456054688 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.8949, loss_val: nan, pos_over_neg: 184.80404663085938 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.8875, loss_val: nan, pos_over_neg: 332.86474609375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.9308, loss_val: nan, pos_over_neg: 186.0360870361328 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.922, loss_val: nan, pos_over_neg: 183.9574432373047 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.9241, loss_val: nan, pos_over_neg: 256.26910400390625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.9589, loss_val: nan, pos_over_neg: 229.33685302734375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.9185, loss_val: nan, pos_over_neg: 198.0929412841797 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.9189, loss_val: nan, pos_over_neg: 275.3895263671875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.9981, loss_val: nan, pos_over_neg: 212.6114044189453 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.9014, loss_val: nan, pos_over_neg: 205.80274963378906 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.8744, loss_val: nan, pos_over_neg: 190.436279296875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.8795, loss_val: nan, pos_over_neg: 164.5907440185547 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.9055, loss_val: nan, pos_over_neg: 173.49468994140625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.9063, loss_val: nan, pos_over_neg: 297.9858093261719 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.9048, loss_val: nan, pos_over_neg: 268.0704345703125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.9382, loss_val: nan, pos_over_neg: 229.8020782470703 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.8817, loss_val: nan, pos_over_neg: 251.9730987548828 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.9391, loss_val: nan, pos_over_neg: 235.79844665527344 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.9084, loss_val: nan, pos_over_neg: 264.3775634765625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.9059, loss_val: nan, pos_over_neg: 209.27923583984375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 164.46875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.895, loss_val: nan, pos_over_neg: 155.14857482910156 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.9396, loss_val: nan, pos_over_neg: 227.4146270751953 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.9009, loss_val: nan, pos_over_neg: 273.44146728515625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.8788, loss_val: nan, pos_over_neg: 262.0380859375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.9288, loss_val: nan, pos_over_neg: 168.1529998779297 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.8766, loss_val: nan, pos_over_neg: 278.3274841308594 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.9415, loss_val: nan, pos_over_neg: 404.10784912109375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.8937, loss_val: nan, pos_over_neg: 378.69610595703125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.8915, loss_val: nan, pos_over_neg: 283.6424255371094 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.9264, loss_val: nan, pos_over_neg: 211.19393920898438 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.8996, loss_val: nan, pos_over_neg: 194.60768127441406 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.9439, loss_val: nan, pos_over_neg: 165.96026611328125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.9408, loss_val: nan, pos_over_neg: 154.8984375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.9138, loss_val: nan, pos_over_neg: 183.5700225830078 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.8774, loss_val: nan, pos_over_neg: 321.5781555175781 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.9053, loss_val: nan, pos_over_neg: 244.03314208984375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.8831, loss_val: nan, pos_over_neg: 289.16278076171875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.8607, loss_val: nan, pos_over_neg: 288.2675476074219 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.8947, loss_val: nan, pos_over_neg: 244.18161010742188 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.917, loss_val: nan, pos_over_neg: 182.75204467773438 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.884, loss_val: nan, pos_over_neg: 278.2235107421875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.9005, loss_val: nan, pos_over_neg: 178.44786071777344 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.9166, loss_val: nan, pos_over_neg: 211.806396484375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.8565, loss_val: nan, pos_over_neg: 266.6071472167969 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.865, loss_val: nan, pos_over_neg: 276.1311340332031 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.9217, loss_val: nan, pos_over_neg: 226.68704223632812 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.9529, loss_val: nan, pos_over_neg: 195.58836364746094 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.9102, loss_val: nan, pos_over_neg: 331.4153137207031 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.8964, loss_val: nan, pos_over_neg: 274.99114990234375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.894, loss_val: nan, pos_over_neg: 264.785400390625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.9176, loss_val: nan, pos_over_neg: 269.4089050292969 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.8981, loss_val: nan, pos_over_neg: 236.82672119140625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.9246, loss_val: nan, pos_over_neg: 176.5669708251953 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.875, loss_val: nan, pos_over_neg: 246.10556030273438 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.8933, loss_val: nan, pos_over_neg: 221.0997314453125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.9345, loss_val: nan, pos_over_neg: 150.1873779296875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.8539, loss_val: nan, pos_over_neg: 348.0118103027344 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.9244, loss_val: nan, pos_over_neg: 283.7414855957031 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.8608, loss_val: nan, pos_over_neg: 314.0223083496094 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.9197, loss_val: nan, pos_over_neg: 226.6104278564453 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.9171, loss_val: nan, pos_over_neg: 193.5044403076172 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.8955, loss_val: nan, pos_over_neg: 191.75953674316406 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.8991, loss_val: nan, pos_over_neg: 237.1844024658203 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 241.0448455810547 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.903, loss_val: nan, pos_over_neg: 328.6744689941406 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.9266, loss_val: nan, pos_over_neg: 217.03822326660156 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.8924, loss_val: nan, pos_over_neg: 275.9599914550781 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.8834, loss_val: nan, pos_over_neg: 291.3635559082031 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.8786, loss_val: nan, pos_over_neg: 242.9215545654297 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.8962, loss_val: nan, pos_over_neg: 224.8543243408203 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 265.2295837402344 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.9071, loss_val: nan, pos_over_neg: 295.6871032714844 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.9186, loss_val: nan, pos_over_neg: 220.6226806640625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.9296, loss_val: nan, pos_over_neg: 196.10037231445312 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.8632, loss_val: nan, pos_over_neg: 265.9005126953125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.8229, loss_val: nan, pos_over_neg: 646.7432250976562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.9011, loss_val: nan, pos_over_neg: 198.3144073486328 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.9281, loss_val: nan, pos_over_neg: 216.2119140625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.9245, loss_val: nan, pos_over_neg: 235.71820068359375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.8669, loss_val: nan, pos_over_neg: 209.52906799316406 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.8754, loss_val: nan, pos_over_neg: 273.02105712890625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.8894, loss_val: nan, pos_over_neg: 280.3934020996094 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.8751, loss_val: nan, pos_over_neg: 217.46261596679688 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.8215, loss_val: nan, pos_over_neg: 226.66720581054688 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.8852, loss_val: nan, pos_over_neg: 223.5946044921875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.8489, loss_val: nan, pos_over_neg: 403.4197082519531 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.9107, loss_val: nan, pos_over_neg: 227.74697875976562 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 281.79913330078125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.8689, loss_val: nan, pos_over_neg: 242.79103088378906 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.9058, loss_val: nan, pos_over_neg: 257.8647766113281 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.8992, loss_val: nan, pos_over_neg: 229.66424560546875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.8719, loss_val: nan, pos_over_neg: 239.90762329101562 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.8661, loss_val: nan, pos_over_neg: 323.8076477050781 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.9431, loss_val: nan, pos_over_neg: 234.44796752929688 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.8467, loss_val: nan, pos_over_neg: 389.6493225097656 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.8702, loss_val: nan, pos_over_neg: 307.3715515136719 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.8821, loss_val: nan, pos_over_neg: 280.6854248046875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.9024, loss_val: nan, pos_over_neg: 229.42391967773438 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.9225, loss_val: nan, pos_over_neg: 187.43292236328125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.8971, loss_val: nan, pos_over_neg: 349.419921875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.8958, loss_val: nan, pos_over_neg: 272.7337646484375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.9142, loss_val: nan, pos_over_neg: 324.5082092285156 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.8845, loss_val: nan, pos_over_neg: 290.6151428222656 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.8768, loss_val: nan, pos_over_neg: 352.6650695800781 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.8748, loss_val: nan, pos_over_neg: 244.30450439453125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.8859, loss_val: nan, pos_over_neg: 240.78451538085938 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.873, loss_val: nan, pos_over_neg: 247.70132446289062 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.8902, loss_val: nan, pos_over_neg: 488.33154296875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.8822, loss_val: nan, pos_over_neg: 194.93280029296875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.8531, loss_val: nan, pos_over_neg: 238.9601593017578 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.9402, loss_val: nan, pos_over_neg: 161.12855529785156 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.9107, loss_val: nan, pos_over_neg: 245.22686767578125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.8701, loss_val: nan, pos_over_neg: 208.6628875732422 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.9059, loss_val: nan, pos_over_neg: 245.5679168701172 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.8655, loss_val: nan, pos_over_neg: 226.31932067871094 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.8605, loss_val: nan, pos_over_neg: 310.9681701660156 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.8476, loss_val: nan, pos_over_neg: 255.8282928466797 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.8756, loss_val: nan, pos_over_neg: 240.32762145996094 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.9036, loss_val: nan, pos_over_neg: 223.05477905273438 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.9102, loss_val: nan, pos_over_neg: 166.97752380371094 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.918, loss_val: nan, pos_over_neg: 276.83538818359375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.8784, loss_val: nan, pos_over_neg: 222.7279815673828 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.95, loss_val: nan, pos_over_neg: 186.79226684570312 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.8966, loss_val: nan, pos_over_neg: 212.9441375732422 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.8779, loss_val: nan, pos_over_neg: 200.5795135498047 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.8815, loss_val: nan, pos_over_neg: 223.50509643554688 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.9177, loss_val: nan, pos_over_neg: 203.79608154296875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.8799, loss_val: nan, pos_over_neg: 335.56793212890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.9235, loss_val: nan, pos_over_neg: 277.09906005859375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 292.025146484375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.8945, loss_val: nan, pos_over_neg: 262.29443359375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.8852, loss_val: nan, pos_over_neg: 199.08544921875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.8657, loss_val: nan, pos_over_neg: 335.6011047363281 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.8695, loss_val: nan, pos_over_neg: 304.7548828125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.9479, loss_val: nan, pos_over_neg: 144.86940002441406 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.8583, loss_val: nan, pos_over_neg: 165.77195739746094 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.9061, loss_val: nan, pos_over_neg: 201.7676239013672 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.8578, loss_val: nan, pos_over_neg: 393.1675109863281 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.878, loss_val: nan, pos_over_neg: 319.75482177734375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [30:34<76280:22:43, 915.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.8625, loss_val: nan, pos_over_neg: 303.62420654296875 lr: 0.00031623\n",
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 3.8476, loss_val: nan, pos_over_neg: 245.13450622558594 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.9036, loss_val: nan, pos_over_neg: 286.5736999511719 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.891, loss_val: nan, pos_over_neg: 213.7187957763672 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.8562, loss_val: nan, pos_over_neg: 218.32717895507812 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.8885, loss_val: nan, pos_over_neg: 150.40284729003906 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.8679, loss_val: nan, pos_over_neg: 165.22605895996094 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.8922, loss_val: nan, pos_over_neg: 216.0522918701172 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.8552, loss_val: nan, pos_over_neg: 275.8893127441406 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.8881, loss_val: nan, pos_over_neg: 310.1696472167969 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.8678, loss_val: nan, pos_over_neg: 353.7990417480469 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.915, loss_val: nan, pos_over_neg: 215.43275451660156 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.8674, loss_val: nan, pos_over_neg: 250.6500244140625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 310.417236328125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.9146, loss_val: nan, pos_over_neg: 186.69729614257812 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.8588, loss_val: nan, pos_over_neg: 252.53179931640625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.8775, loss_val: nan, pos_over_neg: 204.65863037109375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.8604, loss_val: nan, pos_over_neg: 226.1489715576172 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.9109, loss_val: nan, pos_over_neg: 216.05703735351562 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.897, loss_val: nan, pos_over_neg: 221.11769104003906 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.8987, loss_val: nan, pos_over_neg: 239.4251251220703 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.8418, loss_val: nan, pos_over_neg: 229.08546447753906 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.8549, loss_val: nan, pos_over_neg: 252.20327758789062 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.8391, loss_val: nan, pos_over_neg: 315.2811279296875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.9223, loss_val: nan, pos_over_neg: 220.0380401611328 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.9043, loss_val: nan, pos_over_neg: 161.9703369140625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.8667, loss_val: nan, pos_over_neg: 252.74134826660156 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.8773, loss_val: nan, pos_over_neg: 195.3065185546875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.8725, loss_val: nan, pos_over_neg: 233.38241577148438 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 345.8094482421875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.9044, loss_val: nan, pos_over_neg: 223.69090270996094 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.8841, loss_val: nan, pos_over_neg: 295.0347900390625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.8594, loss_val: nan, pos_over_neg: 247.7198486328125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.8741, loss_val: nan, pos_over_neg: 249.5768280029297 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.9051, loss_val: nan, pos_over_neg: 218.2513427734375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.8544, loss_val: nan, pos_over_neg: 215.40707397460938 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.8826, loss_val: nan, pos_over_neg: 225.719482421875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.8738, loss_val: nan, pos_over_neg: 228.12246704101562 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.8837, loss_val: nan, pos_over_neg: 284.1001892089844 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.8637, loss_val: nan, pos_over_neg: 209.68035888671875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.8889, loss_val: nan, pos_over_neg: 257.6700439453125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.9124, loss_val: nan, pos_over_neg: 305.0787353515625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.8844, loss_val: nan, pos_over_neg: 315.8232421875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.9102, loss_val: nan, pos_over_neg: 231.03555297851562 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.9378, loss_val: nan, pos_over_neg: 211.0391387939453 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.9071, loss_val: nan, pos_over_neg: 227.72499084472656 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.8547, loss_val: nan, pos_over_neg: 472.4664001464844 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.844, loss_val: nan, pos_over_neg: 187.37274169921875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.9079, loss_val: nan, pos_over_neg: 179.15863037109375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.872, loss_val: nan, pos_over_neg: 296.4273986816406 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.8189, loss_val: nan, pos_over_neg: 413.98101806640625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.9078, loss_val: nan, pos_over_neg: 252.9897918701172 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 367.5583801269531 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 262.33416748046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.8435, loss_val: nan, pos_over_neg: 287.7015075683594 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.8957, loss_val: nan, pos_over_neg: 242.9165802001953 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.856, loss_val: nan, pos_over_neg: 377.7248229980469 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.8666, loss_val: nan, pos_over_neg: 334.718505859375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.8638, loss_val: nan, pos_over_neg: 237.76939392089844 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.9334, loss_val: nan, pos_over_neg: 207.13018798828125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.8398, loss_val: nan, pos_over_neg: 280.5793151855469 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.8837, loss_val: nan, pos_over_neg: 275.939453125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.8388, loss_val: nan, pos_over_neg: 339.2631530761719 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.8641, loss_val: nan, pos_over_neg: 392.18212890625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.8789, loss_val: nan, pos_over_neg: 491.9550476074219 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.8849, loss_val: nan, pos_over_neg: 632.4902954101562 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 380.8419494628906 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.8531, loss_val: nan, pos_over_neg: 292.3041687011719 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.8323, loss_val: nan, pos_over_neg: 413.2450256347656 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.8449, loss_val: nan, pos_over_neg: 175.04119873046875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.8866, loss_val: nan, pos_over_neg: 198.8292694091797 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.8572, loss_val: nan, pos_over_neg: 282.2693176269531 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.8506, loss_val: nan, pos_over_neg: 297.3099670410156 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.8755, loss_val: nan, pos_over_neg: 504.6128234863281 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.9003, loss_val: nan, pos_over_neg: 302.76165771484375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.8998, loss_val: nan, pos_over_neg: 368.9239196777344 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.8692, loss_val: nan, pos_over_neg: 428.08001708984375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.8295, loss_val: nan, pos_over_neg: 336.1988830566406 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.9377, loss_val: nan, pos_over_neg: 184.0572052001953 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.8695, loss_val: nan, pos_over_neg: 215.32846069335938 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.9063, loss_val: nan, pos_over_neg: 203.8086395263672 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.8774, loss_val: nan, pos_over_neg: 259.7011413574219 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.8879, loss_val: nan, pos_over_neg: 300.6598815917969 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.8918, loss_val: nan, pos_over_neg: 377.97906494140625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.8715, loss_val: nan, pos_over_neg: 337.7567138671875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.8876, loss_val: nan, pos_over_neg: 280.8128967285156 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.8514, loss_val: nan, pos_over_neg: 327.1434631347656 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.8592, loss_val: nan, pos_over_neg: 212.78211975097656 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.8688, loss_val: nan, pos_over_neg: 310.32025146484375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.8813, loss_val: nan, pos_over_neg: 215.57150268554688 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.8815, loss_val: nan, pos_over_neg: 247.48634338378906 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.8994, loss_val: nan, pos_over_neg: 406.01385498046875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.8882, loss_val: nan, pos_over_neg: 299.51788330078125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.8754, loss_val: nan, pos_over_neg: 310.9133605957031 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.8538, loss_val: nan, pos_over_neg: 321.1236572265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.8434, loss_val: nan, pos_over_neg: 238.99937438964844 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.8762, loss_val: nan, pos_over_neg: 277.17559814453125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.8866, loss_val: nan, pos_over_neg: 195.02711486816406 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.8943, loss_val: nan, pos_over_neg: 241.89930725097656 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.8666, loss_val: nan, pos_over_neg: 309.337158203125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.9005, loss_val: nan, pos_over_neg: 172.95826721191406 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.9098, loss_val: nan, pos_over_neg: 210.25323486328125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 456.8231506347656 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.8651, loss_val: nan, pos_over_neg: 256.6800537109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.8805, loss_val: nan, pos_over_neg: 171.29039001464844 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.8502, loss_val: nan, pos_over_neg: 217.61729431152344 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.8409, loss_val: nan, pos_over_neg: 269.75762939453125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.9252, loss_val: nan, pos_over_neg: 142.8802490234375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.8682, loss_val: nan, pos_over_neg: 229.01084899902344 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.8661, loss_val: nan, pos_over_neg: 413.5323181152344 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.8323, loss_val: nan, pos_over_neg: 297.168212890625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.8893, loss_val: nan, pos_over_neg: 177.4922332763672 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.8725, loss_val: nan, pos_over_neg: 179.207763671875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.8349, loss_val: nan, pos_over_neg: 294.7912292480469 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.928, loss_val: nan, pos_over_neg: 182.71359252929688 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.8487, loss_val: nan, pos_over_neg: 177.9574737548828 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.8734, loss_val: nan, pos_over_neg: 225.25096130371094 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.8609, loss_val: nan, pos_over_neg: 287.7615661621094 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.8883, loss_val: nan, pos_over_neg: 172.4877471923828 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.8754, loss_val: nan, pos_over_neg: 196.73460388183594 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.8929, loss_val: nan, pos_over_neg: 227.94752502441406 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 364.5342712402344 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.857, loss_val: nan, pos_over_neg: 313.4095764160156 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.883, loss_val: nan, pos_over_neg: 176.2300262451172 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 245.13125610351562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.8969, loss_val: nan, pos_over_neg: 255.8207550048828 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.8839, loss_val: nan, pos_over_neg: 198.3905792236328 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.8899, loss_val: nan, pos_over_neg: 192.62545776367188 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.8029, loss_val: nan, pos_over_neg: 238.39674377441406 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.8556, loss_val: nan, pos_over_neg: 202.02281188964844 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.9188, loss_val: nan, pos_over_neg: 217.5774383544922 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.8763, loss_val: nan, pos_over_neg: 449.5115051269531 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.8923, loss_val: nan, pos_over_neg: 318.64849853515625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.8872, loss_val: nan, pos_over_neg: 187.53346252441406 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.8672, loss_val: nan, pos_over_neg: 222.24093627929688 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.8839, loss_val: nan, pos_over_neg: 274.98895263671875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.7879, loss_val: nan, pos_over_neg: 430.5592041015625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.8659, loss_val: nan, pos_over_neg: 259.5276794433594 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.8157, loss_val: nan, pos_over_neg: 313.40521240234375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.8087, loss_val: nan, pos_over_neg: 215.29437255859375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.8298, loss_val: nan, pos_over_neg: 206.67919921875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.8546, loss_val: nan, pos_over_neg: 211.70989990234375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.8808, loss_val: nan, pos_over_neg: 246.21807861328125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.8537, loss_val: nan, pos_over_neg: 186.97232055664062 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.8492, loss_val: nan, pos_over_neg: 246.66079711914062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.8319, loss_val: nan, pos_over_neg: 286.5468444824219 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.8845, loss_val: nan, pos_over_neg: 290.0943603515625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.8467, loss_val: nan, pos_over_neg: 223.84841918945312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.9003, loss_val: nan, pos_over_neg: 215.49673461914062 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.832, loss_val: nan, pos_over_neg: 274.4154052734375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.8516, loss_val: nan, pos_over_neg: 384.0875549316406 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.8565, loss_val: nan, pos_over_neg: 301.9141540527344 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.8691, loss_val: nan, pos_over_neg: 203.4160614013672 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.8751, loss_val: nan, pos_over_neg: 166.10301208496094 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.9038, loss_val: nan, pos_over_neg: 161.2859344482422 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.8554, loss_val: nan, pos_over_neg: 234.44973754882812 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.8503, loss_val: nan, pos_over_neg: 228.91954040527344 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.8647, loss_val: nan, pos_over_neg: 450.912353515625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.8747, loss_val: nan, pos_over_neg: 220.88706970214844 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.9053, loss_val: nan, pos_over_neg: 224.37710571289062 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.8756, loss_val: nan, pos_over_neg: 227.61122131347656 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.8891, loss_val: nan, pos_over_neg: 186.6553192138672 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.8562, loss_val: nan, pos_over_neg: 241.8508758544922 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.8745, loss_val: nan, pos_over_neg: 306.6225891113281 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.8946, loss_val: nan, pos_over_neg: 297.6779479980469 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.8859, loss_val: nan, pos_over_neg: 263.6553649902344 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.8216, loss_val: nan, pos_over_neg: 330.96807861328125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.852, loss_val: nan, pos_over_neg: 268.2021179199219 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.9442, loss_val: nan, pos_over_neg: 178.0982208251953 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.8854, loss_val: nan, pos_over_neg: 199.62295532226562 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.8959, loss_val: nan, pos_over_neg: 180.68765258789062 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.8948, loss_val: nan, pos_over_neg: 259.6221618652344 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.8014, loss_val: nan, pos_over_neg: 368.15185546875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.8724, loss_val: nan, pos_over_neg: 195.72618103027344 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.8336, loss_val: nan, pos_over_neg: 235.7403106689453 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.8765, loss_val: nan, pos_over_neg: 190.42176818847656 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.8467, loss_val: nan, pos_over_neg: 223.81150817871094 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 408.5474548339844 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.835, loss_val: nan, pos_over_neg: 358.1683349609375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.8481, loss_val: nan, pos_over_neg: 274.9899597167969 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.8528, loss_val: nan, pos_over_neg: 317.0840759277344 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.8299, loss_val: nan, pos_over_neg: 237.26365661621094 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.8577, loss_val: nan, pos_over_neg: 256.5249938964844 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.8423, loss_val: nan, pos_over_neg: 194.95896911621094 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 338.968017578125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.7984, loss_val: nan, pos_over_neg: 306.7968444824219 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.854, loss_val: nan, pos_over_neg: 249.1943359375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.8796, loss_val: nan, pos_over_neg: 290.4973449707031 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.8739, loss_val: nan, pos_over_neg: 264.43450927734375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.8948, loss_val: nan, pos_over_neg: 219.96112060546875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.8512, loss_val: nan, pos_over_neg: 298.3538513183594 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.8587, loss_val: nan, pos_over_neg: 243.03988647460938 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.8537, loss_val: nan, pos_over_neg: 229.5957794189453 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.9167, loss_val: nan, pos_over_neg: 191.05149841308594 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.8433, loss_val: nan, pos_over_neg: 350.10687255859375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.8441, loss_val: nan, pos_over_neg: 310.5562438964844 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.8257, loss_val: nan, pos_over_neg: 568.1717529296875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.8422, loss_val: nan, pos_over_neg: 305.7298278808594 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.8309, loss_val: nan, pos_over_neg: 300.0946044921875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.8503, loss_val: nan, pos_over_neg: 289.37469482421875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.8259, loss_val: nan, pos_over_neg: 315.9267578125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.856, loss_val: nan, pos_over_neg: 222.46975708007812 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.8726, loss_val: nan, pos_over_neg: 251.6707763671875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 308.23822021484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.8539, loss_val: nan, pos_over_neg: 260.1308288574219 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.8342, loss_val: nan, pos_over_neg: 312.845947265625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 218.65460205078125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.8376, loss_val: nan, pos_over_neg: 260.5998840332031 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.8032, loss_val: nan, pos_over_neg: 313.89898681640625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.8282, loss_val: nan, pos_over_neg: 306.4175720214844 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.9432, loss_val: nan, pos_over_neg: 166.96144104003906 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.877, loss_val: nan, pos_over_neg: 178.93902587890625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.841, loss_val: nan, pos_over_neg: 218.63864135742188 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.8634, loss_val: nan, pos_over_neg: 190.7662811279297 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.8372, loss_val: nan, pos_over_neg: 292.0452575683594 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 296.9571838378906 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.8596, loss_val: nan, pos_over_neg: 264.9922180175781 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.8701, loss_val: nan, pos_over_neg: 218.82374572753906 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.8408, loss_val: nan, pos_over_neg: 304.67352294921875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.8531, loss_val: nan, pos_over_neg: 259.748779296875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.8802, loss_val: nan, pos_over_neg: 165.548828125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.8149, loss_val: nan, pos_over_neg: 255.9958953857422 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.7701, loss_val: nan, pos_over_neg: 338.68658447265625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.8239, loss_val: nan, pos_over_neg: 292.7626037597656 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 282.1746520996094 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.862, loss_val: nan, pos_over_neg: 181.91259765625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.8646, loss_val: nan, pos_over_neg: 262.2158203125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.836, loss_val: nan, pos_over_neg: 221.3657989501953 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.8155, loss_val: nan, pos_over_neg: 282.1365051269531 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.8576, loss_val: nan, pos_over_neg: 242.69549560546875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.8624, loss_val: nan, pos_over_neg: 194.06161499023438 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.8124, loss_val: nan, pos_over_neg: 216.3621063232422 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.9029, loss_val: nan, pos_over_neg: 162.63673400878906 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.8309, loss_val: nan, pos_over_neg: 242.93026733398438 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 329.570068359375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.829, loss_val: nan, pos_over_neg: 335.91949462890625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.8598, loss_val: nan, pos_over_neg: 289.8766174316406 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.8744, loss_val: nan, pos_over_neg: 312.1318664550781 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.8431, loss_val: nan, pos_over_neg: 388.5057373046875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.8408, loss_val: nan, pos_over_neg: 292.2530822753906 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.8065, loss_val: nan, pos_over_neg: 323.7998046875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 328.4460754394531 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.8698, loss_val: nan, pos_over_neg: 216.33457946777344 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.859, loss_val: nan, pos_over_neg: 270.3245544433594 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.8522, loss_val: nan, pos_over_neg: 217.19802856445312 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.8523, loss_val: nan, pos_over_neg: 328.5987243652344 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.8364, loss_val: nan, pos_over_neg: 211.90573120117188 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.8611, loss_val: nan, pos_over_neg: 243.45094299316406 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.8128, loss_val: nan, pos_over_neg: 493.08306884765625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.8421, loss_val: nan, pos_over_neg: 224.13809204101562 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.8813, loss_val: nan, pos_over_neg: 199.79006958007812 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.8559, loss_val: nan, pos_over_neg: 212.1915740966797 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.8686, loss_val: nan, pos_over_neg: 176.79429626464844 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.8213, loss_val: nan, pos_over_neg: 335.45733642578125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.8455, loss_val: nan, pos_over_neg: 237.08631896972656 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.8888, loss_val: nan, pos_over_neg: 453.2814636230469 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.8657, loss_val: nan, pos_over_neg: 320.5154724121094 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.8762, loss_val: nan, pos_over_neg: 211.31211853027344 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.8594, loss_val: nan, pos_over_neg: 239.92645263671875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.8197, loss_val: nan, pos_over_neg: 202.3922119140625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 298.75750732421875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.8302, loss_val: nan, pos_over_neg: 302.88507080078125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.8163, loss_val: nan, pos_over_neg: 296.5511474609375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.8513, loss_val: nan, pos_over_neg: 203.35931396484375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.8506, loss_val: nan, pos_over_neg: 227.4951934814453 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.7867, loss_val: nan, pos_over_neg: 340.47186279296875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.8991, loss_val: nan, pos_over_neg: 252.70938110351562 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.886, loss_val: nan, pos_over_neg: 199.4976348876953 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.8188, loss_val: nan, pos_over_neg: 254.45877075195312 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.8992, loss_val: nan, pos_over_neg: 326.88641357421875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 235.77516174316406 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.8267, loss_val: nan, pos_over_neg: 311.3829040527344 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.831, loss_val: nan, pos_over_neg: 273.135498046875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.8541, loss_val: nan, pos_over_neg: 238.27215576171875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.8226, loss_val: nan, pos_over_neg: 315.7250061035156 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.8133, loss_val: nan, pos_over_neg: 249.11312866210938 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.8575, loss_val: nan, pos_over_neg: 389.5962219238281 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.842, loss_val: nan, pos_over_neg: 304.38787841796875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.926, loss_val: nan, pos_over_neg: 227.45477294921875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.8294, loss_val: nan, pos_over_neg: 262.76934814453125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.8523, loss_val: nan, pos_over_neg: 173.1874237060547 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.8356, loss_val: nan, pos_over_neg: 239.72079467773438 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.8936, loss_val: nan, pos_over_neg: 205.5712432861328 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.8758, loss_val: nan, pos_over_neg: 208.58973693847656 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.8553, loss_val: nan, pos_over_neg: 276.6075439453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.8232, loss_val: nan, pos_over_neg: 320.5782775878906 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.8603, loss_val: nan, pos_over_neg: 307.5253601074219 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.8359, loss_val: nan, pos_over_neg: 374.5881652832031 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.8506, loss_val: nan, pos_over_neg: 294.9326477050781 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.8178, loss_val: nan, pos_over_neg: 287.1100158691406 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.8498, loss_val: nan, pos_over_neg: 224.7838897705078 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.8637, loss_val: nan, pos_over_neg: 184.77745056152344 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 191.37924194335938 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.8339, loss_val: nan, pos_over_neg: 218.5263671875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.8294, loss_val: nan, pos_over_neg: 298.6062316894531 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.8053, loss_val: nan, pos_over_neg: 429.98486328125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.837, loss_val: nan, pos_over_neg: 424.6324157714844 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 290.4598083496094 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.8285, loss_val: nan, pos_over_neg: 335.6246337890625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.8588, loss_val: nan, pos_over_neg: 292.6851806640625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.8353, loss_val: nan, pos_over_neg: 266.9654541015625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.8726, loss_val: nan, pos_over_neg: 221.98019409179688 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.8714, loss_val: nan, pos_over_neg: 203.58116149902344 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.815, loss_val: nan, pos_over_neg: 244.20806884765625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.799, loss_val: nan, pos_over_neg: 312.7400817871094 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.844, loss_val: nan, pos_over_neg: 310.357666015625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.8589, loss_val: nan, pos_over_neg: 207.7249755859375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.833, loss_val: nan, pos_over_neg: 254.28143310546875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.8375, loss_val: nan, pos_over_neg: 269.45526123046875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.8651, loss_val: nan, pos_over_neg: 271.5517272949219 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.8561, loss_val: nan, pos_over_neg: 228.8964385986328 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.9103, loss_val: nan, pos_over_neg: 237.42474365234375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 243.324462890625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.8536, loss_val: nan, pos_over_neg: 247.3065948486328 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.8353, loss_val: nan, pos_over_neg: 217.66522216796875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.8435, loss_val: nan, pos_over_neg: 181.79339599609375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.9039, loss_val: nan, pos_over_neg: 163.3597412109375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.8415, loss_val: nan, pos_over_neg: 301.0270690917969 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.8527, loss_val: nan, pos_over_neg: 220.5707550048828 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.87, loss_val: nan, pos_over_neg: 208.77798461914062 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.8529, loss_val: nan, pos_over_neg: 367.3934631347656 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.8555, loss_val: nan, pos_over_neg: 339.5401611328125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.8366, loss_val: nan, pos_over_neg: 274.20849609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.8416, loss_val: nan, pos_over_neg: 221.42933654785156 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.877, loss_val: nan, pos_over_neg: 147.51100158691406 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.8512, loss_val: nan, pos_over_neg: 176.88253784179688 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.828, loss_val: nan, pos_over_neg: 207.30604553222656 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.8525, loss_val: nan, pos_over_neg: 155.77960205078125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 270.7065124511719 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.8213, loss_val: nan, pos_over_neg: 207.24331665039062 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.8264, loss_val: nan, pos_over_neg: 375.0809020996094 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.8207, loss_val: nan, pos_over_neg: 437.9550476074219 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.8166, loss_val: nan, pos_over_neg: 353.1620788574219 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.8238, loss_val: nan, pos_over_neg: 326.96087646484375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.8437, loss_val: nan, pos_over_neg: 257.6627502441406 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.8526, loss_val: nan, pos_over_neg: 262.5015869140625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.8302, loss_val: nan, pos_over_neg: 282.31695556640625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.881, loss_val: nan, pos_over_neg: 178.7660675048828 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.8564, loss_val: nan, pos_over_neg: 153.0628204345703 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.7785, loss_val: nan, pos_over_neg: 315.211181640625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.8021, loss_val: nan, pos_over_neg: 301.46514892578125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.8315, loss_val: nan, pos_over_neg: 347.5308532714844 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.8788, loss_val: nan, pos_over_neg: 333.0992736816406 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.8069, loss_val: nan, pos_over_neg: 275.15753173828125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.862, loss_val: nan, pos_over_neg: 246.35733032226562 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.8345, loss_val: nan, pos_over_neg: 243.4476776123047 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.8017, loss_val: nan, pos_over_neg: 245.40219116210938 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.8938, loss_val: nan, pos_over_neg: 184.9346466064453 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.8433, loss_val: nan, pos_over_neg: 226.58050537109375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.8194, loss_val: nan, pos_over_neg: 218.6371307373047 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.8395, loss_val: nan, pos_over_neg: 166.08865356445312 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.8026, loss_val: nan, pos_over_neg: 255.3095703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.8172, loss_val: nan, pos_over_neg: 341.3767395019531 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.8542, loss_val: nan, pos_over_neg: 332.3042297363281 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.8407, loss_val: nan, pos_over_neg: 227.87197875976562 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.7973, loss_val: nan, pos_over_neg: 182.76193237304688 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.8934, loss_val: nan, pos_over_neg: 272.251708984375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.8429, loss_val: nan, pos_over_neg: 278.6011962890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.8346, loss_val: nan, pos_over_neg: 199.96832275390625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.8719, loss_val: nan, pos_over_neg: 201.86036682128906 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.8093, loss_val: nan, pos_over_neg: 340.295166015625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 285.4422302246094 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.8062, loss_val: nan, pos_over_neg: 287.4851989746094 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 206.50328063964844 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.8518, loss_val: nan, pos_over_neg: 281.9948425292969 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.8139, loss_val: nan, pos_over_neg: 272.1289367675781 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.8153, loss_val: nan, pos_over_neg: 213.43313598632812 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 231.96011352539062 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.8563, loss_val: nan, pos_over_neg: 258.04498291015625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.8649, loss_val: nan, pos_over_neg: 217.12538146972656 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.7977, loss_val: nan, pos_over_neg: 319.786376953125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.8045, loss_val: nan, pos_over_neg: 489.8693542480469 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.846, loss_val: nan, pos_over_neg: 257.6734619140625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.8411, loss_val: nan, pos_over_neg: 336.2718811035156 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.8526, loss_val: nan, pos_over_neg: 256.4574279785156 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.836, loss_val: nan, pos_over_neg: 266.01708984375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 213.02496337890625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.8458, loss_val: nan, pos_over_neg: 263.99493408203125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.8887, loss_val: nan, pos_over_neg: 162.5700225830078 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.8356, loss_val: nan, pos_over_neg: 230.32115173339844 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.811, loss_val: nan, pos_over_neg: 307.0504150390625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.8233, loss_val: nan, pos_over_neg: 343.4734191894531 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.8619, loss_val: nan, pos_over_neg: 322.8238830566406 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.8184, loss_val: nan, pos_over_neg: 497.4403076171875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.8395, loss_val: nan, pos_over_neg: 392.6290588378906 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.8615, loss_val: nan, pos_over_neg: 489.1824951171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.8375, loss_val: nan, pos_over_neg: 189.76458740234375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.8637, loss_val: nan, pos_over_neg: 227.66517639160156 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.9068, loss_val: nan, pos_over_neg: 166.98886108398438 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.8083, loss_val: nan, pos_over_neg: 312.3795166015625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.9143, loss_val: nan, pos_over_neg: 216.9675750732422 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.8399, loss_val: nan, pos_over_neg: 312.49462890625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.799, loss_val: nan, pos_over_neg: 399.10870361328125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.8464, loss_val: nan, pos_over_neg: 382.8584899902344 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.8386, loss_val: nan, pos_over_neg: 292.8972473144531 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.8639, loss_val: nan, pos_over_neg: 241.0807647705078 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.8783, loss_val: nan, pos_over_neg: 266.61968994140625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.8371, loss_val: nan, pos_over_neg: 320.14361572265625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.8733, loss_val: nan, pos_over_neg: 268.08038330078125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.8758, loss_val: nan, pos_over_neg: 233.24908447265625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 234.65780639648438 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.8489, loss_val: nan, pos_over_neg: 329.5196838378906 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.8463, loss_val: nan, pos_over_neg: 261.51593017578125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.825, loss_val: nan, pos_over_neg: 252.4266357421875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.8489, loss_val: nan, pos_over_neg: 421.9867248535156 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.885, loss_val: nan, pos_over_neg: 448.2823791503906 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.8477, loss_val: nan, pos_over_neg: 367.55224609375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.8076, loss_val: nan, pos_over_neg: 293.8203125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.8609, loss_val: nan, pos_over_neg: 324.0709228515625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.8592, loss_val: nan, pos_over_neg: 305.8775634765625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.871, loss_val: nan, pos_over_neg: 327.4587097167969 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.8602, loss_val: nan, pos_over_neg: 308.3258056640625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.82, loss_val: nan, pos_over_neg: 306.7513732910156 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.8405, loss_val: nan, pos_over_neg: 434.6748352050781 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.7779, loss_val: nan, pos_over_neg: 388.64752197265625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.8625, loss_val: nan, pos_over_neg: 177.04844665527344 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.8118, loss_val: nan, pos_over_neg: 283.7088317871094 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.8397, loss_val: nan, pos_over_neg: 285.54473876953125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.8292, loss_val: nan, pos_over_neg: 333.83740234375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.8433, loss_val: nan, pos_over_neg: 282.97222900390625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.8237, loss_val: nan, pos_over_neg: 307.2719421386719 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.8121, loss_val: nan, pos_over_neg: 327.4861145019531 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.8221, loss_val: nan, pos_over_neg: 283.36279296875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.8401, loss_val: nan, pos_over_neg: 280.4598388671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.7868, loss_val: nan, pos_over_neg: 343.3614807128906 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.812, loss_val: nan, pos_over_neg: 285.5901794433594 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.8346, loss_val: nan, pos_over_neg: 244.04690551757812 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.8336, loss_val: nan, pos_over_neg: 223.780517578125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.8146, loss_val: nan, pos_over_neg: 386.97686767578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.8169, loss_val: nan, pos_over_neg: 270.85076904296875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.8387, loss_val: nan, pos_over_neg: 230.8587188720703 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.7903, loss_val: nan, pos_over_neg: 334.5086364746094 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.8224, loss_val: nan, pos_over_neg: 297.5584716796875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.8922, loss_val: nan, pos_over_neg: 203.1728515625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 196.016845703125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.8288, loss_val: nan, pos_over_neg: 270.282470703125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.8526, loss_val: nan, pos_over_neg: 263.68377685546875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.7971, loss_val: nan, pos_over_neg: 307.16217041015625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.8367, loss_val: nan, pos_over_neg: 232.15904235839844 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.8264, loss_val: nan, pos_over_neg: 388.0388488769531 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.905, loss_val: nan, pos_over_neg: 199.02401733398438 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.8617, loss_val: nan, pos_over_neg: 166.45986938476562 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.8054, loss_val: nan, pos_over_neg: 208.8103485107422 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.8287, loss_val: nan, pos_over_neg: 188.29541015625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.8298, loss_val: nan, pos_over_neg: 293.702392578125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.8855, loss_val: nan, pos_over_neg: 311.3549499511719 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.7719, loss_val: nan, pos_over_neg: 842.970947265625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.8635, loss_val: nan, pos_over_neg: 283.1932067871094 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.8161, loss_val: nan, pos_over_neg: 237.02784729003906 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.8853, loss_val: nan, pos_over_neg: 203.73915100097656 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.8453, loss_val: nan, pos_over_neg: 219.90818786621094 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.85, loss_val: nan, pos_over_neg: 171.53721618652344 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.8415, loss_val: nan, pos_over_neg: 219.75897216796875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.8207, loss_val: nan, pos_over_neg: 259.492919921875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.825, loss_val: nan, pos_over_neg: 216.85060119628906 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.8518, loss_val: nan, pos_over_neg: 204.17054748535156 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.8208, loss_val: nan, pos_over_neg: 199.2643585205078 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.8239, loss_val: nan, pos_over_neg: 287.756103515625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.8357, loss_val: nan, pos_over_neg: 395.15069580078125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.8032, loss_val: nan, pos_over_neg: 328.26690673828125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.7973, loss_val: nan, pos_over_neg: 383.6549072265625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.7849, loss_val: nan, pos_over_neg: 367.7019348144531 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.8223, loss_val: nan, pos_over_neg: 235.0341339111328 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.8438, loss_val: nan, pos_over_neg: 173.5829620361328 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.8439, loss_val: nan, pos_over_neg: 157.11952209472656 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.79, loss_val: nan, pos_over_neg: 277.8437805175781 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.8578, loss_val: nan, pos_over_neg: 195.07577514648438 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.8151, loss_val: nan, pos_over_neg: 303.1978454589844 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.9009, loss_val: nan, pos_over_neg: 241.37733459472656 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.8255, loss_val: nan, pos_over_neg: 329.58990478515625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.8152, loss_val: nan, pos_over_neg: 262.0706481933594 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.7894, loss_val: nan, pos_over_neg: 539.6552124023438 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.8055, loss_val: nan, pos_over_neg: 333.1564636230469 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.8367, loss_val: nan, pos_over_neg: 173.80975341796875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.8072, loss_val: nan, pos_over_neg: 185.5673370361328 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.7726, loss_val: nan, pos_over_neg: 381.8671569824219 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.807, loss_val: nan, pos_over_neg: 226.05995178222656 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.85, loss_val: nan, pos_over_neg: 210.5319061279297 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.8129, loss_val: nan, pos_over_neg: 295.9051818847656 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.803, loss_val: nan, pos_over_neg: 223.94329833984375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.8366, loss_val: nan, pos_over_neg: 185.5615692138672 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 229.63961791992188 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.7777, loss_val: nan, pos_over_neg: 261.7022705078125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 184.62127685546875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.8237, loss_val: nan, pos_over_neg: 197.86782836914062 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.7903, loss_val: nan, pos_over_neg: 193.24050903320312 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.8208, loss_val: nan, pos_over_neg: 256.10870361328125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.7787, loss_val: nan, pos_over_neg: 257.5866394042969 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.7923, loss_val: nan, pos_over_neg: 294.4319152832031 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.849, loss_val: nan, pos_over_neg: 215.27777099609375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.8191, loss_val: nan, pos_over_neg: 196.6386260986328 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.8038, loss_val: nan, pos_over_neg: 247.75192260742188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.8185, loss_val: nan, pos_over_neg: 211.24929809570312 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.8041, loss_val: nan, pos_over_neg: 244.29371643066406 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.7887, loss_val: nan, pos_over_neg: 226.0410919189453 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.8068, loss_val: nan, pos_over_neg: 227.53787231445312 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.817, loss_val: nan, pos_over_neg: 309.0514831542969 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.8652, loss_val: nan, pos_over_neg: 219.6771697998047 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.8067, loss_val: nan, pos_over_neg: 439.2980041503906 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 255.52989196777344 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.8337, loss_val: nan, pos_over_neg: 206.6256866455078 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.8499, loss_val: nan, pos_over_neg: 229.62533569335938 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.8542, loss_val: nan, pos_over_neg: 279.5398864746094 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.8333, loss_val: nan, pos_over_neg: 240.1730499267578 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.7881, loss_val: nan, pos_over_neg: 313.0172119140625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.8286, loss_val: nan, pos_over_neg: 263.21234130859375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.8005, loss_val: nan, pos_over_neg: 371.6527099609375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 330.530029296875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.7823, loss_val: nan, pos_over_neg: 347.8323974609375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 298.5451965332031 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.8732, loss_val: nan, pos_over_neg: 188.35458374023438 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.8096, loss_val: nan, pos_over_neg: 184.01824951171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 226.71774291992188 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.8025, loss_val: nan, pos_over_neg: 303.18023681640625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.8459, loss_val: nan, pos_over_neg: 199.54046630859375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.7966, loss_val: nan, pos_over_neg: 229.8966064453125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.9019, loss_val: nan, pos_over_neg: 183.6881866455078 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.8555, loss_val: nan, pos_over_neg: 239.1269989013672 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.7833, loss_val: nan, pos_over_neg: 387.37384033203125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.8179, loss_val: nan, pos_over_neg: 385.3443603515625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.8305, loss_val: nan, pos_over_neg: 280.4610595703125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.862, loss_val: nan, pos_over_neg: 182.69586181640625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.7854, loss_val: nan, pos_over_neg: 258.62396240234375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.7887, loss_val: nan, pos_over_neg: 242.28482055664062 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.8227, loss_val: nan, pos_over_neg: 209.74884033203125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.811, loss_val: nan, pos_over_neg: 246.64979553222656 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.8305, loss_val: nan, pos_over_neg: 275.01373291015625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.8371, loss_val: nan, pos_over_neg: 270.44940185546875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.8389, loss_val: nan, pos_over_neg: 253.05006408691406 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.8325, loss_val: nan, pos_over_neg: 300.074462890625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.8045, loss_val: nan, pos_over_neg: 307.67999267578125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.8542, loss_val: nan, pos_over_neg: 253.33758544921875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.8269, loss_val: nan, pos_over_neg: 322.88519287109375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.8136, loss_val: nan, pos_over_neg: 226.8080291748047 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.8275, loss_val: nan, pos_over_neg: 211.0928497314453 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.7983, loss_val: nan, pos_over_neg: 345.7970275878906 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.795, loss_val: nan, pos_over_neg: 350.5366516113281 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.7976, loss_val: nan, pos_over_neg: 276.9833068847656 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.8212, loss_val: nan, pos_over_neg: 233.31268310546875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.8154, loss_val: nan, pos_over_neg: 249.34156799316406 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.8631, loss_val: nan, pos_over_neg: 283.1585998535156 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.8582, loss_val: nan, pos_over_neg: 262.9666442871094 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.8267, loss_val: nan, pos_over_neg: 230.93589782714844 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.8452, loss_val: nan, pos_over_neg: 206.3364715576172 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.8521, loss_val: nan, pos_over_neg: 192.72787475585938 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.7923, loss_val: nan, pos_over_neg: 223.2559814453125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 238.55661010742188 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.8226, loss_val: nan, pos_over_neg: 260.21661376953125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.8707, loss_val: nan, pos_over_neg: 222.99087524414062 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.8253, loss_val: nan, pos_over_neg: 389.40948486328125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.8239, loss_val: nan, pos_over_neg: 504.7148742675781 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.8601, loss_val: nan, pos_over_neg: 526.0347290039062 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 252.392578125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.8183, loss_val: nan, pos_over_neg: 262.55889892578125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.8293, loss_val: nan, pos_over_neg: 228.3836669921875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.8505, loss_val: nan, pos_over_neg: 180.48301696777344 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.8223, loss_val: nan, pos_over_neg: 204.96571350097656 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.8485, loss_val: nan, pos_over_neg: 159.85549926757812 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.8491, loss_val: nan, pos_over_neg: 267.4423828125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.8291, loss_val: nan, pos_over_neg: 326.3599853515625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.8498, loss_val: nan, pos_over_neg: 359.40472412109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.7987, loss_val: nan, pos_over_neg: 500.03131103515625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.8357, loss_val: nan, pos_over_neg: 419.67474365234375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.8381, loss_val: nan, pos_over_neg: 627.7339477539062 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.8363, loss_val: nan, pos_over_neg: 380.55029296875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.8525, loss_val: nan, pos_over_neg: 232.91639709472656 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.8368, loss_val: nan, pos_over_neg: 208.60531616210938 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 175.93734741210938 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.8091, loss_val: nan, pos_over_neg: 289.83551025390625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.8488, loss_val: nan, pos_over_neg: 324.43310546875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 291.3175354003906 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.8487, loss_val: nan, pos_over_neg: 304.8495788574219 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 356.0062255859375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.807, loss_val: nan, pos_over_neg: 276.0626220703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.8345, loss_val: nan, pos_over_neg: 273.3647155761719 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.7693, loss_val: nan, pos_over_neg: 380.62884521484375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.8116, loss_val: nan, pos_over_neg: 397.0034484863281 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.8353, loss_val: nan, pos_over_neg: 297.4153137207031 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.8004, loss_val: nan, pos_over_neg: 326.423828125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.8173, loss_val: nan, pos_over_neg: 332.995849609375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.8326, loss_val: nan, pos_over_neg: 228.9451141357422 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.8153, loss_val: nan, pos_over_neg: 444.3958435058594 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.829, loss_val: nan, pos_over_neg: 324.2630310058594 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.8478, loss_val: nan, pos_over_neg: 343.6329650878906 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 355.40936279296875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.785, loss_val: nan, pos_over_neg: 272.90838623046875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.815, loss_val: nan, pos_over_neg: 293.57965087890625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.8062, loss_val: nan, pos_over_neg: 316.0260925292969 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.8075, loss_val: nan, pos_over_neg: 248.75099182128906 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.7651, loss_val: nan, pos_over_neg: 323.973876953125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.8557, loss_val: nan, pos_over_neg: 329.87298583984375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.7785, loss_val: nan, pos_over_neg: 333.210693359375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.8138, loss_val: nan, pos_over_neg: 227.9231719970703 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.8432, loss_val: nan, pos_over_neg: 233.5361328125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.8552, loss_val: nan, pos_over_neg: 247.54727172851562 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.842, loss_val: nan, pos_over_neg: 284.0240173339844 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.7775, loss_val: nan, pos_over_neg: 413.12255859375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 192.3709716796875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.8324, loss_val: nan, pos_over_neg: 200.01150512695312 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.8383, loss_val: nan, pos_over_neg: 217.473876953125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.7987, loss_val: nan, pos_over_neg: 232.0354766845703 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.8169, loss_val: nan, pos_over_neg: 300.15557861328125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.8221, loss_val: nan, pos_over_neg: 404.8358459472656 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.7597, loss_val: nan, pos_over_neg: 439.8192138671875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.7597, loss_val: nan, pos_over_neg: 301.65130615234375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.8076, loss_val: nan, pos_over_neg: 245.74624633789062 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.821, loss_val: nan, pos_over_neg: 267.930419921875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.841, loss_val: nan, pos_over_neg: 192.82614135742188 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.818, loss_val: nan, pos_over_neg: 277.51409912109375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.8753, loss_val: nan, pos_over_neg: 196.14678955078125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.803, loss_val: nan, pos_over_neg: 234.87667846679688 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.8377, loss_val: nan, pos_over_neg: 271.43731689453125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.8217, loss_val: nan, pos_over_neg: 314.2900390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.8229, loss_val: nan, pos_over_neg: 305.1815185546875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.8267, loss_val: nan, pos_over_neg: 482.7270202636719 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.8155, loss_val: nan, pos_over_neg: 295.3874206542969 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.8125, loss_val: nan, pos_over_neg: 212.42147827148438 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.8349, loss_val: nan, pos_over_neg: 316.2769775390625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.7941, loss_val: nan, pos_over_neg: 406.8260803222656 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.8017, loss_val: nan, pos_over_neg: 229.00628662109375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.8095, loss_val: nan, pos_over_neg: 296.69036865234375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.8008, loss_val: nan, pos_over_neg: 269.49005126953125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.8092, loss_val: nan, pos_over_neg: 221.0374755859375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.8185, loss_val: nan, pos_over_neg: 216.73098754882812 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.8223, loss_val: nan, pos_over_neg: 275.7848815917969 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.8287, loss_val: nan, pos_over_neg: 252.07476806640625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.8197, loss_val: nan, pos_over_neg: 489.53424072265625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.8101, loss_val: nan, pos_over_neg: 306.4071960449219 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.8244, loss_val: nan, pos_over_neg: 381.5764465332031 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.8658, loss_val: nan, pos_over_neg: 264.5736389160156 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.8115, loss_val: nan, pos_over_neg: 382.2082824707031 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.7575, loss_val: nan, pos_over_neg: 631.1054077148438 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.7814, loss_val: nan, pos_over_neg: 268.9562072753906 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.8188, loss_val: nan, pos_over_neg: 244.8085174560547 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.7968, loss_val: nan, pos_over_neg: 334.2288818359375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.8044, loss_val: nan, pos_over_neg: 257.08984375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.8316, loss_val: nan, pos_over_neg: 251.0112762451172 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.8434, loss_val: nan, pos_over_neg: 338.77606201171875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.8326, loss_val: nan, pos_over_neg: 264.7123107910156 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.8067, loss_val: nan, pos_over_neg: 385.99298095703125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.7718, loss_val: nan, pos_over_neg: 534.3185424804688 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.8054, loss_val: nan, pos_over_neg: 531.8067016601562 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.8059, loss_val: nan, pos_over_neg: 266.52423095703125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.7977, loss_val: nan, pos_over_neg: 237.51898193359375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.7501, loss_val: nan, pos_over_neg: 309.2857971191406 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.8152, loss_val: nan, pos_over_neg: 253.6996612548828 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.7985, loss_val: nan, pos_over_neg: 226.75514221191406 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.8042, loss_val: nan, pos_over_neg: 291.687744140625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.8205, loss_val: nan, pos_over_neg: 421.4762878417969 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.8838, loss_val: nan, pos_over_neg: 230.43756103515625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.7784, loss_val: nan, pos_over_neg: 278.65093994140625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.8462, loss_val: nan, pos_over_neg: 616.7770385742188 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.7971, loss_val: nan, pos_over_neg: 339.1677551269531 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.7863, loss_val: nan, pos_over_neg: 335.2221984863281 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.8323, loss_val: nan, pos_over_neg: 195.85276794433594 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.8296, loss_val: nan, pos_over_neg: 197.39529418945312 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.8093, loss_val: nan, pos_over_neg: 251.94119262695312 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.8226, loss_val: nan, pos_over_neg: 211.818115234375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.8553, loss_val: nan, pos_over_neg: 236.53395080566406 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.7746, loss_val: nan, pos_over_neg: 401.3123779296875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.7704, loss_val: nan, pos_over_neg: 392.592529296875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.8189, loss_val: nan, pos_over_neg: 333.41876220703125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.812, loss_val: nan, pos_over_neg: 213.79678344726562 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.8058, loss_val: nan, pos_over_neg: 319.5152282714844 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.788, loss_val: nan, pos_over_neg: 351.2662353515625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 214.7406463623047 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.7859, loss_val: nan, pos_over_neg: 286.736328125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.7853, loss_val: nan, pos_over_neg: 277.26507568359375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.82, loss_val: nan, pos_over_neg: 260.74554443359375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.8388, loss_val: nan, pos_over_neg: 230.37619018554688 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.878, loss_val: nan, pos_over_neg: 244.82510375976562 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.8564, loss_val: nan, pos_over_neg: 366.33056640625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.7889, loss_val: nan, pos_over_neg: 492.93341064453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.7815, loss_val: nan, pos_over_neg: 334.52691650390625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.8299, loss_val: nan, pos_over_neg: 229.69464111328125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.8097, loss_val: nan, pos_over_neg: 368.5022888183594 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.7689, loss_val: nan, pos_over_neg: 360.2242736816406 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.8185, loss_val: nan, pos_over_neg: 291.8673400878906 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.8081, loss_val: nan, pos_over_neg: 281.55230712890625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.8084, loss_val: nan, pos_over_neg: 231.33709716796875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.8234, loss_val: nan, pos_over_neg: 218.33480834960938 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.8285, loss_val: nan, pos_over_neg: 187.3742218017578 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.8651, loss_val: nan, pos_over_neg: 238.22731018066406 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.814, loss_val: nan, pos_over_neg: 382.0672912597656 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.8219, loss_val: nan, pos_over_neg: 289.5968322753906 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.8669, loss_val: nan, pos_over_neg: 215.62380981445312 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.799, loss_val: nan, pos_over_neg: 237.4386749267578 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.7982, loss_val: nan, pos_over_neg: 257.3575134277344 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.8046, loss_val: nan, pos_over_neg: 180.44940185546875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.7647, loss_val: nan, pos_over_neg: 235.47952270507812 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.7598, loss_val: nan, pos_over_neg: 335.26165771484375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.7917, loss_val: nan, pos_over_neg: 227.70089721679688 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.8136, loss_val: nan, pos_over_neg: 279.851318359375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.8669, loss_val: nan, pos_over_neg: 299.751220703125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.7814, loss_val: nan, pos_over_neg: 414.8132019042969 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [45:33<75647:01:19, 907.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.8427, loss_val: nan, pos_over_neg: 289.8622131347656 lr: 0.00031623\n",
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 3.794, loss_val: nan, pos_over_neg: 295.88214111328125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.7838, loss_val: nan, pos_over_neg: 302.9064025878906 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 213.50436401367188 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.819, loss_val: nan, pos_over_neg: 196.8424835205078 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.8313, loss_val: nan, pos_over_neg: 268.9628601074219 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.8448, loss_val: nan, pos_over_neg: 272.85076904296875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.8047, loss_val: nan, pos_over_neg: 288.224365234375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.8382, loss_val: nan, pos_over_neg: 229.23497009277344 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.7792, loss_val: nan, pos_over_neg: 311.0002136230469 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.824, loss_val: nan, pos_over_neg: 435.96453857421875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.778, loss_val: nan, pos_over_neg: 338.532470703125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.8072, loss_val: nan, pos_over_neg: 182.05709838867188 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.7748, loss_val: nan, pos_over_neg: 243.0225067138672 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.7835, loss_val: nan, pos_over_neg: 410.4554138183594 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.813, loss_val: nan, pos_over_neg: 198.419189453125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.7459, loss_val: nan, pos_over_neg: 242.34841918945312 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.8023, loss_val: nan, pos_over_neg: 252.21080017089844 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.8589, loss_val: nan, pos_over_neg: 245.53453063964844 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.8358, loss_val: nan, pos_over_neg: 235.1217498779297 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.7897, loss_val: nan, pos_over_neg: 376.8635559082031 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.7961, loss_val: nan, pos_over_neg: 480.657470703125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.7511, loss_val: nan, pos_over_neg: 564.3472290039062 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.751, loss_val: nan, pos_over_neg: 387.5786437988281 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.8417, loss_val: nan, pos_over_neg: 253.80889892578125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.8256, loss_val: nan, pos_over_neg: 208.60107421875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.786, loss_val: nan, pos_over_neg: 356.7837219238281 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 294.55792236328125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.8057, loss_val: nan, pos_over_neg: 418.5142822265625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.8331, loss_val: nan, pos_over_neg: 329.3244934082031 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.7819, loss_val: nan, pos_over_neg: 384.3797607421875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.8523, loss_val: nan, pos_over_neg: 272.92034912109375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.8174, loss_val: nan, pos_over_neg: 198.563720703125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.7953, loss_val: nan, pos_over_neg: 248.86376953125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 379.7228698730469 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.8828, loss_val: nan, pos_over_neg: 256.9696044921875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.7746, loss_val: nan, pos_over_neg: 336.7984924316406 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.8251, loss_val: nan, pos_over_neg: 279.4476318359375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.7802, loss_val: nan, pos_over_neg: 304.4076232910156 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.8435, loss_val: nan, pos_over_neg: 214.85418701171875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.8389, loss_val: nan, pos_over_neg: 211.34872436523438 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.8233, loss_val: nan, pos_over_neg: 220.65760803222656 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.8006, loss_val: nan, pos_over_neg: 294.6739501953125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 291.0843811035156 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.7854, loss_val: nan, pos_over_neg: 296.1626281738281 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 358.6330871582031 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 858.52392578125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 253.12832641601562 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.8164, loss_val: nan, pos_over_neg: 191.6322021484375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.8227, loss_val: nan, pos_over_neg: 212.7113494873047 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.7498, loss_val: nan, pos_over_neg: 359.853515625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.7891, loss_val: nan, pos_over_neg: 366.140869140625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.7707, loss_val: nan, pos_over_neg: 390.443359375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.8235, loss_val: nan, pos_over_neg: 251.13816833496094 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.8796, loss_val: nan, pos_over_neg: 213.45748901367188 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.7883, loss_val: nan, pos_over_neg: 266.69195556640625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.8619, loss_val: nan, pos_over_neg: 220.4544677734375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.8224, loss_val: nan, pos_over_neg: 325.2600402832031 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.787, loss_val: nan, pos_over_neg: 458.29168701171875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.7988, loss_val: nan, pos_over_neg: 258.0777282714844 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.819, loss_val: nan, pos_over_neg: 349.9372863769531 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.8186, loss_val: nan, pos_over_neg: 464.8681335449219 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 312.142578125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 269.0358581542969 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.8083, loss_val: nan, pos_over_neg: 259.0728759765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.7842, loss_val: nan, pos_over_neg: 320.6876220703125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.83, loss_val: nan, pos_over_neg: 206.01893615722656 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.7803, loss_val: nan, pos_over_neg: 249.10040283203125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 281.44549560546875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.8354, loss_val: nan, pos_over_neg: 228.85301208496094 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.8069, loss_val: nan, pos_over_neg: 447.55615234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.8365, loss_val: nan, pos_over_neg: 359.2591247558594 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.8643, loss_val: nan, pos_over_neg: 264.53533935546875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.8038, loss_val: nan, pos_over_neg: 317.8584289550781 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.7864, loss_val: nan, pos_over_neg: 284.2900695800781 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.8094, loss_val: nan, pos_over_neg: 321.2665710449219 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.8156, loss_val: nan, pos_over_neg: 252.88824462890625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.8447, loss_val: nan, pos_over_neg: 300.9579772949219 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.775, loss_val: nan, pos_over_neg: 286.6335144042969 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.8115, loss_val: nan, pos_over_neg: 268.72515869140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.7587, loss_val: nan, pos_over_neg: 291.0101013183594 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.7551, loss_val: nan, pos_over_neg: 201.3573760986328 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.7977, loss_val: nan, pos_over_neg: 210.72479248046875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.8017, loss_val: nan, pos_over_neg: 215.72962951660156 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.7679, loss_val: nan, pos_over_neg: 221.93026733398438 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.8004, loss_val: nan, pos_over_neg: 309.4131774902344 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.8033, loss_val: nan, pos_over_neg: 325.7950744628906 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.7542, loss_val: nan, pos_over_neg: 311.0486145019531 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 469.1271667480469 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.8208, loss_val: nan, pos_over_neg: 460.9183654785156 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.8104, loss_val: nan, pos_over_neg: 251.84620666503906 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.8018, loss_val: nan, pos_over_neg: 207.38356018066406 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.7998, loss_val: nan, pos_over_neg: 178.6997528076172 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.7769, loss_val: nan, pos_over_neg: 277.3250427246094 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.7705, loss_val: nan, pos_over_neg: 266.1351013183594 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.7781, loss_val: nan, pos_over_neg: 241.36595153808594 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.8179, loss_val: nan, pos_over_neg: 225.25762939453125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.7998, loss_val: nan, pos_over_neg: 283.2557678222656 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.7844, loss_val: nan, pos_over_neg: 274.9494934082031 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.7507, loss_val: nan, pos_over_neg: 332.6349182128906 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.7648, loss_val: nan, pos_over_neg: 403.9270935058594 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.8054, loss_val: nan, pos_over_neg: 174.5106658935547 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.8, loss_val: nan, pos_over_neg: 227.9585723876953 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.7948, loss_val: nan, pos_over_neg: 254.23817443847656 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.7673, loss_val: nan, pos_over_neg: 326.3268737792969 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 275.1398010253906 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.8567, loss_val: nan, pos_over_neg: 279.6637268066406 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 239.29391479492188 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.7751, loss_val: nan, pos_over_neg: 279.36944580078125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.7957, loss_val: nan, pos_over_neg: 191.7875213623047 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.8171, loss_val: nan, pos_over_neg: 175.55604553222656 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.8113, loss_val: nan, pos_over_neg: 215.69155883789062 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.8148, loss_val: nan, pos_over_neg: 263.70904541015625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.8043, loss_val: nan, pos_over_neg: 267.5162048339844 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.8072, loss_val: nan, pos_over_neg: 240.46351623535156 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.7936, loss_val: nan, pos_over_neg: 341.1548156738281 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.7701, loss_val: nan, pos_over_neg: 299.2070617675781 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.7984, loss_val: nan, pos_over_neg: 488.2325134277344 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 636.6417846679688 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.7985, loss_val: nan, pos_over_neg: 238.93832397460938 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.868, loss_val: nan, pos_over_neg: 255.94207763671875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.7918, loss_val: nan, pos_over_neg: 317.1543273925781 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 245.32933044433594 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.8123, loss_val: nan, pos_over_neg: 237.76280212402344 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.8403, loss_val: nan, pos_over_neg: 147.58457946777344 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.822, loss_val: nan, pos_over_neg: 212.15560913085938 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 296.8253479003906 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.7822, loss_val: nan, pos_over_neg: 378.8669738769531 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.7704, loss_val: nan, pos_over_neg: 364.6276550292969 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.7885, loss_val: nan, pos_over_neg: 267.5802001953125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.7849, loss_val: nan, pos_over_neg: 297.3738708496094 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.8016, loss_val: nan, pos_over_neg: 392.5330505371094 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.7256, loss_val: nan, pos_over_neg: 542.2239990234375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.7682, loss_val: nan, pos_over_neg: 302.55792236328125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 260.6675109863281 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.8033, loss_val: nan, pos_over_neg: 312.9090576171875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.7648, loss_val: nan, pos_over_neg: 280.3392639160156 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.7606, loss_val: nan, pos_over_neg: 358.64654541015625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.7645, loss_val: nan, pos_over_neg: 505.54400634765625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.8008, loss_val: nan, pos_over_neg: 249.768798828125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.7159, loss_val: nan, pos_over_neg: 250.24832153320312 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.7651, loss_val: nan, pos_over_neg: 319.7049865722656 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.7796, loss_val: nan, pos_over_neg: 304.6645812988281 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.7733, loss_val: nan, pos_over_neg: 271.0357666015625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.7605, loss_val: nan, pos_over_neg: 247.8632354736328 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.7532, loss_val: nan, pos_over_neg: 240.234619140625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.7699, loss_val: nan, pos_over_neg: 264.609130859375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.745, loss_val: nan, pos_over_neg: 308.1875305175781 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.8158, loss_val: nan, pos_over_neg: 149.9822998046875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.8166, loss_val: nan, pos_over_neg: 211.09083557128906 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.8107, loss_val: nan, pos_over_neg: 352.5204772949219 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.7745, loss_val: nan, pos_over_neg: 531.9242553710938 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.7391, loss_val: nan, pos_over_neg: 470.54541015625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.7846, loss_val: nan, pos_over_neg: 294.3782043457031 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.8203, loss_val: nan, pos_over_neg: 279.4535217285156 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.774, loss_val: nan, pos_over_neg: 245.24514770507812 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.8268, loss_val: nan, pos_over_neg: 179.7133026123047 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.7885, loss_val: nan, pos_over_neg: 247.1866455078125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.7887, loss_val: nan, pos_over_neg: 307.0576477050781 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.7984, loss_val: nan, pos_over_neg: 328.35906982421875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.7956, loss_val: nan, pos_over_neg: 296.9027099609375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.801, loss_val: nan, pos_over_neg: 318.8642883300781 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.7809, loss_val: nan, pos_over_neg: 399.603759765625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.8164, loss_val: nan, pos_over_neg: 250.13636779785156 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.8181, loss_val: nan, pos_over_neg: 225.49520874023438 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.7949, loss_val: nan, pos_over_neg: 343.754150390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.7644, loss_val: nan, pos_over_neg: 309.33355712890625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.771, loss_val: nan, pos_over_neg: 304.63372802734375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 379.95501708984375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.8286, loss_val: nan, pos_over_neg: 320.77734375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.7837, loss_val: nan, pos_over_neg: 237.4208526611328 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.8441, loss_val: nan, pos_over_neg: 270.76239013671875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.7486, loss_val: nan, pos_over_neg: 436.3339538574219 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.8139, loss_val: nan, pos_over_neg: 221.4510498046875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.8253, loss_val: nan, pos_over_neg: 256.2939758300781 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.8285, loss_val: nan, pos_over_neg: 272.687744140625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.7713, loss_val: nan, pos_over_neg: 336.9833984375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 211.77670288085938 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.7959, loss_val: nan, pos_over_neg: 387.15264892578125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.7664, loss_val: nan, pos_over_neg: 535.6737060546875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 300.1812438964844 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.7893, loss_val: nan, pos_over_neg: 257.417236328125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.8202, loss_val: nan, pos_over_neg: 347.7665710449219 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.7793, loss_val: nan, pos_over_neg: 391.2130126953125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.8324, loss_val: nan, pos_over_neg: 282.51837158203125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.7777, loss_val: nan, pos_over_neg: 248.89027404785156 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.7391, loss_val: nan, pos_over_neg: 237.38900756835938 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.8126, loss_val: nan, pos_over_neg: 214.53781127929688 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 257.821044921875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.8098, loss_val: nan, pos_over_neg: 389.0534973144531 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.739, loss_val: nan, pos_over_neg: 402.9608154296875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.8002, loss_val: nan, pos_over_neg: 271.1934509277344 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.8408, loss_val: nan, pos_over_neg: 264.38104248046875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.7415, loss_val: nan, pos_over_neg: 484.9814758300781 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.7448, loss_val: nan, pos_over_neg: 478.40594482421875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.7338, loss_val: nan, pos_over_neg: 292.0966491699219 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.806, loss_val: nan, pos_over_neg: 234.84237670898438 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.7606, loss_val: nan, pos_over_neg: 329.30474853515625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.8, loss_val: nan, pos_over_neg: 254.36285400390625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.7822, loss_val: nan, pos_over_neg: 183.5077362060547 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.7712, loss_val: nan, pos_over_neg: 210.77334594726562 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.81, loss_val: nan, pos_over_neg: 177.22996520996094 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.8612, loss_val: nan, pos_over_neg: 262.1737976074219 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.7623, loss_val: nan, pos_over_neg: 275.9133605957031 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.7834, loss_val: nan, pos_over_neg: 281.457763671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.7617, loss_val: nan, pos_over_neg: 460.2453308105469 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.8298, loss_val: nan, pos_over_neg: 228.78094482421875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.8127, loss_val: nan, pos_over_neg: 164.52528381347656 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.8212, loss_val: nan, pos_over_neg: 286.72711181640625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 297.5798034667969 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.7482, loss_val: nan, pos_over_neg: 430.5473937988281 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.8043, loss_val: nan, pos_over_neg: 247.12847900390625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.7857, loss_val: nan, pos_over_neg: 258.2945251464844 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 320.9974060058594 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.7767, loss_val: nan, pos_over_neg: 375.3151550292969 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.8115, loss_val: nan, pos_over_neg: 361.3353271484375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.7874, loss_val: nan, pos_over_neg: 245.71920776367188 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.8432, loss_val: nan, pos_over_neg: 240.2360076904297 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 245.72068786621094 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.8024, loss_val: nan, pos_over_neg: 250.6623992919922 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 241.43292236328125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.8067, loss_val: nan, pos_over_neg: 312.4429626464844 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.7489, loss_val: nan, pos_over_neg: 241.93280029296875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.8073, loss_val: nan, pos_over_neg: 271.3479309082031 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.7882, loss_val: nan, pos_over_neg: 379.96649169921875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.7859, loss_val: nan, pos_over_neg: 319.22650146484375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.8062, loss_val: nan, pos_over_neg: 198.9276885986328 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.7949, loss_val: nan, pos_over_neg: 266.47186279296875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.7727, loss_val: nan, pos_over_neg: 270.2079772949219 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.7576, loss_val: nan, pos_over_neg: 284.0656433105469 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.7625, loss_val: nan, pos_over_neg: 248.28697204589844 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.8325, loss_val: nan, pos_over_neg: 232.31158447265625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.8176, loss_val: nan, pos_over_neg: 289.9819030761719 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.7661, loss_val: nan, pos_over_neg: 408.676025390625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.7866, loss_val: nan, pos_over_neg: 316.3757629394531 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.8074, loss_val: nan, pos_over_neg: 251.02708435058594 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.7347, loss_val: nan, pos_over_neg: 396.5458068847656 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.8359, loss_val: nan, pos_over_neg: 447.56707763671875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.7674, loss_val: nan, pos_over_neg: 574.3699951171875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.7849, loss_val: nan, pos_over_neg: 259.245361328125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.7705, loss_val: nan, pos_over_neg: 240.23458862304688 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.7736, loss_val: nan, pos_over_neg: 207.2765655517578 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.8251, loss_val: nan, pos_over_neg: 172.97463989257812 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.7959, loss_val: nan, pos_over_neg: 200.54730224609375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.8097, loss_val: nan, pos_over_neg: 233.15756225585938 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.7968, loss_val: nan, pos_over_neg: 314.14410400390625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.7715, loss_val: nan, pos_over_neg: 389.4173889160156 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.7952, loss_val: nan, pos_over_neg: 314.06463623046875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.8171, loss_val: nan, pos_over_neg: 270.8255920410156 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.7855, loss_val: nan, pos_over_neg: 217.5838623046875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.8133, loss_val: nan, pos_over_neg: 269.13720703125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.7554, loss_val: nan, pos_over_neg: 297.1560363769531 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.8089, loss_val: nan, pos_over_neg: 186.75660705566406 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.846, loss_val: nan, pos_over_neg: 165.90866088867188 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.7707, loss_val: nan, pos_over_neg: 283.3751525878906 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.7779, loss_val: nan, pos_over_neg: 262.7146301269531 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.8126, loss_val: nan, pos_over_neg: 280.31182861328125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.7886, loss_val: nan, pos_over_neg: 261.18621826171875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.8227, loss_val: nan, pos_over_neg: 258.8714599609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.8143, loss_val: nan, pos_over_neg: 168.09976196289062 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.8235, loss_val: nan, pos_over_neg: 241.03628540039062 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.7485, loss_val: nan, pos_over_neg: 363.15643310546875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.781, loss_val: nan, pos_over_neg: 326.0255432128906 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.8272, loss_val: nan, pos_over_neg: 200.5943145751953 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.79, loss_val: nan, pos_over_neg: 225.79296875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.7734, loss_val: nan, pos_over_neg: 295.22314453125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.7752, loss_val: nan, pos_over_neg: 290.6461181640625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.7218, loss_val: nan, pos_over_neg: 399.4794006347656 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.792, loss_val: nan, pos_over_neg: 286.8309631347656 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.7901, loss_val: nan, pos_over_neg: 446.45806884765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.8005, loss_val: nan, pos_over_neg: 329.7384033203125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.7863, loss_val: nan, pos_over_neg: 373.6593017578125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.7617, loss_val: nan, pos_over_neg: 367.5206298828125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.7779, loss_val: nan, pos_over_neg: 308.5648193359375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.7595, loss_val: nan, pos_over_neg: 374.75775146484375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.7545, loss_val: nan, pos_over_neg: 297.8882751464844 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.822, loss_val: nan, pos_over_neg: 347.5898742675781 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.8445, loss_val: nan, pos_over_neg: 310.3875427246094 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.8035, loss_val: nan, pos_over_neg: 313.65814208984375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.7589, loss_val: nan, pos_over_neg: 376.10162353515625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.785, loss_val: nan, pos_over_neg: 315.1599426269531 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.7603, loss_val: nan, pos_over_neg: 594.9396362304688 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.8085, loss_val: nan, pos_over_neg: 281.9167785644531 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 271.3182067871094 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 339.51666259765625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.8099, loss_val: nan, pos_over_neg: 339.4482421875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.7926, loss_val: nan, pos_over_neg: 411.470458984375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.7544, loss_val: nan, pos_over_neg: 418.4507751464844 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.8383, loss_val: nan, pos_over_neg: 184.87110900878906 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.8311, loss_val: nan, pos_over_neg: 263.89044189453125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.7639, loss_val: nan, pos_over_neg: 307.6481628417969 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.7624, loss_val: nan, pos_over_neg: 285.79644775390625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.7755, loss_val: nan, pos_over_neg: 203.45220947265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.8127, loss_val: nan, pos_over_neg: 222.3380889892578 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.7672, loss_val: nan, pos_over_neg: 333.5277099609375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.7702, loss_val: nan, pos_over_neg: 372.8938293457031 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.7823, loss_val: nan, pos_over_neg: 350.0481262207031 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.8034, loss_val: nan, pos_over_neg: 278.88232421875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.7654, loss_val: nan, pos_over_neg: 311.9019775390625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 347.2854919433594 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.7973, loss_val: nan, pos_over_neg: 244.15829467773438 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.7641, loss_val: nan, pos_over_neg: 273.1832275390625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.7632, loss_val: nan, pos_over_neg: 283.8975830078125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.7917, loss_val: nan, pos_over_neg: 309.75555419921875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.7586, loss_val: nan, pos_over_neg: 400.6754455566406 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.8083, loss_val: nan, pos_over_neg: 202.7401885986328 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.7745, loss_val: nan, pos_over_neg: 357.78717041015625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 258.4133605957031 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.7691, loss_val: nan, pos_over_neg: 257.151611328125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.7726, loss_val: nan, pos_over_neg: 368.3163757324219 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.7658, loss_val: nan, pos_over_neg: 505.21234130859375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.8125, loss_val: nan, pos_over_neg: 233.7439422607422 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.7713, loss_val: nan, pos_over_neg: 256.2889099121094 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.7816, loss_val: nan, pos_over_neg: 271.5064697265625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.8, loss_val: nan, pos_over_neg: 247.2621307373047 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 270.56646728515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 228.50836181640625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.7986, loss_val: nan, pos_over_neg: 345.724853515625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.7955, loss_val: nan, pos_over_neg: 243.77520751953125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 307.6580810546875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.825, loss_val: nan, pos_over_neg: 481.7354736328125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.7496, loss_val: nan, pos_over_neg: 428.143798828125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.8821, loss_val: nan, pos_over_neg: 215.45606994628906 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.761, loss_val: nan, pos_over_neg: 333.8077087402344 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.7805, loss_val: nan, pos_over_neg: 215.97584533691406 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.8052, loss_val: nan, pos_over_neg: 174.7040557861328 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.7672, loss_val: nan, pos_over_neg: 271.9977111816406 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.8274, loss_val: nan, pos_over_neg: 210.8756103515625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 207.19192504882812 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.832, loss_val: nan, pos_over_neg: 256.5815734863281 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.8193, loss_val: nan, pos_over_neg: 445.1197204589844 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.7622, loss_val: nan, pos_over_neg: 465.4480895996094 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.8441, loss_val: nan, pos_over_neg: 230.4772186279297 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.7638, loss_val: nan, pos_over_neg: 270.8027648925781 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.7579, loss_val: nan, pos_over_neg: 204.3052520751953 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.7648, loss_val: nan, pos_over_neg: 198.38157653808594 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.8134, loss_val: nan, pos_over_neg: 225.7476043701172 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.7747, loss_val: nan, pos_over_neg: 308.2073974609375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.8103, loss_val: nan, pos_over_neg: 188.35464477539062 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.7823, loss_val: nan, pos_over_neg: 195.78419494628906 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.8056, loss_val: nan, pos_over_neg: 288.93634033203125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.8316, loss_val: nan, pos_over_neg: 261.5552673339844 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.7863, loss_val: nan, pos_over_neg: 242.4186553955078 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.7756, loss_val: nan, pos_over_neg: 235.4467010498047 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.7996, loss_val: nan, pos_over_neg: 302.410400390625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.7797, loss_val: nan, pos_over_neg: 257.27960205078125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.7769, loss_val: nan, pos_over_neg: 258.7266540527344 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.7502, loss_val: nan, pos_over_neg: 251.35545349121094 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.7789, loss_val: nan, pos_over_neg: 350.9972229003906 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.8117, loss_val: nan, pos_over_neg: 228.8565216064453 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 237.14390563964844 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.7208, loss_val: nan, pos_over_neg: 301.0207824707031 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.7627, loss_val: nan, pos_over_neg: 284.0831604003906 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.7681, loss_val: nan, pos_over_neg: 287.3839416503906 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.8292, loss_val: nan, pos_over_neg: 192.12705993652344 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.7852, loss_val: nan, pos_over_neg: 224.67115783691406 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.7685, loss_val: nan, pos_over_neg: 227.81492614746094 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.7883, loss_val: nan, pos_over_neg: 236.45359802246094 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.7396, loss_val: nan, pos_over_neg: 390.2938537597656 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.756, loss_val: nan, pos_over_neg: 462.27655029296875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.7595, loss_val: nan, pos_over_neg: 691.5354614257812 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.8144, loss_val: nan, pos_over_neg: 332.1661376953125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.7821, loss_val: nan, pos_over_neg: 413.57928466796875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.8199, loss_val: nan, pos_over_neg: 301.7701721191406 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.758, loss_val: nan, pos_over_neg: 281.6473388671875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.7315, loss_val: nan, pos_over_neg: 346.1054992675781 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.8032, loss_val: nan, pos_over_neg: 209.7632598876953 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.769, loss_val: nan, pos_over_neg: 252.42152404785156 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.7506, loss_val: nan, pos_over_neg: 244.40054321289062 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.7558, loss_val: nan, pos_over_neg: 293.61492919921875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.8399, loss_val: nan, pos_over_neg: 222.34800720214844 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 279.96038818359375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.7517, loss_val: nan, pos_over_neg: 242.16036987304688 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.7617, loss_val: nan, pos_over_neg: 464.1312561035156 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.7454, loss_val: nan, pos_over_neg: 427.1199645996094 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 429.7066345214844 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.8009, loss_val: nan, pos_over_neg: 229.70257568359375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.7472, loss_val: nan, pos_over_neg: 279.3770446777344 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.7639, loss_val: nan, pos_over_neg: 337.5641784667969 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 328.80755615234375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.7577, loss_val: nan, pos_over_neg: 390.4163818359375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.7499, loss_val: nan, pos_over_neg: 320.0862121582031 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 281.54254150390625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.7653, loss_val: nan, pos_over_neg: 285.4245300292969 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.7415, loss_val: nan, pos_over_neg: 386.703369140625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.8061, loss_val: nan, pos_over_neg: 186.39163208007812 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.7957, loss_val: nan, pos_over_neg: 193.93959045410156 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.7804, loss_val: nan, pos_over_neg: 338.2210693359375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.8161, loss_val: nan, pos_over_neg: 233.07716369628906 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.7978, loss_val: nan, pos_over_neg: 287.3915100097656 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.7788, loss_val: nan, pos_over_neg: 263.70849609375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.7279, loss_val: nan, pos_over_neg: 405.7523498535156 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.7959, loss_val: nan, pos_over_neg: 313.8825988769531 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.7906, loss_val: nan, pos_over_neg: 357.2621154785156 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 520.2626342773438 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.7742, loss_val: nan, pos_over_neg: 349.9598693847656 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.81, loss_val: nan, pos_over_neg: 236.73524475097656 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.7815, loss_val: nan, pos_over_neg: 344.4580383300781 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.7621, loss_val: nan, pos_over_neg: 282.7490234375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.8167, loss_val: nan, pos_over_neg: 193.52325439453125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.7443, loss_val: nan, pos_over_neg: 264.9131774902344 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.7901, loss_val: nan, pos_over_neg: 242.53878784179688 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.7679, loss_val: nan, pos_over_neg: 305.6241149902344 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 492.0404968261719 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.751, loss_val: nan, pos_over_neg: 410.0878601074219 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.7656, loss_val: nan, pos_over_neg: 351.77978515625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.7532, loss_val: nan, pos_over_neg: 359.0734558105469 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.7865, loss_val: nan, pos_over_neg: 315.84564208984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.8074, loss_val: nan, pos_over_neg: 262.7340393066406 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.7826, loss_val: nan, pos_over_neg: 218.47132873535156 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.7597, loss_val: nan, pos_over_neg: 299.2037353515625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.7912, loss_val: nan, pos_over_neg: 181.74386596679688 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.7652, loss_val: nan, pos_over_neg: 290.2050476074219 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.7704, loss_val: nan, pos_over_neg: 406.0316467285156 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.7914, loss_val: nan, pos_over_neg: 308.4678649902344 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.7546, loss_val: nan, pos_over_neg: 341.30084228515625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.7872, loss_val: nan, pos_over_neg: 302.8843994140625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.7614, loss_val: nan, pos_over_neg: 246.2308807373047 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.7492, loss_val: nan, pos_over_neg: 354.4964294433594 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.7831, loss_val: nan, pos_over_neg: 208.35562133789062 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.7618, loss_val: nan, pos_over_neg: 294.181884765625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.7932, loss_val: nan, pos_over_neg: 213.14561462402344 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.7622, loss_val: nan, pos_over_neg: 250.64093017578125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.8031, loss_val: nan, pos_over_neg: 345.4603271484375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.7735, loss_val: nan, pos_over_neg: 332.9822082519531 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.7847, loss_val: nan, pos_over_neg: 334.7190856933594 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.8019, loss_val: nan, pos_over_neg: 320.8753356933594 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.7606, loss_val: nan, pos_over_neg: 448.3074645996094 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.7627, loss_val: nan, pos_over_neg: 288.79864501953125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.7884, loss_val: nan, pos_over_neg: 279.8744201660156 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.7469, loss_val: nan, pos_over_neg: 318.18951416015625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.7766, loss_val: nan, pos_over_neg: 369.6774597167969 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.7706, loss_val: nan, pos_over_neg: 243.2762451171875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.7816, loss_val: nan, pos_over_neg: 253.8350372314453 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.7528, loss_val: nan, pos_over_neg: 412.27606201171875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 407.5205383300781 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.7885, loss_val: nan, pos_over_neg: 229.5199432373047 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.7366, loss_val: nan, pos_over_neg: 427.7287902832031 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.7541, loss_val: nan, pos_over_neg: 348.0278015136719 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 274.2384033203125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 336.0167541503906 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.7394, loss_val: nan, pos_over_neg: 353.2908630371094 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.7636, loss_val: nan, pos_over_neg: 305.94940185546875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.7464, loss_val: nan, pos_over_neg: 421.9898376464844 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.7682, loss_val: nan, pos_over_neg: 361.5343322753906 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.7607, loss_val: nan, pos_over_neg: 253.99636840820312 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.74, loss_val: nan, pos_over_neg: 324.9497375488281 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.8163, loss_val: nan, pos_over_neg: 239.20579528808594 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.7802, loss_val: nan, pos_over_neg: 263.31201171875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.7858, loss_val: nan, pos_over_neg: 454.61749267578125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.8224, loss_val: nan, pos_over_neg: 352.47247314453125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.7569, loss_val: nan, pos_over_neg: 231.4729766845703 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.7806, loss_val: nan, pos_over_neg: 310.9482116699219 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.7722, loss_val: nan, pos_over_neg: 290.0652770996094 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.7562, loss_val: nan, pos_over_neg: 443.51177978515625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.7274, loss_val: nan, pos_over_neg: 463.4477233886719 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.7697, loss_val: nan, pos_over_neg: 321.43145751953125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.7314, loss_val: nan, pos_over_neg: 387.922607421875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 391.5321350097656 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.794, loss_val: nan, pos_over_neg: 389.0991516113281 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.7776, loss_val: nan, pos_over_neg: 277.6184387207031 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.7495, loss_val: nan, pos_over_neg: 276.402587890625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 434.32550048828125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.7611, loss_val: nan, pos_over_neg: 355.47467041015625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 274.8268127441406 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.7594, loss_val: nan, pos_over_neg: 213.48260498046875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.7767, loss_val: nan, pos_over_neg: 186.08518981933594 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.8026, loss_val: nan, pos_over_neg: 257.0565490722656 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.8211, loss_val: nan, pos_over_neg: 408.2749328613281 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.748, loss_val: nan, pos_over_neg: 546.2862548828125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.7867, loss_val: nan, pos_over_neg: 363.28179931640625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.8707, loss_val: nan, pos_over_neg: 242.49305725097656 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.7424, loss_val: nan, pos_over_neg: 424.2070007324219 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.7556, loss_val: nan, pos_over_neg: 193.28689575195312 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.8129, loss_val: nan, pos_over_neg: 211.7454071044922 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.7352, loss_val: nan, pos_over_neg: 249.4198455810547 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.7686, loss_val: nan, pos_over_neg: 152.5634307861328 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.7355, loss_val: nan, pos_over_neg: 210.255126953125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.7619, loss_val: nan, pos_over_neg: 221.57943725585938 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.7416, loss_val: nan, pos_over_neg: 502.7109375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.7641, loss_val: nan, pos_over_neg: 297.7080993652344 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.7442, loss_val: nan, pos_over_neg: 217.1233367919922 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 259.6412048339844 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.7794, loss_val: nan, pos_over_neg: 326.72442626953125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.82, loss_val: nan, pos_over_neg: 276.076904296875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.7541, loss_val: nan, pos_over_neg: 231.16845703125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.7635, loss_val: nan, pos_over_neg: 211.68267822265625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.7419, loss_val: nan, pos_over_neg: 266.81915283203125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 174.8321075439453 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.7283, loss_val: nan, pos_over_neg: 369.9917907714844 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.7525, loss_val: nan, pos_over_neg: 348.43292236328125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.7357, loss_val: nan, pos_over_neg: 361.3591613769531 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.7137, loss_val: nan, pos_over_neg: 474.9680480957031 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.7691, loss_val: nan, pos_over_neg: 226.18222045898438 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.734, loss_val: nan, pos_over_neg: 265.2393798828125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.7664, loss_val: nan, pos_over_neg: 264.6138916015625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.7833, loss_val: nan, pos_over_neg: 270.64520263671875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.7901, loss_val: nan, pos_over_neg: 230.7956085205078 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.778, loss_val: nan, pos_over_neg: 268.7877197265625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.8064, loss_val: nan, pos_over_neg: 194.10914611816406 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 339.0021057128906 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.8181, loss_val: nan, pos_over_neg: 198.37538146972656 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.7873, loss_val: nan, pos_over_neg: 283.5775451660156 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.7489, loss_val: nan, pos_over_neg: 339.8053894042969 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.7666, loss_val: nan, pos_over_neg: 386.4793701171875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.7717, loss_val: nan, pos_over_neg: 362.161865234375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 334.2381591796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.796, loss_val: nan, pos_over_neg: 224.01748657226562 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.755, loss_val: nan, pos_over_neg: 294.79095458984375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.7968, loss_val: nan, pos_over_neg: 200.4781494140625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.7933, loss_val: nan, pos_over_neg: 181.7267303466797 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.7315, loss_val: nan, pos_over_neg: 200.4088592529297 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.7519, loss_val: nan, pos_over_neg: 218.32254028320312 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.7667, loss_val: nan, pos_over_neg: 333.139404296875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.7781, loss_val: nan, pos_over_neg: 307.1213073730469 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.7131, loss_val: nan, pos_over_neg: 303.6333923339844 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.7873, loss_val: nan, pos_over_neg: 352.97265625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.7499, loss_val: nan, pos_over_neg: 338.2966613769531 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.763, loss_val: nan, pos_over_neg: 379.313720703125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.7792, loss_val: nan, pos_over_neg: 286.50762939453125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.7497, loss_val: nan, pos_over_neg: 309.7909240722656 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.7567, loss_val: nan, pos_over_neg: 262.71234130859375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.84, loss_val: nan, pos_over_neg: 206.56178283691406 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.7343, loss_val: nan, pos_over_neg: 306.0132751464844 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.8186, loss_val: nan, pos_over_neg: 253.0830078125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.7333, loss_val: nan, pos_over_neg: 285.2317810058594 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.7741, loss_val: nan, pos_over_neg: 222.16683959960938 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.7822, loss_val: nan, pos_over_neg: 255.18838500976562 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.7884, loss_val: nan, pos_over_neg: 389.0178527832031 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.7492, loss_val: nan, pos_over_neg: 394.1777648925781 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.7855, loss_val: nan, pos_over_neg: 326.4167785644531 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.739, loss_val: nan, pos_over_neg: 361.63067626953125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.7981, loss_val: nan, pos_over_neg: 430.7336730957031 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.76, loss_val: nan, pos_over_neg: 434.1353759765625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.7175, loss_val: nan, pos_over_neg: 323.3250732421875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.7922, loss_val: nan, pos_over_neg: 241.78390502929688 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.8155, loss_val: nan, pos_over_neg: 400.2135925292969 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.7689, loss_val: nan, pos_over_neg: 215.19508361816406 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.7716, loss_val: nan, pos_over_neg: 209.01113891601562 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.8081, loss_val: nan, pos_over_neg: 242.52923583984375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.7784, loss_val: nan, pos_over_neg: 350.6002502441406 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.7715, loss_val: nan, pos_over_neg: 299.2355041503906 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 492.0743713378906 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.7034, loss_val: nan, pos_over_neg: 394.1957092285156 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.7859, loss_val: nan, pos_over_neg: 235.3999481201172 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.7182, loss_val: nan, pos_over_neg: 411.1957702636719 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.7614, loss_val: nan, pos_over_neg: 302.5465393066406 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.7936, loss_val: nan, pos_over_neg: 317.5660705566406 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.7829, loss_val: nan, pos_over_neg: 276.0685119628906 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.7121, loss_val: nan, pos_over_neg: 328.48468017578125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.7749, loss_val: nan, pos_over_neg: 261.6740417480469 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.7348, loss_val: nan, pos_over_neg: 285.5565185546875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.7278, loss_val: nan, pos_over_neg: 589.0728149414062 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.7889, loss_val: nan, pos_over_neg: 238.52276611328125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.7979, loss_val: nan, pos_over_neg: 292.71368408203125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 285.84625244140625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.7666, loss_val: nan, pos_over_neg: 238.2789764404297 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.7523, loss_val: nan, pos_over_neg: 343.1401062011719 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.7668, loss_val: nan, pos_over_neg: 414.3035583496094 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.7474, loss_val: nan, pos_over_neg: 285.6683044433594 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.7656, loss_val: nan, pos_over_neg: 243.57736206054688 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.754, loss_val: nan, pos_over_neg: 409.56646728515625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.7562, loss_val: nan, pos_over_neg: 483.17059326171875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.7282, loss_val: nan, pos_over_neg: 235.63320922851562 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.7599, loss_val: nan, pos_over_neg: 308.96868896484375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.7607, loss_val: nan, pos_over_neg: 212.5819549560547 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.7891, loss_val: nan, pos_over_neg: 285.1258544921875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.7226, loss_val: nan, pos_over_neg: 329.5724182128906 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.7527, loss_val: nan, pos_over_neg: 376.1911926269531 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 274.2929992675781 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.7016, loss_val: nan, pos_over_neg: 287.3404235839844 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.8201, loss_val: nan, pos_over_neg: 218.39683532714844 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.7741, loss_val: nan, pos_over_neg: 170.61056518554688 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.7377, loss_val: nan, pos_over_neg: 247.76580810546875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.7443, loss_val: nan, pos_over_neg: 217.41668701171875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.7998, loss_val: nan, pos_over_neg: 279.8092041015625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 305.7989501953125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.8042, loss_val: nan, pos_over_neg: 266.96881103515625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.767, loss_val: nan, pos_over_neg: 277.09136962890625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.7381, loss_val: nan, pos_over_neg: 433.83770751953125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.7316, loss_val: nan, pos_over_neg: 405.2833251953125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.7578, loss_val: nan, pos_over_neg: 271.6911926269531 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 349.6580505371094 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.7328, loss_val: nan, pos_over_neg: 270.5780944824219 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.7706, loss_val: nan, pos_over_neg: 287.54766845703125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.7279, loss_val: nan, pos_over_neg: 284.98504638671875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.7792, loss_val: nan, pos_over_neg: 181.9259490966797 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.7385, loss_val: nan, pos_over_neg: 237.83876037597656 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.7521, loss_val: nan, pos_over_neg: 317.841796875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.7371, loss_val: nan, pos_over_neg: 449.26422119140625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.7543, loss_val: nan, pos_over_neg: 360.1706848144531 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.729, loss_val: nan, pos_over_neg: 432.7265930175781 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.7498, loss_val: nan, pos_over_neg: 341.3494567871094 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.7583, loss_val: nan, pos_over_neg: 219.4364776611328 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.7374, loss_val: nan, pos_over_neg: 324.2054443359375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.7416, loss_val: nan, pos_over_neg: 310.2062683105469 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.7214, loss_val: nan, pos_over_neg: 320.4078674316406 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.7585, loss_val: nan, pos_over_neg: 286.965576171875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.7213, loss_val: nan, pos_over_neg: 501.3798522949219 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.7413, loss_val: nan, pos_over_neg: 531.5780639648438 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.7369, loss_val: nan, pos_over_neg: 271.7023620605469 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.7913, loss_val: nan, pos_over_neg: 270.26605224609375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.748, loss_val: nan, pos_over_neg: 342.8481140136719 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 356.2622985839844 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.7282, loss_val: nan, pos_over_neg: 370.9801330566406 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.7692, loss_val: nan, pos_over_neg: 377.42022705078125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.7858, loss_val: nan, pos_over_neg: 381.5550231933594 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.7862, loss_val: nan, pos_over_neg: 283.9819030761719 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.7714, loss_val: nan, pos_over_neg: 296.4984130859375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.7322, loss_val: nan, pos_over_neg: 269.38128662109375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.7602, loss_val: nan, pos_over_neg: 241.15133666992188 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.7425, loss_val: nan, pos_over_neg: 303.6997985839844 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.7657, loss_val: nan, pos_over_neg: 275.8221435546875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.7362, loss_val: nan, pos_over_neg: 314.05078125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.7768, loss_val: nan, pos_over_neg: 351.0836181640625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.7523, loss_val: nan, pos_over_neg: 508.0894470214844 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.7793, loss_val: nan, pos_over_neg: 259.7814636230469 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 328.8482971191406 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.793, loss_val: nan, pos_over_neg: 320.79486083984375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.7551, loss_val: nan, pos_over_neg: 309.6120300292969 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.7757, loss_val: nan, pos_over_neg: 220.55813598632812 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.789, loss_val: nan, pos_over_neg: 188.89993286132812 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.7378, loss_val: nan, pos_over_neg: 268.61077880859375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.7118, loss_val: nan, pos_over_neg: 409.8376159667969 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.7454, loss_val: nan, pos_over_neg: 315.3074645996094 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.718, loss_val: nan, pos_over_neg: 272.6823425292969 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 258.58795166015625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.783, loss_val: nan, pos_over_neg: 280.0304870605469 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.7364, loss_val: nan, pos_over_neg: 384.9459533691406 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.726, loss_val: nan, pos_over_neg: 484.44561767578125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.7561, loss_val: nan, pos_over_neg: 205.66220092773438 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.7886, loss_val: nan, pos_over_neg: 227.3909454345703 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.7707, loss_val: nan, pos_over_neg: 247.62957763671875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.7805, loss_val: nan, pos_over_neg: 199.92367553710938 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.7404, loss_val: nan, pos_over_neg: 195.88450622558594 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.8066, loss_val: nan, pos_over_neg: 270.5015563964844 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.7454, loss_val: nan, pos_over_neg: 431.6625061035156 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.8217, loss_val: nan, pos_over_neg: 249.38978576660156 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 376.5284423828125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.7593, loss_val: nan, pos_over_neg: 278.58941650390625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.7736, loss_val: nan, pos_over_neg: 269.1993408203125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.7385, loss_val: nan, pos_over_neg: 310.31512451171875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.7699, loss_val: nan, pos_over_neg: 231.38990783691406 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 268.7850341796875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.7671, loss_val: nan, pos_over_neg: 301.36566162109375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.7521, loss_val: nan, pos_over_neg: 348.11907958984375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.7132, loss_val: nan, pos_over_neg: 689.0738525390625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.7457, loss_val: nan, pos_over_neg: 391.5179443359375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.7414, loss_val: nan, pos_over_neg: 323.46429443359375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.7442, loss_val: nan, pos_over_neg: 327.3860778808594 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.7943, loss_val: nan, pos_over_neg: 341.44873046875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.7499, loss_val: nan, pos_over_neg: 370.0207214355469 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.804, loss_val: nan, pos_over_neg: 218.1515350341797 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.7481, loss_val: nan, pos_over_neg: 307.72784423828125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.7455, loss_val: nan, pos_over_neg: 306.93756103515625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.7507, loss_val: nan, pos_over_neg: 319.6921081542969 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.7599, loss_val: nan, pos_over_neg: 397.3211975097656 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.7684, loss_val: nan, pos_over_neg: 412.2569580078125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.7784, loss_val: nan, pos_over_neg: 294.38507080078125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.7896, loss_val: nan, pos_over_neg: 322.5496826171875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.8127, loss_val: nan, pos_over_neg: 184.77378845214844 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.7551, loss_val: nan, pos_over_neg: 312.5472717285156 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.8045, loss_val: nan, pos_over_neg: 243.79965209960938 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.7521, loss_val: nan, pos_over_neg: 225.65762329101562 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.733, loss_val: nan, pos_over_neg: 207.88404846191406 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.7147, loss_val: nan, pos_over_neg: 337.33062744140625 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    # model, \n",
    "                                    model_parallel, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.2,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=0, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=0.2'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-3)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
