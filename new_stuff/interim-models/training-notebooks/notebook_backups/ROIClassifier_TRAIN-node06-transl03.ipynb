{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.3, 0.3), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5551, loss_val: nan, pos_over_neg: 1.0484246015548706 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.2133, loss_val: nan, pos_over_neg: 3.5688185691833496 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.8573, loss_val: nan, pos_over_neg: 3.6802279949188232 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7145, loss_val: nan, pos_over_neg: 24.160797119140625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.6131, loss_val: nan, pos_over_neg: 15.037834167480469 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.5319, loss_val: nan, pos_over_neg: 20.639150619506836 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.513, loss_val: nan, pos_over_neg: 27.28033447265625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.4586, loss_val: nan, pos_over_neg: 33.92512512207031 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.4105, loss_val: nan, pos_over_neg: 48.83837127685547 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.3717, loss_val: nan, pos_over_neg: 76.7446060180664 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.3631, loss_val: nan, pos_over_neg: 91.79615783691406 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.3339, loss_val: nan, pos_over_neg: 89.05767059326172 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.2968, loss_val: nan, pos_over_neg: 103.97710418701172 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.29, loss_val: nan, pos_over_neg: 199.87913513183594 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.2631, loss_val: nan, pos_over_neg: 156.28103637695312 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.2552, loss_val: nan, pos_over_neg: 126.50345611572266 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.2352, loss_val: nan, pos_over_neg: 136.68736267089844 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.2189, loss_val: nan, pos_over_neg: 211.88812255859375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.2132, loss_val: nan, pos_over_neg: 311.74163818359375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.2043, loss_val: nan, pos_over_neg: 293.1627502441406 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.1924, loss_val: nan, pos_over_neg: 256.240966796875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.19, loss_val: nan, pos_over_neg: 215.80979919433594 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.1759, loss_val: nan, pos_over_neg: 314.0376281738281 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.164, loss_val: nan, pos_over_neg: 531.3623657226562 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.16, loss_val: nan, pos_over_neg: 686.0769653320312 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.1671, loss_val: nan, pos_over_neg: 1006.37646484375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.1321, loss_val: nan, pos_over_neg: 657.8167724609375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.1362, loss_val: nan, pos_over_neg: 479.35870361328125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.1295, loss_val: nan, pos_over_neg: 843.9074096679688 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.1341, loss_val: nan, pos_over_neg: 397.84210205078125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.1127, loss_val: nan, pos_over_neg: 1278.60546875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.1093, loss_val: nan, pos_over_neg: 922.0474243164062 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.1155, loss_val: nan, pos_over_neg: 214.48858642578125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.1091, loss_val: nan, pos_over_neg: 720.5812377929688 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.1058, loss_val: nan, pos_over_neg: 1284.930419921875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.087, loss_val: nan, pos_over_neg: 25215.892578125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.0941, loss_val: nan, pos_over_neg: 470.86376953125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.0869, loss_val: nan, pos_over_neg: 348.8614196777344 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.0802, loss_val: nan, pos_over_neg: 1076.95703125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.0711, loss_val: nan, pos_over_neg: 1374.82958984375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.0826, loss_val: nan, pos_over_neg: 917.0482177734375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.0634, loss_val: nan, pos_over_neg: 319.1417541503906 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.0607, loss_val: nan, pos_over_neg: 281.4949951171875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.0641, loss_val: nan, pos_over_neg: 730.139404296875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.06, loss_val: nan, pos_over_neg: 362.1573181152344 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.0491, loss_val: nan, pos_over_neg: 486.8103332519531 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.055, loss_val: nan, pos_over_neg: 502.7732849121094 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.0542, loss_val: nan, pos_over_neg: 141.3431396484375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.0573, loss_val: nan, pos_over_neg: 255.43446350097656 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.0421, loss_val: nan, pos_over_neg: 308.99908447265625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.0417, loss_val: nan, pos_over_neg: 713.5274658203125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.0459, loss_val: nan, pos_over_neg: 347.8226318359375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.042, loss_val: nan, pos_over_neg: 186.28489685058594 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.0298, loss_val: nan, pos_over_neg: 1062.6937255859375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.032, loss_val: nan, pos_over_neg: 192.92562866210938 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.0294, loss_val: nan, pos_over_neg: 883.648193359375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.0383, loss_val: nan, pos_over_neg: 195.99420166015625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.0334, loss_val: nan, pos_over_neg: 237.0244140625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.0352, loss_val: nan, pos_over_neg: 465.97076416015625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.0217, loss_val: nan, pos_over_neg: 869.7849731445312 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.0219, loss_val: nan, pos_over_neg: 875.462890625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.0224, loss_val: nan, pos_over_neg: 204.64137268066406 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.0178, loss_val: nan, pos_over_neg: 239.2245635986328 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.0144, loss_val: nan, pos_over_neg: 419.61651611328125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.0139, loss_val: nan, pos_over_neg: 761.0743408203125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.0165, loss_val: nan, pos_over_neg: 287.16845703125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.0081, loss_val: nan, pos_over_neg: 403.940185546875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.0091, loss_val: nan, pos_over_neg: 208.2551727294922 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.0049, loss_val: nan, pos_over_neg: 265.2066650390625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.0067, loss_val: nan, pos_over_neg: 199.6231231689453 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.0011, loss_val: nan, pos_over_neg: 321.1342468261719 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9967, loss_val: nan, pos_over_neg: 330.8597717285156 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9962, loss_val: nan, pos_over_neg: 236.0337371826172 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.985, loss_val: nan, pos_over_neg: 378.8151550292969 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.0109, loss_val: nan, pos_over_neg: 170.06927490234375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9917, loss_val: nan, pos_over_neg: 368.39910888671875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 134.43115234375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9816, loss_val: nan, pos_over_neg: 409.7239685058594 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.98, loss_val: nan, pos_over_neg: 325.5208740234375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9927, loss_val: nan, pos_over_neg: 353.4720764160156 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9944, loss_val: nan, pos_over_neg: 714.676513671875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9898, loss_val: nan, pos_over_neg: 277.84417724609375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9844, loss_val: nan, pos_over_neg: 357.9383544921875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9747, loss_val: nan, pos_over_neg: 212.86880493164062 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.983, loss_val: nan, pos_over_neg: 419.1004638671875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9864, loss_val: nan, pos_over_neg: 412.052001953125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9868, loss_val: nan, pos_over_neg: 185.1998748779297 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9843, loss_val: nan, pos_over_neg: 599.388671875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 283.3501892089844 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9893, loss_val: nan, pos_over_neg: 832.8614501953125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 515.42626953125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9949, loss_val: nan, pos_over_neg: 314.9848937988281 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9778, loss_val: nan, pos_over_neg: 276.8692321777344 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9681, loss_val: nan, pos_over_neg: 489.1535949707031 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9769, loss_val: nan, pos_over_neg: 426.4736633300781 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 395.2182312011719 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9706, loss_val: nan, pos_over_neg: 370.9862060546875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 145.77320861816406 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9693, loss_val: nan, pos_over_neg: 311.02239990234375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9674, loss_val: nan, pos_over_neg: 439.08807373046875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9708, loss_val: nan, pos_over_neg: 535.8477172851562 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 290.90106201171875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 266.89385986328125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.968, loss_val: nan, pos_over_neg: 264.7698669433594 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.959, loss_val: nan, pos_over_neg: 293.7979431152344 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9698, loss_val: nan, pos_over_neg: 439.014404296875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 323.7151794433594 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.963, loss_val: nan, pos_over_neg: 256.7886047363281 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9652, loss_val: nan, pos_over_neg: 251.1029815673828 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9653, loss_val: nan, pos_over_neg: 465.0743408203125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9795, loss_val: nan, pos_over_neg: 182.83712768554688 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9693, loss_val: nan, pos_over_neg: 274.05853271484375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9596, loss_val: nan, pos_over_neg: 251.81024169921875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9564, loss_val: nan, pos_over_neg: 186.87142944335938 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 265.9097595214844 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9736, loss_val: nan, pos_over_neg: 132.5755615234375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9696, loss_val: nan, pos_over_neg: 387.4074401855469 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 287.4881286621094 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9559, loss_val: nan, pos_over_neg: 111.06171417236328 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9514, loss_val: nan, pos_over_neg: 444.0745849609375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9624, loss_val: nan, pos_over_neg: 106.82479858398438 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 225.0027618408203 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 173.739990234375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9631, loss_val: nan, pos_over_neg: 161.54586791992188 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 231.8362579345703 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 218.70079040527344 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9489, loss_val: nan, pos_over_neg: 452.9496154785156 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 339.4901428222656 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 189.15548706054688 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 318.8594665527344 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 543.5140380859375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 360.1456298828125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 313.8999938964844 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 417.7984313964844 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 253.2891387939453 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 299.55841064453125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 570.3905029296875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 368.11895751953125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 302.660400390625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 364.2186279296875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 871.0384521484375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 335.74700927734375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 413.3565673828125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9321, loss_val: nan, pos_over_neg: 262.6603698730469 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 1120.5662841796875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 411.43572998046875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 1054.138427734375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 383.0539855957031 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 444.1671447753906 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 194.6719207763672 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 232.90167236328125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9352, loss_val: nan, pos_over_neg: 620.6047973632812 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 279.1086730957031 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 467.0552062988281 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 338.0992431640625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 665.5132446289062 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 305.3984375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 363.4196472167969 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 464.7879333496094 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 396.5682678222656 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 444.9058532714844 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 399.05267333984375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 515.9754028320312 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 578.3596801757812 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 427.7854309082031 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 294.3413391113281 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 360.7792663574219 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 281.3043212890625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 651.6622924804688 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 475.4942626953125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 342.2286071777344 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 732.0651245117188 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 509.8232421875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 575.4622802734375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 575.725341796875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 559.5785522460938 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 565.9910888671875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 1740.57861328125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1024.1123046875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1217.411376953125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 381.5607604980469 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 1165.9095458984375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 902.7723999023438 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 322.0563049316406 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 829.7872314453125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 522.2071533203125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 1030.1732177734375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 707.5752563476562 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 770.2044677734375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 305.65130615234375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 1693.5362548828125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1043.6539306640625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 369.5112609863281 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 404.097900390625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 173.56520080566406 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 233.56460571289062 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 425.2577819824219 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 187.57928466796875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 277.5278625488281 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 273.2303161621094 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 220.82997131347656 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 450.1003723144531 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 303.4918212890625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 257.4824523925781 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 648.5780639648438 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 393.0571594238281 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1468.784912109375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 1106.45654296875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 367.5523681640625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 392.31951904296875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 632.2403564453125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 417.75689697265625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1106.1260986328125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 681.27880859375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 361.9812316894531 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 1470.94580078125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 783.3517456054688 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 297.58172607421875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 608.1224365234375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 348.7759704589844 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 264.2998046875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 232.40406799316406 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 508.4930114746094 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 290.40753173828125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 411.1463623046875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 480.3012390136719 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 317.68878173828125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 331.9319763183594 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 502.72283935546875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 840.4960327148438 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 315.18170166015625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.899, loss_val: nan, pos_over_neg: 691.91943359375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 494.7287292480469 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 349.177734375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 452.50689697265625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 308.3869934082031 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 393.5145568847656 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 650.7327270507812 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 472.9207763671875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 255.55078125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8922, loss_val: nan, pos_over_neg: 702.1055297851562 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 1125.8634033203125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 307.1457824707031 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 345.2807312011719 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 491.2335510253906 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 343.9172058105469 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 527.01123046875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 435.64556884765625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 302.3922424316406 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8959, loss_val: nan, pos_over_neg: 558.2952880859375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 396.0985412597656 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 417.3798828125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: 603.5252075195312 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 601.8494262695312 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 241.48533630371094 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 660.560546875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 277.9588317871094 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 311.36968994140625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.889, loss_val: nan, pos_over_neg: 461.5524597167969 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 522.5353393554688 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 581.6539306640625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 337.9176940917969 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8911, loss_val: nan, pos_over_neg: 612.635498046875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8847, loss_val: nan, pos_over_neg: 364.0614013671875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 396.1856994628906 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 483.7234191894531 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 455.72637939453125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 789.6532592773438 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 472.74005126953125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8908, loss_val: nan, pos_over_neg: 556.004150390625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8981, loss_val: nan, pos_over_neg: 841.6337890625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 301.8907470703125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8885, loss_val: nan, pos_over_neg: 528.4862060546875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8956, loss_val: nan, pos_over_neg: 770.5388793945312 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8911, loss_val: nan, pos_over_neg: 349.3511047363281 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 538.8287963867188 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 413.10186767578125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 335.6295471191406 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8887, loss_val: nan, pos_over_neg: 396.372314453125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 1386.9208984375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8797, loss_val: nan, pos_over_neg: 365.83331298828125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 267.6379699707031 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8865, loss_val: nan, pos_over_neg: 1063.089111328125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 568.6033325195312 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 587.11572265625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8884, loss_val: nan, pos_over_neg: 727.2957153320312 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 332.6654968261719 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8938, loss_val: nan, pos_over_neg: 292.6165771484375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8926, loss_val: nan, pos_over_neg: 629.9464111328125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8955, loss_val: nan, pos_over_neg: 694.8427124023438 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 461.3630676269531 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8923, loss_val: nan, pos_over_neg: 380.486328125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8952, loss_val: nan, pos_over_neg: 444.85052490234375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8905, loss_val: nan, pos_over_neg: 339.3785095214844 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8872, loss_val: nan, pos_over_neg: 332.17303466796875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 625.77685546875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 434.339111328125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8861, loss_val: nan, pos_over_neg: 322.40069580078125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 775.3779296875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8922, loss_val: nan, pos_over_neg: 544.1438598632812 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8882, loss_val: nan, pos_over_neg: 1363.984375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8823, loss_val: nan, pos_over_neg: 650.97998046875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: 963.3048095703125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 274.800048828125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8746, loss_val: nan, pos_over_neg: 375.4685974121094 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 310.81951904296875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 524.989990234375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8913, loss_val: nan, pos_over_neg: 475.59210205078125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8828, loss_val: nan, pos_over_neg: 213.15899658203125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 469.481201171875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8864, loss_val: nan, pos_over_neg: 453.35198974609375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 386.13800048828125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8895, loss_val: nan, pos_over_neg: 246.0838623046875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 202.37957763671875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8927, loss_val: nan, pos_over_neg: 300.53143310546875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8875, loss_val: nan, pos_over_neg: 368.98211669921875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 323.8780822753906 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 340.8855285644531 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.877, loss_val: nan, pos_over_neg: 663.3784790039062 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 553.7265014648438 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 343.71466064453125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8797, loss_val: nan, pos_over_neg: 289.0843505859375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8922, loss_val: nan, pos_over_neg: 207.05029296875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 217.9549102783203 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8876, loss_val: nan, pos_over_neg: 298.3443908691406 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 433.28173828125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8891, loss_val: nan, pos_over_neg: 336.44000244140625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 289.3844909667969 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 330.1253356933594 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8863, loss_val: nan, pos_over_neg: 457.3512268066406 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8806, loss_val: nan, pos_over_neg: 328.1048889160156 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 317.97943115234375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8862, loss_val: nan, pos_over_neg: 393.5028076171875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8879, loss_val: nan, pos_over_neg: 299.8899841308594 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 301.2206115722656 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8798, loss_val: nan, pos_over_neg: 425.96282958984375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 532.7059936523438 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8761, loss_val: nan, pos_over_neg: 382.4964599609375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 264.0892333984375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 458.4925537109375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 422.1281433105469 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8766, loss_val: nan, pos_over_neg: 354.2361145019531 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 493.3793640136719 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8852, loss_val: nan, pos_over_neg: 571.3372802734375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 839.855712890625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 320.345947265625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 363.40716552734375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8822, loss_val: nan, pos_over_neg: 702.688720703125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 388.0700378417969 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8805, loss_val: nan, pos_over_neg: 448.1993103027344 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8771, loss_val: nan, pos_over_neg: 296.0994873046875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8746, loss_val: nan, pos_over_neg: 1967.35107421875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 353.520751953125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8736, loss_val: nan, pos_over_neg: 439.3712463378906 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8662, loss_val: nan, pos_over_neg: 2848.82763671875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.876, loss_val: nan, pos_over_neg: 544.1487426757812 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8853, loss_val: nan, pos_over_neg: 503.0411376953125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 535.9666137695312 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 357.59124755859375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8784, loss_val: nan, pos_over_neg: 313.0480651855469 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.873, loss_val: nan, pos_over_neg: 706.711669921875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 517.43505859375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8817, loss_val: nan, pos_over_neg: 443.14813232421875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 299.27069091796875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8862, loss_val: nan, pos_over_neg: 539.3604736328125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 562.2870483398438 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8727, loss_val: nan, pos_over_neg: 446.7269287109375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8769, loss_val: nan, pos_over_neg: 808.8350830078125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 618.8728637695312 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8764, loss_val: nan, pos_over_neg: 251.45547485351562 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8743, loss_val: nan, pos_over_neg: 497.83636474609375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8738, loss_val: nan, pos_over_neg: 507.628662109375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.88, loss_val: nan, pos_over_neg: 351.1382751464844 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8825, loss_val: nan, pos_over_neg: 440.1034240722656 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 516.4749145507812 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8753, loss_val: nan, pos_over_neg: 845.1182861328125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 467.5652160644531 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8835, loss_val: nan, pos_over_neg: 324.6579895019531 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8761, loss_val: nan, pos_over_neg: 446.601806640625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 356.7489318847656 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8789, loss_val: nan, pos_over_neg: 358.5160217285156 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.87, loss_val: nan, pos_over_neg: 497.8017883300781 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8765, loss_val: nan, pos_over_neg: 616.2220458984375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 754.0886840820312 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 547.619384765625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 532.0661010742188 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 422.82135009765625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 532.0000610351562 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8892, loss_val: nan, pos_over_neg: 623.23974609375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 553.1820678710938 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 1201.889404296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 664.79833984375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8748, loss_val: nan, pos_over_neg: 405.1017150878906 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 625.3635864257812 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8707, loss_val: nan, pos_over_neg: 868.32080078125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8682, loss_val: nan, pos_over_neg: 565.3446655273438 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 482.82342529296875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8766, loss_val: nan, pos_over_neg: 676.4281616210938 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 696.4393920898438 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 912.488525390625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8775, loss_val: nan, pos_over_neg: 518.924072265625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8735, loss_val: nan, pos_over_neg: 933.9928588867188 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8738, loss_val: nan, pos_over_neg: 1091.55615234375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 416.4371643066406 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 506.92413330078125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 900.556884765625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 629.1758422851562 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8643, loss_val: nan, pos_over_neg: 650.6676025390625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 459.0780029296875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 291.482177734375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 747.1861572265625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8707, loss_val: nan, pos_over_neg: 421.1700744628906 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 437.88079833984375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.87, loss_val: nan, pos_over_neg: 578.9620971679688 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8727, loss_val: nan, pos_over_neg: 744.1974487304688 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 494.7281188964844 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8763, loss_val: nan, pos_over_neg: 397.76531982421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 510.1626281738281 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8762, loss_val: nan, pos_over_neg: 544.8101196289062 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8732, loss_val: nan, pos_over_neg: 626.6380004882812 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 437.7463073730469 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8765, loss_val: nan, pos_over_neg: 440.70697021484375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 412.2371520996094 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 467.7713317871094 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 426.7345886230469 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 477.5335998535156 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 576.8505249023438 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 684.1231079101562 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8719, loss_val: nan, pos_over_neg: 414.4951477050781 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8722, loss_val: nan, pos_over_neg: 474.5161437988281 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8673, loss_val: nan, pos_over_neg: 637.7274780273438 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8762, loss_val: nan, pos_over_neg: 471.2346496582031 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 507.3986511230469 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 758.5181274414062 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 492.7226867675781 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8711, loss_val: nan, pos_over_neg: 623.5428466796875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 768.2344360351562 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8712, loss_val: nan, pos_over_neg: 285.2895812988281 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8764, loss_val: nan, pos_over_neg: 345.62493896484375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 817.4082641601562 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8711, loss_val: nan, pos_over_neg: 915.3255004882812 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 466.5708312988281 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 427.455078125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 596.7472534179688 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8703, loss_val: nan, pos_over_neg: 622.540771484375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8753, loss_val: nan, pos_over_neg: 583.6386108398438 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 575.95263671875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8682, loss_val: nan, pos_over_neg: 713.6893310546875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 1311.5386962890625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8767, loss_val: nan, pos_over_neg: 458.1587829589844 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 953.7171020507812 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8671, loss_val: nan, pos_over_neg: 719.2344970703125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8687, loss_val: nan, pos_over_neg: 884.7061157226562 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 839.0118408203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 664.6285400390625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 668.4000244140625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8721, loss_val: nan, pos_over_neg: 340.04681396484375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 657.8621826171875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 840.8452758789062 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8676, loss_val: nan, pos_over_neg: 521.6482543945312 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 877.1388549804688 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 426.4909362792969 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 480.3025207519531 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 898.4779052734375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 611.0123291015625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 425.5572814941406 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 1249.8292236328125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 530.6109008789062 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 413.4266357421875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 871.8887939453125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 1418.5155029296875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 389.0332946777344 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 538.031005859375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1126.335205078125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 740.8662109375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 297.4981994628906 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 1111.188232421875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 4629.6708984375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 955.0863037109375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 1070.5025634765625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 779.9210205078125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 479.5078430175781 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 537.8533935546875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8713, loss_val: nan, pos_over_neg: 416.1227111816406 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 444.13958740234375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 518.0416259765625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 513.5087280273438 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 396.61279296875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 331.8096008300781 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 688.5282592773438 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 298.6951599121094 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 444.78619384765625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 720.1930541992188 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8638, loss_val: nan, pos_over_neg: 480.3679504394531 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8661, loss_val: nan, pos_over_neg: 738.5467529296875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 2021.2623291015625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 410.87457275390625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.86, loss_val: nan, pos_over_neg: 613.975341796875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 533.193115234375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 574.4544067382812 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8651, loss_val: nan, pos_over_neg: 471.48663330078125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 503.7762145996094 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8645, loss_val: nan, pos_over_neg: 651.8018188476562 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8684, loss_val: nan, pos_over_neg: 795.9449462890625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8607, loss_val: nan, pos_over_neg: 732.5794677734375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 552.6580810546875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 513.482177734375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 540.6351318359375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8625, loss_val: nan, pos_over_neg: 444.5289001464844 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 550.7041015625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 1175.9951171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 510.03668212890625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 705.8540649414062 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 643.2169189453125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 899.063720703125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 1055.4466552734375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8593, loss_val: nan, pos_over_neg: 627.4776000976562 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 427.2381591796875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8638, loss_val: nan, pos_over_neg: 278.4245300292969 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: 625.6013793945312 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 959.1026000976562 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 377.2818908691406 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 437.0392761230469 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 260.2128601074219 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 380.61944580078125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 422.4402770996094 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 485.3880310058594 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 336.9026184082031 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 495.7828063964844 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 751.8897705078125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 890.405517578125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 545.4136962890625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 225.8444061279297 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 4013.30322265625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 432.27484130859375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 377.0912780761719 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 660.7100219726562 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8567, loss_val: nan, pos_over_neg: 1277.2496337890625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 606.6495971679688 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8554, loss_val: nan, pos_over_neg: 414.7242431640625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 1026.2720947265625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 743.2495727539062 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 723.6976318359375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 539.5763549804688 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 2151.388427734375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 402.7799377441406 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 758.9163208007812 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 888.203369140625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 952.324951171875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 683.642578125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 649.6334228515625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1308.322021484375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 386.30462646484375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 650.633056640625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 1115.0501708984375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 490.867431640625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 707.1468505859375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8531, loss_val: nan, pos_over_neg: 1532.69873046875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 645.4531860351562 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 538.687255859375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 803.1094970703125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 690.1348876953125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 480.7414245605469 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 1011.4766845703125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8519, loss_val: nan, pos_over_neg: 1111.76318359375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 538.1582641601562 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 388.23974609375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1834.3292236328125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8603, loss_val: nan, pos_over_neg: 577.4267578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 662.0263671875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 2975.730712890625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 787.0348510742188 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 626.1690673828125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 616.939453125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8554, loss_val: nan, pos_over_neg: 868.6231079101562 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 642.2243041992188 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8554, loss_val: nan, pos_over_neg: 1488.78369140625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8645, loss_val: nan, pos_over_neg: 754.6063232421875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 768.9638671875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 407.00115966796875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 554.9110107421875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 709.0029296875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 916.4273071289062 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 488.8905029296875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 709.4796752929688 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 518.3614501953125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 391.3740539550781 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 970.4256591796875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 1700.42919921875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 545.2025756835938 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 2128.071044921875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8587, loss_val: nan, pos_over_neg: 624.1524658203125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 1052.9346923828125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1179.28662109375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 2510.94921875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 720.484619140625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 750.9036254882812 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 715.1024780273438 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8603, loss_val: nan, pos_over_neg: 463.54840087890625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 646.8151245117188 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 855.4255981445312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 1084.5797119140625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 499.6789245605469 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 415.4781799316406 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 352.3022155761719 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 545.002685546875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 310.6672668457031 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 444.09637451171875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 745.410888671875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8519, loss_val: nan, pos_over_neg: 393.1436767578125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 294.67742919921875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 343.7723388671875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 599.5501708984375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 599.3425903320312 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 405.26312255859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 589.7003173828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 581.9846801757812 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 320.0821228027344 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 605.755859375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 403.249267578125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 545.1461181640625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8531, loss_val: nan, pos_over_neg: 578.135009765625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 534.797607421875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 629.6273803710938 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 376.2219543457031 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 276.8302307128906 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 410.10357666015625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 834.0023803710938 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 395.8108825683594 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 860.8150634765625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 915.4583129882812 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 1890.025634765625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 694.32568359375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8561, loss_val: nan, pos_over_neg: 353.8968811035156 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 911.6181640625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 788.1106567382812 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 747.0640869140625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 532.1005249023438 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 439.25738525390625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 675.7280883789062 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 708.0049438476562 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 759.9979248046875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8528, loss_val: nan, pos_over_neg: 674.3948364257812 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1097.075927734375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 1571.1468505859375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 642.6433715820312 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 366.3608093261719 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8566, loss_val: nan, pos_over_neg: 616.0031127929688 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8488, loss_val: nan, pos_over_neg: 628.3382568359375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 1309.201904296875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1977.665771484375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 603.6143798828125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 678.8840942382812 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 636.0906372070312 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 577.1791381835938 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 585.692138671875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 464.3693542480469 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 537.9146118164062 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1609.5504150390625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1262.90283203125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 738.4000854492188 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 591.0119018554688 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 452.9417724609375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 788.9435424804688 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 468.71075439453125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 319.8955383300781 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 689.2572021484375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 580.7805786132812 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 713.6158447265625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 347.7974548339844 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 689.3754272460938 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 684.6784057617188 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1349.9532470703125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 811.4673461914062 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 501.2963562011719 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 775.5240478515625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1024.977294921875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 668.4639282226562 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 701.985107421875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1256.7230224609375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 3454.458740234375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1718.1759033203125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 555.2750244140625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8488, loss_val: nan, pos_over_neg: 4403.337890625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 639.2219848632812 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 1093.9896240234375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 598.8655395507812 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1086.20947265625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 1546.962890625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 770.7450561523438 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 525.056884765625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 572.21728515625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 781.3575439453125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 415.57379150390625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 447.0589599609375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [21:53<109456:15:19, 1313.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 735.8838500976562 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.856, loss_val: nan, pos_over_neg: 539.3020629882812 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 374.144287109375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8554, loss_val: nan, pos_over_neg: 338.7156677246094 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 599.3985595703125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 491.35650634765625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 713.80029296875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 1023.593994140625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 799.9834594726562 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 444.1842956542969 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1214.87890625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 1194.98291015625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 419.05511474609375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 536.0817260742188 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 668.493408203125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 935.6473999023438 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 539.4008178710938 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 960.3829956054688 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 379.497802734375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 560.9600219726562 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 742.2561645507812 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 709.60400390625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 793.8062133789062 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 756.7492065429688 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 547.9346923828125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 1970.189697265625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 810.9564819335938 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 568.4033813476562 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 595.059326171875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 736.2788696289062 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 657.4774169921875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 2309.753662109375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 711.5072631835938 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 559.4524536132812 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 961.2824096679688 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 847.9429931640625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 376.0220947265625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 755.6331787109375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 960.88427734375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 589.353759765625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 955.9180908203125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 843.0704345703125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1184.0189208984375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 539.1101684570312 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 375.45794677734375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 488.8898010253906 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 434.9318542480469 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 1066.5396728515625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 828.5157470703125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 582.5995483398438 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 465.2792663574219 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 557.6884155273438 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 362.35986328125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 567.4564208984375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 785.8796997070312 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 383.90130615234375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 698.6376953125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 4074.515380859375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 626.1917724609375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 756.5900268554688 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 1047.0921630859375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 568.3240966796875 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 2306.34814453125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 1004.0906982421875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 943.16748046875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 878.4163818359375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 722.0079956054688 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1653.4139404296875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 735.092529296875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 831.6316528320312 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 2315.289794921875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 1606.8170166015625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 2003.7103271484375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 1093.4493408203125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1748.711181640625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 471.0963439941406 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 342.31573486328125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 693.8154907226562 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1316.9652099609375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 876.4839477539062 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 1014.3818969726562 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 1316.4700927734375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 685.9656982421875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 847.0726318359375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 827.2915649414062 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 728.525390625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 726.6472778320312 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 773.5776977539062 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 585.0089111328125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 332.80657958984375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 405.8006591796875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 880.9674072265625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 861.5584716796875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1102.19091796875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 926.3285522460938 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 632.5238037109375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 864.2915649414062 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 667.1458129882812 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 510.6596374511719 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1141.1683349609375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 368.01739501953125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 871.8711547851562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 707.3048706054688 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 383.88262939453125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1090.90771484375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 545.5478515625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 573.7115478515625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 1114.3387451171875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1005.060302734375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 578.6209716796875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 1146.0316162109375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1005.398681640625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 758.9697875976562 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 655.4459228515625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 1142.5228271484375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 586.4906005859375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 1036.625244140625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 530.7467651367188 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 624.2445068359375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 566.8709716796875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8296, loss_val: nan, pos_over_neg: 1146.7760009765625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 836.7564086914062 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 553.18896484375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 958.8759155273438 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 1988.7269287109375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 556.9171142578125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 561.953857421875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 1836.418701171875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 704.1080322265625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 609.6097412109375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 739.2272338867188 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 1022.3446655273438 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 619.1575317382812 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 586.1465454101562 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 719.7025146484375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 623.8262939453125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 729.1797485351562 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 398.5912780761719 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 895.7964477539062 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 1020.4140625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 825.4483642578125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 419.96087646484375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 521.466064453125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 505.0236511230469 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 309.111083984375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 798.3217163085938 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 820.7150268554688 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 324.0302429199219 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1735.9508056640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 439.5881042480469 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 373.183837890625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 427.6236267089844 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 852.9617309570312 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 1023.13232421875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 869.958251953125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 1939.3045654296875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 632.9884643554688 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 998.8133544921875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1593.72216796875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 2095.304443359375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 701.940185546875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 652.6289672851562 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 648.8899536132812 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 4133.09326171875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 638.3872680664062 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 3552.601806640625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 1923.8505859375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 702.4412841796875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1128.1810302734375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 1208.146728515625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 472.7362365722656 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1372.6156005859375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1659.7315673828125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 883.6947021484375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 3690.8740234375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8334, loss_val: nan, pos_over_neg: 3122.888427734375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 865.9884033203125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 752.8065185546875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1469.1728515625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 823.7604370117188 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1216.0777587890625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 1368.0782470703125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1115.6336669921875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 1758.7855224609375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 700.8057250976562 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1705.0802001953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 1906.0299072265625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 658.7782592773438 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1928.9459228515625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 873.1485595703125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 1168.7166748046875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 1464.4273681640625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 440.3594055175781 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 1531.5306396484375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 745.3529052734375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 424.8951110839844 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 2580.44384765625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 590.5914916992188 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 567.0116577148438 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 553.0926513671875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 1363.4464111328125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 454.3749084472656 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 500.2943420410156 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 2767.108642578125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 910.5491333007812 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 729.5055541992188 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1293.146728515625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 853.0169677734375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 1121.3331298828125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 866.991455078125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 629.9894409179688 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 833.5521850585938 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 862.9813842773438 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 590.8529663085938 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 722.0298461914062 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 1155.017333984375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1227.0831298828125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 741.466552734375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 374.81640625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 1224.8402099609375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 823.1429443359375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 585.6395263671875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 829.5475463867188 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 2047.6751708984375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1357.531005859375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 541.86962890625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 758.2202758789062 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 1167.5765380859375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 1038.6512451171875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 367.21710205078125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 672.5833129882812 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1354.1102294921875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 820.2487182617188 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 1482.8485107421875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 689.620849609375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 575.374755859375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 3791.694580078125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 635.3575439453125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 1572.0777587890625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 499.5149230957031 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 961.6845092773438 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1554.3822021484375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 530.1624145507812 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 460.4881286621094 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 507.8850402832031 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 694.9454956054688 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 1086.7861328125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 845.3795166015625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 973.2344970703125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 752.4856567382812 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1065.802490234375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8256, loss_val: nan, pos_over_neg: 1147.08251953125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 674.9683227539062 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 1031.415283203125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1145.7781982421875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.824, loss_val: nan, pos_over_neg: 1371.34716796875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 915.4588623046875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1043.9212646484375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 1117.453369140625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 797.0000610351562 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 1815.71142578125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 1382.716064453125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 798.1685180664062 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 684.5491333007812 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 1275.3499755859375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 837.9889526367188 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 916.0978393554688 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 781.7476806640625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 4427.46923828125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 761.4953002929688 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 412.0032653808594 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 620.3987426757812 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 488.41998291015625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 724.8762817382812 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 1342.66162109375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 1015.5736694335938 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 2314.331298828125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 1269.2606201171875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 1219.0064697265625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 482.7469787597656 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 403.77899169921875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 458.01336669921875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 552.9652099609375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 2280.904541015625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.827, loss_val: nan, pos_over_neg: 609.12255859375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 402.3609313964844 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 618.480712890625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 533.41650390625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 626.63134765625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8206, loss_val: nan, pos_over_neg: 528.4796752929688 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 622.9479370117188 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 541.2891845703125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 820.0455322265625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 721.0333251953125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 588.796875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 544.7211303710938 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 833.452880859375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 532.4398193359375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 852.7383422851562 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 678.0560913085938 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 395.8444519042969 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 363.7026062011719 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 483.4271545410156 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 457.98602294921875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 392.2026062011719 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 741.8116455078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 594.5984497070312 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 809.9546508789062 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 690.771484375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 419.96148681640625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 468.63153076171875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 473.13336181640625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 648.95947265625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 1477.2822265625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 997.16357421875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 427.92047119140625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 859.1617431640625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 656.7826538085938 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 729.8228149414062 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 1659.609130859375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 519.423583984375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 335.20367431640625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 942.9640502929688 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 2514.549072265625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 784.4979248046875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1153.8935546875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 737.9176635742188 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 635.9695434570312 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 652.1944580078125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 518.817138671875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 666.1868896484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 653.6775512695312 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8252, loss_val: nan, pos_over_neg: 600.4680786132812 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 564.2531127929688 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 880.8336791992188 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 994.1187744140625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 875.4586181640625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 992.0266723632812 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 1142.5191650390625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 1162.76611328125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 521.0949096679688 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 533.5493774414062 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 624.1002197265625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 752.023193359375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 630.537353515625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 1232.71728515625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 744.2540283203125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 996.23291015625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 483.5665588378906 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 675.7061157226562 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 669.8749389648438 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 588.5173950195312 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 581.6004028320312 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 389.5562438964844 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 820.3821411132812 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 1187.6356201171875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 322.7423400878906 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 544.5199584960938 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 461.9681701660156 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 505.5257873535156 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 449.9226379394531 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 898.539794921875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8307, loss_val: nan, pos_over_neg: 415.7070007324219 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 646.3855590820312 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 535.7826538085938 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 454.2756042480469 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 564.3367309570312 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 410.8138732910156 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 953.6060180664062 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 1108.621337890625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 3090.65478515625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 402.3147277832031 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 509.7325744628906 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 640.7617797851562 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8233, loss_val: nan, pos_over_neg: 1703.2030029296875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 835.7334594726562 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 591.2636108398438 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 951.1547241210938 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1312.2889404296875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 532.5952758789062 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 292.3970642089844 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 554.5222778320312 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 963.8571166992188 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 489.306884765625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.829, loss_val: nan, pos_over_neg: 755.8967895507812 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 1514.1866455078125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 525.6788330078125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 827.1690063476562 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 1007.9837036132812 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8334, loss_val: nan, pos_over_neg: 1227.657958984375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1184.13037109375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.827, loss_val: nan, pos_over_neg: 1256.2908935546875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 714.7097778320312 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 746.9735107421875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 516.8033447265625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 996.0203857421875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 3015.72021484375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 482.28375244140625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 964.8539428710938 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1200.5396728515625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1388.8720703125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 480.8212890625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 618.8334350585938 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 807.4137573242188 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 1168.4764404296875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 1511.672607421875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 750.653564453125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 866.4933471679688 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 713.7390747070312 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 627.1483154296875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 880.4890747070312 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 550.2879638671875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 404.0060729980469 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 536.1649780273438 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 599.3959350585938 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 825.6041259765625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 493.8533630371094 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 812.0504760742188 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 830.5180053710938 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1000.666748046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 666.0108642578125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 542.3024291992188 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 386.9091796875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 1096.453857421875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 1061.2760009765625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 583.88623046875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 1149.831298828125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1322.47509765625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1715.2781982421875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 807.017333984375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8223, loss_val: nan, pos_over_neg: 1054.1243896484375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 340.6123962402344 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8191, loss_val: nan, pos_over_neg: 739.7691040039062 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 1079.13037109375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 511.306396484375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 392.45367431640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 950.6506958007812 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8232, loss_val: nan, pos_over_neg: 810.2706298828125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 892.9066162109375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 747.5547485351562 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 912.8367309570312 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 740.1292114257812 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 611.391357421875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 850.3570556640625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 1137.2623291015625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 816.61865234375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 406.0463562011719 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8222, loss_val: nan, pos_over_neg: 718.7655029296875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 906.8411254882812 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 580.2538452148438 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8222, loss_val: nan, pos_over_neg: 409.76910400390625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 690.4309692382812 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 655.0487060546875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.823, loss_val: nan, pos_over_neg: 980.32666015625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 760.4379272460938 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 700.42041015625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 851.2681274414062 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 907.2162475585938 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 339.106201171875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8199, loss_val: nan, pos_over_neg: 388.7300109863281 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 388.6324157714844 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 661.3865356445312 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8222, loss_val: nan, pos_over_neg: 680.731201171875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 511.9212341308594 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 670.406494140625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 553.8038940429688 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 548.06005859375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.823, loss_val: nan, pos_over_neg: 748.5616455078125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 925.9430541992188 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.824, loss_val: nan, pos_over_neg: 431.1288146972656 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 664.6826782226562 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 839.3848876953125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 1047.07568359375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8233, loss_val: nan, pos_over_neg: 406.02960205078125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8212, loss_val: nan, pos_over_neg: 451.4162292480469 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 682.0133666992188 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 2852.427734375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 478.2091064453125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 993.1691284179688 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 733.0787963867188 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 1005.095458984375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 727.50537109375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 465.8154296875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1326.990966796875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 1110.1903076171875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 1173.2606201171875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 1001.3715209960938 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 1015.383544921875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 756.3823852539062 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 741.3196411132812 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8211, loss_val: nan, pos_over_neg: 654.40087890625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 571.7369384765625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 588.5037841796875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8188, loss_val: nan, pos_over_neg: 711.3507690429688 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1388.1656494140625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 833.3433227539062 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 603.0953979492188 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 1404.5006103515625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 2280.281494140625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 884.424560546875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 995.8612060546875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 1516.7462158203125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 1052.7938232421875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 430.241455078125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 905.5882568359375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.826, loss_val: nan, pos_over_neg: 2023.393798828125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 1041.3568115234375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 547.2474975585938 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8209, loss_val: nan, pos_over_neg: 2137.544189453125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8279, loss_val: nan, pos_over_neg: 720.31201171875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 1430.3773193359375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 1072.9271240234375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 743.1192626953125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 635.3270874023438 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 548.0198974609375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 1241.66845703125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 5693.45361328125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1637.2099609375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 996.4407348632812 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 1149.31298828125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1095.0379638671875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 675.8551025390625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 745.179931640625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8224, loss_val: nan, pos_over_neg: 793.53564453125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 993.0579833984375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 800.1992797851562 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 1247.823486328125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 412.736572265625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 690.4893188476562 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 698.7774658203125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 1021.561767578125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 626.6277465820312 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 1473.451904296875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 1145.078125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 1625.8505859375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8211, loss_val: nan, pos_over_neg: 1120.0992431640625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8256, loss_val: nan, pos_over_neg: 1259.0238037109375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8145, loss_val: nan, pos_over_neg: 607.60693359375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8217, loss_val: nan, pos_over_neg: 1244.476806640625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.814, loss_val: nan, pos_over_neg: 1262.5496826171875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 864.799560546875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 612.883056640625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 1508.90283203125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1159.299560546875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.819, loss_val: nan, pos_over_neg: 1405.9700927734375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 1572.0799560546875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8188, loss_val: nan, pos_over_neg: 926.8639526367188 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8199, loss_val: nan, pos_over_neg: 1343.0125732421875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 679.3287353515625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1141.07421875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8224, loss_val: nan, pos_over_neg: 1369.32275390625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.823, loss_val: nan, pos_over_neg: 849.1468505859375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 2637.806640625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 16626.439453125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 1274.6866455078125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8274, loss_val: nan, pos_over_neg: 1237.215087890625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1387.6148681640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 1440.5623779296875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 1730.9041748046875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 501.38153076171875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8205, loss_val: nan, pos_over_neg: 683.3966064453125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1300.1666259765625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 1180.4599609375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 1255.9036865234375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 1608.59814453125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 627.7022094726562 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 595.6490478515625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.815, loss_val: nan, pos_over_neg: 486.053955078125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 990.0757446289062 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 679.032470703125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 522.4088134765625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 726.5518188476562 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 612.07861328125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 382.1439514160156 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 333.65570068359375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 560.6350708007812 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 2046.3973388671875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 851.2824096679688 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 661.628662109375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 623.8578491210938 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 751.474365234375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 775.8043823242188 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 610.3887939453125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 528.3375244140625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.827, loss_val: nan, pos_over_neg: 338.379150390625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 558.5872802734375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8276, loss_val: nan, pos_over_neg: 905.9341430664062 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 556.8289184570312 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 685.7572021484375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 975.1888427734375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 464.1654968261719 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 614.4258422851562 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 1856.75732421875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8223, loss_val: nan, pos_over_neg: 485.632568359375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 652.9890747070312 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 2610.13720703125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 529.2869873046875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8224, loss_val: nan, pos_over_neg: 691.2847290039062 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.824, loss_val: nan, pos_over_neg: 606.349609375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 2097.58935546875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 1755.7572021484375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 909.8660278320312 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.824, loss_val: nan, pos_over_neg: 823.3795776367188 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 982.6937255859375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 1192.6666259765625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 889.8322143554688 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1052.4339599609375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 653.6011962890625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8159, loss_val: nan, pos_over_neg: 551.7127075195312 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1351.8526611328125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 982.2164306640625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 542.0691528320312 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 959.4057006835938 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 795.8329467773438 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 694.1204223632812 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 1566.8153076171875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 1984.87890625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 1634.4390869140625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1230.680908203125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 759.6921997070312 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 604.2171630859375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 1018.003173828125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 623.321044921875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 897.6112670898438 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 617.733642578125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 755.771240234375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 1253.2486572265625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 1463.45556640625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8181, loss_val: nan, pos_over_neg: 610.2841186523438 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 685.6611938476562 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 1255.772705078125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 769.845947265625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 2016.13818359375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 978.59814453125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 339.6835632324219 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 604.8245239257812 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 872.052490234375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 941.5106201171875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 595.1862182617188 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 590.7221069335938 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1514.4873046875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8198, loss_val: nan, pos_over_neg: 620.4676513671875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 698.8312377929688 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 628.8290405273438 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 728.1582641601562 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8209, loss_val: nan, pos_over_neg: 1682.9619140625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 1100.0950927734375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 1719.2523193359375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 1927.5435791015625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 611.8788452148438 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 966.4771728515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 2322.08642578125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 964.5213623046875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 760.0887451171875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8181, loss_val: nan, pos_over_neg: 640.8098754882812 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1159.454345703125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 417.2110900878906 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 328.5785827636719 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 564.6764526367188 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 970.516357421875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 303.61370849609375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 723.7322387695312 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 764.33056640625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 984.2896728515625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 491.8321228027344 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 462.5333251953125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 510.4317626953125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 824.7308959960938 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 1274.892578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8188, loss_val: nan, pos_over_neg: 623.344970703125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 735.6444702148438 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 577.0947265625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 1195.6995849609375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 735.5936279296875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1248.046630859375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 844.5302124023438 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8212, loss_val: nan, pos_over_neg: 905.4119262695312 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 2491.93212890625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 1479.264404296875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 1278.7659912109375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 1758.704345703125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1851.9119873046875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 1312.8248291015625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 903.83837890625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8095, loss_val: nan, pos_over_neg: 915.3524169921875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 793.5430297851562 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 960.2680053710938 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 598.755859375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 659.1558837890625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1544.9481201171875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 845.8018798828125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 945.983154296875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1450.649658203125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 822.7178955078125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 767.1154174804688 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [43:35<108896:20:51, 1306.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 840.0328369140625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8104, loss_val: nan, pos_over_neg: 1851.130615234375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 946.815185546875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1453.4508056640625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 1283.681640625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 945.3308715820312 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8153, loss_val: nan, pos_over_neg: 1126.2083740234375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8126, loss_val: nan, pos_over_neg: 1736.9150390625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 929.3849487304688 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 521.6591796875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 769.8367309570312 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1873.0826416015625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 1397.6302490234375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 924.5736083984375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 1091.7916259765625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 1329.8155517578125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 826.8931274414062 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 347.5818786621094 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 542.7757568359375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 836.3201904296875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 941.551025390625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 430.0881652832031 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 1016.1535034179688 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8118, loss_val: nan, pos_over_neg: -7080.1904296875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8221, loss_val: nan, pos_over_neg: 1200.2781982421875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 957.4395751953125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 976.5534057617188 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 1029.8973388671875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 1345.509521484375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 817.2870483398438 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8137, loss_val: nan, pos_over_neg: 715.2125244140625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 803.724853515625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 1708.1234130859375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1719.755126953125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 958.5328369140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 666.20068359375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 810.21044921875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 1497.319091796875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 910.2638549804688 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 460.5090637207031 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.814, loss_val: nan, pos_over_neg: 631.0946044921875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 585.5983276367188 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 617.7091064453125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 628.3768920898438 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1362.77783203125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 2025.4451904296875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 1369.0289306640625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 666.3209228515625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8212, loss_val: nan, pos_over_neg: 1116.879638671875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 1167.30078125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 664.2264404296875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 493.87255859375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 759.2735595703125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 1179.9844970703125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 482.4732360839844 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8069, loss_val: nan, pos_over_neg: 909.8079223632812 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 964.3743896484375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 820.4310913085938 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 591.0331420898438 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.815, loss_val: nan, pos_over_neg: 631.187744140625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 1415.2969970703125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 1081.7232666015625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8194, loss_val: nan, pos_over_neg: 381.65130615234375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 659.9984130859375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1801.7154541015625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 826.9812622070312 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 739.98095703125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 844.8226928710938 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 822.1829223632812 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 620.4656982421875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 513.2680053710938 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 626.6068725585938 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 981.41259765625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1196.1484375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 551.2123413085938 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 480.8427734375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 681.4012451171875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8137, loss_val: nan, pos_over_neg: 497.01080322265625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 667.335205078125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1041.4129638671875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 467.63153076171875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 631.6254272460938 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1699.9488525390625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 1901.05712890625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 2779.017333984375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8206, loss_val: nan, pos_over_neg: 499.0561828613281 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 1072.7032470703125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 940.1716918945312 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8157, loss_val: nan, pos_over_neg: 1536.8765869140625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 636.43212890625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 534.635498046875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 804.7213134765625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1072.260009765625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 1394.5772705078125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 387.37469482421875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 565.646484375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.815, loss_val: nan, pos_over_neg: 512.1292114257812 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8188, loss_val: nan, pos_over_neg: 573.112060546875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 658.7681884765625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 495.4329528808594 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 636.2489624023438 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 616.2868041992188 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 2072.01416015625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 615.9381713867188 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 584.8605346679688 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 1592.52685546875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 1403.1409912109375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 537.4733276367188 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 1028.00048828125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 1245.231689453125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 588.906494140625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 817.0841064453125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 473.66015625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 1582.1151123046875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 907.7557373046875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8159, loss_val: nan, pos_over_neg: 484.7861022949219 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 839.629638671875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 1142.0006103515625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 1026.749267578125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 891.7152099609375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 967.1814575195312 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 4384.99755859375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 595.0736083984375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 1139.6507568359375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8153, loss_val: nan, pos_over_neg: 1076.945556640625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 911.21484375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 1645.4326171875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8248, loss_val: nan, pos_over_neg: 1010.6504516601562 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 789.229248046875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8129, loss_val: nan, pos_over_neg: 893.1405029296875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 1334.9168701171875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8224, loss_val: nan, pos_over_neg: 1317.593994140625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1215.316162109375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 1991.760498046875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 2135.70458984375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 544.3431396484375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 1947.054443359375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1059.773681640625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 750.0233764648438 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 672.3333129882812 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.814, loss_val: nan, pos_over_neg: 792.1138916015625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 1087.7020263671875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1542.6202392578125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1247.8876953125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 707.3021240234375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 505.606201171875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 812.7978515625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1097.2987060546875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 755.270751953125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1099.83154296875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 2317.49365234375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 809.3907470703125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 785.6021118164062 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 2509.978271484375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 946.3729248046875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 694.2122802734375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 632.4400634765625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 2090.284423828125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 2058.737060546875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 3769.516357421875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 590.8358154296875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1470.3656005859375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 965.781494140625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 2796.793212890625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1170.439697265625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8126, loss_val: nan, pos_over_neg: 1198.2193603515625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1333.7911376953125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 945.5957641601562 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 930.94140625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 776.0074462890625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1192.9326171875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8209, loss_val: nan, pos_over_neg: 676.6429443359375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 669.3710327148438 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 738.55078125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 1442.622314453125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 970.289794921875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8171, loss_val: nan, pos_over_neg: 1053.724365234375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8118, loss_val: nan, pos_over_neg: 972.1830444335938 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 2168.62841796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 742.930419921875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 488.1380310058594 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 949.3396606445312 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 790.1891479492188 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 1759.03515625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 1537.8724365234375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 836.5643920898438 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 1594.9991455078125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 985.3470458984375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8181, loss_val: nan, pos_over_neg: 1506.9368896484375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 694.5128784179688 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 645.4790649414062 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 855.9744262695312 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 635.0068969726562 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 768.7131958007812 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 782.52001953125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 973.3289184570312 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1113.004638671875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8137, loss_val: nan, pos_over_neg: 967.7288208007812 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 934.8977661132812 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 505.41802978515625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 387.41107177734375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 561.0439453125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 630.4318237304688 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1096.21533203125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8129, loss_val: nan, pos_over_neg: 881.2362670898438 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 613.4302368164062 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8191, loss_val: nan, pos_over_neg: 665.8531494140625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 4360.20263671875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8145, loss_val: nan, pos_over_neg: 853.8920288085938 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 831.0306396484375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 1253.7938232421875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 1282.631591796875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 1349.885498046875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 516.8084716796875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 662.587890625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8118, loss_val: nan, pos_over_neg: 1230.247314453125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8159, loss_val: nan, pos_over_neg: 1139.3204345703125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 3252.78662109375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8137, loss_val: nan, pos_over_neg: 5933.453125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1387.6083984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 722.0673828125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 840.6100463867188 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 541.7846069335938 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1785.8482666015625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 682.68212890625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 656.841552734375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 808.34765625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 1628.5692138671875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1988.7860107421875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 1531.5194091796875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 630.4183349609375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 621.1244506835938 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 1301.1363525390625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 538.933349609375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 802.70556640625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 1448.705322265625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8145, loss_val: nan, pos_over_neg: 973.0709228515625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 727.7041015625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 817.4497680664062 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 1233.42724609375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 763.2543334960938 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 1023.848388671875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1642.7398681640625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 1227.1470947265625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 800.5631103515625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 753.8583984375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 787.9330444335938 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 844.4761352539062 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 450.5773620605469 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8112, loss_val: nan, pos_over_neg: 612.0731201171875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1429.645751953125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 1698.7576904296875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 6617.58203125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 782.231689453125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1249.3408203125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 816.9315795898438 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 963.180419921875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 2755.68408203125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 1461.6953125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 851.2960815429688 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 681.1965942382812 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 1328.0166015625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 1296.572509765625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 2354.817138671875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1198.8194580078125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 4391.40185546875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 2270.3154296875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1745.07373046875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8157, loss_val: nan, pos_over_neg: 891.6824951171875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 724.8126831054688 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 985.5157470703125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 2117.054931640625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1395.7528076171875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 719.7784423828125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 654.2849731445312 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8231, loss_val: nan, pos_over_neg: 527.2840576171875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 2291.090576171875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 1807.3682861328125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 527.0283203125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 853.19775390625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1664.781982421875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1833.4093017578125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 742.333251953125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 678.5286254882812 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1469.0911865234375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 906.8068237304688 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 1042.9669189453125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 817.5919189453125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8145, loss_val: nan, pos_over_neg: 780.0570678710938 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 1665.820068359375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 2614.353271484375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 481.6351013183594 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 1961.950439453125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 1956.203125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 2352.286376953125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1393.2899169921875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 940.8101196289062 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1030.45556640625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 972.427978515625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2380.602294921875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 632.7758178710938 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 503.3031311035156 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 1378.999267578125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 733.7877807617188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 312.73565673828125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 539.468505859375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 1885.05859375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 573.459716796875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 758.2114868164062 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 900.3516235351562 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 1527.7557373046875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 976.687255859375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 1361.258544921875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1409.074951171875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 1056.2098388671875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 573.0548095703125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 907.8424682617188 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 849.4375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 898.9259643554688 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1592.0184326171875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 602.6063842773438 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1748.4256591796875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 486.7821960449219 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 2086.577880859375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 654.801513671875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 1358.83984375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1152.17529296875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 866.9754638671875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8224, loss_val: nan, pos_over_neg: 1013.3084716796875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 774.0802612304688 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.819, loss_val: nan, pos_over_neg: 736.9522705078125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 325.9066162109375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8157, loss_val: nan, pos_over_neg: 1052.30615234375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 958.353271484375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8139, loss_val: nan, pos_over_neg: 1292.76806640625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1208.541748046875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 673.332275390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 1054.927978515625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 2064.5234375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 2745.442138671875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 915.3753051757812 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 487.8439025878906 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 584.3193359375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 409.9290466308594 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 801.5880737304688 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 548.0286865234375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 681.180419921875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1017.4075317382812 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1180.9246826171875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 707.5505981445312 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 774.8712158203125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1309.009033203125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1011.4368286132812 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8074, loss_val: nan, pos_over_neg: 1020.0396118164062 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.819, loss_val: nan, pos_over_neg: 1332.4207763671875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 390.7821350097656 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1132.1873779296875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1163.1005859375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 1120.2381591796875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 635.9119262695312 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 742.2487182617188 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 787.131591796875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 623.48876953125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 1398.589599609375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 670.2734985351562 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8223, loss_val: nan, pos_over_neg: 434.13348388671875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 1042.173095703125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 485.4329528808594 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1113.3328857421875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 715.5870361328125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 1401.064208984375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 440.9894714355469 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 526.8695678710938 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 646.6749267578125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 2137.264892578125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 491.04278564453125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 438.6600341796875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 539.853759765625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 959.1782836914062 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 411.67498779296875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 524.948486328125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 386.93011474609375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 614.6754760742188 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 1388.898193359375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8112, loss_val: nan, pos_over_neg: 571.1426391601562 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 604.6431274414062 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 469.1051330566406 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 2226.6279296875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1467.6029052734375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 491.4854431152344 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 655.7872314453125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 835.5824584960938 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 604.080810546875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1366.3309326171875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8104, loss_val: nan, pos_over_neg: 668.370361328125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 878.2836303710938 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 705.6235961914062 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 2947.978515625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 4440.5029296875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 666.7273559570312 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1839.8297119140625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1737.31787109375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1001.8001098632812 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 360.548095703125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 552.3517456054688 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 504.98223876953125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 1450.6722412109375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 831.2516479492188 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1125.284423828125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1557.7119140625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 679.7149658203125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1272.8719482421875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1819.49169921875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 1392.691650390625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 775.4127197265625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 1524.9595947265625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 578.6861572265625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 982.150146484375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 941.5633544921875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 1113.481201171875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1105.8006591796875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 764.1583862304688 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 858.0387573242188 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 947.7512817382812 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 908.3350830078125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1927.3900146484375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 835.1245727539062 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 870.3170776367188 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 611.2266845703125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8145, loss_val: nan, pos_over_neg: 801.16064453125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 1963.206298828125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 1012.9674682617188 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1052.0198974609375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1372.32421875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 2631.95068359375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 3251.1708984375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 1404.6470947265625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 5472.26953125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 2587.3017578125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1803.576171875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 16748.30859375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 2268.88916015625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 1171.060302734375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 965.4505004882812 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 635.5877075195312 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1546.1690673828125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 871.8956298828125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 2015.0191650390625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 840.3570556640625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 2304.054931640625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1778.5023193359375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 807.2244873046875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 584.019775390625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 1502.4798583984375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1280.8209228515625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 676.3416748046875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 857.683837890625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 646.4835205078125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 607.0997924804688 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 460.5923767089844 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1331.4852294921875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1083.3509521484375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 729.0612182617188 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 695.9125366210938 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1569.246337890625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 2440.531494140625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1292.385498046875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 649.8003540039062 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 920.7081909179688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1417.4141845703125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 842.5838012695312 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1292.0428466796875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 682.4788208007812 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 825.9360961914062 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1013.2322387695312 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 3322.912353515625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1935.133544921875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 590.2965087890625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 13335.3603515625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 3599.916015625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 681.19873046875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 728.8628540039062 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 2705.21142578125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1605.613525390625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 1204.09521484375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8069, loss_val: nan, pos_over_neg: 2126.947509765625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: 1323.4940185546875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 1685.83740234375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 1943.2064208984375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1950.947021484375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8118, loss_val: nan, pos_over_neg: 856.42431640625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 1143.81640625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 1092.794189453125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 1623.935302734375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 494.3871765136719 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 778.0266723632812 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 1547.1263427734375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1260.7822265625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 1673.2440185546875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 916.9600830078125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 913.408935546875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1134.0938720703125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 8847.2548828125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 616.004638671875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 543.3988037109375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1154.46826171875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 1209.96533203125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 508.7784423828125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 658.1074829101562 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 1274.12158203125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: -6137.55859375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1168.466796875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 986.4822998046875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 3158.31005859375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 698.8204345703125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 1205.10009765625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 1182.359375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 582.21142578125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 568.2288818359375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 697.9451904296875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1313.441162109375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 921.0747680664062 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 645.6529541015625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 463.8211364746094 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 707.6507568359375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 677.9158935546875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 402.8150634765625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 556.0999145507812 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 789.9478149414062 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 491.97027587890625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 557.5791625976562 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 653.5260620117188 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 1201.2647705078125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 1274.05126953125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 496.275634765625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 478.3486022949219 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 927.0455932617188 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 995.1071166992188 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 884.0859985351562 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 919.4065551757812 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 1721.5147705078125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 1219.4244384765625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1201.0457763671875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1683.9337158203125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 1057.375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 846.0484008789062 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 574.0533447265625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1163.5772705078125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1161.0877685546875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1859.784912109375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8104, loss_val: nan, pos_over_neg: 3355.292236328125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1700.75244140625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1817.347900390625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 2824.23681640625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 2448.849365234375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 1541.0792236328125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 722.0978393554688 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1195.2484130859375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1064.608642578125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 3638.405517578125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 1015.2088623046875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 651.5350952148438 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1058.062744140625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 2133.494873046875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 2093.21435546875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 796.1036987304688 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1261.2728271484375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1822.6280517578125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1551.00341796875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 1975.6160888671875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 2102.88623046875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 630.3505859375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1241.79443359375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 847.0764770507812 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 1475.89501953125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1285.2447509765625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 666.720947265625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1418.8289794921875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 1478.7611083984375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 861.4689331054688 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 1127.4007568359375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 963.611572265625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 954.6412963867188 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 1206.591796875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1077.3389892578125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 677.630859375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1042.391357421875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 804.8121337890625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 7948.31787109375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1003.32470703125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8205, loss_val: nan, pos_over_neg: 1985.10009765625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 1043.3721923828125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 1323.0196533203125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1445.6707763671875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 1041.8665771484375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 965.3502197265625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 810.8387451171875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 2627.739501953125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1758.113037109375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 822.4653930664062 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 894.5262451171875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 1193.9715576171875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 648.0659790039062 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 735.5238647460938 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1470.6710205078125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 1720.135009765625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2158.165771484375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 5293.3828125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1071.439453125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 1635.320068359375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1572.807861328125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1271.4913330078125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 878.4506225585938 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1625.7769775390625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1225.540771484375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 1017.167236328125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2348.22998046875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 2802.724853515625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1979.216064453125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 2706.655029296875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 1655.162841796875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 1306.8804931640625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 749.0558471679688 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1442.8236083984375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1929.7772216796875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 3087.803466796875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1361.4056396484375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 861.7765502929688 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1494.7496337890625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1478.1998291015625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1228.2451171875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 599.9105834960938 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1084.5413818359375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 2585.2568359375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 600.8696899414062 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 1037.746826171875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1385.9840087890625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 1659.41455078125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 744.7095336914062 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 977.2130126953125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 901.4886474609375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 694.7477416992188 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1363.848388671875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 501.2359619140625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 932.5005493164062 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 862.2417602539062 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1296.3582763671875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1034.0986328125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 815.2283325195312 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1118.1806640625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1734.140625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 567.7673950195312 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 910.0947875976562 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 3415.42626953125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 1654.9654541015625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 2760.934326171875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 949.0150146484375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 605.9356689453125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 522.6254272460938 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 922.200439453125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1215.435791015625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 9446.6923828125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 824.05322265625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 624.8770751953125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 714.7884521484375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1221.4940185546875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 599.8478393554688 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 811.3832397460938 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 677.8424072265625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 1900.2410888671875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1050.636962890625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 712.6572265625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 524.6228637695312 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 789.7670288085938 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1034.732177734375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 2614.761474609375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 1221.9200439453125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 475.8097839355469 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1045.206787109375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1136.88134765625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1060.1209716796875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 824.2235717773438 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 753.2491455078125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 616.8902587890625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 900.6046142578125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1636.5220947265625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 802.0545654296875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 794.2728881835938 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 635.464111328125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1371.6229248046875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 485.2123107910156 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 844.9244995117188 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1047.840087890625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 708.0873413085938 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 1157.833740234375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:05:47<109838:33:49, 1318.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 894.0908203125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 821.8094482421875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 582.472900390625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 809.307861328125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1254.7540283203125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 853.0436401367188 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 729.8728637695312 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1194.024169921875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 2430.735107421875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 1094.984375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 1994.951416015625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 547.005615234375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1849.343017578125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 3305.535400390625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 4023.4716796875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 2310.972412109375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 895.1378784179688 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 832.0963134765625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8112, loss_val: nan, pos_over_neg: 778.7249145507812 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 767.5697021484375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1098.2154541015625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1576.714599609375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 2628.576416015625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 5225.96533203125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1474.4078369140625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1097.228759765625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 1846.93359375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 5232.44091796875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 2751.36767578125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1210.4112548828125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 697.4314575195312 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 935.5930786132812 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 3967.46923828125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 2819.715087890625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 449.81292724609375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8095, loss_val: nan, pos_over_neg: 545.9756469726562 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1193.1243896484375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 825.947021484375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1130.802490234375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 914.6625366210938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 863.2257080078125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 717.1134643554688 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 4816.6669921875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 771.8302001953125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 880.3755493164062 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 898.4290161132812 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 752.2894287109375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1208.9757080078125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 521.0445556640625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 584.1329956054688 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 2307.7724609375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 942.8948974609375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 680.2185668945312 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 4776.765625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 3152.01171875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 503.5144348144531 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 1486.6861572265625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 428.74951171875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1843.329833984375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1656.9381103515625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 1473.0821533203125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1134.943115234375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1283.9439697265625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1536.916259765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 2469.642333984375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1094.830810546875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 900.18212890625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 478.14434814453125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1394.2105712890625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 853.9583740234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 652.44091796875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 507.7746276855469 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 890.0485229492188 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1481.725341796875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 815.806640625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1124.436279296875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 1273.54443359375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1548.107421875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 1067.6624755859375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 665.8058471679688 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 777.459716796875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1129.66552734375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 707.0780639648438 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1189.470947265625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 1189.021240234375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8153, loss_val: nan, pos_over_neg: 511.90130615234375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 790.0535888671875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 829.7310791015625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1008.1939697265625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 911.03271484375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 797.0999755859375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 581.3541870117188 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 636.9752197265625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1165.2703857421875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 875.8690185546875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 2546.3974609375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 696.74658203125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 1005.5677490234375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 1008.763916015625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 974.92578125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 929.8278198242188 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 978.6489868164062 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1945.2528076171875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 2064.124755859375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 894.0078125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1977.769287109375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1373.7840576171875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 927.1535034179688 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 572.0489501953125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1521.7969970703125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 1254.1072998046875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1270.0107421875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 497.6504211425781 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 983.4983520507812 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1135.8001708984375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1269.295654296875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1440.5987548828125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1630.7967529296875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 565.6057739257812 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 713.9909057617188 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 2267.128173828125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1255.86962890625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1788.448974609375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1251.6666259765625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1086.322265625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1877.232421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 5497.00048828125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1201.47998046875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 927.719970703125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 777.988037109375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 1122.6480712890625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 2284.487060546875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 1821.1170654296875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1439.9085693359375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1465.4036865234375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 1327.646484375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1747.9422607421875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 863.501708984375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1037.7275390625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 1622.0391845703125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1572.68505859375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1186.658935546875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 4142.03662109375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1400.9981689453125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 1194.0616455078125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1126.345703125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 2119.39111328125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 3497.472412109375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 518.3018798828125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8104, loss_val: nan, pos_over_neg: 843.2987060546875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 738.9947509765625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1091.69091796875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 2161.15478515625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1003.911865234375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1036.9952392578125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 888.798583984375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1143.9288330078125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 941.46435546875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 641.3917846679688 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1844.655517578125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 819.3219604492188 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 582.6453857421875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 862.2131958007812 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 579.8265991210938 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 575.1785278320312 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 726.352783203125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 888.2811279296875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 633.18408203125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 866.3928833007812 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 893.2494506835938 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 1035.5872802734375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 716.4474487304688 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 749.804931640625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1079.189208984375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1089.2801513671875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 2210.68310546875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 524.873046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1224.4481201171875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1151.7847900390625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1434.8741455078125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 440.8564758300781 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 894.535888671875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 686.0187377929688 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 554.4798583984375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 617.5881958007812 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1277.0672607421875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 688.8775024414062 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 497.5779113769531 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 892.5850830078125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1303.0040283203125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1632.703369140625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1425.8233642578125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 676.5538330078125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 840.6604614257812 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1063.7236328125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 1341.7255859375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 1103.2310791015625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1029.905029296875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 2516.22900390625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 2943.496337890625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 2108.31787109375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 2076.222412109375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 2023.4197998046875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 1175.45947265625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1137.599609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 1794.2406005859375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2520.262451171875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1885.1993408203125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1446.7379150390625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 768.873046875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1033.86669921875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 2184.315673828125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2921.871826171875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 780.35986328125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 5687.28564453125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 962.8800659179688 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 842.4920043945312 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1340.4833984375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1389.4498291015625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 897.7612915039062 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 713.330322265625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1311.567138671875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 981.0010375976562 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1264.544921875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 884.9596557617188 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1848.0482177734375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1259.0596923828125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 4060.413818359375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 1598.2742919921875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 3981.599609375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1744.161376953125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1827.4022216796875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1574.79833984375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 496.05780029296875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 563.8812866210938 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 1597.1077880859375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 578.6006469726562 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 666.9036254882812 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 2969.393798828125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 4952.82568359375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1196.6788330078125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 1053.1402587890625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 927.2606811523438 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 698.5830688476562 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 963.6516723632812 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1396.1480712890625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1532.281982421875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 3422.60693359375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1404.018798828125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 1463.3927001953125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 695.0158081054688 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 797.70263671875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1183.5330810546875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 635.93115234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 591.9598388671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 560.1073608398438 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1010.56787109375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 915.5245971679688 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1165.5823974609375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 583.2166748046875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 638.1408081054688 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2608.650634765625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1146.0443115234375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 780.9188232421875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 1009.564208984375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 497.1656188964844 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1024.3023681640625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1350.789794921875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 565.9790649414062 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 736.0497436523438 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 640.6669921875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 696.26953125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 942.0933227539062 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 751.1751708984375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 1019.9515380859375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 516.04150390625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 679.7314453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 669.0779418945312 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 715.8839721679688 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1397.5166015625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 8516.5615234375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1214.029296875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 791.1910400390625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1037.9512939453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 963.85546875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 2870.869873046875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 689.8599853515625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1035.876220703125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 974.8710327148438 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1423.654052734375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 1562.0352783203125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 571.8294677734375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 861.5327758789062 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 867.6892700195312 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 710.1411743164062 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 583.1986083984375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1143.94091796875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1170.7603759765625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 630.867919921875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 965.3798828125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1884.0877685546875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1652.7506103515625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 993.0367431640625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 804.5829467773438 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 460.33355712890625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 897.60205078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 864.4453735351562 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 586.7342529296875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1374.095703125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 1099.1353759765625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 921.88720703125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 677.707275390625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2122.374267578125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 2317.092041015625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1150.0518798828125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 652.5780029296875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1639.5726318359375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 687.0247802734375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 680.8040161132812 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 905.6116943359375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 664.7568969726562 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 2827.45458984375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1356.123046875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 1286.54150390625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 817.5569458007812 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 896.4353637695312 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1363.6605224609375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 2499.39208984375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1379.980712890625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1030.88427734375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 759.7565307617188 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 407.572021484375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1511.4400634765625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1304.12060546875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 1697.7791748046875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 754.2299194335938 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 626.1489868164062 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 796.5247802734375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1513.2705078125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1184.28955078125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 689.604736328125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 938.419189453125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 667.5108032226562 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 949.7499389648438 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 679.7542114257812 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 930.3511352539062 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1307.925537109375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 774.6640014648438 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1112.30517578125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1381.301513671875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1257.3515625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1682.657958984375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 630.9739990234375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1269.7708740234375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 564.6124267578125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 724.4678955078125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1130.652587890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1203.3587646484375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1795.506591796875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1918.3907470703125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 5228.609375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1455.906494140625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 2738.688720703125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 4026.938720703125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1088.3172607421875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 980.8729858398438 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1919.8568115234375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 838.7800903320312 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1474.9010009765625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1464.6170654296875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 1413.3658447265625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1974.3218994140625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 4336.2626953125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 2632.68603515625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 2254.581787109375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1193.375732421875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 880.2822875976562 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 640.1387329101562 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 623.875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 749.6607055664062 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1234.074462890625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 2351.964599609375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 623.9141235351562 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1009.5386962890625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 1458.930419921875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 2933.658935546875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1372.31640625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 642.6591186523438 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 766.313720703125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 1229.0665283203125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1251.49169921875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 501.6407165527344 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 776.2196655273438 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 387.6761779785156 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 901.1390380859375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 970.7026977539062 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 775.4124145507812 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 534.7854614257812 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 1290.8179931640625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1478.373779296875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1034.4454345703125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 2263.302978515625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 3785.982177734375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 2652.209228515625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 2035.5860595703125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 832.5822143554688 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 951.8870849609375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 925.8881225585938 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 997.9501342773438 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1061.6912841796875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 923.0294799804688 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 976.0833129882812 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 853.9247436523438 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 741.1679077148438 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 1949.4063720703125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1380.049072265625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1742.113525390625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 19867.11328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 1272.195556640625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 3485.944091796875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1023.181884765625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1797.4520263671875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1118.6082763671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1271.1243896484375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1163.88720703125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1291.1104736328125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1510.0235595703125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1028.586669921875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 3076.655517578125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1045.630859375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1425.939208984375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 706.8386840820312 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 861.5498046875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 7411.53759765625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1234.857666015625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1531.634033203125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 767.0765380859375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 879.7506713867188 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 9945.4267578125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1677.9605712890625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1750.426513671875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1747.6700439453125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 827.727783203125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 703.7373657226562 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 816.0924072265625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 4788.9794921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 729.1220703125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 876.18994140625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 466.4876708984375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 2618.72607421875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 2798.096923828125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1132.906982421875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 956.5546264648438 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1460.737548828125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 5532.33837890625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 992.6857299804688 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1031.1795654296875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 3158.3603515625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1509.2825927734375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1566.43603515625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 1054.3284912109375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 2000.525634765625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1008.4484252929688 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 783.1841430664062 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1004.534423828125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1328.450927734375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1081.3524169921875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1465.406005859375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1960.453369140625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 2664.585205078125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1579.8319091796875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1356.0086669921875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1338.322021484375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 2561.336669921875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1339.93603515625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 970.9733276367188 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 862.0650634765625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 729.797607421875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 720.5386352539062 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1182.1976318359375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1233.826171875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 635.7966918945312 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 957.3015747070312 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 666.8903198242188 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 945.2591552734375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 2028.3668212890625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1504.119384765625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 571.642822265625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 829.5745849609375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 759.9501953125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1136.593994140625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 924.8900756835938 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 760.3455810546875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 897.5672607421875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 1655.35302734375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 2039.78271484375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 2000.2401123046875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2426.454345703125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1517.1624755859375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 2976.78955078125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1260.955810546875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1232.4522705078125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 629.379150390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1929.6138916015625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1165.9346923828125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 802.2843017578125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 353.8306884765625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1286.424560546875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 3390.8515625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1310.4857177734375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 805.888916015625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 2426.350341796875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 913.6168823242188 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1493.1497802734375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1365.095458984375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 645.79345703125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 2334.590576171875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 819.3123168945312 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1775.6031494140625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 572.1376953125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1019.9910888671875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1846.909912109375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8095, loss_val: nan, pos_over_neg: 675.8578491210938 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 902.0291137695312 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 936.6378173828125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1787.044189453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1074.34814453125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1431.1138916015625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 610.2377319335938 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 12650.791015625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1501.608154296875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 599.647705078125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 765.3319702148438 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 2047.5604248046875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 858.2874755859375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 646.4605712890625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1235.2855224609375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 888.7083129882812 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 854.6207275390625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 678.0294189453125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1403.3726806640625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1397.87060546875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 891.1853637695312 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 634.94189453125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1698.5872802734375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1008.2252197265625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 866.7141723632812 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1122.1744384765625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 4571.22314453125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 2860.866943359375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 1554.1591796875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1614.7568359375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 599.7195434570312 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 11311.2265625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1533.715087890625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 695.6275634765625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 563.3839111328125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2007.431396484375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 806.5073852539062 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 2157.58984375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 2383.54638671875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1468.204833984375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 3026.961181640625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 572.6069946289062 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1030.4766845703125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1353.4903564453125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 837.3339233398438 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 676.295166015625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 908.5560302734375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1165.75341796875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1181.81201171875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 884.0926513671875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 893.3436889648438 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1316.252685546875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1037.817626953125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 3430.557861328125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 2588.037109375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 960.5491943359375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 891.4141845703125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 2342.57177734375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 563.9463500976562 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 850.7525024414062 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 2323.62060546875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 894.9552612304688 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1080.8548583984375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1862.4925537109375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 3090.0498046875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1162.351318359375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1301.0369873046875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1492.91748046875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 696.3370971679688 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 970.723876953125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1920.8726806640625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1023.6322631835938 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 5332.02734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 904.9529418945312 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1009.906005859375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 2338.1025390625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 4015.960693359375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 951.096923828125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 757.3182373046875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1108.80517578125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1710.2022705078125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 2409.56103515625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 984.75390625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 2608.81494140625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 1807.248291015625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 4378.52099609375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1215.428955078125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 979.615478515625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 857.9228515625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 2595.3369140625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1414.6182861328125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1071.0548095703125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1099.064697265625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1048.6923828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1432.067138671875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 693.4909057617188 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 843.307861328125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1926.49951171875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1121.4063720703125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 3480.003662109375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 540.5089111328125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1314.828857421875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 2606.659912109375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1464.009765625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 853.2749633789062 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 994.96826171875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 571.2363891601562 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1502.018798828125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 2289.70849609375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1007.4186401367188 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 779.6959228515625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2032.845458984375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 950.6567993164062 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 644.8507690429688 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 1167.742919921875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1622.6727294921875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 884.12353515625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 811.1852416992188 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 927.545654296875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 578.6007080078125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1261.2755126953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1250.64453125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1631.7733154296875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 852.3353881835938 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1505.2296142578125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1235.7406005859375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 875.7772216796875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 779.1757202148438 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 670.9567260742188 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 991.5248413085938 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1053.0260009765625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 903.1796875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1080.79931640625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1611.296875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 772.1757202148438 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 1259.9676513671875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1113.8564453125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 606.8927612304688 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 484.58905029296875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 1062.625732421875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 865.2105712890625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 781.9484252929688 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 623.7115478515625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2568.019775390625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 697.3997192382812 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 773.628662109375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1090.9874267578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 1890.65576171875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 633.2192993164062 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 699.7572021484375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1040.2491455078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 2098.152099609375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 3498.127197265625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 485.5760498046875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 851.6434936523438 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 556.649169921875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1282.877197265625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 1277.3507080078125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1157.0648193359375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 730.0617065429688 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1389.7440185546875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1494.9273681640625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 658.11328125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1096.32421875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 850.6380004882812 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 822.000244140625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 2540.257080078125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1633.493408203125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 2110.1708984375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 632.4321899414062 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 728.5062866210938 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 13096.1298828125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 615.4669799804688 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:27:51<110030:57:50, 1320.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 973.0382690429688 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1111.464111328125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 3325.84033203125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 735.8663330078125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 950.3479614257812 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1513.1331787109375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 3097.7265625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 558.4404907226562 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 464.3021240234375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1128.0784912109375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1009.8767700195312 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 643.9745483398438 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 2529.780029296875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1249.1162109375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 3690.1748046875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 942.8831787109375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 491.8390808105469 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1867.197021484375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 807.6635131835938 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 4374.41357421875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1462.847900390625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1422.7861328125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1099.9327392578125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1390.86474609375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1320.2108154296875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 793.5508422851562 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 675.4495849609375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 702.7080078125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 887.41845703125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 426.8484191894531 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 904.24755859375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 494.9541320800781 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 717.5657958984375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 977.779541015625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 1731.00048828125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1201.213134765625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 879.025390625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1152.9312744140625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1341.323974609375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 704.9752807617188 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 455.36358642578125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 782.6871948242188 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 911.7454833984375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 1345.0982666015625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 942.5836791992188 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 2344.06884765625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1077.8724365234375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1357.3524169921875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 3443.587890625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 755.0176391601562 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 1747.5625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1284.87841796875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 652.102294921875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1093.3524169921875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 779.9335327148438 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1222.69091796875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 4046.0439453125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 950.6049194335938 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 827.4896240234375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 4246.02978515625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1883.22802734375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1009.4801025390625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 532.930908203125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 2523.161865234375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1087.7811279296875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1795.1031494140625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 975.5570678710938 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 2935.198486328125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 985.142578125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 830.8880615234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1377.456298828125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 163684.1875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 744.0466918945312 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 814.38037109375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 2470.376220703125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1353.6259765625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 2011.5467529296875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 499.1493835449219 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 1023.7803955078125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1284.6688232421875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 906.1904296875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1011.9746704101562 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 3624.10107421875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1510.191650390625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1615.5843505859375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 4765.97802734375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1288.9886474609375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 969.4445190429688 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2643.751220703125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1379.00634765625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1768.85009765625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 967.2245483398438 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 805.4931030273438 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1775.5572509765625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1094.896240234375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1134.4031982421875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 653.716796875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1029.2122802734375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1050.82275390625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 1447.28759765625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1223.7353515625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 2499.045166015625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 2248.12841796875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 679.45556640625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 729.3755493164062 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 652.3568115234375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 852.3575439453125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 983.797607421875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1405.754638671875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 643.46826171875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1082.126953125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 770.8373413085938 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1243.3360595703125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1146.5474853515625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1481.2388916015625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1608.0850830078125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1049.9705810546875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1167.4979248046875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 8018.01318359375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 1207.0340576171875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 767.9469604492188 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1422.1856689453125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 781.0164184570312 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 683.4860229492188 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 562.1551513671875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 881.2142333984375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 937.3990478515625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 691.2326049804688 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1402.2767333984375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1157.6983642578125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 1436.5743408203125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1130.224853515625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 910.561767578125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1224.2958984375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 2220.0830078125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 816.1187133789062 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 1108.4349365234375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 752.0940551757812 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 696.7860717773438 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1362.8564453125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 870.609375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1230.2626953125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 687.6884155273438 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1515.753662109375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 739.1670532226562 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 745.3546752929688 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1776.426025390625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 469.4962463378906 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 874.8274536132812 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1871.46484375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 6215.59326171875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1159.4346923828125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1808.36962890625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1550.08544921875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1058.1236572265625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1760.6884765625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1603.9122314453125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1137.8450927734375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1088.4576416015625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1066.396484375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1629.6639404296875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1131.9793701171875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 2692.2587890625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 2179.230224609375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1654.513427734375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 2943.60498046875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1804.807373046875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 2001.9796142578125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1915.9227294921875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: -9531.6015625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1387.4288330078125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1122.9346923828125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 894.7307739257812 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1944.8184814453125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2447.020263671875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1865.882568359375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 625.6416015625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 2079.23974609375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1421.429443359375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 1886.7305908203125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 953.7879638671875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1503.9202880859375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1563.9222412109375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 2637.607177734375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 728.3873901367188 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1295.60498046875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1682.275634765625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1408.181640625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 3313.2529296875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 934.1116333007812 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 473.0965270996094 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 939.8563842773438 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 1248.46435546875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 2095.628662109375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1470.86083984375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1301.1766357421875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 938.2794799804688 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 3407.813232421875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 771.4629516601562 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1006.9681396484375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1061.9630126953125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1672.10107421875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 670.56201171875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1214.4927978515625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 968.5364990234375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 4184.57080078125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1031.3857421875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1300.2099609375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 833.8763427734375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 2071.381591796875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 835.6553344726562 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1133.548095703125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 832.5651245117188 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: -27558.666015625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1809.3365478515625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 1108.3179931640625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 1224.957275390625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 723.1110229492188 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 2152.357177734375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1329.9959716796875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1193.5234375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 3006.962646484375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 2086.07568359375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 2028.7076416015625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 2309.207763671875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1807.533935546875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1292.285400390625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1980.3863525390625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1138.3421630859375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 818.2874145507812 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 2063.212646484375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 3376.23291015625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1322.299072265625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 660.5148315429688 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1057.8170166015625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1109.2647705078125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1294.003662109375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 8707.7216796875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1347.0103759765625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1554.1971435546875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1072.2254638671875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 838.0048217773438 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 72271.84375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1838.2806396484375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 910.7446899414062 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 875.3436279296875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1543.2144775390625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1612.2037353515625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2594.8271484375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1835.6068115234375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1326.1612548828125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1678.5704345703125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1486.35498046875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1406.221923828125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1392.0379638671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1644.6419677734375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1512.5142822265625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 447.53289794921875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1109.9468994140625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 2255.698486328125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1847.303466796875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1334.3333740234375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 948.8348388671875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1285.72265625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 4504.43017578125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1096.451416015625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1053.2808837890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 849.70751953125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 3078.905517578125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1197.247802734375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1515.7269287109375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1771.8692626953125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1589.3447265625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 975.7005004882812 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 1844.4071044921875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 2877.965576171875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 2717.652587890625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 916.8976440429688 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 2484.343994140625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1608.1903076171875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1720.2708740234375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1313.72314453125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1849.3690185546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 829.4208374023438 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 753.9874267578125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 2209.10986328125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1701.7452392578125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1126.5045166015625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 880.930908203125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1603.5621337890625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1823.5267333984375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1260.7357177734375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 3970.00830078125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 766.5731201171875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 658.4041748046875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 890.6978759765625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 3650.110595703125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1042.263671875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 479.13812255859375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 981.1262817382812 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 2106.909423828125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1727.8372802734375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 621.6659545898438 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 2271.706298828125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 843.4744262695312 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1224.1842041015625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1192.7548828125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1092.1826171875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 930.7946166992188 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 864.7592163085938 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1930.7061767578125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1135.1065673828125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 2267.98779296875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1604.6429443359375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 2832.6162109375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1226.9407958984375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1616.1734619140625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 668.1349487304688 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1068.7265625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 2151.850341796875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1585.820068359375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 910.7830810546875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1273.6551513671875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1050.1322021484375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 955.0607299804688 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1906.838134765625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1693.4920654296875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 576.6124267578125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1515.989013671875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1205.1500244140625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1361.5987548828125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 608.2960205078125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 673.235595703125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1944.18994140625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 885.9924926757812 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1209.435791015625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1262.7615966796875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1353.80712890625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 943.008056640625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1276.81103515625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 730.3895874023438 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 938.2608642578125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1315.442626953125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1474.81982421875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1367.4691162109375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1365.0594482421875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1589.81298828125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1133.1021728515625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 708.6866455078125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1333.152099609375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1087.5738525390625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 2687.4736328125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1641.5057373046875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 2042.3089599609375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 596.7582397460938 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 906.0309448242188 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1207.6441650390625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 2404.373291015625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 2928.01806640625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 947.7555541992188 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1620.212890625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1421.6318359375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 1231.1275634765625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1541.326171875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1459.7020263671875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1424.476318359375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1130.359619140625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 3483.545166015625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 3737.6845703125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1401.1683349609375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1574.3629150390625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 543.0516357421875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1682.94970703125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1283.690185546875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 4234.5849609375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 5068.67236328125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1188.860107421875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1290.9927978515625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 3083.923828125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 9115.9443359375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1558.9932861328125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 2887.244384765625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 1084.488037109375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: -7284.6025390625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1993.559814453125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 2840.55908203125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1789.5596923828125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: -24683.240234375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 969.19970703125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 6481.2705078125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 2313.8671875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 3135.4453125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 3929.100830078125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2706.631103515625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 937.15380859375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 931.771484375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 3971.956298828125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1503.6627197265625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 836.984130859375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1229.3402099609375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 951.0693359375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1402.070556640625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 3746.72900390625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1446.701416015625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 585.9620361328125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 831.5526123046875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1025.9114990234375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: -39736.24609375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1144.8780517578125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 622.1939086914062 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1011.8406372070312 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 572.9505615234375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1508.15380859375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 908.4818115234375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 732.1840209960938 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 966.2217407226562 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1581.6416015625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 718.8158569335938 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1635.0421142578125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 608.0814208984375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 770.2230834960938 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 844.3641357421875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1283.7781982421875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1328.472412109375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 680.5231323242188 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1286.8876953125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 719.7400512695312 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1751.354248046875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1348.12158203125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 630.4166870117188 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 745.1076049804688 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1900.7427978515625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 776.2394409179688 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 3047.56640625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1187.41943359375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 2145.856689453125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1240.697021484375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1594.272705078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1572.205322265625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 3301.36865234375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1621.633544921875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 884.7387084960938 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 840.0650634765625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 2031.295654296875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 731.0674438476562 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1342.5404052734375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 2362.689697265625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 2270.664794921875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 3240.7236328125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1276.8726806640625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1405.268798828125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 3202.722412109375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1286.311767578125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1375.3028564453125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1472.3416748046875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 2973.60986328125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1168.9012451171875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2394.396728515625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 2199.972412109375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 2461.08935546875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1066.1544189453125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 1138.1336669921875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1878.9149169921875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2975.106201171875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1267.7977294921875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1077.2059326171875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1628.9683837890625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1023.2447509765625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 2428.508056640625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 972.856201171875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1229.5260009765625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 772.8284912109375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 987.2998046875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 835.343505859375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 675.7166748046875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1611.61279296875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1703.7762451171875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1481.430908203125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 2931.267333984375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 2324.83935546875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 785.7803955078125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 975.8969116210938 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 781.592529296875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1367.308837890625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 10023.2900390625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 927.5523681640625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 1157.07763671875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1963.4384765625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1772.0711669921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1661.7247314453125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 766.8046875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1065.0345458984375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 835.5682983398438 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1545.097412109375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 673.6807861328125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 592.316162109375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 779.3043212890625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1203.82177734375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1006.7744750976562 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 805.7059936523438 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2689.55126953125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1015.7061767578125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 805.374267578125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 1299.208984375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1236.1751708984375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 957.3226928710938 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 879.9205322265625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1111.4681396484375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 1294.4986572265625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 3025.600830078125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1938.2662353515625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1789.9127197265625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1715.52099609375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1382.6270751953125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1438.7391357421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1406.677001953125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1697.0054931640625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 941.9854736328125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 653.585205078125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 906.5147094726562 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 753.8636474609375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1245.79833984375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 2332.671142578125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 784.4231567382812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 898.6356201171875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 761.086669921875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1529.4195556640625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 838.983154296875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 2027.6544189453125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1410.3997802734375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 2063.01708984375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1040.265869140625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 831.9813232421875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1010.995849609375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 904.1358642578125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 913.127685546875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1296.884033203125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1007.8541259765625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1487.862548828125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 2437.939208984375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 965.0435180664062 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1453.01123046875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 2244.6416015625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1290.5653076171875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1796.0052490234375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1238.117919921875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 723.0740966796875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1786.9293212890625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 2316.73291015625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 3294.934326171875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1436.2001953125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1800.26611328125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1024.9984130859375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 3859.09375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1022.1286010742188 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 3507.5517578125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 955.3045654296875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 944.6270141601562 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 792.6467895507812 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 9799.42578125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1181.477783203125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1208.8056640625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1306.7386474609375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 986.8817749023438 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1347.9058837890625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1621.382080078125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 746.7252197265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 736.327392578125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 476.5833740234375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 708.3199462890625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1143.5374755859375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 663.885009765625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1139.1331787109375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1089.634521484375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1076.3193359375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 4630.89111328125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1389.706298828125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 956.5995483398438 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1480.951904296875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 3322.8330078125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 838.846923828125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 922.4751586914062 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 3097.20751953125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1188.602294921875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 618.8829345703125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 878.2069702148438 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 1685.4189453125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1278.328125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 551.525634765625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 880.9041137695312 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1063.37841796875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1002.6776123046875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 871.158935546875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1092.0047607421875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1831.159912109375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1861.4619140625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1701.5767822265625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1457.6593017578125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 741.2946166992188 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1007.0399169921875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 1664.652099609375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 3998.18115234375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1891.2994384765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1385.7518310546875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1183.4381103515625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 5931.95166015625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 885.62744140625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1404.808837890625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 609.765869140625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 970.3425903320312 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 790.3322143554688 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1343.75390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1047.3330078125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1591.1990966796875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1676.4298095703125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1167.4132080078125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1752.004150390625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1145.8212890625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 2425.028564453125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 955.2713012695312 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 820.2150268554688 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1757.1669921875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1294.0262451171875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 2146.43017578125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1195.996826171875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 28232.451171875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1084.07861328125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1438.05078125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1259.49658203125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1816.4345703125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1123.843994140625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 916.3119506835938 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 874.7830200195312 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1324.4522705078125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1318.8226318359375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 818.5647583007812 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1525.0147705078125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1042.189208984375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 979.4842529296875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1873.870361328125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 954.7425537109375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1440.214111328125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 883.2280883789062 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1253.2738037109375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1295.18603515625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 10751.7646484375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 1828.8472900390625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 672.92578125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 585.3968505859375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 655.3260498046875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1422.10498046875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 896.5509643554688 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 3401.53515625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1484.0709228515625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1135.2291259765625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 897.3750610351562 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1626.21875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1145.7535400390625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 1205.44677734375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1768.9278564453125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 801.1713256835938 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 19306.337890625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 2985.7841796875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1958.5548095703125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 3236.025390625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1329.5927734375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1335.245849609375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1427.3212890625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 901.8612670898438 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1362.50927734375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1395.360107421875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 1586.7117919921875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 2567.984619140625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1110.39208984375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 3562.88134765625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 5339.06494140625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 2644.28076171875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 4418.50732421875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 760.959228515625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 3569.796875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 6453.50830078125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1606.9442138671875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1277.9400634765625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 985.8922729492188 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1585.696044921875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1928.133056640625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1739.3817138671875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1717.977294921875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 1172.7938232421875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 3054.63232421875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 997.9581298828125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 2119.8232421875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1629.5068359375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 2023.819091796875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1216.9051513671875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:49:54<110121:51:25, 1321.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1929.7564697265625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 810.0769653320312 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1255.203369140625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1323.590576171875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 725.99951171875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1181.7103271484375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 2128.85400390625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1290.9169921875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1038.0609130859375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 750.8768310546875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1169.0921630859375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 833.7208251953125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 611.9735717773438 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 645.5074462890625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1670.49169921875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 931.827880859375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1729.7232666015625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1162.9669189453125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1838.5625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1014.0643310546875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 821.7769165039062 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 878.6148681640625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1011.6292114257812 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1732.66162109375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1333.911865234375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1297.2752685546875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1615.9439697265625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 837.8275756835938 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 701.1566162109375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 2382.835205078125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 887.347900390625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 491.8491516113281 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1070.1767578125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1382.5816650390625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 908.6187744140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 788.4745483398438 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 2083.635009765625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 814.3578491210938 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 581.8934936523438 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1343.6134033203125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1839.3780517578125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 2899.8603515625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 949.2139892578125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 2590.936279296875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1420.5093994140625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 3910.941162109375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 3183.855712890625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1026.0406494140625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1074.5035400390625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 875.0760498046875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1509.672119140625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 809.7152709960938 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 2315.31396484375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1945.531982421875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 8961.0458984375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 2109.73681640625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 6388.7822265625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1050.07080078125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 2643.935791015625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 572.0264282226562 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1799.840087890625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 2597.007568359375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 2040.881103515625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1142.3580322265625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1392.38037109375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1077.4716796875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 8246.38671875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 929.7343139648438 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1435.2220458984375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1335.1651611328125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1557.309814453125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 2098.0693359375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 788.1837158203125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 805.8587036132812 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1012.3963012695312 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 3948.013671875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1436.31640625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 2205.94970703125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 692.313232421875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 2182.733154296875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1411.065673828125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 7366.68603515625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1117.341552734375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1078.774169921875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 743.5040893554688 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1834.1953125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 2766.905029296875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1268.05078125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1322.4659423828125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1390.3878173828125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 2303.00634765625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 1219.8428955078125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 3289.861328125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 846.3580322265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1429.13525390625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1906.761474609375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 2514.09326171875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1586.5595703125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 932.8448486328125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1566.8560791015625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 2273.85107421875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1175.702880859375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 2067.70654296875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 775.4708862304688 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1109.393310546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1176.4644775390625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 2401.7939453125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1219.7305908203125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1043.1160888671875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1603.83154296875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1504.30712890625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 2267.5048828125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 896.9967651367188 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 833.0853881835938 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1313.905029296875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1591.58056640625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1343.95947265625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2468.46923828125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1807.917236328125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1059.6551513671875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 912.79052734375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 2327.686279296875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 2395.822998046875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2071.0625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 489.73773193359375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 712.9202880859375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1914.4210205078125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 2899.458984375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 858.6534423828125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1238.6270751953125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 5446.390625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1809.751220703125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 2660.58935546875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 824.5309448242188 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1274.3338623046875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1715.0557861328125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 891.12451171875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1487.3621826171875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1175.7711181640625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1005.6959228515625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1107.8623046875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1130.84912109375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 690.2615966796875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 746.33447265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 2284.489501953125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1124.8121337890625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1686.5550537109375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1208.0634765625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 3217.621337890625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1157.5133056640625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 2048.564208984375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1114.3084716796875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1936.1700439453125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1698.607666015625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1209.19189453125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 7085.806640625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1874.9482421875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 808.2765502929688 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 896.1807250976562 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1397.7218017578125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1266.4603271484375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 571.1509399414062 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1913.6683349609375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1391.7254638671875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1669.9801025390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1646.220703125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 910.7513427734375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 896.6863403320312 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1119.2979736328125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 743.9188842773438 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 733.0949096679688 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1724.364990234375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1320.319580078125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 3090.835205078125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1396.58251953125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1697.3797607421875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1430.3214111328125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1244.96875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1736.2757568359375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 864.0169067382812 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 836.6693115234375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 932.7772827148438 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 849.3916015625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1150.5023193359375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1899.096435546875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 699.4351196289062 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 868.9730834960938 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1116.8372802734375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 499.7177734375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1018.4390258789062 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 2719.010498046875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1018.2979736328125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1129.0452880859375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 888.4204711914062 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1485.9580078125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1282.0341796875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1244.362548828125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1438.985107421875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 3401.206787109375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1654.1019287109375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 7631.2275390625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1247.9854736328125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 4084.3125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1529.9676513671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 2673.496337890625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 2703.3896484375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1917.3931884765625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 2651.448974609375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1069.51953125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 2808.6318359375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1566.322509765625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 2452.04248046875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 964.3139038085938 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1138.8173828125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1855.864501953125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 6638.9482421875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: -6412.6640625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 824.8035888671875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 788.7849731445312 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1672.9468994140625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1547.066162109375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 943.6035766601562 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: -4821.7001953125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1145.8944091796875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1765.943115234375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1130.089599609375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1321.13525390625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1193.452880859375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1200.6116943359375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 530.819091796875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1714.451416015625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1119.1448974609375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1177.5916748046875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 2231.0009765625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 869.78369140625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1021.1165771484375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 958.0076904296875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 3718.447265625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 2207.801025390625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1492.5950927734375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1033.0018310546875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 846.6919555664062 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 2938.1416015625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1006.4891357421875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1581.69921875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 2309.3271484375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 2435.182861328125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 2137.444091796875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 629.4476928710938 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1884.8759765625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 4295.3671875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 10322.7568359375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1345.02001953125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 962.81396484375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 2045.4197998046875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1352.7943115234375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1244.690185546875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1183.9256591796875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 590.753662109375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1016.0285034179688 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1165.1865234375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 715.5664672851562 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 883.8585815429688 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1772.836181640625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 903.0320434570312 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1007.9517822265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 2985.866455078125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 970.4309692382812 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1855.112060546875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 2400.43310546875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 2863.93212890625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1641.6021728515625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1419.2276611328125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1161.5245361328125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 2305.892333984375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1093.198486328125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 1211.6514892578125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1240.815673828125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: -26901.11328125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 5099.49951171875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1489.431884765625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1042.5843505859375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1591.96630859375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 2260.6865234375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1752.6236572265625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 726.6629638671875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 580.4219360351562 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 954.7758178710938 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 804.9237670898438 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1813.5595703125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 744.5986938476562 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 739.6015014648438 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1566.61767578125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1285.0716552734375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 4823.888671875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1447.291748046875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 811.2002563476562 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1088.2135009765625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1191.7027587890625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1315.3115234375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 831.0100708007812 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1119.6968994140625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 2718.539794921875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1730.54736328125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 872.6865844726562 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 4740.451171875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1911.965576171875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 730.1956787109375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1862.7154541015625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 2540.751953125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1928.19091796875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 954.5558471679688 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1084.5546875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 948.0648193359375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1768.039306640625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1883.3353271484375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1126.2490234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 747.7925415039062 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 3121.89013671875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 843.953125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 954.1989135742188 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 666.1449584960938 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 866.6505126953125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1490.6746826171875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 816.8587036132812 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 799.0328369140625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1483.3790283203125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1641.2506103515625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1137.5250244140625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1419.289306640625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 654.0032348632812 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 2467.78076171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1162.8843994140625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 2913.864990234375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 2635.485107421875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 912.0782470703125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1392.6959228515625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1801.2247314453125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 922.3534545898438 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 3493.802001953125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 901.6944580078125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1175.1455078125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1167.084716796875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 529.9760131835938 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 721.4529418945312 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1921.5718994140625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1567.956787109375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1646.2447509765625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1440.0279541015625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1519.94140625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1009.1760864257812 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 3566.783203125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1940.9146728515625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 21024.716796875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 3280.5537109375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1708.2801513671875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: -9975.3544921875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 2344.53173828125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 2465.236572265625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1416.941162109375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1795.376953125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1216.2578125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1682.67236328125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1195.95849609375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 2329.59619140625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 2199.864013671875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 936.7805786132812 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 4196.55029296875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2801.091552734375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 2458.121826171875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1658.59765625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 2699.130126953125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 11165.064453125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1242.1982421875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1830.672607421875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1279.4952392578125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1910.54833984375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 3800.379638671875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1718.3536376953125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 902.3259887695312 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 2377.494140625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1007.7014770507812 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1896.80517578125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 628.6096801757812 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1555.1131591796875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1815.0126953125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1185.60400390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 2129.265380859375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1795.275634765625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 790.1746215820312 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1026.324462890625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1163.2742919921875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 2653.708984375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 783.8204956054688 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 656.3126220703125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1917.1148681640625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 19665.302734375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1453.5655517578125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 754.9882202148438 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1286.998291015625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 2018.6466064453125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 3480.796630859375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 817.471923828125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1292.986572265625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1244.446533203125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 937.81005859375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: -6503.14208984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 2645.837158203125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1017.9605712890625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 585.1702880859375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1761.888671875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 2667.1201171875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1968.91455078125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 377.10748291015625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 823.9188232421875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1045.470458984375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 2155.64892578125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 826.1168212890625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1288.07763671875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 636.1458740234375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 2464.907958984375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1875.311279296875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1548.2742919921875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 722.3680419921875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1924.2626953125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 6722.94677734375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1708.791748046875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 818.7904663085938 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1085.975341796875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1650.3824462890625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1330.725341796875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 18186.60546875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1176.9814453125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 2107.735107421875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1965.2271728515625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2204.410888671875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 2153.91162109375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 2508.44921875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 2618.234375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 2207.493896484375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1831.5960693359375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 2948.8525390625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1497.5592041015625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1554.7276611328125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1904.7147216796875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 8609.0244140625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 2339.764404296875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 773.4105834960938 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1994.997802734375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1107.132568359375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 872.6878662109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1795.060546875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 1531.4554443359375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1151.5589599609375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1070.828857421875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 4452.8623046875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1699.4833984375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1402.77294921875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 658.1436767578125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 4900.703125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 748.6022338867188 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 2119.281494140625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1372.60693359375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 896.8129272460938 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 797.38916015625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1029.9329833984375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 23262.3359375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 2026.4520263671875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1464.9019775390625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1028.5047607421875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1144.1065673828125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 994.0396728515625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1956.0284423828125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 928.6303100585938 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 2228.84521484375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 2906.51708984375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1500.8941650390625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 900.8604736328125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 756.6737670898438 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1308.139892578125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 2019.7755126953125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 871.7874145507812 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1070.650390625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1391.94580078125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1421.3072509765625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1937.3348388671875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1299.62353515625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 3924.400390625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1413.1553955078125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1603.3416748046875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1286.928466796875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1187.015869140625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1071.1627197265625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1192.4613037109375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1577.7242431640625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1358.868408203125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 1543.7662353515625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1631.8460693359375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 2550.005126953125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1237.7806396484375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1284.10302734375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1153.7003173828125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1312.8216552734375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 836.4187622070312 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 797.4226684570312 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 3069.0712890625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 767.4107666015625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 908.489501953125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1476.89306640625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1298.4739990234375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1601.0284423828125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1044.6270751953125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1202.2904052734375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1322.7879638671875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1177.5274658203125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1151.1767578125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 810.9122314453125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 756.7362060546875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1490.1463623046875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1908.3955078125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1045.7611083984375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 909.669677734375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 608.8903198242188 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1782.646728515625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1178.9832763671875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 950.2322998046875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 989.5202026367188 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 955.1177978515625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1730.84521484375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 514.4212646484375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1639.6824951171875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1069.0084228515625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1095.9940185546875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1121.2237548828125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1086.053955078125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1339.234130859375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1584.4005126953125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 933.2469482421875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 887.5697021484375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 2691.028076171875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 986.5983276367188 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 593.1511840820312 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 505.94635009765625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 726.1594848632812 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1765.064208984375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1310.560546875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 368.1723327636719 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 805.7088012695312 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1366.8857421875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1001.0748291015625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1604.530517578125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1001.1006469726562 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1211.152587890625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 3247.471923828125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1003.3651123046875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 766.1893310546875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1952.94970703125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 2431.66015625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1217.1143798828125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1060.6556396484375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: -31245.615234375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 5994.681640625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 732.7103271484375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1432.0009765625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2559.083251953125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 2669.41455078125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 2314.618896484375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 2289.53759765625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1947.5382080078125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1283.7564697265625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1431.6131591796875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1251.381103515625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 501.02178955078125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 990.6307373046875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 2143.793212890625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 1043.559326171875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 740.615478515625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 624.4281616210938 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 565.6259155273438 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1287.0196533203125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1505.529296875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 6975.96484375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 809.474365234375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 833.581787109375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 2115.875244140625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 719.6132202148438 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1529.561279296875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1124.8594970703125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 743.3560791015625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 853.4039306640625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1562.827392578125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 891.5794067382812 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1333.3275146484375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 8284.982421875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1863.7550048828125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 891.323974609375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1731.71044921875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1192.9017333984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 741.1619262695312 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 843.85546875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1574.2431640625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 582.2155151367188 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 609.3531494140625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1075.794921875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 2406.20361328125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1100.711181640625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1552.2745361328125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 916.87646484375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1089.74462890625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 876.2903442382812 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1472.816162109375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1601.535888671875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 857.4716186523438 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 361.3291320800781 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 963.0243530273438 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 701.930908203125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2216.3857421875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 11403.421875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 948.2664184570312 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 629.3780517578125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 2098.443115234375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1649.677734375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1671.398681640625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 667.7015380859375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 982.9166259765625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1047.051513671875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1399.16650390625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 844.9049072265625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 2970.4541015625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1926.986083984375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1812.3865966796875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 980.88818359375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1068.8846435546875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1565.4603271484375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 3547.007080078125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1486.3798828125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1115.4512939453125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 579.584228515625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 657.145751953125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1476.30078125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 636.8353271484375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1168.9932861328125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1150.9874267578125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2529.580322265625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 3862.467529296875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1264.017822265625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 3880.2724609375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 4790.8603515625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1538.093017578125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 2498.339111328125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1343.408447265625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1820.268798828125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 813.16845703125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1073.374755859375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 813.1502075195312 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1093.7669677734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 2215.2548828125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 927.4398193359375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1126.041259765625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1428.7708740234375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1798.1141357421875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 4038.90576171875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 987.58056640625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1038.0924072265625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 518.8577270507812 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 638.9735107421875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 564.2354125976562 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1533.4793701171875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1208.048828125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 666.1633911132812 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1084.1593017578125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1343.2950439453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1269.0098876953125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 569.57275390625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 2476.27734375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1871.0811767578125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1135.9930419921875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 549.5150756835938 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 2997.655517578125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1635.6168212890625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 919.122802734375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 864.466064453125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 743.6339721679688 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1440.33740234375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 846.8840942382812 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 101719.8203125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 838.2782592773438 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 931.8280029296875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 999.042724609375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1410.2696533203125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 504.0891418457031 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 931.3370971679688 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 664.2066650390625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1100.4481201171875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 784.5540771484375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:12:06<110410:05:53, 1324.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 723.7308349609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 686.2610473632812 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 949.408447265625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 578.7352905273438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 634.6777954101562 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 981.5191040039062 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 924.8552856445312 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1413.13134765625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 2208.082763671875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1028.2708740234375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 756.6226806640625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1126.5172119140625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 2822.598388671875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1855.2257080078125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 2948.1767578125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1100.87548828125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1316.7288818359375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1495.0826416015625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1052.957763671875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 2197.669189453125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1675.2022705078125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 3820.717529296875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1034.8167724609375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1081.8785400390625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 2476.565185546875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1443.4429931640625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1414.4803466796875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1054.819091796875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 650.2765502929688 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1724.2861328125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 937.970458984375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 2716.112548828125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 2412.153076171875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 874.146484375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 5385.26025390625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 898.9965209960938 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1016.423583984375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 2309.731689453125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1926.582275390625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1706.594482421875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 2984.66552734375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1732.1680908203125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 4149.74853515625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 884.262451171875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2234.896240234375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 3271.547607421875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1144.621826171875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1240.4788818359375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 820.038818359375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: -33949.3984375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1131.8077392578125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 3136.29150390625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1217.4405517578125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1348.177490234375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1611.410888671875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 2330.0087890625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 790.0358276367188 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 876.815185546875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1447.957275390625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1333.4952392578125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1326.755859375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 981.6571655273438 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1586.634033203125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1936.9779052734375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1599.9549560546875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1126.7547607421875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 826.9063720703125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 866.8892211914062 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1412.227783203125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 2061.99267578125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 882.889892578125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1408.73486328125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 596.0202026367188 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1814.8238525390625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1014.0639038085938 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1689.8587646484375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 842.0084838867188 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1608.16015625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 696.2487182617188 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 728.0291137695312 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 2270.068603515625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1649.4136962890625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 866.2620849609375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1606.4046630859375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 964.8077392578125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 984.166748046875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 633.643310546875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 2755.092529296875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 730.6826171875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 931.5692138671875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 830.9912719726562 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1978.399658203125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 949.3754272460938 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 3000.814697265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1884.7880859375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1306.7764892578125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1070.648681640625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2113.240478515625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1127.0648193359375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1664.8968505859375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 4408.46728515625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 11811.85546875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 2143.026123046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1764.8580322265625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 2458.27490234375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1175.080322265625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 2348.6748046875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 910.28515625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 849.7514038085938 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 938.5429077148438 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1102.0684814453125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 563.2054443359375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1741.5052490234375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 978.2094116210938 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1623.0809326171875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 841.4334106445312 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 953.33642578125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 6534.2890625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1476.8255615234375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 2120.5185546875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1112.1981201171875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1886.8734130859375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 9137.861328125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1403.8760986328125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 2700.543212890625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 6191.02392578125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 760.4583740234375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1851.688720703125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 2088.543212890625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1082.04931640625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1163.04833984375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1689.5673828125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1286.1878662109375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 3447.847900390625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1892.417236328125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1195.9166259765625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1241.811767578125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1560.9141845703125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1613.8631591796875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1851.7862548828125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 2363.379638671875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1364.5469970703125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1494.291259765625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1493.3587646484375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1804.3599853515625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 5265.0400390625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1939.6593017578125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1914.5634765625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1678.0205078125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1489.881591796875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 882.8193969726562 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1557.95068359375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1580.829833984375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1524.6162109375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 2550.64697265625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1843.9517822265625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1555.5924072265625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1031.5316162109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 887.6439819335938 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1913.3328857421875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 2160.8359375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1040.64599609375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1025.1981201171875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 4370.77978515625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1167.85986328125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 764.2659912109375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1728.2601318359375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1952.534912109375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 4570.13134765625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 3974.19091796875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 604.6464233398438 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1160.4039306640625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 2137.874755859375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 2342.89013671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 2533.767822265625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1617.9503173828125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2603.843017578125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 5454.77001953125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 3421.119384765625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1520.375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1306.362548828125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 5840.2158203125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 2578.90234375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1165.0521240234375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1344.90478515625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1097.026123046875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1727.0943603515625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1978.15869140625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 7286.3017578125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 938.1529541015625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1451.0162353515625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1543.654296875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 2326.8857421875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1125.2247314453125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1994.2451171875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1834.24609375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1393.9154052734375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1349.749755859375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 570.9835815429688 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1031.329833984375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1414.173095703125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 2826.302734375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1050.680908203125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1384.1661376953125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 2501.2705078125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 3672.5654296875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 5148.9501953125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1151.685791015625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1112.095458984375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 2634.027099609375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 4872.3115234375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1789.7283935546875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1069.7523193359375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1141.2193603515625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1430.410888671875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1097.224853515625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1659.7802734375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1612.0352783203125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 2571.001220703125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 778.8580322265625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 774.4962768554688 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 891.2099609375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 4719.43115234375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 3192.643310546875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1233.9510498046875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1157.561767578125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 3439.29833984375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 2752.285400390625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 2702.035888671875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1519.193359375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1022.9329223632812 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1920.69677734375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1200.0146484375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1072.9677734375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1448.415771484375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1378.381591796875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1114.8023681640625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1381.6807861328125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 566.1890869140625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 3089.03955078125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1956.513427734375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1154.1300048828125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 786.9826049804688 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 984.4445190429688 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 997.9463500976562 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 970.141845703125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1931.8604736328125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 844.7188110351562 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1482.7393798828125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 903.0684204101562 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1800.813232421875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1900.211669921875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 901.748046875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 685.2449340820312 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 969.3649291992188 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 864.385498046875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 3197.734375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1961.249755859375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 2313.6650390625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2876.334716796875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1345.513671875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1072.8056640625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1624.234619140625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1917.7763671875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2183.70458984375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1041.4356689453125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1022.9827270507812 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 5571.09423828125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1855.5 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 3778.0205078125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 804.5950927734375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1457.454345703125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1817.8128662109375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 3378.8720703125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1598.2215576171875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1122.2796630859375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1504.971923828125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 992.8359375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1380.5506591796875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 3391.6708984375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 4909.349609375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1486.9974365234375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 543.2498168945312 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1752.670654296875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1771.02001953125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1820.34228515625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1260.4429931640625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1444.259521484375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1555.0733642578125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1035.4483642578125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 2410.69873046875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1126.072265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 786.607666015625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1064.670654296875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1138.5946044921875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1729.2320556640625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1711.3310546875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1072.375732421875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1819.705078125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1300.08349609375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 12978.8232421875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1527.5560302734375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1116.8568115234375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 779.8087158203125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1268.6370849609375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1750.2884521484375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1424.8636474609375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1133.6959228515625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1798.9287109375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1691.370361328125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 9658.564453125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 993.7861328125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 2979.4951171875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1936.084716796875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 938.3418579101562 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1822.1971435546875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1335.567138671875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1291.637939453125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1516.724365234375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 743.8429565429688 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 989.0872192382812 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1012.6138916015625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 2981.260986328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 3651.736083984375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 899.3139038085938 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1436.732666015625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1241.7039794921875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: -111288.3984375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1517.794189453125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 2186.5751953125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 3233.103759765625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1598.98046875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1417.0416259765625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 3032.771240234375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 4313.12939453125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 908.4175415039062 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1454.1429443359375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1357.775146484375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1242.7420654296875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1375.1754150390625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 6880.5634765625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1458.9752197265625 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1920.4849853515625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1493.904541015625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1171.2369384765625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1565.7777099609375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 987.8053588867188 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 2013.75830078125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1194.530029296875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 940.4761962890625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 747.73388671875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 981.6310424804688 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 946.3131103515625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1624.372314453125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 947.4442138671875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1288.61572265625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 880.5048828125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 3010.39501953125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1734.9759521484375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1427.725341796875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1660.9219970703125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1321.8038330078125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1599.414306640625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 762.04443359375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1011.6399536132812 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1465.1885986328125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1517.08447265625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 804.0880126953125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 791.920654296875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 887.9812622070312 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1012.593017578125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1566.6734619140625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 2898.51953125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 3552.351806640625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 661.3465576171875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 9070.552734375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 842.5762939453125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 922.473388671875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 3362.1240234375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 961.6436157226562 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 2206.623046875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1884.6485595703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1232.2403564453125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 610.103759765625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1371.0408935546875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1469.6458740234375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 33100.60546875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1991.47314453125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 676.7637939453125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 644.0755615234375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1369.9530029296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 819.6100463867188 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 908.3052978515625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1570.8773193359375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1264.4764404296875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1248.767578125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2496.48876953125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 3042.219970703125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1093.1331787109375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2246.75830078125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1269.5665283203125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 2714.545166015625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 5860.94580078125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2143.429443359375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 2343.953857421875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1390.522216796875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1521.3238525390625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1469.954833984375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 2282.1689453125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1348.905029296875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1009.3167114257812 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 3384.462158203125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1451.782958984375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1179.763916015625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 2347.92919921875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1643.0032958984375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 3504.85400390625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1527.840087890625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 929.8958129882812 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1783.217041015625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 2347.095703125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 2870.609619140625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 3907.070556640625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: -60582.6796875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 629.0921020507812 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1934.808837890625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 34655.23828125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 3034.370849609375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: -8527.2099609375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 3144.50439453125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 2572.610107421875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1358.534423828125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 783.21923828125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 2101.117431640625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1751.80419921875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1202.524169921875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1406.2919921875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1117.467529296875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1119.2236328125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 10439.7470703125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 913.0091552734375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 2174.45458984375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 2673.930419921875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1062.1181640625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1152.4193115234375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1181.1336669921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 10194.45703125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1268.4554443359375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1228.140625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1407.468994140625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1440.7880859375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1216.060302734375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1656.1748046875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 806.451171875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1860.3582763671875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1004.8953247070312 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1513.70556640625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 877.4458618164062 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 806.3430786132812 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 905.3908081054688 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1028.510498046875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 737.7316284179688 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 676.9513549804688 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 751.5526123046875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 674.54638671875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 993.03173828125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1082.7174072265625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1601.104248046875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1041.68896484375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1121.53466796875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1357.3092041015625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1146.6156005859375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1040.1112060546875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 769.6820068359375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1537.392822265625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 641.709716796875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1694.1787109375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 827.2504272460938 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 3732.619873046875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1184.2279052734375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 648.2720336914062 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 580.9345703125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1757.4854736328125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: -489214.3125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1060.1922607421875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 801.2926635742188 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 671.7217407226562 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 864.224609375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 2469.51416015625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 2163.42138671875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 682.1903076171875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1123.80615234375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 12010.642578125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 2061.10888671875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 3238.813232421875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 2374.460693359375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 7108.4912109375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 2946.702880859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1516.4052734375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 7099.41259765625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 4593.33203125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 879.6516723632812 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1882.83935546875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1961.4146728515625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 3897.81103515625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 2280.029296875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 2535.19677734375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1321.07666015625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1116.7603759765625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1314.9691162109375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 2029.2977294921875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 1011.0692749023438 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1489.1484375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 3796.665283203125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1400.608154296875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1605.05810546875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 3423.76708984375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1359.5740966796875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1773.7686767578125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1321.8944091796875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 629.421875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1202.9808349609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 2706.679443359375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 2736.459228515625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 3418.7802734375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2715.685791015625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1445.2869873046875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1191.2403564453125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 3656.43896484375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1747.6612548828125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2392.156005859375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1366.44384765625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1479.054443359375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1420.000732421875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 957.8892211914062 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 787.8048706054688 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 2227.0546875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 2485.78125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1773.69970703125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1152.6473388671875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 4900.75537109375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: -40322.2578125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 550.5310668945312 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1361.7589111328125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1321.227783203125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 2126.308837890625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 3595.105712890625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 763.2669067382812 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1581076.375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1689.269287109375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1427.435302734375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 3223.348388671875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1112.12890625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1482.2379150390625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2946.560546875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.778, loss_val: nan, pos_over_neg: -17738.828125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 3531.17626953125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1145.2777099609375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 2094.246826171875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2541.985595703125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 2204.47509765625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 853.8636474609375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1575.87451171875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 4681.59375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1682.0732421875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1518.7020263671875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 4189.8759765625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1714.382568359375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 3988.364013671875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1306.95751953125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 3109.3017578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1868.1605224609375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 5801.72314453125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1033.6151123046875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 2599.262451171875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1076.434814453125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 609.118896484375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1550.5052490234375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1321.4283447265625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 523.4588012695312 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 5208.5830078125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 737.4367065429688 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 2980.91259765625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 997.8480834960938 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 947.0595703125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1863.9371337890625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 2443.723876953125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 851.7197875976562 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1182.157958984375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1276.3824462890625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 2125.63427734375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1968.9508056640625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1142.120849609375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2413.113525390625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 2459.451171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: -55758.21484375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1646.199951171875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 3691.9912109375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1005.3034057617188 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 2052.117431640625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 984.0825805664062 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1256.9371337890625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1994.60498046875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 892.5628051757812 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1405.71923828125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 28330.58984375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1106.699462890625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 651.0401000976562 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 2755.19921875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 2246.111572265625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2107.593994140625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 939.5067749023438 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 994.5242919921875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: -47568.53515625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 7251.45654296875 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 2482.765625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1244.3211669921875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 14198.3134765625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 6799.36376953125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 6120.98828125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1796.053955078125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1674.6988525390625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1339.5560302734375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1199.0438232421875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 3295.391357421875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 2087.6708984375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1819.8843994140625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 3815.252197265625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1990.0699462890625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1245.5076904296875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 973.471923828125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 5401.49951171875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1910.9818115234375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 880.4723510742188 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 2075.640625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1095.556640625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1339.6221923828125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 2240.92529296875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1428.455810546875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1177.6429443359375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 2350.34375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 2283.115966796875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 3903.74267578125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 3445.09033203125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1782.60595703125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 2133.2880859375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: -2997.000732421875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1799.48828125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 2559.52001953125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 21464.416015625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1272.9293212890625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1586.8831787109375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1292.452880859375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 3007.2509765625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 8449.4375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1671.7354736328125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1301.419189453125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 3717.369873046875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: -8262.935546875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1860.2470703125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1268.4068603515625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1467.071044921875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2709.911376953125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 893.8897094726562 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 676.9685668945312 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 2119.734375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1967.84130859375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 675.2055053710938 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1080.438720703125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 2085.369384765625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 2230.438720703125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1111.904541015625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 3729.809326171875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1364.576171875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1307.91650390625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1392.5081787109375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 994.7532958984375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1965.3919677734375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 2717.990966796875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1386.9793701171875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 3049.954833984375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1880.915283203125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 4454.15869140625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1281.974853515625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 979.4419555664062 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 2584.369140625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 2289.444091796875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 2260.65234375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 6330.228515625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: -18713.681640625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1402.3760986328125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1132.12890625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 3988.36279296875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 992.804931640625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 2791.098876953125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:33:38<109522:56:36, 1314.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 723.6150512695312 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 865.625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1529.9383544921875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1541.7872314453125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 2306.195068359375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1493.4031982421875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 800.9917602539062 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1818.179443359375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 2143.234619140625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 4702.7666015625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 679.8903198242188 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1572.9052734375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: -21165.5859375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 972.8129272460938 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 907.6561889648438 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 2083.16259765625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1535.232177734375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 3519.295166015625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1207.249755859375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 3771.852294921875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1218.4278564453125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 2862.840576171875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1161.8057861328125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 2602.113525390625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 4490.75927734375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1222.91259765625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 799.492431640625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 715.2830200195312 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1454.111328125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1048.7998046875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1151.85791015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 9049.1337890625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1774.406005859375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 783.7947387695312 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1463.3753662109375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 960.7164916992188 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1548.0152587890625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1320.88134765625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 2004.7696533203125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 2581.133544921875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1887.001953125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1908.0357666015625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1159.0523681640625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1372.6915283203125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 6362.6123046875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1208.9940185546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 832.4268188476562 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1080.1627197265625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 2263.595703125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1087.68212890625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 3094.779541015625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1324.643310546875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 2421.789306640625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 4454.0869140625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: -10427.8154296875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 2240.303466796875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1508.7119140625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 980.2141723632812 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 20742.771484375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 2677.923095703125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 760.056640625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1063.376953125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 672.0896606445312 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1132.947509765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 983.326416015625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1110.0947265625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1449.8966064453125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1535.9796142578125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1171.7708740234375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1413.4151611328125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 4490.16064453125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1142.3780517578125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 635.9994506835938 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1579.9918212890625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 850.5380249023438 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 26295.19921875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 2642.3408203125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1175.8154296875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1065.4437255859375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 3998.710693359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 5182.751953125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1148.8487548828125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1454.41748046875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1457.975341796875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1514.2403564453125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1638.2666015625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1105.064208984375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1735.502685546875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1825.7291259765625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1290.379150390625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 536.3690185546875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 879.8900756835938 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1733.39404296875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 784.115478515625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1139.69091796875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2004.4520263671875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 2376.528076171875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 2232.252197265625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 899.2642211914062 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 2555.192626953125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 2127.0009765625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 2937.326171875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1794.5650634765625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1749.0172119140625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 2852.517333984375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 3064.771484375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 3006.5439453125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1913.7047119140625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 533.4434204101562 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1098.970703125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1755.5684814453125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1322.64013671875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 2108.7685546875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1824.2152099609375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1000.6657104492188 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 2506.599609375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1408.2764892578125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1872.901123046875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1532.50537109375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.775, loss_val: nan, pos_over_neg: -243061.015625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1045.408935546875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1226.7166748046875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 918.3654174804688 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 6902.13818359375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 4420.6455078125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1186.846435546875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2977.50048828125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1943.9228515625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1174.944091796875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 826.1351928710938 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2041.9854736328125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1824.277099609375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 728.0971069335938 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1554.297119140625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: -14758.3505859375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2156.62060546875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 2893.445068359375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1530.224365234375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 26519.92578125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: -313297.8125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 6918.7685546875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1331.39697265625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 3139.90478515625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 5244.4423828125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1063.3125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 863.9578247070312 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 3276.51513671875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1466.438720703125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1155.8524169921875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1364.8736572265625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 4084.32470703125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1731.9188232421875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1798.345458984375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1341.9698486328125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1068.21728515625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 728.9857177734375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 2469.032958984375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 2498.927490234375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 2986.15869140625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 784.6685180664062 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 835.7575073242188 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 2113.61669921875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1140.7239990234375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1306.154541015625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 569.6651000976562 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 784.8818359375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1916.9168701171875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1362.279541015625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1056.032470703125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 940.5630493164062 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 978.3419189453125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 1477.31494140625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2644.9111328125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1082.4708251953125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 2443.0087890625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1997.0233154296875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1611.2161865234375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1031.4718017578125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 2189.570556640625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 2037.7235107421875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: -75912.609375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1416.85302734375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 684.5103759765625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1594.77880859375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1871.9068603515625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1201.7596435546875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1363.462646484375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 2415.51220703125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1898.3458251953125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 3773.5546875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1611.6767578125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1500.7567138671875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1374.71826171875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1333.564453125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1767.361328125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1765.2249755859375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 2082.7568359375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1296.42431640625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1153.2425537109375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1202.3167724609375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 2770.608642578125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 2236.0771484375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1429.1207275390625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 3628.40234375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1548.6416015625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 973.5226440429688 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 862.0645141601562 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 3870.90673828125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 13984.8525390625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1333.078125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 3780.986572265625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 2105.419677734375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1597.8575439453125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 6173.9462890625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1116.6307373046875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 6523.57861328125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1596.77880859375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 2966.304931640625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 964.7304077148438 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 3538.36572265625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 889.8461303710938 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1648.0838623046875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1851.5902099609375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1286.260498046875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1306.14892578125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 959.2409057617188 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 8241.5107421875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 2297.2939453125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1171.7412109375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1367.2086181640625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1872.986572265625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1756.0599365234375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2230.753173828125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1012.089599609375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 939.0341796875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1535.0518798828125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: -18663.701171875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 6282.70556640625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1082.3612060546875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1622.925537109375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 2153.895751953125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1393.1988525390625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1735.6776123046875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1080.846923828125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 2201.02783203125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 2017.8173828125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1239.102783203125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 2756.259765625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1714.7972412109375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1152.4378662109375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 819.4088745117188 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 993.4852905273438 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1157.072509765625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 2710.396728515625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1010.7750854492188 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 2131.246337890625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1128.92138671875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 2436.493896484375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1224.3236083984375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1072.5645751953125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1682.2763671875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1694.5450439453125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1026.7039794921875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 910.8841552734375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1676.3985595703125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1696.783203125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 2116.625244140625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1298.2545166015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 3854.37939453125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1126.454833984375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1474.5213623046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1898.26220703125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 2174.50244140625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1911.151123046875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 983.4825439453125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 960.9529418945312 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1138.5111083984375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1088.352294921875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 2099.05224609375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1129.8480224609375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 937.6542358398438 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1088.3599853515625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1572.644287109375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 770.0509643554688 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1352.752197265625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1512.9486083984375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1742.9337158203125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 3239.70068359375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1510.5069580078125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1058.194580078125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1480.2703857421875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1226.57421875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1156.0924072265625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 2022.66796875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 895.7014770507812 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 791.6226806640625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1056.709716796875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1325.351318359375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1474.8060302734375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1528.684326171875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1249.70751953125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 3308.094482421875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1035.9708251953125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1088.66748046875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1105.656982421875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1510.1236572265625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 9678.53515625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1705.19091796875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1412.3006591796875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 2149.955322265625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1389.3768310546875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1175.0745849609375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1586.1297607421875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 2909.279052734375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1226.0631103515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1167.3436279296875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1252.933837890625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1958.261474609375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 5249.943359375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1012.3731689453125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1983.3226318359375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1729.20166015625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1807.1278076171875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1621.4775390625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1355.6793212890625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 892.010986328125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1712.8775634765625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 2675.015380859375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 15980.734375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1127.0765380859375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1239.3609619140625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1388.450927734375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1242.894287109375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 4369.52392578125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1227.560302734375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 612.5306396484375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 771.888916015625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1465.0465087890625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 2080.36962890625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: -14767.01953125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1534.118408203125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 2527.828857421875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1065.7301025390625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1137.7752685546875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1090.7672119140625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 710.4298095703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 757.20166015625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 977.4931030273438 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1432.318115234375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 2402.798095703125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1919.6771240234375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2983.565673828125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 783.781982421875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 906.02099609375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1478.2978515625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1184.7056884765625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 664.281005859375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 597.74169921875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 589.1612548828125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 981.959228515625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 2178.1953125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1424.8009033203125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1476.6380615234375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 780.1873779296875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 4168.80517578125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 5840.810546875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1351.61669921875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 641.1669921875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1010.3817138671875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1593.6435546875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 842.0332641601562 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1094.96728515625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 763.2645874023438 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1697.13818359375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 4176.54931640625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 2366.102294921875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1223.503173828125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 704.506103515625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1067.8214111328125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 6444.24072265625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 2942.387451171875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1255.339599609375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 883.1138305664062 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1832.9283447265625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1063.8646240234375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1095.6676025390625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1929.75048828125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 2267.97802734375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1129.8822021484375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1215.0068359375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 2707.216796875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1305.45166015625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 2170.904052734375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1111.0994873046875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1248.273193359375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 2055.32470703125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1063.527587890625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1208.8665771484375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1035.0035400390625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 2088.072998046875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 2674.509033203125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 4546.69384765625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 923.3768310546875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 2177.200439453125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 63320.9609375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 7638.33203125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1744.5091552734375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 3415.532958984375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 6771.921875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 2628.400634765625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 2707.0791015625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 883.3221435546875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1339.536376953125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1543.478515625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1774.6029052734375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 6796.52099609375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 8923.4248046875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 895.0901489257812 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 22812.8984375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1799.015869140625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 9156.349609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: -9473.7734375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 8870.3232421875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1971.3116455078125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 2100.486328125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1494.7889404296875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 796.5386352539062 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 3625.947021484375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 3515.806640625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1564.7462158203125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 784.1261596679688 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 918.9898681640625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 3555.98974609375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 5464.72802734375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1285.7196044921875 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=transl03'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss  Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix  Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE  RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
