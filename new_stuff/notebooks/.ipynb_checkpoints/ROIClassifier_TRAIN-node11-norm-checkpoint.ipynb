{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/josh/opt/anaconda3/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225],\n",
    "                                     inplace=False),\n",
    "    \n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5hElEQVR4nO19T4wsyVnn74vIzKrufm+ePZ4Z8I4tPNbOSpi94LVsJBBCYhG2tZL3AsIHxMESF6MFicMO+MDJEnDgtOIwEhasxNprCaSdgyXktZAsDrBGyIDtke2xdwHD4Jk3b2Zev+6uqsyMbw8RkRUZGREZmV39Op/dP6nU1VmR8S+/+P5HJDEzbnCDHIjr7sANHh3cEMsNsnFDLDfIxg2x3CAbN8Ryg2zcEMsNsnFlxEJEHySirxPRS0T03FW1c4OHB7oKPwsRSQDfAPAzAL4D4EsAPsrMXzt4Yzd4aLgqzvJ+AC8x87eZeQfgMwA+ckVt3eAhobiiep8G8E/O/98B8IFY4YrWvKaTwC+W69FIc6yLki5LRABz/25K1OGUnQbudZEC/XTrpd61/Y29/nV96Y8p2Pag5nz4bdoaT/neXWZ+MnTPVRHL2OhARL8M4JcBYI1j/NjqQ/oH5RRjZQoLQDhVKu/RsgIrBgkCpARJCSgFZtZlBelrPsEwA0qZr/uyA/jt2TKKwW1rumja9tro9YFo/78dm71HGCZv+922+zHZ8Yfmxs6Pj9R8mb64bdq+fn7zx/8wrEzjqojlOwDe6fz/DgD/4hZg5ucBPA8Aj4m36dH4gwKGhAIMJw7YT2oMSsV/c+uNXbfteWXI/m/aZuYBwdh7Bvqh21+/fyRAQu3L2PbZXEuNNTaOEQz67Vc7q9ZxfAnAs0T0DBFVAH4BwAvZd7scJQf+5NmJtytYMbjVK5Zbtf/ElHvF+4+FoDDRSjl8oG7XyFvhtozTX2bu9UWvelO3Czsv9q/tD6vhtRh6HEflLSJ7a3bJCWDmBsCvAPgzAC8C+CwzfzW7AvfBm8GNUb1b1vTB6ZBHPPajvAc0A92DtVA8IMJe3a74sBzL9CNIvKGHn7uIJsAn2BCuSgyBmT8H4HOTbkqsisFAAqKom3RXPfInlsSelfsixeoUI6szSli2XsVgESln++O3YfSa3jj98dm+x373x5GJTo8awZURyxQQ0is7OnBH0dwXdpRdILwKEyszqKBOQYRgiAg8xhBiD4wd3cX+5aGO0ymuqTrtghLoKdxWUU9hEcTCiCiGcB5WzCKx32PKsV82YwX5VkKvyoniKjauvJvz9YmHgUUQi0VS4ewKWf0jYCUBfSsi9LtHWIOHKbzl71hByYcuSPcpglkcysLX4RQPRRIrAJ5CbMsDwUXSU6qtMp1gMMshlpwVbyZHixmVJJiYYjzQCYye0CtHjqnrEqcME8tA1/HGEuWOcxEjGMVdHwd6S0hHiv0WwXKIxV3xroz2wBEZPLutGMHATLgjymyZbC4Rc/JNQYpDApqIJ4grO0YW/f7liMpFpCgQ0dAEBfa+Df+6UWJ7/gUfCUuhqztUVqnO/Q0hBu33uETIH5OL0Li6Rvqe625+/L52gxJd2YG/Jti26M33fk7S5LAIYgGwH0D096GHll1O1P8h2VSQYOD7ZrgrG3SsBdqI1TtAikhyPLQjzsIY5+u5+GdgOWIohAAbJykBDky2uxqnOK1SD1apsL7hE4pTR09/iXGdmHgKOCO7JsdEn6cvhcx0Zga5sTC/7hHzeTnE4j6YnuvekatSmDIyuro7OPGYHps1kxXSPfas3gsuAvG2Ag99QDA5mOKQBIL6RleO1d4yc8rYcMdcLINYMhRG17kFWAUtotwFVmXMKspSWENteNzLl/sE6AUA7nM9C/uwPa40aDplSbkcSgjNGdz4UYjDXsJ3swhisU45ANHVmHRupVzo7nU3bcFc99mx20bndbWrNJYyYSFlXycweTWd7yLwALM81yFC8etS/VjXQL8bc+lniO5FEAuA/oPNNTedVUIyMDm2jBJ7Ge6KLzUkshDBQNKefXtu9T2RC62kS0dnaludl+L32X8wwnvomWMejC0FN9rt+Kv0pUfNz5KBofPMedhTJtxFIJi4/81x+1sPrSUU4/EkZoAIVBb6WlGAikLf2zRAWQC7WhNN26ZX95R+9+5zxJFPiDFTe4Y4Wg6xpDLUZkSBiUjrCz4ncMVKtC8RwnMJxeUiUmgCkRKoSnBVgssC1LagzQ60q8HbLbDdAXU9FH0RIgnqUyE3v7tofKstZGFN5CgWyyEWH4lEoqgb2+oI1uoZs0ZC4iSCjiBdQrFcpCiAQoLLAqhKqOMK7UqCWoa8KEHbGnSuOREDIMtlDIKONMdyG3YmwRn8kEMoHcNyyYncZbnEEsHgodpUgLksPFSnDyFMbq8wxCJBhcNFVhV4JaGqAmot0awl2rWAaBiiriA3LYoHK4j1CuJ8A95sQdstuGn6osnPYoOX2hBTYMc4RMCU7iGmsHtYCLHwcMI8rpD07irWIsfzKfTiP9JbWbZlz+IIEp6UmoNUpeEkEpASfLSCOirRHpVojySaI4FmLdCuCG0FkAJEA8itRHUiUR6VKO6XEKcFWBDoYmMsNMtlAlFjdzyXiDNx22qx44pgR3zlxIYWQizoydeQrI6azr2oa2CFmHqT3KMXGxpmuRGRFjlVBZQFuCzAqwLquEJzXKI5lqhPBOpjQrsG2rUmFkATi6gJ9TFhdSRQrQSqUkLCWCPMQMBR5vqBQnOS5ZMx4opTeoytKyMMsBxiycBoyqEPdyWNheRdH4ohmK5qGNO8LDQ3WRdQ6xLNSYHmWKA+FqhPNEE0xzAEw/pG1hymvEWob0tUJwJHRxKrSqJYVxCn5+DTU3DdaIsp4AcaxKYmuhZ6imxgnjole4RgFkIs/SBhSC8ZfcBA0BvaheJ9DuSV19WZPBlg70izdReao6h1gfa4QnNSoD4RaI4MkZwQ6mOgucVojhl83AKSQYLBDDS3C+weaKJqVxKqWGNdChRCQDADZ+faYtrt9u36ebmOWPW532DOImmXvTkNOfYSWAixGCSDegGCca2CyL2dCQ1LDOlVGSwjSFs7qz1HGRDKCdCcMJpbCrjdYH2yQ1U2WJUNAODBxQqb8woXxyVYSgACoArrllHWDcg68Cyx+OOemgbhzk3KO2uV30hil4vlEEsoKJaapO6BDpXCYaokAe24X4EEgRUPdzeuVsbqKdGupFFgCc2K9PeVFjvNMYNPWhzd2uKJ22d4bLXBY+UGK9ng9e0x7m2O8cr6FjZ0DLAEKWGspTVE2wJNC9rthuLWjQZHErOD8+Ynd7s6nZfYnpMrvAhiIaQSdcYdciHltRu8ECCldMZbyjdh/A6WYACdDkFVCSpLqMoQylqgXRmLZ0VQFdBWmlj4qEV1ssPjt87xjltv4O3rN/FUdYo78hz3mlt4rT7BN9dP4ev8FLbtMUQjUGwEygcVyk0DcbEFS6m5DPaiJeqWD/hieuIX2Ft5biqqO0+KQWEjbIBFEIuPrJzVSLR2sGfZncyxYJnPuo37HmWh/SgrCVUR2hJQJcAFoAqAC4YqAZQKVdXgVrXFE6sHeHr1Bn6ouosni/s4VWu81tzCSjQ43a3wD5sC9dkK9RtahylK2cWXusQrE7V2CdjOS8q3FFo8Ma46xYu7DGKxppufbJTa/O3tfxmsMs+iGSCklxjWTUJ7f8m48bksoKoCbSXQlgRVEJQ0hCL1B4JBklEIhUq2WIkGd+Q5nizu453yAc7FBZ6UpwCAl2/fwb07xzh9o0K7Eub+QJ+6fom94h3byObPn3f/ACT2HMURRSksg1h8hBKbYsqa63DzczqsjI7tOYqlJyqhdRX7KSS4EFAVQZVkuAlpQhHoHjYRQwiFgjSxnIgtnhTn+AFZQUFhwzsAL+Mfj5/Av9y+g/u3TtCuK7AgxI4E6aVJ+HudTcR5YFr74wzlG89w8C2DWMzRF0nKjomQ2KBj5VkNzeJom9QRCheWo+yJRCtb0AEfBXArsGsKbNoSF22JDZfYsoSCgoDAmoDbVOOOvMDtagNRtYZDOcTibVbv5/kEgohWwffnb4wYZuw8yMmEuHL0kp9iIXW78o14COa9KI/7hFYW0Fk8Pvfqr1AbgBNgKaFKAXYIhQX2qcAMUEvgncBuV+C8rvBGfYw32mO8oY5wqhpsuTEEo3AstrhdbiELBS78uox4DW1UNxaan3HXK5vKB0qkiA4S0wNYBLEAwCC52SZeexvG/PTK/f0jgUQrl50tJD7B9A7aMdtAWAqgEFBSix8lYcSG01UGqAXQEJpa4mxX4c16jbv1bbzW3sI9JXHKCltuoACU1KASDWShdJ0FgQvRcZdUdn63hWPuSQqpXQkjWIYYsvADgQaxwYxuYve5VGcamxiUO+n+qiMCCQEWAqrQXEVJ0svLbVZpQqGGQLWA2kqcbSrcrW7hn8st7hTnKKnBW+Q5HqMt7vMx7rW3sFMFiBiqYG2Cr0yKg5TaCkq5A7q5yrR5LWLKLvAoxYYcwrAZaMCeWwixz1VxJjLp7gbCfhXXCnCLusd9GfEDIbTOIkmLC1dXYcNRGCBFIMUQDdDWArttidfPj7AuHsORrAEAj8szvEWeY8MlXm9OcNGWALRF1ayBZi1RlYXewdCKcYIJHPFl56Qn0kNWnzsdEzbtL4RYPF3BToT9OzKg1Nktrn+CHB2mN6nWQ2oVRqsXGeUWksBuHxidUgu2nMWIia1AW0ic0wqvyhMUQqFlwlvKCzxenKFmiX/dPoYH9QpKaXGmSoKqCFwaj7FU42K1G2siSQrIV2QfnUCigfWNtG1/9URk+GjSEjn+iaxycp+7YtIkWUqto0BHj6k1JjJpkcTSqDgtAYLBO4KSEg2A+zgGM2HbFrhdbnCnug0BxqvbW3hjc4SmLkDKKswEFgIkJIgazWsHolHPh95ekjEnIZPZ+61X/pHZZGZhXd1GdkfzYWO+E/d/Gx8JiKNgbiuMi98QDJcFUAiHWBjUkq6atH+HWnNrAwgQmBiAgGoJjSK8qQjbusDx6givr7YohMKDXYWzbYV2K1EqAGTEWyFAhQTX/fjNvotqP5522Hd/HoKhkFCcaGzDnsFyiGVqZDVQfqDD+AG1VF32/s4RpzPiVCm0HwSas4gWUMYtIhp0EpQVwK02oduWIVpCqySUIpzXArtVifNVCSkV6rpAU0tgJyAaw7EYWh/z9v8Ec1FCC8CO1RM7A4IJOTX9BKnYNI0VIKJPEdErRPQV59rjRPR5Ivqm+ftW57ffIH1e/9eJ6GdHe9Drzf5hBbO37N4cX4e5RLph3+Np6pZaoeZCgI1/BbBiSCuyOgOOIXaA3AFyCxQbQG6A4oIgLwjFOUGeStBpgeZ+hfP7azy4f4TtWYXmogBtBURNoJb1p1FA0+wfrvWp2DiV62MKuBZcTjHI9BOBjwMSNBonyjHW/xDAB71rzwH4AjM/C+AL5n8Q0XugjzH9EXPP7xPlxjT3fgTyd/YNet0/MsIiqMOEJsCuwFCOr30Q0pjNUvR1FpdgakDWDLllyI39YP85J5RnhOKB0ERzvwROS+CsAG0k5IZAlvAaBtUt2Owx6uZD6p0E7rz4Dz/qULMWnTu3sdBABkaJhZm/COCed/kjAP7IfP8jAP/Zuf4ZZt4y8/8F8BL0Of5JEDJNOOMKZ5t+CAQJZvKRXLAOORWcQGLWm8mY+wRjraDugWtuYzmN3BpOc6a5THFOkGcC4lxAXJDmSltNdNR4SqvzoOPzsff2xsezN62j8+JyqQTm6iw/wMwvm068TERPmetPA/hLp9x3zLU0fL9BKvTupgO6JvBYjm1XSSCS7Uaw3W4xgxTvTWTXZNZ3QDB1vhfWBpHpj1Z+uQaEBFQLqIagSmPxEUFuSBPWjiFqBlqlx2N9SkCQYHrzYBTV3jUAsW2tk+JHHg6t4IZaD5IzuWf308ne6RaBOxnRhB0nfJ8NqxjafcPM+0Gw0SWMFYRCW0QggJjAbOJMlliEvaY5z/460LbUeXrtdc152BBLhm/lMttb3fEGcJVbQb5LRG83XOXtAF4x10fP7Ldg5+z+O/IJtt5Zr0ywcXKtl0HFI3mnsb3AveBcq1d5oz+iFmChxU4XFyLW3IZIW0ICnZhioo5ImEwKAxjEmmAsFyougOKCUZwryE0Dqhuotu0vmhSBDDbY90MAU86JyRHdcwOJLwD4JfP9lwD8L+f6LxDRioieAfAsgP8zWluKUHxl1FpLMtB1d/L88+YSlkAPVm9RCmhaiLqFqBVEy5ozKO6+i9bqLpqQrN4iaza6C2sFuNNh9gqwJZTyglGct6CL2ttAP8JpAgpu7+Mqw275S2CUsxDRpwH8FIAniOg7AH4LwG8D+CwRfQzAPwL4OQBg5q8S0WcBfA1AA+DjvN9uFwUjEdsABtwilKzcbfkIYcokMesH1jSgXQ3aFBDCeGsJ2oy2XMOGrEjHhnwdQXtlARZsbgZI7PvYcZXzWm+eb/aKuxs0TK367NjOmNs/Q3SPEgszfzTy009Hyn8SwCdHW463513wkn3c7DBHKR5klMXEkKsEhsoYa4t2NSAEBNFe0W0ZXArtqLOvL3IJx+2q1cEYAAgCrCVXVx9QbBhy00JsDLGo/roajaID/fcUWAjqQiYhZTjo2c1YUMvx4PqUH6N0l2DsNRNM6xEMMJwAz1tJ0iljLaJW9XQGMrv1hNL6i1oXICUB66wjMk4w7L25RCCwTuiG0XFMwBFgI8qA4sLoKpsaMDsSB2P3x+Bbc24+jh0Xa19RcIPdJbAcYknFdizcHFRllrIboXYJZgRJb6VicNM47Ro/S6uJBqoEVKHzcAXpFAZBXbYbkXnYLZmAOe8D5wyt49QMuW0hLmrQdgfe1f3DAUMLJ5HAPuau3++2jG++H8NyiMVgsFnKhRExduDMFNwOEZT1jtXjZ7VHYXYIEit9rorSjjOh9HdVSnAloFjombQEY5tSbEQTdVyHlCYUahhi24J2jT7kZ7vVnMV/6IlUSD8vN/kmFKe+rGPFAlgUsfjxDD/JSacvODc4Aw+tjLmyuasbmpvA6CzctiClQK0CqwpYlVAoICpAkQATei/VZEf0dH1qtbdW7hTErgVta/B2p89qqZuevtVtvY3sW3a35vq/DebC51ResDEHiyCWnrs/9voWu83jsuhtQMtz3nHbAkSgHboXohIzRKtASkG1BVBJAFLnt3Qih4z4ch6MYhMHUqC67Q4p9M+tAwIKbug8FXNP8k0oVreJbH3NxSKIpUMqFmL1kVhaJA9fwmB9DoNJd4/osvt9Y8ejOvewiQ/pYxEaUKMftGwqEFeAJCiYzDpB2vpRjg4DdBFmYRx+8N/VGNTdAkTijFFz10i81tX97P02TVVP3CA1M4ZlEQuwH4SDQaJyxGeQPADIcev7Z/6zXZk2/zeQCM7G/0Im54SkPuKLmkZbQ5JAxd4Jxppdaj3HgT46TIG2rbaAmqZ7WFG/iuuxDnDe0cR1P1/HTygT4hF8k1kiDzTXMRU6DAdAWORY3cDbWhFty24ZNVyGBGmxJCWoLiHqFhCauxBB+8cVm6RuE2OqFcS2gdhp97411VNtdv0SGD07L7jPOSXiA8nvMSyCWAAM828R8eZ68Pf6+pHXXlzEsRqSZ7XE/ByuLtG2QG08t1LqEymrwnAXgITs6tEWkFFoa2MB7YxvxXXExdodiSQP5ioEfxG6IZacdAgsiViAXqoAMELtRhSFzhXpEULE9IwenRUyXUPOPaDTY2hXA1UD2jUgE48R1mdizGyxbUAbE0KoG6BpwXW9V25zkSmCo+gWn7OlJiOzH1gasViEvLmhc/NdgnEHnzKP/fvHvJuJujrR2bZa96gbkJQQkqCs15Z1uqS4qEEXW81RLGfy/SohV7wbSY7Fd1JWXW8eHW+vI6478fZIbQWZA48bAQh7fe11YSKySg1P256YZghgbyE1reYYUgJiH87XebVKE8r5heYmTnvspFB2KZDuCykcC6h3KKKrDKcI3oZGUq/F8crEsBximeL7GFtJCfd4T8c5hN8GMByiBXb1vn7LCJXSukndgOta59jC0TEGvpR5PpCBC0Bf7JcJiaocxd5gEcTSc8qNHIQ33GYaKO8QTE9/6Qgy4joH8ojV33ZhCIKBfb6u7WeXG9Ps82TY+F1D/XAV+glv/QjPx973Ejanp+2VXgSx9OD5AAapB65vZMpYYzsBYgHMAGIWBxsn3V4XUfq4dmePNhsHHpgHR7DvuVHAOeYepR7qL7D30LpOt9AZL0BP78nJl3GxCGKxfhbAUbRGXnKQexZatg9lKgYWktl5oKQ2p1UBY0Pr3+1+IH9cqcBeMN8mYgUmzrNNBWfHXBMuFkEs3YqkoaLl76brwuwjkVVbfjISkzYIbDp96767HMZJa3QJZZDZ57fpp0AmzHntlY2b0oPNZs7vQWU3gWUQC7QuQhJxGd3pFAmWHJr0XISIJBS9DUy+vxWFVbO/33XRe/VEk5P8jLaMhGsXPhHHwgRTE6MWQywplttDigAytjkExVAGofi6wKg4C/pDvHt9Dsj9B9lbNJ5vydYTJYRA27H8nlwsg1jI0y2UNyjX1zBR5whGaWN1hF7kZGGVVRt0813zIeJKwb/fRobdXQktBtHw/euK8xEUNzNE9MycqUODsjs/NRcj3w0eWZWdXpFatf02en3MWfXAntimPEQ1DEAmOWfo5AX/k8AyOAuQdmPHLAbH/EwhuK1iirfWizYn++y1FyUSV6wMAoiOT8BTgt2XcHXiKOBcHIw38AaVR9J0jiI1oaHosg8vvB+6ng2bZun3bQZc/adLm4xksdlIfL8vngKs0nXsG97PU7DMyLwsm1gmImulxCYkFnn2wweht4DB0wvEMH+4B3dHAjOCJ0iOnRWXQFCRDfS5h0fGzwKkFdnYKg7lpI7BffgxbuSwbOvHYP+hxiLA9jdTf5RojGc3unUl9YKK0HhDL58C9gQeUNynKsoLIZZIpwOuaQD5vofYw4xZLkKA0QIt9ic1uJFg15HWnWxpHqR74qVr1SXTJbQ4ciPC6QSmxHj8ufDeJ+Rjju62EGLpE0Ivb3ZObQHulPK1dG06x7nbN4MEj/zsdCVGb8+qubdDIAbT9SOQhBQdgxu4dOqNwpjhg5M6XSV5BhZCLOhNQDRY58JTcEP7ny2C+6LdleXHWoxF4lo0AwJTZqNbu++7PdlhQKSeM8+Nf3UpmsA+jUIKDHJb9g33x2vH4P7tGo8Qeej3DE/uQvwsDpwVZLdyzEIo6yvierdtWoKxx5sCnmgwBGp3B3Rn//cU4n08iPz2nHY6Ylb6pROhF08MclsC7xuIj9/znXTRes46mTKE5XAWF5FQu0Vvr4zv/g6UZzanOZmHNNfXohsPsHf3VS5OH0Pj8vulq3Sus+ZW5JXp6vUTnHIgCO5r/ew4+grxo5iDO4gF9c+xH6xa6sdbgoRgdQ/7u/LYemjyUxwtxL4V6+hv4qBnVw/xo+m9ugzBJNudCkswzv+We3YppiNYHrFMReB1dyFMDvyNWCWd3tO6t3iKsn/dRcyX092UNp1nwbYTML1Ht5JgacTieVwpsio7OJ7NXjh/pO6eZzhyoE80092f6Jj73Hv4yZU7FgF2uV/IxxJxI0w9GGBMP1wOsYQyzk0cpGOXwH77hL/yZFgZ7rnDbVm4ek+gPDCa6d6JR9/gMNZN7/W67gmaMf9QLNXRnRcp8ziCQxBJjjHRQ7wca2iMtbqTlPPihkvC1ZG0fhSOyo7myvhIKNWxrLbOAxuIMqfaCDn53MMCRp2AHkaJhYjeSUR/TkQvEtFXiehXzfWDnd9PiFsPPSdW6ATsUH6tGPo79oVUcNLtJMZY8eD30MZ0a/aT2Jvf9mRN14R1c1YMEQ4CiG5CuUuobQt2Tl4IibxeX/y+++8/yCVA5HGWBsCvM/MPA/gxAB8nfUb/4c/vz1Hc3FMQ/N2FIYSue74R190/7JPYf+zt7oqMeXeFPXffO2I0gCihmN/Iih/XzzLVV2LH4LblpoBkcJlRYmHml5n5b8z3UwAvQh+x/hEc8Pz+Dk7Qa7ZDzpXFrl8hdGCyu7JCZ/Ha82g9ThLNV/E5XSwNk4bE5z+wgQfZP0cuxlU89JyAvovBwk8DCWCSgktE7wLwowD+Coc+v9/CDxxmvuzAwjVTO2Wxl8sSyncV4XwXQeGHZ/uVyse1eoObj+L7OgL97y2SgFNycNxXDCEHoHNCxRxkEwsR3QLwJwB+jZnvJxoN/TAYFfln9yOgt4jASiKB/sFytuxQ0RwQTSxa7W/ksvACd4MEaSRWt5uY5fbRF7W+pWade34bXT/k0Bq093roJUfZukRgnt3+JpBlDRFRCU0of8zMf2ouf5f0uf2gGef3M/PzzPw+Zn5fiVWYFbsiwBUDsTyUKXI85Q2N5KT2RI+7ed2/x5rLsfcEjKAn/lL9H3n1yyAAGbru15dAjjVEAP4AwIvM/HvOTy/gkOf3e/CJp7e63LemD/bc7B9gVGnzk6MDaQX+/z1Xfav0iU0RR5hWSofJRn65rn7/QamwDtPrU06agv14W0x6fycgRwz9OIBfBPD3RPRlc+03ceDz+zvk5mxYceISTc4Ki90/FakTGuxl8nYeekg7zDLmISTS5iLDEs05u/8vENZDgKs4v9/zPnr1hsuGcmT9LDU3JmJzSVzlM7RnKZVj4x2FnoVA1l/MkzzgADHCcK/HvMOhkzj9shmEthwPLvrOo5AS5if7BMuFHqA/8bG8lkAdfTPZ04tCXCmkmAJhKy/w/sKeeAqNIYCUM7GrS2S0MUL8y4kNIRKdtQhEl4Mrcuw8W93QsLy9xxNNo97NgRIb2aI6B6k84+T4xkVkECMieVHEAoQfTm+gVmFzg3NAMirr6g2xBKlu8i+hI/T67j9kP30zdlLEWJsGQRGtuLfhLGQ1BvuYmfKwOGIJrqTQaVChfA9XJ+nqG1pLXcKyZ1X4jkD/0J0Bxhxkfh99gkFe8LGnCLu6mMtt7SJyfDbJN4F448rhPIvSWWIJQJeKKEfeAR1sz5S3GHC0HPiBwquCu1E/FjA9MOjQof1ZnSB6FcAZgLvX3ZcJeALfm/39IWZ+MvTDIogFAIjor5n5fdfdj1x8P/Z3WWLoBovGDbHcIBtLIpbnr7sDE/F919/F6Cw3WD6WxFlusHDcEMsNsnHtxEJEHzS7AF4ioueuuz8AQESfIqJXiOgrzrWD7Wa4gv5e+Q4MAOEko4f1gfZDfwvAuwFUAP4WwHuus0+mXz8J4L0AvuJc+10Az5nvzwH4HfP9PabfKwDPmPHIh9zftwN4r/l+G8A3TL8O2ufr5izvB/ASM3+bmXcAPgO9O+BawcxfBHDPu/wRXMVuhgOAH9IOjOsmlqcB/JPz/7SdAA8Xvd0MANzdDIsZQ2oHBi7Z5+smlqydAAvHYsbg78BIFQ1cG+3zdRNL1k6AheBSuxmuGlexA8PHdRPLlwA8S0TPEFEFve31hWvuUwxXupvhMnhoOzAWYHl8GFp7/xaAT1x3f0yfPg3gZQA19Cr8GIC3Qe/p/qb5+7hT/hOm/18H8KFr6O9PQIuRvwPwZfP58KH7fOPuv0E2rkwMLdHZdoPL4Uo4C+kjNr4B4Geg2fiXAHyUmb928MZu8NBwVZxlkc62G1wOV5XdH3L6fMAt4J6iICH/wzEey6+d3C8c9xAkD3qwuxjzm+3XG3FVXIZRu1WOjSl3zBz5zb/f/HbKr9/lSA7uVRHLqNOHmZ+HSch5jB7nD4j/mL3vuHfwb+jUaaee4IkLztbTOadN9w4udjeCuadS+S+DSmXb241g7nZZvx5TzpbpjTkxXnd8wfq9cX2+/sw/xLp5VcQyz1EVmtAAAfXedhE7r8Up26+OxrdJxN4j6O6l9je6uYQS6ru/NzoyrgHsfZc5MDkGb5xjC+eqdJbLOdsCSjcJir4OJXpEubuHx3yCxOPfE7vfXOudte/Xm7NnKOfBRwhrcPa+f3TY2PhibWX0+0o4CzM3RPQrAP4MOg3hU8z81cybJzSkhqv3ELB9iO3SM+0O3uczwjlC4mKwmn1idURPsHywe9y14ZZPvX8oB1e2fZWZPwfgc9k3dBM5cSddhi7Qv5SzQTxzNRqCSbUX7hYNvkdFEPoPuUcAkfH13loSabe7/5F7CTg5g4y8A8dOWGxyk5PulXH1jl75qVs+Q+9M9FZzr914x0BCJQnPJ5i5/Qu1nYtlEEsmYoTgr9SYpp+NmP6T+j3RxmT2f0hl1iEY24+57xu67qizBic4QmK1h5TeWD0DDmLqDbLmUP2h8+fG2hkpFzOPZ8HXh3zdx+1fQPHPwTKI5aow9TSDiL9iQDBIW2djbftKp6uQjtab6vfA1A8oz6ymGREOlkMsmT6WyxxnEVr1uZwgxMH8ewc6h2fyTmkv1P7oK+t8DkXU70f3d55FtHyd5TLye8ShN/XBBeV9zIEXQKq9lF4TItSoyd67Z3iID4dOGM/EcjiLg1w2PHeVHgwZ+tQssYL9HKSUZv9/9+NU1OMuIZGauyCXw1m8wUQV17G4TwZyrKosJCbZj8lEHWs95bPvgMt1xsXEayy0EXRTZGA5xAKP6r1B5q6w2D255VLI5mQx66ZbEIE4krnPiolQ3/1rY/1J+XvmiOLFiKFsEzXjjPlefc7/l3F157TX1yMSQcUJmKuAm5uHZnPOfREsg7PE+htIJXDfqXwwcZKBqat4YHEEOGVKFPge6xSCoQB9YfTeKVgGsbjzHHBRu7kbVq4H5X7u5IyE5i9NbJ4YjdU/JlpzPL+p3+dafTEsQwwNsrvSvpQ53t4pSPpPQm3GfEQJrjGl7Sle4Zz65mIZnMVgkNU18vB7rDyDUFLR2lRfspAI2oX8M1OVZW5hAo4jXM87NDlcZKgw52BRxOJijAVH5XS/kK0sq81Dsu1Zosy3orqEK456XbO47AiH6/U18bKfxRBL1gOaqrCNWALZdWREoZPZernwUygTHHMSYY9wvVwsg1iCeT/haHB0cAfU/IMKdCTPpiMU+zpd90WbbWZSuE8ggw6lOezAAsog0NxovYtlEIuHKdSeUuqmioLJ95F92bfQxFKa6VQMblu94aQFgHbWw7FtpDhD8p7oz496IDGQpGMxx3ub8/vc+0jqAB1VFagogKoElSVQlTrbv66BptGfugGa/nYRwOMIY1s55nDNBJH5iyJ3npZDLMBAkz+IkniI/jiwuglJAVqtQKsKWK/AVQmuShAzsN2BNjtwXYM6JZvNK2lEmGAiuFQZT3S7/qo5c7wsYrHIYLvXEXF2CQWWk6xX4FUFXpdQqwLEAEmjw+wKoDAf1YLqRhNM22p9JjP+FcLAzTAFzAACCecjWCaxzHBmjd0bLz6SXe9tx7CEQkUBKgpwIYFCggsBlgJMAIoVaF2C6hXErgF2NahpgaYF1TV4uwMuLsCNGuUcs7iBR4Tsv+CcIpbbCJZFLGOy+1AI7MfJ6ROk1BxDCsMx9P9cCPOCTIIqBVjoN6qKliG2BURdgeoWaBRou9NvJtvtAI+7BBOqpzgb3fK9QOaItZWJZRFLBHMy2pL3RRRpX/y5kfDO4qkqraesKq2nlAW4EFCFJRICF/qvEgwmApcK1EhAMUQpIdi8p/H8Yq8MT8CoIy6UY3uALbCLJJZDeVJzlMNBhr3LdUhzC5Jyb/WsVppQViW4KsClhCqNGCoIIIBa3r/AsyC0hQRKBiloLgRAEEEUBfj8Qoukdr8Ntp+3MiMNcmQn5VwskliAeKT2IHmz/YbMX2vGUl+RFUITymoFqkrwkSYWtdaEwlJ0nITtQ2aAGGABfV0CbB6SaMzDEppgSClw0wC805aS528fpDKEIuwxQgqFOwL3P3LufhcPzdLpmdlWgTV6SVFoJ1tZgSpt+XBVglcFuCqgKk0oWkcxdfh6ltFdOoIRBC4ApgJcEKQUkEQQAHhTgLfbYb9SfU8531LJYzGMcJ3lEMsB3fUhp16SAF1/hJRa7FQVaL0CjtZa5JRSix1j9ahSgCV1xEIKHXdiocURCFpnMQSjJIBCQEmGqghFqduV2Dv6fF/MMKnK4RYj22di4w5ez5j/5RDLZTFHHvusmMxrfaUEpNC6ydEK6qjs6SWaUxBUQVCl0VMagFpoAjG/AQCxVnKVBFRJUJJADID1vUwrlFKgkALUtqDdTouldgaHTSWzm8j1ZRK7HgliGd35d+j67Yu1SRNFRxyl0A9Y0v6v1OVIMkSrOYkqNMHo/um/bUVQJaAKdCKvWRPaitAcS6xWEhUAwQzebKAuNoCaYCWFsgt7v4/4VjLm8ZEgliD8wc3YIhIqZw4H7im8WonVXEWVAqoitKUVL6aYJChlrknsOYvS9KJKoF1ZQkInuppjgtwxVFGCmmOUuxpg1t7euhn0sxdPmrFQQk69RzM2lIsDpU8G61XmKbat5jCtIR7sFVRVmIduFFj3PEMQoKQmiO46aUJpK000bAgG0KKLGgJYoNhUkJsTSMXa67urgyGByfrGGNfJxKNJLAdI5AGG0VdzEdwooJba21qVoFKCSmklitPgnktoZXZvLqP7romjrQBVAe2KNcGVpu0WEA1BtISLrYTYrbFqGfJiA9pu97GkRKDwYWFxxHKZrZ7Z8CLTLtF0sZidjuFQUUCUBdSRM1XMICYoQxBWR+kIB+g4jOUk7YrRrgC1UuCKgcr0oSG0DYFaCbEjyF0BuV1BvrkGnZ0DSk06LTXHETkXyyGWmd7FWdp9L7+1n0fTdadtdfxGClBZQKxKFCuJRkiogjSXseax8aNYfQQOEamK0VZAe6LAxy3kUYPVqsbxqoZiYFuX2G5K1GoFuZGozwjNkUS1KrVVJuwugYS3bO48TMQosRDRpwD8JwCvMPO/N9ceB/A/AbwLwP8D8PPM/Lr57Teg36LRAvgvzPxnl+3kWFQ2dyPW1A1Y3DQ6N6XYQBQShQCY1mhX+2ChFU2WeFganaXU3ERVjPZIQTxW4y2PnePJkzO8dX2OJ6oz1Czw6uYWXjm/jZfbO6jP1miOCGpFYCmNOZ+/ySwwOQcVVTmc5Q8B/DcA/9259hyALzDzb5uXODwH4L8S0XugjzH9EQD/BsD/JqJ/x8zpZZFADkuNEUxqc3pW3W0L3mzBRCDjaZWrAtRK2L1OxDAKMHU6i6oAtWJNLEcKdNzgLXfO8Ozjd/FvT17FO6p7+MHyTWxUiW+vn8I3q6dwtq3w5v0KzbFEsxJAIQAhu2g2sUPsiQSvwZgiBJO7D9zFKNnxQ365pLsb7zL7fC8tt3l/cjc3DXi3A11sIc9rFGctigsFuWWIGhCN+bTorCImgAsGSoVy1eDx4ws8vX4Dz6xexburV/Du8i7eVd7FO6rX8PT6DTx+cg6cNGiOGc0RoT0qtXJdFMPxeMdoZCWEH2B+5uosvRc1EpH7osa/dMpFX9Tont2/xnGP+nP3MPc4SmBftPmnU2KzVtMgcYi1Gb2rASFBZxuU6wIstTOtWWtHnBAMFgRhLG4wAAFQqbBa1bhTXeCp6hRPl6/jB4tT/KBscUYN7hdv4s3yBG9bn+FfTu6gPilRHwu0RwXKVaVNaKJRJXcywUzMkgMOr+CGhGewR/7Z/bMaC20XiRy812s7NEmhspaF2yRsItBFBXlaoiTjtidpTGcCE4MLApVaPOkuMUrZYi0bHIsd1lRjTS1KEEpirEWNY7HFSbFDVTbYrhhqBa0XlQWokGAR2N+dmJOlWUPfJaK3G67y0F4u6XKHYA5qyKsb4hQhpLZcsAIz6SSlzQZUSEiyvhQCICCMv8TGgnTWHKFdSWyOStzdnOBbqychSOFfmwd4izzHhku80jyGf969Fa9ubuFiU0JsCdRA113q1E1IqeNWIr4PKRg4nbgjcwxzieUF6Bc0/jaGL2r8H0T0e9AK7sFfLhlKIdQnK0wM1Y+UHcaLGKy0dQQSEMyAdcBxAVELTSzKikUd91FHEtttidfOT/Bt+QTOmhVOii1WQrvyL9oSb9RHuHt+gvqsQnVBEDUbYtH5vTpLT+g9SNHz45wxsOovqgNZRDmm86cB/BSAJ4joOwB+C5pIPktEHwPwjwB+DgCY+atE9FkAXwPQAPj4ZSyhJKaImMRv2WzbtlfXWq6ygpAShRAgBlQlIHZCW0qmOlUQuBCoRYV7LWFTF3h1fYJSKEhD3K0S2DQFXrt3C/JeiepNQnnGEFsFsiGHVJ9cQsjY9noZjBILM3808tNPR8p/EsAnJ/ck84yV/YrpT0hyu+lIXalTDgZKcatA0PoLLjYgKSCV0rm1VQFRM0QtIVoBagG5IxTnBeoTibOTCg/WJ0ChQJIBBrgWQC1Qvi5x9Crh6FWFo7stqtc2EPfPwecbcN0Et4641lDo1IkxcdUr8z0bdSZ71r1HEL5ym8N+HbY9mDjHinJ/4xbAbqf9H4COEJc6LCB2JURdQdQScitRnhPq+4TmmNCcSLQrCVUwuDRxoR1BboHV64z1vRZHd2uU9y4g7p2CT0/Bu1pHn2PJUN1wvZCFN44sjBym/GgSSwxz2O/ErRb6e2viR+YhNC2okEChiYW2LeRFgfJBAVUJNGuJ5ligOSI0K53XwpJALUNuGeUFozptUb2+Q/HGBej0HPzgzOS08N7nkzWc6Rlw+7IUsV01Fk0sUX0i9YAnTE40/TKzDm5bYAe9/8dsFcGugLjYAoWElBJcSJRVCXVcoj0utUksdeIUNYxi00JeNBDnO4gHG+D8Arzd6Xzcdq/uheZi1BmZ2Occuz+FZRFLxC3dRYIjjqSks81nxYHz5MYODYqe1KAa/UBJx4rIxHG60kJ0m9KKoyMUJp8XQugQQt2ANlvw+Qaod1B1Y1Iq2+BDdftyiMDh1FjZcoglZ8vqJfNsYzrNmMc4ukuw02mUlkwOQfZ0CClBrQLVdec3IcDEnjY6/mT3DVluksjZcUMhcwh9ahmL5RBLLhIEM1uxy0DWpAaUUFYMgkl3aFtNOJYDsfYMc9NMXuVu/bH+jiFlDYawDGKhtPk6gEMwl2LHBw7h6yoDTsMW2qkXy6nNOAosOM5L9N+vr/t/8ZvMOCMQNhJmt//3EppD1zLPbwk9nJQCPCXtMScHJyudItH/1Hw++ic/TUQsiKgfaJ/rhIjIrSfHyoiVnxO4C/Y9s/3LINruiA/H4pEklrl5uoeKxkatryvQlXpIcMU5yUz9qh8VnWUEfqQ55StwzezLtBXCHFEZq2cW8Y6InTGFdSD+MjmKxcPdS3BJJJ1QvpIY8HpO4kgTiM2mbLqfMUyxYnLrnIPe/I14sx8JYglxlFF4mXexNM1JGXPub3OjuiP3TUknDd07q3zmWJYhhmKLxnN+DW6LyelQimZiu2uovn43AnpJAln+mBHOlWMxTcGonpWBxXCW4a7/dHrkmMXSuzd0fFbmJMU5T9g/cikl2vZ1oi7ht+/rdJdVfi2WQSw5fpa5bL+XHOQkd+f6SqL1DhXr7PvHDtrJSF4/CCbO6TKIBQiLCX9SY8lOAfQUTpPDGiOSfhN+OsKg4uQ9sX6k7h+r361rKkKcZi6WQyxAmNITBBOvJqzfRK2KAPtPEqMfnLyqUx2CTactr5TeNQg+TnQvLEPBtQi56EPwFMQsn0Ki7CgyiSG08+AQYiTEGca42VhG3Rwsh1hCgcGQR3TCKp6kQwQ2yh+SY2TFemLISKeYBHdeJ3CXZYkhH6HBPOQzSQAYy2cZiualzelLhCSWw1ki20yDyMxy8ycmyJ5zotCUUFAzlO7sXJgxK2lKfTGM+ZcWn6JgkUidBMKsfNQD67wBw61jcF/oQbniaWwlZpxWkCo3el8GcvSZuXUDSyOWAGaz3W47asb9meZsUO+YsqPgwDpQdrLYoBvzvMPLIZaRVMlZCuLY2SWJdic92ES+bLTuSAL5vkhejm3oHvf3lFU0VZwtg1gon4XOSbWcLN8nZNPlRLaD7WeKt8vGiFLzOjVGtAxicZDcepG+8ZCdCF7OWe1z6j0EZs3ZRCyOWCYhwgGuJJ7Ss3oOu2tg2FR+/0edl7qRZPuxvGIfCyEWmqQoHjJFchLsm9tzLBpvPFeRvJRFKPZ7wlLLNZ2X45Sb6FI3/1xRZ2KNx2MqgziNl2ZwmaQmF4cKCgYxMp8LIZa8SYxOzszs+Ox0xYC4m/ugUkQT+m2qmMoNEg4WXcbCWwix7DElN9X8MKudbCJJcLyxOlIPNYfL5Fh1sZxd82Vy/CeFhegsaUzhFKHvh9ZvYu248C2nlLWSq4PFys1xFfT690jl4BrMysY3ytuUjWGhtoL1zkTqgV61ievrNLl+q5y5XpwYimJChtt4VSO6ih/C91j55IeboT+EEprGxFjOAgjGpiK5w2PjGiUWInonEf05Eb1IRF8lol811x8nos8T0TfN37c69/wGEb1ERF8nop8da2MqYpHjnAH7ImTKXp9cgkkln0/dY2THFBvb4LcrzNrL4SwNgF9n5h8G8GMAPm7O6Lfn9z8L4Avmf3jn938QwO8TkTxUh6PR4oQymssJYg8u9mAPFt3NiWH5cFNBvbTQqyKYUWJh5peZ+W/M91MAL0Ifsf4RHOr8/kh2f3bsYmSyU6s4J7g41Ufi+lauDL1dC2L4f+o+5/cpXG6SgktE7wLwowD+Cpc8v39wdr/FhEyunmy/hHkYDNZl5rIcNM1xKjKyCHPN7YP6WYjoFoA/AfBrzHw/VTRwbdBjZn6emd/HzO8rsTIXhxvLYjI7x3wdQyixOpr9jyvmFJdASA+K6U1zCQXIJBYiKqEJ5Y+Z+U/N5e+ac/txsPP7c1kp0qImOiEZdfa4zEMIJ/iLYSpBzhWt3g1ZbeVYQwTgDwC8yMy/5/z0AvS5/cDw/P5fIKIVET2Dqef3Ow81ukImrPSUBZHuRnr34iQE6hhYMIFTIOYgpS9luQsSyNFZfhzALwL4eyL6srn2m7ii8/uTFkdGJDXr3gl9ySWq2WkFsTFdUZ7uSMeSP+ec3f8XCOshwIHP7x81TSckUPv6yKUfagC+3pTM4otXkkX4qbZjCDntxsITKSzK3d8hldvi/eZPQCqi6/zTry9Vttd04gXcl/HxeKbsrDoyESKY3PqXSSxjImOOE8u/f+aWjJR39nsdiyIWn+pnJ17nJG8fWCe4bHpjVj0HuneWuMSSAokZ8Z2piUrJXI+ROnNzQmL97e4fOZRoMcjo23KIJYSpZmtKz3kYKZg5xLBEgslNaeWr3PCdCSJ6FcAZgLvX3ZcJeALfm/39IWZ+MvTDIogFAIjor5n5fdfdj1x8P/Z32WLoBovCDbHcIBtLIpbnr7sDE/F919/F6Cw3WD6WxFlusHBcO7EQ0QdNYvdLRPTcdfcHAIjoU0T0ChF9xbl2bQnqGf19OEn1zHxtHwASwLcAvBtABeBvAbznOvtk+vWTAN4L4CvOtd8F8Jz5/hyA3zHf32P6vQLwjBmPfMj9fTuA95rvtwF8w/TroH2+bs7yfgAvMfO3mXkH4DPQCd/XCmb+IoB73uXDJagfGPwwkupx/WLoaQD/5PwfTO5eCHoJ6gDcBPXFjCGVVI9L9vm6iSUruXvhWMwYDp1U7+O6iWV+cvfDx+ET1A+Ih5FUf93E8iUAzxLRM0RUQe9kfOGa+xTD1SSoHwAPLal+AZbHh6G1928B+MR198f06dMAXgZQQ6/CjwF4G/Q23W+av4875T9h+v91AB+6hv7+BLQY+TsAXzafDx+6zzce3Btk47rF0A0eIdwQyw2ycUMsN8jGDbHcIBs3xHKDbNwQyw2ycUMsN8jGDbHcIBv/H9VA640z/bmaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.561, loss_val: nan, pos_over_neg: 1.0406272411346436 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.1437, loss_val: nan, pos_over_neg: 2.7838664054870605 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.895, loss_val: nan, pos_over_neg: 2.958254337310791 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.6375, loss_val: nan, pos_over_neg: 16.179611206054688 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.5539, loss_val: nan, pos_over_neg: 17.238676071166992 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.4695, loss_val: nan, pos_over_neg: 20.072927474975586 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.4008, loss_val: nan, pos_over_neg: 36.53895568847656 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.3493, loss_val: nan, pos_over_neg: 51.54439926147461 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.3351, loss_val: nan, pos_over_neg: 57.25225830078125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.2888, loss_val: nan, pos_over_neg: 73.40570068359375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.2712, loss_val: nan, pos_over_neg: 89.42797088623047 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.2624, loss_val: nan, pos_over_neg: 102.28662872314453 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.2387, loss_val: nan, pos_over_neg: 108.83406066894531 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.2314, loss_val: nan, pos_over_neg: 122.15565490722656 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.2159, loss_val: nan, pos_over_neg: 138.1888427734375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.2031, loss_val: nan, pos_over_neg: 288.6463623046875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.1917, loss_val: nan, pos_over_neg: 290.41534423828125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.1747, loss_val: nan, pos_over_neg: 242.0494384765625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.1801, loss_val: nan, pos_over_neg: 229.5741729736328 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.1568, loss_val: nan, pos_over_neg: 174.9648895263672 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.1423, loss_val: nan, pos_over_neg: 267.6479797363281 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.1402, loss_val: nan, pos_over_neg: 281.8436584472656 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.1298, loss_val: nan, pos_over_neg: 553.6534423828125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.1186, loss_val: nan, pos_over_neg: 594.5422973632812 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.1328, loss_val: nan, pos_over_neg: 482.88531494140625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.111, loss_val: nan, pos_over_neg: 405.0965576171875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.1086, loss_val: nan, pos_over_neg: 401.2702331542969 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.0988, loss_val: nan, pos_over_neg: 581.5630493164062 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.0823, loss_val: nan, pos_over_neg: 1654.7904052734375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.0914, loss_val: nan, pos_over_neg: 4858.28173828125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.0872, loss_val: nan, pos_over_neg: 714.7122802734375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.0778, loss_val: nan, pos_over_neg: 1275.96728515625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.0677, loss_val: nan, pos_over_neg: 455.51361083984375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.0575, loss_val: nan, pos_over_neg: 576.1466674804688 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.0515, loss_val: nan, pos_over_neg: 658.7351684570312 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.0421, loss_val: nan, pos_over_neg: 1438.4547119140625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.0495, loss_val: nan, pos_over_neg: 415.7337951660156 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.0488, loss_val: nan, pos_over_neg: 311.3133239746094 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.0504, loss_val: nan, pos_over_neg: 462.8885192871094 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 512.9998168945312 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.033, loss_val: nan, pos_over_neg: 283.20068359375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.0357, loss_val: nan, pos_over_neg: 431.7403869628906 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.0191, loss_val: nan, pos_over_neg: 474.0910949707031 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.0199, loss_val: nan, pos_over_neg: 826.376220703125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 1117.655029296875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.0118, loss_val: nan, pos_over_neg: 589.2134399414062 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.0083, loss_val: nan, pos_over_neg: 967.7493896484375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.0239, loss_val: nan, pos_over_neg: 676.8668212890625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 697.9849243164062 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 589.5675659179688 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 343.64117431640625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9921, loss_val: nan, pos_over_neg: 561.0369262695312 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 858.2838134765625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9917, loss_val: nan, pos_over_neg: 587.3544311523438 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.985, loss_val: nan, pos_over_neg: 640.604248046875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9882, loss_val: nan, pos_over_neg: 805.6868286132812 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9857, loss_val: nan, pos_over_neg: 1245.908203125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9871, loss_val: nan, pos_over_neg: 780.0598754882812 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9761, loss_val: nan, pos_over_neg: 452.32275390625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9711, loss_val: nan, pos_over_neg: 883.5457153320312 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9698, loss_val: nan, pos_over_neg: 1065.2318115234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9639, loss_val: nan, pos_over_neg: 835.459228515625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 599.2987060546875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9612, loss_val: nan, pos_over_neg: 1152.0286865234375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 494.8822021484375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9597, loss_val: nan, pos_over_neg: 752.4922485351562 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9629, loss_val: nan, pos_over_neg: 353.690673828125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9613, loss_val: nan, pos_over_neg: 281.2787170410156 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 480.4079895019531 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 432.58380126953125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 516.8991088867188 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9524, loss_val: nan, pos_over_neg: 363.9932556152344 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9439, loss_val: nan, pos_over_neg: 695.94482421875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9538, loss_val: nan, pos_over_neg: 240.46153259277344 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 469.71844482421875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 384.4450378417969 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 219.31138610839844 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9463, loss_val: nan, pos_over_neg: 265.6125183105469 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 212.2339324951172 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 275.24542236328125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 204.02203369140625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 169.3885040283203 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 225.54306030273438 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 267.5456237792969 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 200.01193237304688 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9352, loss_val: nan, pos_over_neg: 314.9439697265625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: 195.321533203125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 292.6367492675781 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 161.81204223632812 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 166.3397216796875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 166.070556640625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 264.6392517089844 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 212.5203857421875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 378.0916748046875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 226.96575927734375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 206.55650329589844 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 219.1084442138672 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 408.9872131347656 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 171.7003631591797 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 429.77557373046875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 140.95254516601562 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 326.42584228515625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 187.3746337890625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 204.83778381347656 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 422.80303955078125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 376.8047180175781 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 346.8846435546875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 306.9805603027344 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 446.6350402832031 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 236.72947692871094 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 575.9143676757812 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 271.65972900390625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 427.26873779296875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 558.9450073242188 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 355.63092041015625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 422.7732238769531 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 1470.525390625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 1026.944580078125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 469.07196044921875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8981, loss_val: nan, pos_over_neg: 845.41845703125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 438.24420166015625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 279.9639892578125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 406.466552734375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 498.6605529785156 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 602.8720092773438 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 512.4273071289062 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 383.2997741699219 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: 784.532470703125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 492.3697204589844 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 522.5752563476562 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8925, loss_val: nan, pos_over_neg: 998.9774169921875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 286.24560546875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 861.4822998046875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 719.625244140625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8891, loss_val: nan, pos_over_neg: 552.5120239257812 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 629.3596801757812 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 295.4257507324219 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 375.0988464355469 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 447.3165283203125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 643.9840087890625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 304.4423828125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 320.6342468261719 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 559.8406982421875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 228.02149963378906 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8932, loss_val: nan, pos_over_neg: 499.7993469238281 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 299.4985046386719 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 437.75445556640625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8923, loss_val: nan, pos_over_neg: 393.3487548828125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 308.4505615234375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8877, loss_val: nan, pos_over_neg: 192.4589080810547 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 423.0740966796875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 400.94610595703125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 248.07977294921875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8858, loss_val: nan, pos_over_neg: 307.95745849609375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8824, loss_val: nan, pos_over_neg: 315.98663330078125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8961, loss_val: nan, pos_over_neg: 167.09388732910156 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8833, loss_val: nan, pos_over_neg: 411.69512939453125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 264.308349609375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 280.62652587890625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 346.1852111816406 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8865, loss_val: nan, pos_over_neg: 322.11328125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8888, loss_val: nan, pos_over_neg: 244.31822204589844 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8948, loss_val: nan, pos_over_neg: 264.3986511230469 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8942, loss_val: nan, pos_over_neg: 288.2492370605469 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8804, loss_val: nan, pos_over_neg: 327.0099792480469 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8862, loss_val: nan, pos_over_neg: 294.8103332519531 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.889, loss_val: nan, pos_over_neg: 320.892333984375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 206.89479064941406 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 392.11627197265625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 609.7481079101562 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8902, loss_val: nan, pos_over_neg: 280.57818603515625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 309.51580810546875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 538.66845703125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8879, loss_val: nan, pos_over_neg: 493.6439208984375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.881, loss_val: nan, pos_over_neg: 1251.6148681640625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 340.30340576171875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8784, loss_val: nan, pos_over_neg: 338.56787109375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8796, loss_val: nan, pos_over_neg: 232.83863830566406 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 352.8668518066406 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.876, loss_val: nan, pos_over_neg: 378.9056701660156 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8786, loss_val: nan, pos_over_neg: 243.36172485351562 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 320.53411865234375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8783, loss_val: nan, pos_over_neg: 266.1648864746094 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8789, loss_val: nan, pos_over_neg: 286.6021423339844 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 243.3916778564453 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8692, loss_val: nan, pos_over_neg: 388.0435791015625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 282.40106201171875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8807, loss_val: nan, pos_over_neg: 552.4700927734375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8743, loss_val: nan, pos_over_neg: 350.5192565917969 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 339.1234436035156 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8831, loss_val: nan, pos_over_neg: 216.7344970703125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 297.3126525878906 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8842, loss_val: nan, pos_over_neg: 339.361328125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 466.44921875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8746, loss_val: nan, pos_over_neg: 528.0011596679688 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8674, loss_val: nan, pos_over_neg: 414.3583679199219 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.868, loss_val: nan, pos_over_neg: 401.9936828613281 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8622, loss_val: nan, pos_over_neg: 501.553955078125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 277.1014099121094 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 307.5799865722656 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 262.11669921875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 267.3433532714844 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 290.78131103515625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 834.3775024414062 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 333.5639343261719 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8713, loss_val: nan, pos_over_neg: 437.6472473144531 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8795, loss_val: nan, pos_over_neg: 315.6172180175781 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 338.26300048828125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 315.3893737792969 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 474.364013671875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8694, loss_val: nan, pos_over_neg: 374.4551696777344 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8693, loss_val: nan, pos_over_neg: 545.8118896484375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8618, loss_val: nan, pos_over_neg: 605.3755493164062 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 487.1444091796875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 675.61767578125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 984.7408447265625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 497.2686767578125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 1054.962646484375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 744.9117431640625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 638.9451904296875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 991.3621826171875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 419.9337158203125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 386.2287902832031 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8737, loss_val: nan, pos_over_neg: 860.4512329101562 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 362.415283203125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 327.218017578125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1233.129638671875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8606, loss_val: nan, pos_over_neg: 392.8208312988281 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 399.7010803222656 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 1115.1905517578125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8575, loss_val: nan, pos_over_neg: 435.1968078613281 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.868, loss_val: nan, pos_over_neg: 229.2433319091797 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 481.14764404296875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 268.7359619140625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 281.1131896972656 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8642, loss_val: nan, pos_over_neg: 540.1484985351562 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 371.12744140625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 287.71148681640625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 267.9091491699219 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 330.2658996582031 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 282.7842102050781 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8668, loss_val: nan, pos_over_neg: 260.5259094238281 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 408.23345947265625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 165.9580841064453 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 262.2378234863281 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.856, loss_val: nan, pos_over_neg: 292.0878601074219 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 255.77223205566406 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8639, loss_val: nan, pos_over_neg: 281.4734191894531 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 223.33876037597656 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 235.99365234375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 180.2228240966797 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 130.5395050048828 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8645, loss_val: nan, pos_over_neg: 237.50289916992188 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 210.38226318359375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 215.42343139648438 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8639, loss_val: nan, pos_over_neg: 291.7784729003906 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8625, loss_val: nan, pos_over_neg: 475.14044189453125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 235.4254608154297 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 278.3420104980469 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 363.1103210449219 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 365.60577392578125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 309.80084228515625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 325.69659423828125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 342.4680480957031 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 488.03271484375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 289.1060791015625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 200.2051544189453 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 637.1259765625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 403.2501220703125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 269.5799255371094 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 575.3956298828125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8561, loss_val: nan, pos_over_neg: 624.0142211914062 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8531, loss_val: nan, pos_over_neg: 783.2334594726562 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 478.2590026855469 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 933.1692504882812 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 445.59619140625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 391.0372619628906 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 528.6085815429688 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 499.8476867675781 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8531, loss_val: nan, pos_over_neg: 469.7001953125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1452.3665771484375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 522.7847900390625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 710.61962890625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 428.4845275878906 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 534.1168823242188 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 507.5722961425781 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 483.0872497558594 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 730.9231567382812 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 658.0223999023438 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 796.097900390625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 342.75048828125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 477.692626953125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 910.8497314453125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 311.4239807128906 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 737.5087890625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 251.1446990966797 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 323.2689208984375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 1113.2369384765625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 470.1921081542969 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 291.21112060546875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 1100.474365234375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 304.08343505859375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 400.7959899902344 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 402.2886962890625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 491.84869384765625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 344.42822265625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 390.1330871582031 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 532.982177734375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 1485.2762451171875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 302.8625793457031 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 690.6773681640625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 236.8928985595703 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 438.095703125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 885.794921875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 410.04693603515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 515.7009887695312 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 925.9277954101562 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 318.9963073730469 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 397.5552673339844 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 453.3985595703125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 490.0007019042969 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 355.4294128417969 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 631.9004516601562 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 513.765380859375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 556.7110595703125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 495.3802490234375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 325.4133605957031 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 417.92919921875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 370.0782775878906 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 336.356201171875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 462.3110656738281 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 534.3580322265625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 582.9722290039062 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 462.7986145019531 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 396.55157470703125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 449.54833984375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 253.9103546142578 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 484.8041687011719 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 326.8728332519531 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 229.04112243652344 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 448.9242248535156 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 346.060302734375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 303.9241638183594 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 405.29681396484375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 865.3277587890625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 301.094482421875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8467, loss_val: nan, pos_over_neg: 371.636474609375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 548.7706298828125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 314.8899841308594 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 443.3572082519531 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 739.8182373046875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 611.8123779296875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 665.4151000976562 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 459.4111633300781 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8333, loss_val: nan, pos_over_neg: 561.8016357421875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1247.267578125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8488, loss_val: nan, pos_over_neg: 462.523193359375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 469.3202209472656 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 582.9827880859375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 404.0508117675781 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 24253.458984375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 757.4710693359375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 788.7235107421875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1082.4471435546875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 441.6356506347656 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 526.005615234375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 582.7532348632812 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 404.91192626953125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 592.9266357421875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 416.617431640625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 377.7923583984375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 371.81005859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 335.4534606933594 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 392.9441223144531 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 424.55694580078125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 441.8601379394531 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 653.4637451171875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 2062.734130859375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 1094.6788330078125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 561.4603271484375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 894.576416015625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1058.4271240234375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 403.68341064453125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 1202.648193359375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 508.2920837402344 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 590.0029907226562 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 502.0337219238281 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 656.1820678710938 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 240.60960388183594 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 1541.0582275390625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 609.4596557617188 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 336.8987121582031 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 682.138671875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 403.2839660644531 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 1056.2738037109375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 988.6829223632812 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 448.8362121582031 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 488.8137512207031 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1326.185791015625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 461.16845703125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8274, loss_val: nan, pos_over_neg: 612.3759765625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 607.6264038085938 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 473.9265441894531 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8296, loss_val: nan, pos_over_neg: 757.5050048828125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 428.7920227050781 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 851.7434692382812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 755.2245483398438 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 456.1540832519531 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 212.89202880859375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 362.0408935546875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 536.1411743164062 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 324.4808349609375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 463.3096008300781 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 673.5831298828125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 584.4996948242188 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 521.4400024414062 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 524.4166870117188 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 1106.46337890625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 505.1425476074219 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 331.0378723144531 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 397.5867614746094 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 410.8880615234375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 460.75018310546875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 374.7461242675781 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 377.9346008300781 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 899.4092407226562 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 366.8294982910156 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 324.20428466796875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 305.2547912597656 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 327.5373840332031 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 377.54901123046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.827, loss_val: nan, pos_over_neg: 227.3107452392578 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 428.8118591308594 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 271.6235046386719 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 427.5341796875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 604.8531494140625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 634.5294799804688 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 507.052001953125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 427.12445068359375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1498.0072021484375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 708.2941284179688 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 368.7894287109375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 953.913330078125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 1707.705322265625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 605.7098999023438 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 1474.2969970703125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1107.9892578125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8306, loss_val: nan, pos_over_neg: 721.0105590820312 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 501.4684143066406 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 555.0899047851562 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 569.4574584960938 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 553.6539306640625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 420.6556396484375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8205, loss_val: nan, pos_over_neg: 1905.7747802734375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 805.2633666992188 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 769.9915771484375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 635.6024780273438 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 489.603271484375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 460.08935546875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 933.4190673828125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 683.637451171875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 929.3563232421875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 2004.572265625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 705.263427734375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 724.5135498046875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 905.7218627929688 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 549.8933715820312 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 977.36181640625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 1079.8863525390625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8333, loss_val: nan, pos_over_neg: 393.5815734863281 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 691.3600463867188 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 585.5517578125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 585.46044921875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 492.4238586425781 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8206, loss_val: nan, pos_over_neg: 358.7685241699219 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 549.2636108398438 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 404.30889892578125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 278.0289611816406 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8236, loss_val: nan, pos_over_neg: 532.10888671875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8217, loss_val: nan, pos_over_neg: 399.9735412597656 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 469.8783264160156 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 851.5350341796875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 506.2495422363281 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 430.37384033203125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 288.73492431640625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 261.22528076171875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 532.5771484375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 545.8233642578125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 266.9793395996094 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 603.203369140625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 1398.166015625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 296.1729431152344 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 220.26632690429688 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8279, loss_val: nan, pos_over_neg: 373.2925109863281 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 247.08978271484375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 358.1337585449219 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 653.3931884765625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 368.8937072753906 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8272, loss_val: nan, pos_over_neg: 757.2488403320312 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 518.1210327148438 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 541.4395751953125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 578.38525390625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 369.8306579589844 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 417.7370300292969 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8272, loss_val: nan, pos_over_neg: 533.0194091796875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 704.5704345703125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 430.9649353027344 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 776.2390747070312 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 500.5988464355469 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 615.875244140625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 272.8201599121094 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 436.0481872558594 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 968.8899536132812 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 533.0858764648438 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 271.9205322265625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 677.6376342773438 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 664.41943359375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 522.358154296875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 443.0562438964844 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 565.8646850585938 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 374.2767639160156 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 511.3064270019531 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 885.1849975585938 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 532.5111694335938 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 791.8751831054688 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 671.7706909179688 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 387.8023681640625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 396.030029296875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 428.6699523925781 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 474.2444152832031 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 500.81634521484375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 296.7005310058594 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 542.2988891601562 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 737.6129150390625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.823, loss_val: nan, pos_over_neg: 395.46502685546875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 612.5830688476562 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 929.9400634765625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 3156.257080078125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 551.0802612304688 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 334.4405212402344 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 1329.7021484375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 1734.467041015625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 742.895751953125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 773.7490234375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 695.05908203125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 438.8953857421875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8171, loss_val: nan, pos_over_neg: 804.5739135742188 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 1345.6414794921875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 708.7081298828125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 517.9635009765625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 2074.002197265625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 1011.5122680664062 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 796.4813842773438 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 697.0620727539062 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 805.4195556640625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 596.5200805664062 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8199, loss_val: nan, pos_over_neg: 917.5835571289062 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 918.7103881835938 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 632.4913330078125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 2109.052978515625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 517.6177368164062 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 858.7730712890625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1378.643310546875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1307.0643310546875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 562.8764038085938 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 664.9507446289062 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8157, loss_val: nan, pos_over_neg: 871.3294067382812 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1090.21044921875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 647.6016845703125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 596.237548828125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 461.1412658691406 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 507.3037109375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 876.1061401367188 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8196, loss_val: nan, pos_over_neg: 526.4063110351562 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 491.1146545410156 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8194, loss_val: nan, pos_over_neg: 325.65838623046875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8191, loss_val: nan, pos_over_neg: 452.0570068359375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 497.76947021484375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 575.5966796875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 304.4831848144531 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 447.8306579589844 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 589.876220703125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 395.2566833496094 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 446.8480224609375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 546.42333984375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8232, loss_val: nan, pos_over_neg: 216.3643035888672 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 324.7570495605469 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 444.7350769042969 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 346.58544921875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 274.80804443359375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 472.2458801269531 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 345.70086669921875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 540.1668090820312 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 369.057373046875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8212, loss_val: nan, pos_over_neg: 325.8246765136719 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 457.1015625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 498.19561767578125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 630.5281372070312 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 556.6649169921875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 539.3636474609375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 787.2900390625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8114, loss_val: nan, pos_over_neg: 834.4735717773438 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 319.1437683105469 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8139, loss_val: nan, pos_over_neg: 490.6275634765625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 783.4057006835938 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 413.47320556640625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 310.4715881347656 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.823, loss_val: nan, pos_over_neg: 448.96331787109375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 554.5848388671875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 621.8999633789062 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 724.2010498046875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 522.0313720703125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 329.88250732421875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.814, loss_val: nan, pos_over_neg: 1402.2760009765625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 732.6361083984375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 401.0792236328125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1310.8868408203125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 732.2576904296875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 696.5208740234375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 516.8157348632812 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 904.5936279296875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 2028.158203125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 822.6516723632812 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 483.8235168457031 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 516.9620971679688 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 626.4306640625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 1085.09619140625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 651.0115356445312 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 1519.575439453125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 876.9779052734375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1254.2998046875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.814, loss_val: nan, pos_over_neg: 591.240966796875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8137, loss_val: nan, pos_over_neg: 945.0021362304688 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 831.6617431640625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 772.8836669921875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 2275.15478515625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 561.7546997070312 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 393.9616394042969 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 465.0767822265625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 610.8115234375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 764.437744140625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 733.19482421875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 536.2705078125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 939.5437622070312 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 2365.692626953125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 488.6971435546875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 1675.0833740234375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 1118.17333984375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8114, loss_val: nan, pos_over_neg: 419.1265563964844 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 585.2516479492188 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 596.4060668945312 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 562.629638671875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 501.3486328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 613.9730834960938 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 3063.007568359375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 573.69189453125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 432.7169494628906 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 683.4788208007812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 812.3922119140625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8126, loss_val: nan, pos_over_neg: 745.3037109375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1010.173583984375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1949.3153076171875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1881.47998046875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 461.79876708984375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1690.588134765625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 597.4830322265625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8129, loss_val: nan, pos_over_neg: 960.5799560546875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 552.2374267578125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 535.4627685546875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 475.09368896484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 1254.201416015625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 721.427001953125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 626.8548583984375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 779.476806640625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 509.0385437011719 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 336.7978820800781 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1040.5977783203125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 841.3007202148438 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 546.6625366210938 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 1620.1614990234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 1017.53466796875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 407.9725341796875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 790.0106201171875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 701.2212524414062 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 744.3230590820312 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 423.28472900390625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 563.2438354492188 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 650.8733520507812 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 488.2717590332031 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 2292.03515625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 905.5582275390625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 293.05731201171875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 615.2809448242188 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 809.2991943359375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 470.21795654296875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [21:09<105761:08:49, 1269.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 411.4357604980469 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 441.9308166503906 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8129, loss_val: nan, pos_over_neg: 548.5442504882812 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 625.7948608398438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 340.034423828125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 663.1629638671875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 764.852783203125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 511.5382995605469 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 309.436279296875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 593.6736450195312 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 381.29327392578125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 706.7661743164062 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 367.1083984375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 384.9373474121094 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 703.977294921875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 773.3689575195312 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 671.6004028320312 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 339.748046875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: 365.0990905761719 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 281.101318359375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 786.2470703125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 408.451904296875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 375.31463623046875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 425.71258544921875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 408.52777099609375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 768.962646484375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 896.811767578125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 448.00909423828125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 501.8985595703125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1447.458251953125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 505.5872802734375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 544.8988647460938 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 265.9494934082031 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 887.1777954101562 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 1056.9332275390625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 477.3116455078125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1014.866455078125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 414.42010498046875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 700.8162231445312 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 543.0391845703125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 395.6590881347656 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8129, loss_val: nan, pos_over_neg: 203.7183380126953 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 399.16351318359375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 559.1076049804688 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 638.3746337890625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 663.7412109375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 427.0718688964844 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 588.2395629882812 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 644.7877807617188 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 451.3780212402344 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 428.34539794921875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 460.0118713378906 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 739.40185546875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 666.3450317382812 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 557.2849731445312 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 481.4781188964844 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 671.2811279296875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 720.655029296875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 427.92230224609375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 364.3743591308594 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 610.6373901367188 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 418.2248840332031 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 869.4940185546875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1907.8232421875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 674.16552734375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1103.6864013671875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 463.0487976074219 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1429.8311767578125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 700.3576049804688 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 523.439208984375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 768.0654296875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1217.8212890625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 620.1249389648438 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 953.2798461914062 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 823.2791748046875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 811.1448974609375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 987.6314697265625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 691.8639526367188 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 1006.3565673828125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1028.9649658203125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 816.75732421875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 891.281982421875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 941.4754028320312 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 599.7623901367188 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 640.1271362304688 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1395.428955078125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 372.4421081542969 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 347.2255859375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 547.4993896484375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 743.50048828125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 594.1508178710938 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 826.5723876953125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 562.8524780273438 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 631.1559448242188 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 450.5823669433594 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 597.289306640625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 401.495849609375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 319.6968078613281 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 845.2449340820312 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 711.4481201171875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 443.2649230957031 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 752.2896118164062 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 569.2706298828125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 560.9719848632812 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 411.9532470703125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 462.7806701660156 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 864.1448974609375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 696.330322265625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 439.0582275390625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 1075.685791015625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1061.8033447265625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1487.5787353515625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1075.694091796875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 607.0352783203125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 803.557373046875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 541.4727172851562 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 445.7922668457031 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 496.3188781738281 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 578.7356567382812 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 410.3091735839844 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 794.308349609375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 694.1817016601562 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 811.2012329101562 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1541.6904296875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 557.7388916015625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 374.6335754394531 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 412.169189453125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1147.0399169921875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1188.5146484375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 786.389404296875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1037.315673828125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1229.5748291015625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 450.6485900878906 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 530.33056640625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 576.43408203125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 641.2955932617188 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 726.4240112304688 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1019.0338134765625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1401.904052734375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 1358.570068359375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 481.35394287109375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 937.9093627929688 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 800.2337036132812 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 312.966064453125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 721.405029296875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 625.7799682617188 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 965.4033203125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 869.92626953125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 671.7395629882812 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1056.3465576171875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 748.2986450195312 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 1630.6663818359375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 860.010009765625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 484.28173828125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 666.71533203125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 672.9400634765625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 489.60003662109375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 879.7110595703125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1415.5699462890625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 894.7138671875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 915.9667358398438 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 3785.327880859375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 893.8786010742188 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 507.90576171875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 617.6884155273438 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 831.45947265625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 674.5081176757812 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 669.5361938476562 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 430.28082275390625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 634.4034423828125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 648.7440795898438 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1268.88037109375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 436.267578125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 362.3338317871094 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 658.9067993164062 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 562.7015991210938 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 479.8179016113281 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 335.1031494140625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 579.8041381835938 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 619.4749755859375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 356.9808654785156 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 624.9392700195312 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 424.6992492675781 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 411.9871520996094 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 271.72528076171875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 550.7449951171875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 338.4610900878906 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8039, loss_val: nan, pos_over_neg: 368.0489807128906 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 490.55303955078125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 407.5688171386719 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 706.4464721679688 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 369.5985107421875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 492.7365417480469 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 678.2855224609375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 754.4158325195312 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 516.2196044921875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 706.940185546875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 416.33251953125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 521.9970703125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 387.09271240234375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 279.6359558105469 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 558.2520751953125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1104.439453125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 602.2987670898438 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 518.1812133789062 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 628.2603759765625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 399.0069580078125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 551.6101684570312 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 480.9111633300781 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 688.3733520507812 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 783.44921875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 462.93951416015625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 583.5615234375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 842.5060424804688 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 556.6525268554688 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8095, loss_val: nan, pos_over_neg: 455.1469421386719 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 715.7119750976562 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 436.4982604980469 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 620.0426025390625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 733.8350830078125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 357.4748229980469 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 1101.0689697265625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 655.8244018554688 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1126.700927734375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 817.8324584960938 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 452.711181640625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1148.373046875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1529.51611328125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 386.1565856933594 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 545.2349853515625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 315.6604919433594 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 505.5344543457031 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 503.04241943359375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 701.0741577148438 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 349.341552734375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1057.72900390625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 805.3333740234375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 432.4193420410156 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8025, loss_val: nan, pos_over_neg: 338.25994873046875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 393.77337646484375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 549.7742309570312 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 707.6109619140625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 464.1556091308594 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 687.8579711914062 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 520.0086669921875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 630.7752685546875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 554.4949340820312 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 546.1420288085938 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 441.7290954589844 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 594.1826171875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 605.7225952148438 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 536.5400390625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 590.5443725585938 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 933.422119140625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 938.42236328125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 745.5758666992188 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 645.7759399414062 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 677.1992797851562 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 611.257568359375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 878.2985229492188 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 501.1336975097656 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 886.8729248046875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1841.5875244140625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1283.7265625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 571.7619018554688 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 975.8978881835938 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1755.2166748046875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 540.961181640625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 608.7926635742188 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 562.1647338867188 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 668.7490844726562 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 1761.56201171875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 665.1770629882812 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 337.3029479980469 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 855.9215698242188 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1164.3917236328125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 863.8309936523438 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 385.635009765625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 498.27923583984375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 638.104248046875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1049.1619873046875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 613.5525512695312 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 686.3516845703125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 2760.2548828125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 691.2598876953125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 573.3783569335938 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 753.1553344726562 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 836.7385864257812 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 695.408447265625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 383.9664611816406 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 568.8944702148438 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 765.2030029296875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 704.6050415039062 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 580.3472900390625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 814.1325073242188 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 728.8803100585938 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1077.5179443359375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 762.7635498046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 542.4561767578125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 633.1295776367188 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 739.1494750976562 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 354.3547058105469 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 846.861572265625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 950.0494384765625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 788.0696411132812 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 439.32843017578125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 560.1195678710938 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 659.2874145507812 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 424.21026611328125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 1021.182861328125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 639.768310546875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 371.921630859375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 396.38470458984375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1495.7532958984375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 686.0239868164062 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 329.73931884765625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 460.942138671875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1062.5281982421875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 351.15869140625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 604.7850952148438 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 778.3364868164062 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 657.3436279296875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 659.6640625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 456.1341857910156 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 802.912841796875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 741.12158203125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1174.0303955078125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 947.0949096679688 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 464.5418701171875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 768.1688842773438 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1137.453857421875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 598.140380859375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 723.7034301757812 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 729.7212524414062 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1799.326416015625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1253.301513671875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 517.4629516601562 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 844.023681640625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 721.1162109375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 1131.6614990234375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 635.8103637695312 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 832.3656005859375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1202.460205078125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 793.0881958007812 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1150.149658203125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 590.8857421875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 628.1993408203125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 761.9524536132812 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 473.0592956542969 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 816.8649291992188 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 430.3982849121094 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 359.0216064453125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 921.2319946289062 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 839.8099365234375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1163.1763916015625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 694.1792602539062 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 887.6126098632812 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 788.69140625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1360.975830078125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 671.7020263671875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 1028.9141845703125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1379.0106201171875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 722.29052734375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 697.289306640625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1959.9478759765625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1460.9547119140625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 646.5511474609375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 472.9409484863281 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 953.18408203125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 780.8448486328125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 610.7235717773438 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1082.8096923828125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 884.990234375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 626.3887329101562 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1442.5306396484375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 606.3357543945312 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 471.4305419921875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1143.57666015625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 4688.484375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 492.96435546875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 538.7582397460938 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1274.5643310546875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 2670.81884765625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 884.4400024414062 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 875.39892578125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 953.7317504882812 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1118.04443359375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 422.9687805175781 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 664.23291015625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1090.7724609375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 731.7713012695312 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 455.94354248046875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 899.861083984375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 605.1776123046875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 629.721435546875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 610.8857421875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 408.2197570800781 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 596.691162109375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 558.0272216796875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 458.27777099609375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 395.55743408203125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 434.07135009765625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 981.229248046875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 439.40863037109375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 324.195556640625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 562.2156982421875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 643.3550415039062 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 711.0613403320312 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 459.6934814453125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 368.2478942871094 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 485.5893249511719 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 1298.300537109375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 599.9948120117188 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 783.2188110351562 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 485.078857421875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 608.584228515625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 997.5014038085938 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 540.668701171875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 508.66253662109375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 828.391357421875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 476.02239990234375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 704.1290893554688 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 766.017333984375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 619.7603759765625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 820.62451171875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 711.1371459960938 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 560.5843505859375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 561.8804321289062 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 356.19793701171875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 501.8939514160156 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 609.9070434570312 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 622.46533203125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 345.7736511230469 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1020.43798828125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 781.633056640625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1110.899169921875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 999.2767333984375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 598.5380249023438 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1072.1661376953125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 380.9016418457031 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 813.9073486328125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 851.9843139648438 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 519.0782470703125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1729.147705078125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 952.2814331054688 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 674.308837890625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 596.247314453125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1854.599365234375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 942.933349609375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 302.4424133300781 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 658.8050537109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 481.8876647949219 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1597.7711181640625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 486.5631408691406 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 812.916748046875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 662.204345703125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1094.7298583984375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1197.3543701171875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 928.8523559570312 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1369.905029296875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1563.7550048828125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1040.4912109375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 784.9609985351562 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 565.0635375976562 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 561.0894165039062 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 745.6925659179688 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 485.599853515625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1788.485595703125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 755.9600219726562 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1365.66162109375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 422.2992858886719 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 3245.2080078125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 2350.158203125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 430.1709899902344 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 878.5853881835938 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 799.8499145507812 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1169.0628662109375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 571.76904296875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1837.039794921875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 700.9682006835938 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 623.2105102539062 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 815.6268310546875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 816.8721923828125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1181.7259521484375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 684.6925659179688 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 597.6754760742188 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 536.0631713867188 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 746.7327270507812 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 972.8255004882812 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 668.8121337890625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 876.2958374023438 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1183.347412109375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 558.29345703125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2237.501220703125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1225.1861572265625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 585.3165283203125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 981.6746826171875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1438.9010009765625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1016.4746704101562 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 467.094970703125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1211.8282470703125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 582.775634765625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 902.523193359375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 765.0098266601562 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 550.9370727539062 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 852.9620971679688 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 732.8117065429688 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 989.2486572265625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 569.1103515625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 695.4290771484375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 332.46112060546875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 682.3330688476562 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 766.1167602539062 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 762.3014526367188 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 555.5827026367188 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 586.1013793945312 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 405.8687744140625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 897.8941650390625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 707.8665161132812 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 497.68682861328125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 403.8254089355469 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 998.2440795898438 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1519.308349609375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 556.0709228515625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 405.29937744140625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 631.00927734375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 635.3531494140625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 523.96484375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 682.20947265625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 417.38323974609375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 409.09844970703125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 423.8073425292969 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 549.324462890625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 472.8113098144531 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 310.85125732421875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 536.7109375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 812.138916015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 934.3421020507812 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 400.3092041015625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1023.5828247070312 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1476.4605712890625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 423.90057373046875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 426.7050476074219 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 921.9664916992188 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 691.6124267578125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 814.6439819335938 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 489.8974914550781 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 482.08563232421875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1019.5154418945312 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1005.4547729492188 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1037.6041259765625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 968.3027954101562 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 430.2437438964844 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 724.5416870117188 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 718.9248657226562 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 749.8214111328125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 791.3407592773438 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 559.1336669921875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1099.3985595703125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1341.0880126953125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 643.9382934570312 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 490.984130859375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 651.6869506835938 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 868.0152587890625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 603.7095336914062 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 610.5396118164062 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 414.4884948730469 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 536.7345581054688 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 561.6036987304688 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 693.6701049804688 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 550.4547119140625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 351.3260498046875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 540.4237670898438 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 673.8900146484375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 695.419677734375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 547.140625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 892.271240234375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 608.030517578125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 440.4887390136719 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 982.73486328125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1034.5360107421875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 673.3018798828125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 363.2403564453125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 483.2425842285156 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1580.7904052734375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 655.9921264648438 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1133.9246826171875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 740.9113159179688 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 477.82366943359375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1417.9285888671875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 1818.4052734375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 807.1002197265625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 530.2977905273438 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 715.9771728515625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 572.022705078125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 733.6203002929688 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1607.7490234375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 914.9843139648438 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1400.05810546875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1185.3670654296875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1073.050048828125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1732.5999755859375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 499.5026550292969 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 910.0120849609375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 793.5068359375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1281.3463134765625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1546.1044921875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 791.1187744140625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 3124.387939453125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1547.53857421875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1289.54931640625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 937.7408447265625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 6449.88818359375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 900.1801147460938 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 710.8009643554688 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 503.6309509277344 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 650.0076904296875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2298.555419921875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1609.8861083984375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 877.7861328125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 774.8558959960938 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1481.766845703125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 981.8223876953125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1224.684326171875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 761.7861938476562 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 772.3460693359375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 788.501708984375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 859.5729370117188 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1391.0797119140625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 749.7675170898438 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 763.4199829101562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 640.1917724609375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 3778.9306640625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 641.86865234375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 835.2682495117188 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 656.8838500976562 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 537.3087768554688 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1220.033447265625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 573.56689453125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 743.0751953125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 910.08837890625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 972.7346801757812 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 628.2967529296875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 846.2655639648438 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 566.3248901367188 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 350.58428955078125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 759.9192504882812 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 609.195556640625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 867.2511596679688 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 539.4342651367188 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1631.3135986328125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1205.90625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 849.10498046875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1066.3935546875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 904.180419921875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 744.3596801757812 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1034.2596435546875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 3021.983642578125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 466.5281982421875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 519.6638793945312 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1228.3021240234375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1439.1285400390625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 894.82958984375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 540.2778930664062 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 2376.779296875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 479.2532653808594 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 709.5068359375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1590.3875732421875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1145.288330078125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 810.2061767578125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1277.596435546875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 813.5945434570312 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 605.9818725585938 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 565.6001586914062 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 541.4803466796875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1100.4603271484375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 777.9249877929688 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1960.45703125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 953.185791015625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 837.6643676757812 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 959.2085571289062 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 2617.9248046875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 706.0177612304688 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 597.6246948242188 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 922.2098388671875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1009.18798828125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 534.6734008789062 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 938.3145751953125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1791.490478515625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1166.6749267578125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 882.2510986328125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 545.8314208984375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1352.137939453125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1369.751708984375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 778.572509765625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [42:02<104990:30:49, 1259.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 317.5196838378906 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 765.0432739257812 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1412.412353515625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 959.1016845703125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 970.4857788085938 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1206.8153076171875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 954.6618041992188 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1044.353515625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 755.0987548828125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 520.8925170898438 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1051.038330078125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 527.1644897460938 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 825.2650756835938 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 572.5674438476562 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1019.407470703125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 671.3934326171875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 819.9957275390625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1505.298095703125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 502.23779296875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1234.3388671875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 951.2353515625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1403.6361083984375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1158.245849609375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1122.8720703125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 2611.7431640625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 947.1796875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1032.5701904296875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 779.4474487304688 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 432.9601745605469 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1000.0007934570312 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 898.793701171875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 553.8202514648438 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 972.6700439453125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1091.2362060546875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 774.2949829101562 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 760.8050537109375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 894.5283203125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 797.3535766601562 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 2992.117431640625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 823.7444458007812 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 783.069091796875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 937.6146850585938 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 653.6614379882812 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1304.0501708984375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 883.072998046875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 536.8834838867188 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 685.4006958007812 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 2515.667236328125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 736.8858032226562 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1223.4251708984375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 926.1856079101562 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 996.9074096679688 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 604.4052734375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 391.2821350097656 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 956.4369506835938 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 766.918212890625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 413.90484619140625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1018.8450927734375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 856.9133911132812 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 584.3442993164062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 413.46728515625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 527.521240234375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 460.65869140625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 714.05078125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 755.84228515625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 742.6920166015625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 734.9965209960938 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 577.8946533203125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1283.32958984375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 498.93212890625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 639.3104858398438 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 403.89495849609375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 486.6859436035156 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1270.58544921875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1250.63427734375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 631.1104125976562 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 761.9981689453125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 826.8389892578125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 812.8717041015625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 814.7399291992188 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 568.1881103515625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1114.023193359375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1314.84716796875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 519.3499755859375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 614.562255859375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 728.1384887695312 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1281.9749755859375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1041.0711669921875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 729.150146484375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1595.3563232421875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 874.0986328125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 854.2259521484375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1521.395263671875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1126.4176025390625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 2223.96826171875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 611.0685424804688 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 919.7163696289062 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 548.17041015625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 618.9705200195312 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 784.9833374023438 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 692.8358764648438 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1517.9217529296875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 800.9600219726562 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 602.968505859375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 2001.8603515625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 685.4325561523438 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1013.0791015625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 584.6558837890625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1063.0511474609375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 587.6151733398438 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 838.5177612304688 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 921.9139404296875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 467.1585388183594 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 817.677734375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1205.7335205078125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 625.432861328125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 497.23992919921875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 777.9880981445312 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 526.3262329101562 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 531.1695556640625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 573.1082153320312 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 834.4144897460938 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 984.0156860351562 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 891.805419921875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 709.1934814453125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 578.9955444335938 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1167.8785400390625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 738.6358642578125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 975.0689697265625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 621.4689331054688 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1559.3057861328125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1499.5498046875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1460.8358154296875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 657.2974853515625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1392.1177978515625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 950.0413208007812 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1482.5223388671875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 2854.38623046875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1312.8302001953125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 580.86083984375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1055.2828369140625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 731.5421142578125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 890.7760009765625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 813.4744873046875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 690.066162109375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1285.415283203125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 712.58349609375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 845.22705078125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1390.5714111328125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 3904.607177734375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 2203.361328125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 954.1452026367188 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 726.9613647460938 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 653.0349731445312 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1543.5054931640625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 931.8441162109375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 811.2511596679688 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 724.0559692382812 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1816.87890625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 608.4755249023438 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 2164.451904296875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1312.195556640625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1180.8975830078125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1027.2886962890625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1749.1434326171875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1641.3134765625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1259.30615234375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1921.2137451171875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 385.0941467285156 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 535.2994384765625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1059.5240478515625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1154.694091796875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1845.8695068359375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 490.3055114746094 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 427.7525634765625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 598.8974609375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 777.9888305664062 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 344.0201416015625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 586.4719848632812 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 439.8569030761719 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 658.5538940429688 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 595.4745483398438 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 894.0109252929688 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 679.2775268554688 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 543.89794921875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 991.522705078125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 734.1981201171875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 507.5479431152344 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 714.7899169921875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 825.7786865234375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1057.0679931640625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 535.5145874023438 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 350.8124084472656 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 711.00390625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 908.3502807617188 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 655.5643310546875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 512.3942260742188 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 661.95703125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 556.8114624023438 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1047.7213134765625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1085.6376953125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 598.9246826171875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 688.9345703125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 430.1196594238281 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 638.5618896484375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1243.7427978515625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1154.0625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 482.9713439941406 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 610.28271484375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 874.7562255859375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 668.2162475585938 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 681.30322265625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 557.1087646484375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 544.69677734375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 472.98187255859375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 768.9339599609375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 823.5184326171875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1920.3795166015625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 901.5087890625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 693.005859375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 622.977294921875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 975.77197265625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 991.1339111328125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 783.004638671875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 522.197998046875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 784.1846923828125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1021.092041015625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1011.5189208984375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 476.663818359375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 643.9967041015625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 886.1261596679688 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 751.215576171875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 946.5968017578125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 693.5673217773438 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1569.3402099609375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1042.5936279296875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1460.99169921875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 799.0787963867188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1951.9425048828125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 949.5840454101562 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1089.5146484375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 924.1845703125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1426.18896484375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1017.0874633789062 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 630.1486206054688 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 659.4320068359375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1249.0740966796875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 508.7702331542969 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 539.5145263671875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 585.8065185546875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 2247.79052734375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 551.0025024414062 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 499.5011291503906 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 857.0736083984375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1076.4979248046875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 572.0009765625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 862.7725219726562 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 521.1781616210938 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 2318.60205078125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 666.8963623046875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 789.7034301757812 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 883.3385009765625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 770.142333984375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 627.93359375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 928.54052734375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 743.3428955078125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 2077.0107421875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1635.5478515625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 926.5509033203125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2543.234130859375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 745.3468017578125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 680.18017578125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1669.7288818359375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1056.5706787109375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1352.0155029296875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 568.8365478515625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 956.4927368164062 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 904.2987060546875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 976.96142578125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 836.4287719726562 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 512.5209350585938 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 920.3434448242188 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 787.2871704101562 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 699.9488525390625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 773.06103515625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 567.485595703125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 729.2116088867188 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 892.3340454101562 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 629.5668334960938 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 691.0551147460938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 556.2852172851562 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 916.906982421875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 523.9746704101562 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 711.936279296875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 815.7359619140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 507.42755126953125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 965.2133178710938 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1923.4610595703125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1190.46923828125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 397.4098205566406 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 593.5804443359375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 906.5828857421875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 993.342529296875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 569.4363403320312 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1316.3770751953125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1637.8162841796875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 2975.996337890625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 675.5499267578125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 994.0668334960938 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 774.572021484375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1471.6622314453125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 881.2963256835938 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1230.487060546875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 652.2512817382812 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 432.6997375488281 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 736.3615112304688 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 904.3054809570312 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 884.8058471679688 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1643.6876220703125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 916.4152221679688 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 791.5105590820312 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2028.68408203125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 842.6627807617188 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 761.9918823242188 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 603.4447631835938 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 438.68707275390625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1346.2833251953125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 834.6602172851562 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 893.82373046875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 2431.05859375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 776.6923828125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 827.5567016601562 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 679.6133422851562 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1526.163330078125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 671.621826171875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 453.7218933105469 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1175.900390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1085.2030029296875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 787.043212890625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 982.0264282226562 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1141.48388671875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1126.3262939453125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 885.2564697265625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1035.930908203125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1289.03125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 815.1806640625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 594.5098266601562 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1222.5267333984375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 636.43115234375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1711.841064453125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 2199.419921875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 872.6474609375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 981.7276611328125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 727.569580078125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 2249.639892578125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 911.9644775390625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 719.2597045898438 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 539.6719970703125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 564.2831420898438 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 805.155517578125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 2036.24462890625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 619.1696166992188 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 609.0817260742188 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 671.2537841796875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1407.1812744140625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 816.556884765625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 345.5896301269531 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1011.7644653320312 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1048.20654296875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 632.4913940429688 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1263.8956298828125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 588.179443359375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 826.85888671875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1315.73681640625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 766.4071044921875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 667.1337280273438 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 821.8479614257812 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 719.7462768554688 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 743.167236328125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 876.478271484375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 476.41363525390625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 753.9680786132812 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 706.4592895507812 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1014.3822021484375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 2331.549072265625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 687.3941650390625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 522.3562622070312 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 889.5570678710938 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1758.27099609375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1973.8994140625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 757.0771484375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 638.8193359375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 813.8541259765625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 582.4292602539062 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 730.839599609375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1225.649169921875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 772.583251953125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 945.9657592773438 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 689.5503540039062 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 649.0597534179688 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 735.1433715820312 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1341.884765625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 579.3843383789062 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 559.7332763671875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 904.71240234375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 328.8608093261719 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 565.9751586914062 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 882.9318237304688 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 816.2816772460938 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1018.192138671875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 467.0560607910156 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 513.1451416015625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 823.3535766601562 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 756.0221557617188 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 496.3812255859375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 388.77032470703125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 639.2117309570312 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 787.1857299804688 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1419.189208984375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 430.2574157714844 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 529.9310302734375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 787.1436767578125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1521.33251953125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 732.1348266601562 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 550.2127685546875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1018.9744262695312 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 710.0099487304688 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 976.533203125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1105.1566162109375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 561.2450561523438 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 942.1699829101562 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 735.6634521484375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1090.9959716796875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 831.02734375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1056.535400390625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1553.7467041015625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1801.53466796875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 2693.091796875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 954.0372314453125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 499.71319580078125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1755.6507568359375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1178.7486572265625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 852.263427734375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 905.2890625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1404.154296875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 908.3554077148438 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1426.49365234375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 833.1561889648438 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1632.8448486328125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1077.349609375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1599.4300537109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 912.2205810546875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1348.38671875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1165.61962890625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1052.887939453125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 905.7593994140625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1296.5849609375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1056.0972900390625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 673.4979858398438 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1351.5235595703125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 2031.765869140625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 958.2192993164062 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 725.603759765625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 630.6297607421875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 887.6143798828125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 781.860107421875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 884.414794921875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1197.730712890625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 547.7852172851562 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1184.7869873046875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 647.1817626953125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 693.4445190429688 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 620.6521606445312 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1269.2965087890625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 707.8642578125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 575.3477172851562 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1303.9552001953125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 426.5964050292969 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 773.5585327148438 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 505.5827331542969 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 686.0084228515625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1000.4929809570312 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1436.1483154296875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1121.6839599609375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 661.9635620117188 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 661.4717407226562 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 8119.11669921875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 884.68701171875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 671.5823974609375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 667.82958984375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 607.5787963867188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1070.5911865234375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1806.31396484375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1398.2496337890625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 772.406005859375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 908.9298706054688 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1957.6533203125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 956.6282958984375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 663.17724609375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 837.2005615234375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 483.270263671875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 529.8792114257812 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 557.3530883789062 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 470.5025329589844 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 543.2766723632812 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1774.2672119140625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1033.0914306640625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 755.06689453125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1022.9923706054688 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 691.6776123046875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1245.0814208984375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1100.1392822265625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 771.9168701171875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1249.9539794921875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1811.823974609375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 978.7547607421875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 889.9755859375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 827.8721923828125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 891.819580078125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 673.4271850585938 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1139.205810546875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 672.8888549804688 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 653.8877563476562 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1439.47412109375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1270.4197998046875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 802.443115234375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 692.5633544921875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 812.858154296875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1187.53759765625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1346.9886474609375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 885.5117797851562 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 724.54931640625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 577.9712524414062 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 697.0294189453125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 899.9943237304688 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 700.5113525390625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 862.4441528320312 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 558.8685302734375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 590.4115600585938 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 680.850341796875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 767.1428833007812 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1344.756591796875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 790.181884765625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 622.84765625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1279.724365234375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1487.1552734375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 835.1206665039062 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 516.22265625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 711.9949951171875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 737.8572387695312 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 902.6316528320312 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 899.5292358398438 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 867.4300537109375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 677.102783203125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 555.109375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 788.300537109375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1748.5196533203125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1072.6142578125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 886.927978515625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 890.2034912109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 845.1917724609375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 943.1088256835938 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1281.742431640625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1095.136962890625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1115.2412109375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2897.21923828125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1070.5718994140625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 635.4409790039062 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 624.1686401367188 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 923.6461791992188 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1109.39306640625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 912.2523193359375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 476.7066650390625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 491.7234802246094 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 923.1942138671875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 799.463134765625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 488.96636962890625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 829.0205688476562 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 619.679931640625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 882.499755859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 791.2030029296875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 859.6832275390625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 648.5775146484375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 895.484619140625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 847.4746704101562 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 950.4017944335938 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 649.2870483398438 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 501.7254943847656 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 754.1195068359375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 851.9185180664062 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 2242.984375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1182.609130859375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 800.2380981445312 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 873.2980346679688 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 755.2626953125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1563.0888671875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1854.11474609375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 733.9044189453125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1177.777099609375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 866.2806396484375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1289.7008056640625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1296.961181640625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 921.202392578125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1470.8756103515625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2344.763427734375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 2624.023681640625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 2706.385498046875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1424.4224853515625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1519.5748291015625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 996.072509765625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1299.121826171875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 475.15655517578125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 805.8367919921875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 717.8346557617188 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 723.8619384765625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1463.26220703125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1083.2298583984375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1731.80322265625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 889.4901733398438 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1074.6982421875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 677.2921752929688 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1020.9435424804688 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 884.3079223632812 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 734.7608032226562 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 899.3764038085938 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1255.3819580078125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1002.4563598632812 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 716.1954956054688 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1133.3238525390625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1362.323974609375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1034.100341796875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 847.9386596679688 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 528.2449951171875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 715.5308227539062 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 850.1622924804688 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 895.48486328125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 627.6074829101562 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 543.021484375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1314.5899658203125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 706.0731811523438 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1594.578857421875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1602.791015625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 2029.6407470703125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 942.3311157226562 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 881.9305419921875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1733.8857421875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 568.9697265625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 607.5945434570312 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 441.60986328125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 654.8695678710938 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 915.0538330078125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 803.973388671875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 428.6796875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 800.2479248046875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 853.10791015625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 542.927734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1279.6861572265625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1378.6622314453125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1154.1126708984375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 484.90191650390625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 901.49560546875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 2528.420654296875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 898.9057006835938 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 964.4124755859375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 674.4990234375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 861.3703002929688 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 437.95166015625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 685.8624267578125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 651.2570190429688 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 835.1185913085938 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1048.0958251953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 571.8429565429688 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 576.8700561523438 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 626.1192016601562 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1137.9609375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1126.8875732421875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 638.9449462890625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 737.6900024414062 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 717.5396118164062 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1035.14501953125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 868.8671264648438 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 809.7537231445312 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 771.58935546875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 775.2291259765625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1499.0794677734375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 820.9620361328125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 755.5009765625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 775.4320678710938 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 794.3646240234375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 920.3684692382812 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 473.35260009765625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 618.0338745117188 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 718.2493896484375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1615.8607177734375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1293.5025634765625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:02:47<104415:01:58, 1252.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 752.6302490234375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1854.033447265625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 676.272705078125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 830.5025634765625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 632.4591064453125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 637.07763671875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 473.3608703613281 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 796.97900390625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1206.25 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 815.3682861328125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 718.0364379882812 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 711.584716796875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 886.0274658203125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1141.36669921875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 686.4074096679688 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 516.3157348632812 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 767.301513671875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 546.2442626953125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 726.9853515625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 484.7882080078125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 932.7803344726562 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 697.8864135742188 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 686.029296875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 817.4807739257812 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1582.4268798828125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1771.531494140625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 608.9551391601562 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 351.9706726074219 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 840.9676513671875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1631.060791015625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 743.319091796875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 430.2059020996094 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 795.6112670898438 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 801.0863647460938 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1038.62744140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 604.0894775390625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 667.8538818359375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 484.13385009765625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 2192.46875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1903.9989013671875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 473.5129089355469 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 633.017333984375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1242.0322265625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 4101.7822265625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1001.1124877929688 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 828.8983764648438 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1428.86669921875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1460.8377685546875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1054.14404296875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 2447.614990234375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1476.22021484375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1027.3157958984375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 919.9747924804688 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 545.7027587890625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1162.7322998046875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1100.969482421875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 649.3986206054688 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1215.9910888671875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 962.0673217773438 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 591.2293090820312 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1571.279052734375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 542.0701293945312 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 720.5740356445312 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 724.8817749023438 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 532.2508544921875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 880.3099365234375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1807.2769775390625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 2798.1396484375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1623.036865234375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 834.9408569335938 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1362.919189453125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 944.2123413085938 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1163.4891357421875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 997.5790405273438 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 520.1543579101562 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 750.3590087890625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 689.4610595703125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1314.81201171875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 910.0624389648438 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 633.1007080078125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1724.185791015625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 598.9776000976562 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 491.85211181640625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1010.0963134765625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1240.0189208984375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 472.5697326660156 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 665.381103515625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 451.4809265136719 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 2034.736328125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1159.648193359375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1060.2850341796875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 789.6033325195312 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 914.1262817382812 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 557.4773559570312 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 507.3248291015625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 588.1160888671875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1170.670166015625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 683.6580200195312 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 858.3048095703125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 703.116943359375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1331.114501953125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 663.2982788085938 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 859.5159912109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1287.68896484375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1437.178466796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 445.9004211425781 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 738.4046630859375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1084.301513671875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 836.5344848632812 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 912.154296875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 656.328125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 363.8453674316406 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 786.0750732421875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 567.248291015625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 749.0780639648438 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 3318.830810546875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1693.4381103515625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 795.9981079101562 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1040.4658203125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1114.85791015625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1409.3494873046875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 978.5269165039062 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1204.75732421875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1206.0560302734375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 487.776123046875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1418.6497802734375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1108.736083984375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 631.7078857421875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2398.003662109375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 597.019775390625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 969.7259521484375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 911.5182495117188 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1515.8984375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 850.8447265625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 693.719970703125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1103.62744140625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 567.2476806640625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1134.2589111328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1056.4168701171875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 980.419921875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1311.492919921875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 946.7494506835938 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 863.7369384765625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1021.580322265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1702.9344482421875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 692.5447998046875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 777.82861328125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1539.5596923828125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 882.7793579101562 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 826.9776611328125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 597.8704223632812 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 897.4530029296875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 566.4630126953125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 978.1953125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 782.9306640625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 607.8667602539062 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 934.906005859375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1422.193359375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 651.6165161132812 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 753.0838623046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1013.1224975585938 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1425.785888671875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 2028.7421875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1473.1484375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1490.9862060546875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 1477.04736328125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1074.7515869140625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 659.2498168945312 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 732.6100463867188 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 2751.54345703125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 866.5230102539062 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 786.0323486328125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 2681.206787109375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 906.100830078125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 565.170166015625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1474.36572265625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1446.3941650390625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 831.9812622070312 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1818.736083984375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1289.712890625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 590.94384765625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 960.9463500976562 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 620.3491821289062 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1017.0099487304688 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 775.8660888671875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 361.3968505859375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 574.4774169921875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 967.466796875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 663.1326904296875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1526.8677978515625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 839.8634033203125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 694.0169067382812 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 483.6210632324219 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 3186.66455078125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 570.78662109375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1130.42822265625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 678.4588012695312 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1024.7420654296875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 956.7122192382812 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 554.9157104492188 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 810.8012084960938 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 858.368408203125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 712.8370361328125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 910.338134765625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1036.5848388671875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 849.1522827148438 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1083.28759765625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1221.9429931640625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 863.2880249023438 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1514.701904296875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 552.961669921875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 481.1908874511719 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2115.140625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 724.4871215820312 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 535.0050659179688 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 3358.5361328125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1449.6724853515625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 846.2418823242188 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 731.8829956054688 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 734.2454833984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 2611.296875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1040.6456298828125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 920.082763671875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1758.8072509765625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 662.7799682617188 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 662.4098510742188 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 647.6442260742188 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 732.4305419921875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1581.0970458984375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 818.7759399414062 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1046.384033203125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 503.0062255859375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 745.9645385742188 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 873.72314453125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 902.2605590820312 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 679.8785400390625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 621.0835571289062 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1392.759521484375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 956.23486328125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 908.4443359375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 638.895751953125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 484.9748840332031 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 984.4732055664062 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1043.4842529296875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 701.3189697265625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 658.6640014648438 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 644.8764038085938 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1960.1376953125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 885.9971313476562 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1188.077392578125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 854.8844604492188 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 458.3414611816406 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 840.0487670898438 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 827.3888549804688 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1643.5511474609375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 549.8980712890625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 827.7335205078125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 683.0062255859375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 822.6077270507812 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 665.1373901367188 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1018.3329467773438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 679.04443359375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1111.612060546875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 570.2957153320312 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 548.338134765625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 413.67572021484375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 889.2481689453125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 432.7895202636719 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 748.9539184570312 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 855.8731689453125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 623.7417602539062 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1077.2845458984375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 7551.744140625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 495.7235107421875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2565.458740234375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 784.4547119140625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 698.492919921875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1200.4432373046875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 529.982666015625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 857.0781860351562 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 749.2090454101562 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1213.9169921875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 847.058349609375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 920.0631103515625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 3575.134521484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 755.03369140625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 769.880126953125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1012.893798828125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1045.43359375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 996.4842529296875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 821.200439453125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1049.9317626953125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 2002.9749755859375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1297.143310546875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 938.0015869140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 691.7063598632812 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1587.459228515625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 2310.10595703125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1243.605224609375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1594.1663818359375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2430.51806640625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1384.645263671875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 999.99755859375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1245.0596923828125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1654.025146484375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1461.8883056640625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1479.0494384765625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1512.3553466796875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1485.1513671875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1155.3143310546875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1399.9361572265625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1711.2613525390625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 4525.9833984375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1068.3209228515625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1551.7054443359375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1020.5931396484375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 5758.27880859375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 930.7359008789062 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 6257841.0 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1933.9036865234375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1771.6021728515625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1245.7188720703125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1442.799072265625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1495.292236328125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1156.62451171875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 783.8126831054688 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 668.0992431640625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1249.196533203125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 2153.32666015625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 763.8530883789062 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1152.23583984375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 690.207275390625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2843.366455078125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 4012.118408203125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 958.6683959960938 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1154.042724609375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 7200.33251953125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1523.114501953125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1082.66015625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 759.7896118164062 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1383.026611328125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1227.654296875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 934.591796875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 685.0221557617188 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1566.4188232421875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 542.1039428710938 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1519.123779296875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 641.0933837890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 607.0935668945312 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 3727.3837890625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 822.6994018554688 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1946.2427978515625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1950.6456298828125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1093.583740234375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1115.0360107421875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1555.3902587890625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1590.54296875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 557.4959716796875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 594.9761962890625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 5910.4287109375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 2693.591064453125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1488.4180908203125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 886.1740112304688 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1080.861572265625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1068.4830322265625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 679.9800415039062 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 529.6212768554688 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1018.0817260742188 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 513.2288208007812 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1662.7833251953125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1238.072265625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1266.0966796875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 481.5189514160156 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 836.9315185546875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 902.038818359375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 861.6201782226562 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1541.341796875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1329.4266357421875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 9253.0107421875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1013.174072265625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 563.4149169921875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 781.0018920898438 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1255.1248779296875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1626.062255859375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1038.002197265625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 418.2406311035156 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1005.5177001953125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1422.0377197265625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 673.4801635742188 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 896.1392211914062 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 2158.802490234375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 871.519775390625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 444.7428894042969 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 587.9107055664062 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1707.4163818359375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 816.8713989257812 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 687.82421875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 533.9451904296875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1803.0931396484375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1031.5914306640625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1065.2900390625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 640.47216796875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1499.966552734375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 988.0282592773438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1172.927734375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1025.94921875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 779.6097412109375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 878.0726928710938 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 679.6130981445312 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1256.9024658203125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 968.8993530273438 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 665.538818359375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 493.05364990234375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 940.7200927734375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 527.4146728515625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 482.0544128417969 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 732.8845825195312 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 763.3590698242188 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 532.7415161132812 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 757.29052734375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 527.3627319335938 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1721.6923828125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1015.6331787109375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 937.8826904296875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 967.5370483398438 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1364.5928955078125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1890.2447509765625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1003.5274658203125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 646.1605224609375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 611.9567260742188 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1408.2666015625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 704.14208984375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 552.2146606445312 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 947.963623046875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 725.1715087890625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 791.3745727539062 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1111.483642578125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 757.8718872070312 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1152.0042724609375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 853.7694091796875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 406.67059326171875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 525.5833129882812 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 547.8624267578125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1602.457763671875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 683.8076171875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 517.96484375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 966.2818603515625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 2308.549560546875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 868.0465698242188 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1201.080810546875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 510.6693115234375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 653.3547973632812 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 705.7061767578125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1049.2689208984375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 957.0310668945312 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 592.4335327148438 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 631.1317749023438 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1127.2958984375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 2456.801513671875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1587.6436767578125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 860.347900390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 794.5639038085938 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1019.7584838867188 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1333.2572021484375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1490.291748046875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1087.791748046875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 449.785888671875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1461.6593017578125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 3217.64501953125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 3473.32666015625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1329.4716796875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 3045.007080078125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1056.939697265625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 638.58837890625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1371.29345703125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1839.762939453125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 952.658447265625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 553.767578125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 898.4788208007812 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 708.5914306640625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1125.3623046875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 837.6692504882812 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1662.943359375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1199.312255859375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1069.4215087890625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1329.615966796875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 935.6962280273438 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 896.85595703125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 774.5982055664062 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 760.0567016601562 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1299.1470947265625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 629.648193359375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 649.9920654296875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1365.3050537109375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1042.13134765625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 833.8573608398438 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1063.3184814453125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 944.4312133789062 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1346.231201171875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 2187.96337890625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1276.1151123046875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 976.7811889648438 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1440.6351318359375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 836.8564453125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1128.07080078125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1282.6357421875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1193.6822509765625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1528.5433349609375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1575.7825927734375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1719.267822265625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1583.6072998046875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1210.18798828125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 2187.6689453125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2548.958251953125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1176.4298095703125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 517.0341796875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 690.673095703125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 743.776611328125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1849.4708251953125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 2020.865234375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 548.0266723632812 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1071.6424560546875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 4111.92431640625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 792.7728271484375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 745.0397338867188 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 947.3028564453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 668.16796875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 2306.2275390625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1175.16357421875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 560.0952758789062 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2895.81103515625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 687.8598022460938 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 533.1341552734375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 751.3395385742188 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 787.2071533203125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1414.3729248046875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1465.049072265625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1244.56982421875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1514.60400390625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 2323.88916015625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 9129.8486328125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2009.2955322265625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 785.16748046875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 946.902587890625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1007.0406494140625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 675.0151977539062 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 630.3981323242188 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 2245.1279296875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 919.45947265625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 2915.57421875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1147.739013671875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 909.6846313476562 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 761.091064453125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1096.541748046875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1122.6395263671875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 2679.12890625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 2411.91015625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 661.5535278320312 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 885.0274047851562 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 4902.2265625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1983.8922119140625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1822.4937744140625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 2595.245849609375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 637.0679931640625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 850.8917236328125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1522.9617919921875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 840.0327758789062 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 2111.72216796875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 615.3065795898438 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 800.8495483398438 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1318.0889892578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 921.3604736328125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 800.10009765625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 881.9541625976562 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 965.1041259765625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1225.45556640625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1044.258056640625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1028.0528564453125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 739.2972412109375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 681.6310424804688 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 980.8894653320312 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 742.4735717773438 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 417.59075927734375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1212.198486328125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 457.0159606933594 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 656.4566650390625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 610.8565063476562 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1257.686279296875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 2346.686279296875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1022.455078125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 467.28424072265625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 851.8810424804688 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1046.805419921875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1582.977294921875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1307.0816650390625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1330.965087890625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1188.3748779296875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1864.532958984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1081.5242919921875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 3795.9990234375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 818.94580078125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1001.8418579101562 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1116.93994140625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 674.68310546875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1023.797607421875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1429.10498046875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1051.893798828125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1584.6256103515625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1016.453369140625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 641.0611572265625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1409.3563232421875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1464.867431640625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 5332.97021484375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 3044.176025390625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1431.845703125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1329.18798828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 623.65576171875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 729.703369140625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 736.87646484375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1832.21826171875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 846.3067016601562 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 2228.271484375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1284.3656005859375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1750.9478759765625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 525.0588989257812 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1263.262451171875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 2063.8173828125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2659.21728515625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 757.3685302734375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 512.3563232421875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 887.310546875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 560.267333984375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1975.244873046875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 854.7683715820312 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 998.169189453125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1483.290771484375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 2296.993408203125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1264.7952880859375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1277.5439453125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 848.29150390625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 739.556884765625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 835.4185180664062 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1187.130859375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 812.3251953125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1624.98291015625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1387.9512939453125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1013.2819213867188 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1971.9005126953125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 3476.921630859375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 931.2136840820312 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1232.5997314453125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1372.5494384765625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2057.2919921875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 842.2469482421875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 860.8494873046875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1027.219970703125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1801.6339111328125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1618.732666015625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 2310.85546875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1535.797119140625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1201.7763671875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1566.3262939453125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 873.6555786132812 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1920.848876953125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1273.2454833984375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 710.8993530273438 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 770.4006958007812 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1452.9578857421875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1174.1651611328125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1345.11083984375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1622.2967529296875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 959.14990234375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1341.1397705078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 892.1445922851562 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1352.827880859375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 533.7803344726562 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 927.2144165039062 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1301.4686279296875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 715.6748046875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 698.9829711914062 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 575.3885498046875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1360.7301025390625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 781.3133544921875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1003.4856567382812 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 760.96484375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1128.6837158203125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 759.82421875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 540.1314086914062 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 460.3612976074219 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1083.019775390625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1061.5367431640625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1061.06103515625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 755.7830810546875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 503.4947509765625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1265.6322021484375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:23:33<104182:25:27, 1250.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1584.7200927734375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 2522.063720703125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 783.1817016601562 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1825.206787109375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1010.2967529296875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1604.8709716796875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 771.7916870117188 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 860.9619750976562 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 803.2359619140625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1176.213134765625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 975.2572631835938 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1031.744140625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 527.2427368164062 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1207.408935546875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 929.5577392578125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 539.5501098632812 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 829.91650390625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1248.2078857421875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 2327.460693359375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2123.90283203125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1383.4783935546875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1066.2979736328125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1161.9910888671875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1391.2325439453125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 733.9522094726562 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1699.6217041015625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 748.254150390625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 543.7529907226562 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 962.3690185546875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1640.9290771484375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1227.267333984375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 870.9204711914062 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 694.8004150390625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 603.9124755859375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1061.6978759765625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 776.8067016601562 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 893.63916015625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1804.1346435546875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 942.750244140625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 977.5189208984375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 873.0130615234375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1520.846435546875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1015.1547241210938 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 681.7649536132812 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1071.1519775390625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1817.87548828125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1261.338134765625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 813.5053100585938 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 7428.54541015625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1447.020751953125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1265.09521484375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1595.0130615234375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1310.4267578125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 969.7203979492188 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1216.6103515625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 2775.18408203125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 758.4135131835938 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1573.0115966796875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2521.747314453125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1100.0152587890625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1022.7073364257812 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 946.753173828125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1559.1998291015625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 2885.540771484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1270.880615234375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 504.85491943359375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1842.45361328125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 988.71875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1116.8275146484375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 656.904052734375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1539.28271484375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 498.19610595703125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 2693.589111328125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1493.9989013671875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1985.125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 851.5733032226562 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 653.4430541992188 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1044.5267333984375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1207.5244140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1015.6820678710938 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 846.3663330078125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1166.40283203125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1169.080810546875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1273.0458984375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1217.1444091796875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 902.968994140625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 601.2560424804688 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 667.97900390625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 977.5143432617188 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 857.49951171875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 965.5616455078125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1541.13134765625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1449.6728515625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 599.2990112304688 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1059.3656005859375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1153.451904296875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1027.526123046875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 946.6946411132812 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 2521.154296875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1948.09521484375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1046.6072998046875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 7129.06396484375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 709.6723022460938 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1908.02783203125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 2497.61474609375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 2404.927978515625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 890.3314208984375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 720.342041015625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1034.3499755859375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 934.140869140625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 725.7160034179688 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1214.9061279296875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 4928.9541015625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 571.637939453125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 809.9182739257812 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1208.475341796875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1523.5213623046875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1971.3839111328125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 613.354248046875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 830.3164672851562 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1440.173095703125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 933.21923828125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1691.1019287109375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 808.6168823242188 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1693.9515380859375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 2874.408203125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 2950.615966796875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1843.427734375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 861.874755859375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1952.9957275390625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 538.4989624023438 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 2228.681396484375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 985.5089721679688 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1153.9669189453125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 945.6641845703125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1179.7220458984375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1843.804931640625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1174.173828125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 883.67626953125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 799.1065673828125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1492.385498046875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 2065.237060546875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 970.1724853515625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 600.7402954101562 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 2357.679443359375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1070.5555419921875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 6638.04248046875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1351.23388671875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 846.5435180664062 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1048.5914306640625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1133.9642333984375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 3989.85986328125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 827.5047607421875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 928.5962524414062 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 600.3515014648438 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1779.047119140625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 2411.290283203125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1192.2515869140625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 640.1885986328125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 881.1749267578125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1674.1258544921875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1146.9600830078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1380.6700439453125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 890.5250854492188 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1111.1815185546875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1132.6258544921875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1903.8253173828125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 983.5913696289062 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 925.9964599609375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 893.3265991210938 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 524.44384765625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 2735.89453125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1341.6959228515625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 760.837890625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 585.1689453125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 689.1036376953125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 826.8919067382812 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 596.2318725585938 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2282.56396484375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 836.66796875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 836.2520751953125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1134.10205078125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 3385.679931640625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1788.809326171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 886.1516723632812 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1496.8204345703125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 445.17645263671875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1868.0069580078125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1581.000244140625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1344.910400390625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1690.061279296875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1165.82763671875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1596.61767578125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 2312.496337890625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 3623.76416015625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1226.3367919921875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1018.9065551757812 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2346.38134765625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 778.5728149414062 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 765.6546020507812 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 768.7073364257812 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 865.3034057617188 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 888.1141357421875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1361.4022216796875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 862.1702270507812 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1971.549560546875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 978.7467041015625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1337.3287353515625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2351.843017578125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1246.4898681640625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 2098.007080078125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1043.692626953125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1542.473388671875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 393.4815368652344 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 665.98486328125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 843.8029174804688 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1288.3909912109375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 956.327880859375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1625.176513671875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 606.3300170898438 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1218.7357177734375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1869.2103271484375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 2171.41748046875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1083.4351806640625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 893.766845703125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 758.9449462890625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 947.0878295898438 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 871.9526977539062 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1632.0594482421875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 590.8011474609375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 974.1805419921875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1176.895263671875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1096.8011474609375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1164.7183837890625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 844.8687744140625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 643.9083251953125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 689.318359375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 2809.149169921875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1189.023681640625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 612.5004272460938 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 967.982177734375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1696.113037109375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1346.4503173828125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 3931.60986328125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 956.9602661132812 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1202.4256591796875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 888.379150390625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 2316.525634765625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 933.6183471679688 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1764.5645751953125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1742.901123046875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 2050.807373046875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 3455.321044921875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 2864.46875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1235.9931640625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1203.7491455078125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1249.5477294921875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1636.0140380859375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1123.3055419921875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 558.64599609375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1448.039794921875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 2236.565185546875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 703.378662109375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1398.3424072265625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 950.6317138671875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 770.7864990234375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 882.7366943359375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1411.754638671875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1270.4903564453125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 809.1133422851562 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 857.4282836914062 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 3027.007080078125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1000.3558349609375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 670.2030639648438 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 847.6633911132812 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 2602.140869140625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1110.260986328125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 650.6322631835938 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1078.15185546875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1043.6309814453125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1065.6064453125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 970.888916015625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 937.8013305664062 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1697.3409423828125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1050.789794921875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 850.3291015625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1899.08154296875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1037.474853515625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1075.8431396484375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1110.641845703125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1005.7223510742188 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 4303.17138671875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1183.0400390625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 976.0846557617188 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1150.851318359375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 5541.26171875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1541.0936279296875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1401.291259765625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 598.12060546875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 3585.359375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1258.403564453125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1088.787109375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1577.6875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1308.45654296875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 957.1454467773438 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1047.422607421875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 31551.919921875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 2794.0283203125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1744.34716796875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 924.2145385742188 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1720.2452392578125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 3225.13427734375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1466.0682373046875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1401.1510009765625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 999.5131225585938 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 925.4232788085938 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 2322.253662109375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1553.8433837890625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1629.2484130859375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1127.132080078125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1754.555419921875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 4481.08740234375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 2300.44287109375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1334.0692138671875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1549.5230712890625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1996.595947265625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1118.5274658203125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 998.8025512695312 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1503.214111328125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 3513.916748046875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 712.8074340820312 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1509.8748779296875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 861.1102294921875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1464.1707763671875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1180.44091796875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 687.306884765625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 698.5330200195312 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 3212.2890625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 957.0125122070312 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 703.8308715820312 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 989.61669921875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 902.2254028320312 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 734.4569702148438 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 581.3987426757812 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 888.288330078125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 929.21826171875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 731.419921875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 669.5099487304688 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 701.625732421875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1598.6220703125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 622.64990234375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 611.6812133789062 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 924.1514892578125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1421.2254638671875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1090.9056396484375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1204.6895751953125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1337.115234375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1957.327880859375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1465.2437744140625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 848.9862060546875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 807.8248291015625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 914.1660766601562 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1915.1522216796875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2226.654052734375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 808.632080078125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 863.3804931640625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1036.12646484375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2445.4619140625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 2015.2308349609375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1258.7806396484375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 722.3617553710938 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1147.0804443359375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1463.3612060546875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 579.2542114257812 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1104.190673828125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 937.7050170898438 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 853.0517578125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 928.9243774414062 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1071.8531494140625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 899.471923828125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 882.1117553710938 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1047.1419677734375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 901.4927368164062 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 688.4920043945312 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 788.1695556640625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 721.2926635742188 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1833.9141845703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1291.0499267578125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 646.5770874023438 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1052.901123046875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 2426.8994140625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1483.3162841796875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 825.252685546875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 761.8389282226562 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1310.9744873046875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2370.696044921875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1463.0594482421875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 898.16162109375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 586.6929931640625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1359.5318603515625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1181.8419189453125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1028.122802734375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1102.54296875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 787.6887817382812 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1655.4072265625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 772.4896850585938 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1571.376708984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 975.047119140625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 601.1258544921875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1318.5931396484375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1467.0037841796875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1314.151123046875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 855.625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1572.35205078125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 783.4077758789062 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1482.44189453125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 793.8539428710938 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1220.2847900390625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 882.3593139648438 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1201.6038818359375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1120.3731689453125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 759.5671997070312 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 763.8788452148438 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 669.3632202148438 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 705.4119262695312 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1588.212890625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 965.9791259765625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 767.9126586914062 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 891.6498413085938 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 829.654541015625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1658.8218994140625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 807.5745849609375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 627.4996337890625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1269.7442626953125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 611.8668212890625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 649.1166381835938 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1344.17138671875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 619.5704345703125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 670.733642578125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1412.4456787109375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 2956.485595703125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 2168.599853515625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1135.172607421875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 884.4180297851562 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 700.2810668945312 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 681.853271484375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 465.0174255371094 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 961.191650390625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 828.9849853515625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1351.885009765625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 2393.57080078125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1655.3822021484375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1437.2752685546875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1213.0048828125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 872.676513671875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 990.0921630859375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 821.576416015625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 688.5897216796875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 627.2753295898438 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1445.4237060546875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1725.80322265625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1544.8660888671875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1139.2547607421875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1356.793212890625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 7269.14697265625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 681.1259765625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1424.934326171875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 2017.212158203125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1175.62841796875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 934.5777587890625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 2005.9306640625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1191.90234375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1148.8282470703125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 882.0452270507812 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 705.1607666015625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1308.8992919921875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 854.6876831054688 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1448.6990966796875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 891.9042358398438 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 942.4061889648438 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 920.7347412109375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1796.672119140625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 915.594970703125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 794.9534301757812 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 688.311279296875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2200.2158203125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 2129.443359375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 737.6328125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 758.3342895507812 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 641.1708984375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1994.603759765625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1147.2266845703125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1045.6060791015625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1040.14794921875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 714.707275390625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1007.7368774414062 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1181.1585693359375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 997.3995971679688 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 779.208251953125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 963.1482543945312 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 679.2528076171875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 777.1640625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 856.554931640625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 810.5385131835938 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1375.0545654296875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 861.1743774414062 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 448.061279296875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1501.6588134765625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2394.40087890625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1750.6104736328125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 731.7855834960938 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 976.607177734375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1420.7099609375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1822.21630859375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 723.6099853515625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1134.48486328125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1174.6312255859375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 838.8399047851562 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 727.6786499023438 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2360.9287109375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 773.0401611328125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 748.2798461914062 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 782.4791259765625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1519.889892578125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1230.7821044921875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1020.6005859375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 717.9065551757812 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 2691.472900390625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1189.966064453125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 829.5652465820312 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 761.4827270507812 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1810.0491943359375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 646.971923828125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2066.854736328125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 810.2385864257812 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 915.2288208007812 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 2458.17138671875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1013.3037109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1430.776611328125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 966.1958618164062 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1393.1326904296875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1372.1470947265625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1011.6531982421875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 649.0753173828125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1397.6622314453125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1138.1041259765625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1054.53759765625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1174.027587890625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1102.9639892578125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1104.50146484375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2831.953125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1100.076171875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1518.8511962890625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1073.9737548828125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1118.5538330078125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1615.0465087890625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 897.7220458984375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1071.8839111328125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 929.1012573242188 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1519.3150634765625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1539.9647216796875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 611.9158935546875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 992.4979858398438 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1659.2152099609375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1908.83935546875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2340.540771484375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1281.4755859375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1365.38720703125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 2121.18701171875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 463.6986999511719 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1046.6514892578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1013.4507446289062 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1103.632080078125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 780.7938232421875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 2350.7822265625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 992.2197265625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1359.7882080078125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 919.7520141601562 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1513.1568603515625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 850.4583740234375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 2206.68359375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 999.9443359375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1000.3018188476562 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 930.5833129882812 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 2955.703125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2549.080322265625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1086.2222900390625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 2685.04443359375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1912.0946044921875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1500.67041015625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1509.1033935546875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1923.7056884765625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 3943.385986328125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 892.3356323242188 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 950.4168701171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1384.7091064453125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1011.9259033203125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 855.7854614257812 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1038.1700439453125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 826.0151977539062 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 3243.169189453125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1940.8536376953125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1229.8018798828125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 2522.6171875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 5326.10791015625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1118.75537109375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1076.194091796875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1807.064453125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1041.9908447265625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 807.5970458984375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 758.1817016601562 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1759.2144775390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 865.2874145507812 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1709.22705078125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1888.89599609375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 845.0960693359375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1339.61962890625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1768.9151611328125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1216.506103515625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1324.0994873046875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 2082.950439453125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1615.9613037109375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 724.8759765625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 700.0806274414062 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1993.0521240234375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1005.7318115234375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 2309.27587890625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2238.010498046875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1060.41943359375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1215.270263671875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 807.4981689453125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3550.173095703125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1012.4375610351562 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1129.0411376953125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1260.4132080078125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1660.5076904296875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 825.5433349609375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 830.8043823242188 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1355.5531005859375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2115.149658203125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1303.35009765625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1500.709716796875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1154.81005859375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1608.6376953125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 851.8482666015625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 991.4935302734375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 968.9657592773438 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1269.6903076171875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1105.6671142578125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1317.436279296875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 662.9321899414062 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1064.185546875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1195.0616455078125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1980.1861572265625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1027.3819580078125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 899.6862182617188 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1333.532958984375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1470.1141357421875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 2267.165771484375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 589.4225463867188 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1095.0406494140625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1614.9932861328125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1511.28662109375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 902.6524658203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 903.6901245117188 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 2050.006591796875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 958.5694580078125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 3565.9169921875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 2136.287353515625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 3602.953857421875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1034.9530029296875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2273.76171875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1786.73193359375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 752.3960571289062 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 828.1586303710938 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1464.3529052734375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1204.4259033203125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 2297.064453125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1781.53125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 3976.5576171875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 970.51904296875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1735.7347412109375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1038.092041015625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 4474.619140625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 2512.485595703125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 739.74365234375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 2740.51904296875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 2091.660888671875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1058.4027099609375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1482.371337890625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 4059.833740234375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1133.12744140625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3519.493408203125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 2213.3583984375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1512.4361572265625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:44:33<104480:26:54, 1253.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 3069.833740234375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 986.4855346679688 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1511.8751220703125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 581.5512084960938 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 658.9463500976562 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 841.07763671875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 872.7089233398438 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1217.7423095703125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 981.8101196289062 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1541.69775390625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1063.6180419921875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1141.69775390625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1659.769775390625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 2128.571533203125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 992.7410278320312 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 676.1179809570312 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 778.4612426757812 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 840.3602294921875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1216.20654296875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1019.2770385742188 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1014.5979614257812 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1050.937255859375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 2137.65625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1499.766357421875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 3524.005615234375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1267.432373046875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 3504.056884765625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1517.89599609375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1383.6475830078125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 3222.42431640625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1068.497314453125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 958.97900390625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1669.850341796875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1072.94189453125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 2831.8955078125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1955.43359375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1056.070556640625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1504.995361328125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1344.78173828125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1240.04931640625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 591.41357421875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 983.7432250976562 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1372.9560546875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 2723.050048828125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1203.2236328125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1641.4898681640625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2069.058349609375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1129.689697265625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1572.79443359375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1550.888916015625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 744.279296875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 664.9472045898438 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1023.7056884765625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1012.625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1029.099365234375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 933.8231811523438 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 995.3403930664062 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2151.224853515625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1112.8807373046875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 669.0955200195312 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 841.5715942382812 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1899.3790283203125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 2060.58642578125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 2045.017822265625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 977.6285400390625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 982.2420654296875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2609.06201171875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 5393.78125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1493.4014892578125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1740.7113037109375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 663.5794067382812 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1369.92041015625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1410.950439453125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 5447.509765625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1020.6549072265625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 700.167724609375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 901.9656982421875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1459.0465087890625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1404.626220703125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1246.1990966796875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 815.6856079101562 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1688.7305908203125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 647.6046752929688 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1392.2342529296875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1367.1409912109375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 688.8440551757812 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 2167.66357421875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1238.9884033203125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1310.166748046875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 2192.31298828125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1596.1058349609375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 928.037841796875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 855.5074462890625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1891.892822265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1361.95556640625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1532.6829833984375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1190.2071533203125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 895.655517578125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 897.3193359375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1380.2330322265625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1052.818359375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1252.4669189453125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 821.292724609375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 853.7139282226562 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 633.5595092773438 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1198.457763671875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1116.721923828125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1613.770263671875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1146.3516845703125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1653.6910400390625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1249.8917236328125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 2584.137939453125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 782.0192260742188 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 711.6060180664062 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 858.8560180664062 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 957.1721801757812 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1458.6536865234375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 884.8864135742188 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 595.9680786132812 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 3078.17236328125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1701.03759765625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1161.0400390625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 545.41357421875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1937.638916015625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1574.0555419921875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 883.2238159179688 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1022.9749755859375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 4025.56787109375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 995.9054565429688 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 612.4468383789062 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 623.9320678710938 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 575.400390625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1045.523193359375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1630.2236328125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 951.7683715820312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 792.10888671875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1038.7257080078125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 894.0101928710938 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1191.424560546875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1030.5982666015625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 768.1102294921875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 646.8504028320312 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 568.2908935546875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 503.3907165527344 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1275.635009765625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 662.4821166992188 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 964.33642578125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 945.2653198242188 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1345.701904296875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1746.80908203125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1260.219970703125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1199.069580078125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 957.567626953125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 3663.71240234375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1949.87109375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1665.3421630859375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1701.831787109375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1245.7017822265625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1592.62890625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1257.0693359375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 5075.8896484375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 796.0205688476562 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2930.89599609375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 868.4003295898438 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 652.66015625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 954.6854248046875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1609.377685546875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 990.6064453125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1253.4178466796875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1085.314453125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1035.7445068359375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 498.2243957519531 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 710.5499267578125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 789.3501586914062 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1039.455322265625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1071.0167236328125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1394.81396484375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1215.281982421875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1281.5045166015625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1099.7369384765625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1462.0389404296875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1455.8939208984375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 694.1082153320312 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1140.7098388671875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 864.521484375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1034.6688232421875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1233.280029296875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2595.415771484375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1136.005859375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1315.8822021484375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1107.199951171875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1041.8466796875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1130.535400390625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1235.2401123046875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 696.0608520507812 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 453.93646240234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 744.5962524414062 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1049.077880859375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3031.30859375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 929.772705078125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1036.83642578125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 2122.515380859375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 871.4343872070312 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1824.648681640625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 910.6201171875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1874.451171875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 815.4550170898438 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1159.93359375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1210.1729736328125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 829.6990356445312 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1399.7939453125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1172.6553955078125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 4116.900390625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 952.1651611328125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 823.5408935546875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 878.7156372070312 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 784.9095458984375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 809.8585815429688 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1412.827392578125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1079.9906005859375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 711.0538330078125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1024.9320068359375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 699.5089111328125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 2378.487060546875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1474.5391845703125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1116.2020263671875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1847.8148193359375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 3548.7958984375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 3052.322265625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1438.9119873046875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 3328.061767578125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1152.68212890625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1493.461181640625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2321.182861328125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1908.4287109375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1362.2181396484375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 896.3215942382812 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2141.567138671875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1043.096435546875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 3061.935302734375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1042.265625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 599.990478515625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 983.3089599609375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 577.0659790039062 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1819.468994140625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 876.0057373046875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 715.99169921875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1303.1336669921875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1290.3814697265625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1554.6630859375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1219.08154296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1671.0279541015625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1453.74365234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1139.311279296875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1405.2684326171875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1355.3848876953125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1175.7347412109375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1035.6334228515625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1389.1444091796875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7439, loss_val: nan, pos_over_neg: 2694.969970703125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1019.6591796875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1086.3125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 883.3861083984375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 454.3593444824219 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 583.70361328125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 723.5941162109375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 871.8128051757812 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 937.9981689453125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1125.355712890625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 852.3876342773438 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 938.7114868164062 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1009.5423583984375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 562.1534423828125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1328.4517822265625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1174.563232421875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2038.97607421875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1284.0677490234375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1885.329345703125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 642.1168212890625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1283.9554443359375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 731.1715087890625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1307.053955078125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1024.8006591796875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1045.9969482421875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1194.6981201171875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1589.869140625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 37275.21875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 2050.90380859375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 827.91943359375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1200.3568115234375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1901.768310546875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 7119.0810546875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1201.42578125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 653.29931640625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 715.4605712890625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1304.6578369140625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1156.3463134765625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1631.13525390625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 940.138427734375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1068.255859375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1058.6478271484375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 600.6016845703125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1025.130126953125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1563.3240966796875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1576.8505859375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1460.27294921875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1176.503662109375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1169.6224365234375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1225.799560546875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1118.97900390625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 758.5171508789062 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1953.6868896484375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1324.670166015625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1512.148681640625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1406.5184326171875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1574.5858154296875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1199.6700439453125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1542.1898193359375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 921.398681640625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 2069.62158203125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1422.516357421875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 487.67498779296875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1523.6510009765625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 934.3790893554688 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1278.44384765625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 4318.76123046875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 911.81982421875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1092.4776611328125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 4017.358642578125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 762.8836059570312 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 929.5321044921875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 898.857421875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1950.19873046875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 523.1935424804688 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1022.7012939453125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 793.9417724609375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 800.5081787109375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 814.2659301757812 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 790.3522338867188 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 924.538818359375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 704.2457275390625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1398.367919921875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1186.0048828125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 710.0820922851562 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 714.261962890625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 834.1001586914062 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 679.4585571289062 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 829.8972778320312 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 804.2353515625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 971.2357177734375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 942.0325317382812 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 739.0188598632812 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 639.2225341796875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1476.087890625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 649.0316772460938 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 725.5220947265625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1041.7777099609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 783.1079711914062 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 797.7412719726562 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 604.8470458984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 510.7853698730469 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2124.066650390625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 2430.882080078125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 994.1760864257812 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 753.2942504882812 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 964.0527954101562 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2949.37060546875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 5835.46435546875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1618.600830078125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 593.8553466796875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 537.9887084960938 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1036.9793701171875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1592.0338134765625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 970.4735107421875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1176.155029296875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 970.71337890625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 2961.69580078125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 768.2310791015625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 819.7341918945312 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 505.5947570800781 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 807.4635620117188 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1230.2655029296875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 922.0537719726562 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 901.5565185546875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1236.82275390625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1381.6767578125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 3701.10205078125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 3035.35693359375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 895.7798461914062 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 955.4979858398438 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1157.4298095703125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 794.7677001953125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 861.2474975585938 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 3518.421142578125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 517.5601196289062 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 791.8385620117188 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 705.8136596679688 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1411.8328857421875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 614.029052734375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 754.1129150390625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1157.773193359375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 740.17919921875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 1020.031982421875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 909.0256958007812 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 696.9793090820312 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1035.69775390625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 913.8090209960938 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 896.4554443359375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1266.9979248046875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 943.8406982421875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1070.4130859375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1442.992919921875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 2078.56640625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 10250.8115234375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1202.474609375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 2105.38720703125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1075.830078125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 824.8428955078125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1273.43603515625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1293.7767333984375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1034.9495849609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 866.1497192382812 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1052.3328857421875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 915.2048950195312 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 3736.386962890625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1082.2852783203125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 976.4584350585938 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1071.3223876953125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1732.762451171875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1210.9617919921875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1074.710693359375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 4451.498046875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 2773.480224609375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1034.0218505859375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1112.6875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1086.1943359375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1254.1849365234375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2721.3330078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 833.09033203125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 955.7181396484375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 2879.05712890625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1372.2904052734375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1036.4495849609375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 2035.01904296875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 848.15625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 880.9561157226562 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 884.9105834960938 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1075.711181640625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 996.0806884765625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 2643.859130859375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 765.325439453125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1157.809326171875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1338.72021484375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 4118.12548828125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1885.247314453125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1055.316162109375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1249.9439697265625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 2518.440185546875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1008.3065185546875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 2003.7022705078125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2017.3734130859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1290.7613525390625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1086.2279052734375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1548.1273193359375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 2347.472900390625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 79596.546875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1526.84326171875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2474.353271484375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 2299.412109375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1832.900634765625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 956.6063842773438 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2146.088623046875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1855.4632568359375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 3787.297119140625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1859.710693359375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1667.65673828125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1461.0667724609375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1984.787109375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1256.70703125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 3038.938720703125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1857.665771484375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1635.3934326171875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1388.2210693359375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 970.5587768554688 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 689.3915405273438 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 20249.34765625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 7428.49853515625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1183.1661376953125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 727.0314331054688 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1434.4622802734375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1418.07373046875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1226.22314453125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1074.466064453125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 890.5250854492188 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1744.023681640625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1327.545654296875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 777.8035888671875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1187.0361328125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 713.4138793945312 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1005.2424926757812 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1697.4658203125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1171.458984375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1220.9107666015625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 542.4144287109375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 743.42333984375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 672.59033203125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1622.282470703125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 828.95751953125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1464.3516845703125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 3511.077880859375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 894.630126953125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 898.0928344726562 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 570.227294921875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2122.190185546875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1255.938232421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 2092.628173828125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 646.294921875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1079.1614990234375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 596.47607421875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1543.443603515625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 984.0050659179688 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 746.536376953125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 796.1641845703125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 961.8411254882812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1238.2884521484375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1178.1085205078125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1180.7296142578125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 926.9739990234375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 791.127685546875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1321.5404052734375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1029.849853515625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1945.100341796875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1866.4542236328125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2598.385009765625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1021.23974609375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1235.2528076171875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2864.364990234375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 834.4163818359375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 2466.656494140625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1461.0103759765625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1731.2781982421875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1361.0045166015625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1415.7802734375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 98098.578125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1678.4737548828125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1445.42578125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1978.7076416015625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 3901.18701171875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1013.7471923828125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1402.3453369140625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 958.7936401367188 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 828.6106567382812 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 738.3519897460938 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 911.8396606445312 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1515.013427734375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 972.1373291015625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 453.70245361328125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 883.558837890625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1018.3958740234375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1047.2730712890625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 780.9832153320312 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1818.5477294921875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1054.804931640625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1850.21728515625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 792.0269165039062 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1096.48291015625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 926.8504638671875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1213.4283447265625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 795.7630004882812 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1328.455810546875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1087.030517578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 888.7462158203125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1637.0831298828125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 956.6893310546875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 621.3638305664062 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 942.9007568359375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1801.3046875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 713.6348266601562 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 697.9544067382812 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 625.6970825195312 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 998.028564453125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 743.9781494140625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1063.9215087890625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 494.9631042480469 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 700.9376831054688 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 862.06298828125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 610.7010498046875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 981.1697998046875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1504.017333984375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1114.4163818359375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 618.0828857421875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 976.1187744140625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 791.9912109375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1920.3973388671875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1491.9158935546875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 796.5288696289062 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 749.24951171875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 432.2076110839844 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 902.7377319335938 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 819.72509765625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 689.1937866210938 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 991.0494995117188 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1458.7532958984375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 685.5252075195312 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 737.228759765625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 737.2921752929688 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1211.7952880859375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 3018.66064453125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1208.062744140625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 961.908447265625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1032.9776611328125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1264.6568603515625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1036.0235595703125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1186.0789794921875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1010.5968017578125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1430.4437255859375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 723.3914794921875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 570.68115234375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 743.7426147460938 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1895.083251953125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 2363.140625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 995.5715942382812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 671.7284545898438 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1052.6195068359375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 996.9547729492188 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1598.287109375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 700.832275390625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 705.7233276367188 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 803.0849609375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 737.7392578125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1675.5950927734375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1045.11962890625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1190.8343505859375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1850.40771484375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1278.634521484375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 2598.47216796875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 827.5897216796875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 964.881103515625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1148.72607421875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1152.2315673828125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1078.6253662109375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 855.099853515625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1809.5140380859375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1122.5052490234375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1784.587646484375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 2060.6240234375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 3008.1787109375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1044.380126953125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1593.9305419921875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 9539.177734375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1399.53466796875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 657.5552978515625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 780.1533813476562 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 844.35009765625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1270.6988525390625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1435.9822998046875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1726.6182861328125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 889.1416625976562 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: -5325.67333984375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 911.6483764648438 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 11712.34375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 765.0488891601562 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 35412.84375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1814.0830078125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 2472.986328125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 798.7938842773438 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 614.0294189453125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 977.4511108398438 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1244.531494140625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 911.722412109375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1073.1982421875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1117.151611328125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 627.1998291015625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1259.787353515625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 821.9151000976562 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1421.82666015625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1532.4600830078125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1031.7904052734375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2435.48486328125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 717.652099609375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 560.794921875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 647.5032348632812 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 2619.189453125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1037.9593505859375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1573.736572265625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 765.139892578125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 696.9185180664062 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1038.13232421875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1209.5096435546875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2730.58203125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1008.5255126953125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1079.72900390625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 638.1651611328125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1890.47998046875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 3624.2333984375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:05:35<104708:52:56, 1256.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1352.6531982421875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 814.0870971679688 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1039.388916015625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1678.6942138671875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1529.18408203125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1030.980712890625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 898.2947998046875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 857.4622192382812 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 937.5931396484375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 805.1488647460938 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1438.6220703125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1528.9136962890625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 765.0289916992188 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 660.3489990234375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 891.1591186523438 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1017.521728515625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 903.9287109375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1731.854736328125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 826.795166015625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1472.6427001953125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 951.0283813476562 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1575.060546875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 787.2499389648438 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 909.4263916015625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 625.3065795898438 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 679.7838745117188 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 885.6015625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 880.8698120117188 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1831.71533203125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1438.2783203125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 734.2785034179688 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1532.501220703125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 948.122802734375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1029.8509521484375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1115.9525146484375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1984.19873046875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 767.2022705078125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2966.618408203125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1481.1578369140625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1973.6112060546875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 2692.541259765625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2969.007568359375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1519.187255859375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1599.135498046875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 918.80029296875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1315.86669921875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1154.78271484375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1922.31201171875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 3160.53662109375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1107.7978515625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 585.8803100585938 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1180.8941650390625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1841.017578125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1740.2340087890625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 774.09375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 649.4603881835938 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1235.70458984375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 966.6477661132812 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1387.7637939453125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1190.5565185546875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 810.1123657226562 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 744.3491821289062 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1168.3302001953125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1443.4822998046875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1019.5783081054688 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1558.922119140625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1084.631591796875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 733.0227661132812 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 946.302978515625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 3199.335205078125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1376.8331298828125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1888.3277587890625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1442.156982421875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 5473.9462890625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1018.5196533203125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1349.4986572265625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1089.52880859375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1371.5565185546875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1837.614990234375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 925.2999267578125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 625.2672729492188 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1388.86328125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1872.134765625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1132.7275390625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 600.6470336914062 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 810.4141235351562 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 624.308837890625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1927.2205810546875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2990.05322265625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 898.9406127929688 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 689.6250610351562 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 715.8523559570312 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 884.9759521484375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1146.35888671875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7439, loss_val: nan, pos_over_neg: 1891.2459716796875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 621.0264892578125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 655.347900390625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 523.3939208984375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 701.3890991210938 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1414.380615234375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1159.8330078125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 699.5629272460938 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 778.4874877929688 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1046.447509765625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2130.42333984375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1004.235595703125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7417, loss_val: nan, pos_over_neg: 1108.32861328125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1007.02099609375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 732.5521850585938 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 517.4886474609375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 532.6355590820312 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 955.2907104492188 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 649.0530395507812 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 693.576904296875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1221.2342529296875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 801.4838256835938 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 621.2518310546875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 796.9249267578125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7439, loss_val: nan, pos_over_neg: 1585.716552734375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1495.193359375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 671.3770751953125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 555.7725219726562 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 943.09033203125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 663.2698364257812 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 814.2056884765625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7416, loss_val: nan, pos_over_neg: 3221.627685546875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 4574.67138671875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 921.0407104492188 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 908.5798950195312 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 989.8049926757812 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1252.1468505859375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1316.965576171875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 769.4356689453125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 812.6336669921875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 756.0709838867188 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1347.603759765625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1286.3553466796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1008.7338256835938 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 645.5806274414062 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 715.4946899414062 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1738.778564453125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 601.9530639648438 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 816.6593017578125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 796.4246215820312 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1065.9417724609375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 744.8297729492188 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 881.8012084960938 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 652.1240234375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 801.7645263671875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 1092.5452880859375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 925.1517333984375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 839.118408203125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1266.982421875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 945.9071044921875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1899.7840576171875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1232.1883544921875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2234.317626953125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 723.355224609375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1510.5107421875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 2128.710693359375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1046.4189453125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1265.5361328125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 961.6945190429688 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 968.094970703125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1188.24169921875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 720.3468017578125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 776.46240234375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1011.1119995117188 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1733.5888671875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 904.191162109375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1080.4609375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1271.7666015625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2577.645751953125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2060.477294921875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1161.5548095703125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 715.7708129882812 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 933.8890380859375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1617.463623046875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1450.7413330078125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1804.4605712890625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1067.477783203125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1495.2911376953125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1159.582763671875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1364.767822265625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1967.970703125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1880.187744140625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2044.085205078125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1354.2281494140625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 543.598876953125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1094.34130859375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 2537.724365234375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 2719.387939453125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1109.0015869140625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 656.1044921875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1917.6680908203125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1311.5457763671875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1272.2642822265625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 857.2018432617188 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 1059.6871337890625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1240.604736328125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 719.5844116210938 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 739.998779296875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 723.7073974609375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1417.953857421875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 986.3058471679688 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 721.0171508789062 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 982.71533203125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 2016.6201171875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 924.9891967773438 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1035.385498046875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1658.654052734375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 2278.650390625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 810.8988037109375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 587.755126953125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1092.805419921875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1003.99560546875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1097.925537109375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1072.3671875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 623.0415649414062 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 761.9325561523438 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 985.6917724609375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1509.0118408203125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 3986.30908203125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1198.285400390625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 885.9495849609375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 910.7539672851562 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1214.41455078125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1108.39404296875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1378.2314453125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1232.7216796875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1359.193359375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1431.982421875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 881.1954956054688 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1426.152587890625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1030.506103515625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1648.321533203125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 821.239501953125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1262.4232177734375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 952.2459716796875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 3753.487060546875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 988.463134765625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 7137.78173828125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1016.1837158203125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 4199.568359375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 2090.32080078125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1442.484619140625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1441.8876953125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1170.5234375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 922.4154663085938 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1852.3179931640625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1368.0404052734375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1733.2198486328125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1085.1826171875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1218.5093994140625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1111.67041015625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1096.7354736328125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1262.267578125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2061.10205078125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1370.1197509765625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 2666.032470703125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1146.99609375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1242.3585205078125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 2038.426513671875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1560.7301025390625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 707.8578491210938 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1206.261474609375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 866.721923828125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 809.177001953125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 3561.926025390625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1076.6820068359375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1286.7034912109375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1156.8226318359375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1015.785888671875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1446.1363525390625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 856.6065673828125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1415.856689453125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1136.5833740234375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1078.8087158203125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 809.9495239257812 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 608.6032104492188 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 900.2069702148438 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 848.2586669921875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1386.2589111328125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1942.984375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 3503.641845703125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 873.7086791992188 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1693.830322265625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 5334.837890625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1576.358642578125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 2964.799560546875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 971.5635986328125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1264.4581298828125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1368.323486328125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 920.8076171875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 950.7762451171875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1358.5697021484375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2204.595703125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1761.9129638671875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1646.2052001953125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 966.9017333984375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 3433.458740234375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1100.2379150390625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1537.62744140625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 2854.634765625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1733.0362548828125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1352.8812255859375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1314.12646484375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1707.98876953125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 860.7528686523438 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 2453.82470703125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1190.83447265625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1067.435302734375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 881.8399047851562 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 2600.322998046875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1037.0115966796875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1209.69140625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1040.962158203125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 965.9701538085938 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1183.9010009765625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 2472.64453125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 2670.564697265625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 790.1727905273438 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1132.0833740234375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 831.1131591796875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 2155.6474609375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1285.615478515625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1563.369140625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 668.1363525390625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 719.6752319335938 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 961.4992065429688 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1102.791748046875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 911.1807250976562 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1390.5640869140625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1506.7840576171875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 980.0867309570312 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 849.4387817382812 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1422.7464599609375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 908.4302368164062 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1384.2454833984375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 813.1572875976562 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1591.859130859375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2386.67578125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1780.048095703125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1429.673095703125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 958.9070434570312 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1594.21923828125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1155.7752685546875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1102.5723876953125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 910.5999755859375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1119.8717041015625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 5898.74365234375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1454.8148193359375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 3573.0634765625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1092.8878173828125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1045.3297119140625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1181.63134765625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 971.652587890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1438.7708740234375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 907.0438232421875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 1092.127197265625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1755.0908203125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1937.6121826171875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1414.9820556640625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1013.1980590820312 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 636.8154907226562 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 4427.76220703125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 2031.9246826171875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 2679.39599609375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1976.4190673828125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 3441.612548828125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 3127.033203125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 78311.765625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 3645.89501953125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1494.4385986328125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1586.74169921875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 3365.25537109375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1123.0843505859375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 861.3684692382812 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1087.8111572265625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 6655.9775390625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 2470.029541015625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1397.8280029296875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 766.572021484375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1823.3604736328125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1888.9063720703125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 2560.298095703125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 658.3461303710938 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2310.52392578125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 3028.75048828125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1160.5074462890625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 670.3888549804688 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1247.4727783203125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 2477.130859375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 787.8599853515625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7423, loss_val: nan, pos_over_neg: 1982.6685791015625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1181.1900634765625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 951.9498901367188 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1344.4400634765625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1482.398193359375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1149.7506103515625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1427.1678466796875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1353.4688720703125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1280.696044921875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1006.24365234375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1093.8023681640625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1768.2730712890625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1585.4263916015625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2115.540283203125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1723.0843505859375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1225.481689453125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1448.154296875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 2106.349853515625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1449.093505859375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1613.18359375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1610.75341796875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 784.427978515625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 1054.9876708984375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 940.2445068359375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 3877.610107421875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 877.3434448242188 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 868.7289428710938 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1061.839599609375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 4049.01123046875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1448.7705078125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 4434.02099609375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1638.1077880859375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 675.35498046875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1274.7344970703125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 723.1429443359375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1191.0848388671875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1534.1099853515625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 911.52001953125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 2183.09912109375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1052.0751953125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1802.7674560546875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 1155.2340087890625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 4148.3466796875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1320.5694580078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 971.4535522460938 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 892.5523681640625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1505.5062255859375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1491.4127197265625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1102.4139404296875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 3826.421875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2358.07080078125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 1171.5396728515625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1768.9962158203125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 639.0535278320312 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1584.4007568359375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 669.8151245117188 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 935.212158203125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 707.8997802734375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 2528.515869140625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1357.6435546875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1254.6636962890625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 500.85296630859375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1330.1806640625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1014.5557861328125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1165.8162841796875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1820.0528564453125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 3712.34375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 919.6561889648438 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2077.197998046875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 825.8566284179688 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 5110.55029296875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1447.9049072265625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1297.2178955078125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 966.7622680664062 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1001.8501586914062 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 888.8226928710938 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1581.388671875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 678.7180786132812 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 602.2090454101562 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1039.9954833984375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1051.1732177734375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 893.346435546875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 14003.2099609375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1579.665771484375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1887.5911865234375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1140.5750732421875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1109.9378662109375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2976.187255859375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 3692.9296875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 2781.27392578125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1361.2757568359375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 4289.4296875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1513.0399169921875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 796.54931640625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1018.038330078125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1051.505615234375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2369.967529296875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 2155.420654296875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 772.2933349609375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1268.2506103515625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1615.8961181640625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 955.415771484375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 3464.838134765625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1517.6351318359375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1481.2718505859375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 842.3043823242188 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 849.531494140625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1105.49072265625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1234.466064453125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1508.34130859375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 790.0000610351562 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 741.9198608398438 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 1702.6846923828125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 978.6160278320312 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 582.229248046875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 791.0322265625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1224.934326171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1088.5103759765625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1667.1995849609375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1309.71484375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1394.100830078125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1329.6844482421875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 2250.546142578125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2765.6181640625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1790.5126953125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 944.4745483398438 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1404.4937744140625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 965.774658203125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7428, loss_val: nan, pos_over_neg: 1980.7674560546875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1048.063720703125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1339.7850341796875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1508.4698486328125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1152.34423828125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 676.3026123046875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1920.70166015625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 2324.399169921875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 966.4392700195312 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 870.7655639648438 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 4725.91650390625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 1993.241455078125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 3449.92626953125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1411.9837646484375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 685.782470703125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1579.7901611328125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1213.867431640625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 5567.6533203125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 2006.1507568359375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 2380.5771484375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1206.275390625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1419.365478515625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 2058.8369140625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1977.4228515625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 803.1829223632812 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 989.172119140625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3501.3291015625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 3061.527099609375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 2501.541259765625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 2969.148193359375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1033.4420166015625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 3429.46240234375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2121.3681640625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 2181.853271484375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 956.3701171875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1424.8623046875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 793.4212036132812 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1342.3084716796875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1312.8367919921875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1393.3834228515625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 713.7401123046875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1803.1099853515625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 2341.36083984375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1260.418212890625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1549.4915771484375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1486.5654296875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 7785.34716796875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 3706.199951171875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 5182.4326171875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 3782.386962890625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 2248.75439453125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2386.544189453125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1850.012451171875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1309.814453125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7422, loss_val: nan, pos_over_neg: 5810.7529296875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1086.4912109375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1361.0906982421875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 994.7180786132812 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 2854.28125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1483.46142578125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 4245.142578125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 5936.64306640625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1179.7845458984375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2516.7822265625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 2467.604736328125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 5328.8330078125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1504.39794921875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 677.385986328125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 799.7884521484375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 4058.9677734375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 1132.6651611328125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2421.056396484375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 3367.604736328125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1105.605224609375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 730.17626953125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1921.791259765625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1850.873291015625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 1578.0712890625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1786.3890380859375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1453.0833740234375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1019.2738037109375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 761.088134765625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 844.7018432617188 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2705.634521484375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 1013.0745239257812 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 931.951904296875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 2923.888671875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1857.7789306640625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 800.966796875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1072.278564453125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2947.79296875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 3556.296630859375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1342.593994140625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 2482.049072265625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2500.990478515625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 17951.869140625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1294.642822265625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 2230.82666015625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 5481.8837890625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7423, loss_val: nan, pos_over_neg: 1398.6959228515625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 2123.92919921875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1353.6341552734375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2077.116455078125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1061.0206298828125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1301.795654296875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1279.8277587890625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1541.7073974609375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1302.7891845703125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1011.2559814453125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1091.4586181640625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 1028.8424072265625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 146467.765625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1011.3136596679688 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 930.4120483398438 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7407, loss_val: nan, pos_over_neg: 27886.62890625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1388.3206787109375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1040.701416015625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1166.2906494140625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7423, loss_val: nan, pos_over_neg: 3296.396728515625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1666.9754638671875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 3820.08740234375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 882.8739013671875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 1233.4150390625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1813.0577392578125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7407, loss_val: nan, pos_over_neg: 1178.91015625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1408.5604248046875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 853.8032836914062 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 924.9339599609375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 894.8341064453125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 896.5061645507812 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 929.1566772460938 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 890.080078125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1100.028564453125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 729.0162353515625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 772.2166137695312 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1234.86279296875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1475.0474853515625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1303.2489013671875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 2147.6796875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 821.6740112304688 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1497.0072021484375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 2059.70703125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 2333.2080078125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 2519.7314453125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1426.73974609375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 2062.0625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1729.59912109375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1708.9971923828125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1139.286376953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1405.286376953125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2873.141845703125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 2675.262451171875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1600.268798828125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1059.235595703125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 608.7599487304688 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 2059.8232421875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 873.5914306640625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 738.7235717773438 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1086.770751953125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1055.413330078125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 700.192138671875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 2412.376953125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 4724.39208984375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1256.924560546875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1370.7100830078125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1821.7928466796875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 95745.7109375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 19547.67578125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1060.05224609375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 2036.5699462890625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1078.541259765625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1089.9107666015625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1229.614013671875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:26:30<104662:47:09, 1255.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 2178.368408203125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1977.6885986328125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1649.319580078125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1141.8828125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 3062.919921875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1321.56787109375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1183.137939453125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 858.1251831054688 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1055.830810546875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7406, loss_val: nan, pos_over_neg: 2262.04248046875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 1065.662841796875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2055.615966796875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1104.3614501953125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1882.5989990234375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1871.2366943359375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 1499.6329345703125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1364.96142578125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1165.484375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1611.923583984375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 804.4178466796875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 1424.1337890625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 709.275634765625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1208.0233154296875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1708.5283203125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1092.700439453125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 926.923828125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 960.944091796875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 891.8330078125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.742, loss_val: nan, pos_over_neg: 2222.063720703125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7428, loss_val: nan, pos_over_neg: 2542.90869140625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 2446.1865234375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1184.6602783203125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7434, loss_val: nan, pos_over_neg: 1688.13916015625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 13872.3251953125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1984.42724609375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 1838.606201171875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 1162.44580078125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7425, loss_val: nan, pos_over_neg: 2140.499267578125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 919.4498901367188 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 951.3759765625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1082.9915771484375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7416, loss_val: nan, pos_over_neg: 2526.7197265625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1474.0692138671875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 892.2796020507812 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2065.135009765625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1389.4959716796875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 6660.49951171875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1379.508544921875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 4180.8037109375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 3673.370849609375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1702.8685302734375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1593.75439453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1207.1248779296875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 2466.2900390625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: -19650.537109375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1446.862060546875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 1301.4364013671875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2659.618896484375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1012.0228881835938 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1847.562255859375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1337.208740234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 3852.3583984375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1404.85302734375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 1044.0240478515625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7377, loss_val: nan, pos_over_neg: 1147.06396484375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1670.519287109375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2127.9189453125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1120.394287109375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1303.3612060546875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1218.294189453125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 875.25 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1140.9564208984375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1845.6539306640625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1428.1427001953125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 610.5512084960938 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 814.1182250976562 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 928.85302734375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7419, loss_val: nan, pos_over_neg: 2275.326416015625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1166.4918212890625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 665.1099243164062 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1544.5172119140625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 1893.2615966796875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1941.87548828125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 7755.71533203125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 2997.254150390625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1188.4912109375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 942.3095092773438 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1689.2447509765625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 598.83154296875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1014.6917114257812 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 1875.6048583984375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 2026.0250244140625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 659.453857421875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 2060.378662109375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1719.843017578125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 584.73876953125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 805.9550170898438 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1126.6383056640625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1043.50537109375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1230.7928466796875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1409.4927978515625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 2580.7802734375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 2057.923095703125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1000.2311401367188 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 1128.435302734375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1819.144287109375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 6265.400390625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 988.9352416992188 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1186.10498046875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 559.246826171875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 2469.298095703125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2101.342041015625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 511.15277099609375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 641.7147827148438 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1001.3436279296875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1660.8568115234375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1146.752685546875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1079.03173828125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 839.7098999023438 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 1418.2762451171875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1172.154052734375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1229.1153564453125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1259.2371826171875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1046.44189453125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1176.342041015625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 859.1194458007812 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1010.8826293945312 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 647.27392578125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 610.801513671875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 837.4481201171875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 930.6107177734375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 725.1723022460938 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 890.2251586914062 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 550.9617309570312 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 792.3152465820312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 574.1099853515625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 969.6693725585938 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1151.75390625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 702.9744262695312 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 654.8008422851562 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 795.0962524414062 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 994.4910278320312 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1067.529296875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1747.6199951171875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1308.9154052734375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 919.4649658203125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 677.5281372070312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1563.6258544921875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1813.0758056640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 880.3777465820312 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1093.859375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1183.4884033203125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1440.2100830078125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1360.3311767578125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 643.670654296875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 981.0443725585938 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1029.24658203125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1210.1591796875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1258.803955078125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 967.8684692382812 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1188.004638671875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1693.9444580078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1039.8704833984375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1090.7056884765625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 708.3568115234375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1860.313232421875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1910.4874267578125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1018.167236328125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1136.5736083984375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 816.4346313476562 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1242.531005859375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 2530.041748046875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 3381.978759765625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.741, loss_val: nan, pos_over_neg: 2446.279296875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 1311.453369140625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 817.5034790039062 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1857.239501953125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 2416.140625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 3569.79833984375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 690.5856323242188 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 1198.0804443359375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1231.8543701171875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1802.7158203125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 652.3994750976562 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 2926.40478515625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 2655.958984375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1725.454345703125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1687.0921630859375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1133.28662109375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1910.5267333984375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 2098.416015625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1664.010498046875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1273.614501953125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1622.5206298828125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1255.9898681640625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 909.9005737304688 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1132.660888671875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1074.591796875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1125.691650390625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1054.3905029296875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 788.1627197265625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 855.8720703125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 529.47216796875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1827.8206787109375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 1091.588134765625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1355.0775146484375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 965.9808959960938 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1503.344482421875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1987.19384765625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7417, loss_val: nan, pos_over_neg: 1387.4324951171875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1288.1785888671875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1033.494384765625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7392, loss_val: nan, pos_over_neg: 1291.154052734375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1206.392822265625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1123.28515625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1096.368408203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 718.5955810546875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 680.220947265625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1365.9434814453125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7415, loss_val: nan, pos_over_neg: 1383.3704833984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1108.9774169921875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1678.895751953125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 1033.607421875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 3230.1435546875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 2117.78125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1175.6103515625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 678.1357421875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 3268.280517578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1223.6334228515625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 1291.7864990234375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1764.8572998046875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 2212.676513671875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1760.18896484375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1576.77294921875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1403.4908447265625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1778.8897705078125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 721.4906616210938 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.741, loss_val: nan, pos_over_neg: 2319.78955078125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 985.6204223632812 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7425, loss_val: nan, pos_over_neg: 1071.0579833984375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 987.6754760742188 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 752.1749267578125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 993.0816650390625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1250.056396484375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1425.8221435546875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7439, loss_val: nan, pos_over_neg: 1574.7413330078125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1741.2462158203125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1199.3494873046875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1158.5087890625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 3876.121826171875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 685.01806640625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1399.1851806640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 761.878173828125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 881.7431030273438 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1291.5455322265625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1300.5845947265625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1282.3978271484375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 1180.1160888671875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 522.1754150390625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 4196.96142578125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1238.261474609375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1033.1409912109375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1000.2443237304688 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1696.7515869140625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1861.95654296875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 924.6527099609375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 1399.309326171875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 776.5697631835938 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2321.448974609375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 689.24462890625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1125.7879638671875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 1366.620361328125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1820.5750732421875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 3429.855224609375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 837.70166015625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 498.9585876464844 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1025.916259765625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 928.28759765625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 777.9574584960938 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 922.88916015625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 760.551513671875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1009.9031982421875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1168.7525634765625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7412, loss_val: nan, pos_over_neg: 1509.0380859375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1142.1456298828125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1143.7451171875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1313.9261474609375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1173.4061279296875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1005.0529174804688 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1305.9427490234375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7394, loss_val: nan, pos_over_neg: 1870.9940185546875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 2172.596435546875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1962.4312744140625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1550.4197998046875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1207.8560791015625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 3413.81787109375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 2154.78076171875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1677.0145263671875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1530.1912841796875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1135.5980224609375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1319.221923828125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 981.7310180664062 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1687.0010986328125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1376.3028564453125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1134.3824462890625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 3328.665771484375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1286.95751953125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 3651.3095703125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1210.2515869140625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1997.81005859375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 926.1343994140625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1136.414306640625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 995.5890502929688 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 3033.210205078125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7428, loss_val: nan, pos_over_neg: 2376.716552734375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1297.7322998046875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 2122.843994140625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1136.803466796875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1431.272705078125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 1733.5263671875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 573.38134765625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 2213.728759765625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1133.8990478515625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1034.6912841796875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 845.4024658203125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 835.2683715820312 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1317.917724609375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1031.63525390625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 971.9782104492188 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 632.5833740234375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 710.6508178710938 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 733.7684326171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1204.3463134765625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 913.5250244140625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 956.595703125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1203.530029296875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1464.4520263671875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1109.2244873046875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1430.478515625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 591.9285278320312 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1290.609130859375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1330.9232177734375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1287.4600830078125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1118.40380859375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1999.8502197265625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 3446.823974609375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1199.9554443359375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1159.249755859375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1093.2054443359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 4357.49462890625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 3108.586181640625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 2307.6083984375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 954.2706298828125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1261.3714599609375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 3598.26708984375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 2719.943359375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 2456.0751953125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 2904.4111328125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 3022.9833984375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1752.6373291015625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 2347.84033203125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1777.4698486328125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2887.07177734375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1158.088623046875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7422, loss_val: nan, pos_over_neg: 1653.2060546875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1051.6297607421875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 627.9974365234375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 878.9954223632812 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 6769.888671875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1590.0240478515625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1979.4793701171875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 897.8546752929688 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 1973.1136474609375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 826.2766723632812 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1817.3192138671875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1321.0703125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 3201.348876953125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 745.7026977539062 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 2124.51171875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.741, loss_val: nan, pos_over_neg: 933.6627807617188 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1130.922119140625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 2835.873779296875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1185.8607177734375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 2284.4267578125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1593.4508056640625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 5185.27197265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1802.8572998046875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1363.144287109375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1727.2772216796875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 3541.171875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1373.7470703125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 1271.699462890625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1378.879638671875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7383, loss_val: nan, pos_over_neg: 3081.9287109375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 3301.829833984375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1070.0792236328125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1978.8662109375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.741, loss_val: nan, pos_over_neg: 1197.26904296875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.742, loss_val: nan, pos_over_neg: 1164.1552734375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 2020.9102783203125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2328.543212890625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 2652.0830078125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 2501.284912109375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1236.5003662109375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1267.4173583984375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 23123.572265625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 10397.1474609375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 2762.13720703125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1747.3636474609375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 570.3499145507812 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1318.691650390625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 4149.30517578125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1509.805419921875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1093.367431640625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 3277.897216796875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 2594.083984375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1147.607177734375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1339.988037109375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 1332.7415771484375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 1193.538818359375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 1236.002685546875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 525.57861328125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1241.3790283203125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 2049.269775390625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 13122.5625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 1991.8585205078125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 640.5835571289062 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 2419.876220703125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 2059.882568359375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1719.21435546875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1280.0469970703125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 2248.47314453125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 999.8804931640625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7396, loss_val: nan, pos_over_neg: 867.9155883789062 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1184.097900390625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7442, loss_val: nan, pos_over_neg: 1816.1724853515625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 3982.051513671875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 2501.347412109375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 1726.2242431640625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 835.2611694335938 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 962.2510986328125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.744, loss_val: nan, pos_over_neg: -28985.533203125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 3072.548583984375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.742, loss_val: nan, pos_over_neg: 1241.9239501953125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1104.9920654296875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 953.46875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 5451.51220703125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 2260.523681640625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.741, loss_val: nan, pos_over_neg: 1168.4169921875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1371.0303955078125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 937.0829467773438 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7422, loss_val: nan, pos_over_neg: 2116.191162109375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1090.736572265625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7428, loss_val: nan, pos_over_neg: 4149.68408203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7415, loss_val: nan, pos_over_neg: 2041.236572265625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 2450.353271484375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 647.88525390625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1029.5111083984375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1300.950439453125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2169.366455078125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1694.9852294921875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 3301.650390625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 855.1409301757812 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1299.0128173828125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1240.9249267578125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1005.9697875976562 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 976.4590454101562 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1212.7718505859375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1147.69091796875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 983.9713134765625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 691.5484008789062 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 819.4492797851562 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 2883.126708984375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1004.6107788085938 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 976.526611328125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1334.65576171875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7393, loss_val: nan, pos_over_neg: 1320.9710693359375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1778.78076171875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 1636.535400390625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1179.2197265625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1138.1748046875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 1884.4405517578125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 5739.78955078125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 2734.96142578125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 1103.448486328125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 1295.0419921875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 1330.4884033203125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.741, loss_val: nan, pos_over_neg: 1746.82470703125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 757.4761352539062 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 839.43212890625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7397, loss_val: nan, pos_over_neg: 2893.10986328125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 2128.091796875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1150.4658203125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1452.9736328125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 979.97021484375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 2182.652587890625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1321.5858154296875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1002.6881713867188 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 1694.27587890625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1119.5335693359375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1346.3876953125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 922.9098510742188 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 2772.256591796875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 1598.8228759765625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1467.8709716796875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 1063.5008544921875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 5444.09228515625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: -3030.15625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 2375.455322265625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 1411.583251953125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 677.4754638671875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 2853.152587890625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 747.1361083984375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1238.72705078125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 934.8794555664062 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1406.2413330078125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7425, loss_val: nan, pos_over_neg: 841.789306640625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7395, loss_val: nan, pos_over_neg: 1556.4833984375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1320.10009765625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 3194.166015625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1939.068115234375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 2307.256103515625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 2460.00439453125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 926.578125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1210.0213623046875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1455.906494140625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 2154.030029296875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1776.940185546875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1131.6153564453125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 1832.647216796875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1140.342041015625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2119.565673828125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 1254.8841552734375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1722.4351806640625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1727.9735107421875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1373.0146484375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 809.1121215820312 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1210.4056396484375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 6385.49853515625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1387.5330810546875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1523.7056884765625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1240.9208984375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1722.690185546875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1931.1737060546875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 1639.5166015625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7439, loss_val: nan, pos_over_neg: 1234.52392578125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 1149.5343017578125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1187.096435546875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 3270.598388671875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7392, loss_val: nan, pos_over_neg: 1972.300537109375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 563.2391967773438 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 778.1829833984375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 998.2021484375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1945.10595703125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7399, loss_val: nan, pos_over_neg: 2736.73046875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1199.402587890625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1362.618896484375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1153.3529052734375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 1029.5343017578125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 695.9781494140625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7406, loss_val: nan, pos_over_neg: 1674.6605224609375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 700.1084594726562 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 622.8894653320312 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1141.2105712890625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1211.6009521484375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 894.713134765625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 1710.000244140625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1821.2305908203125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1644.1640625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 2318.348876953125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7417, loss_val: nan, pos_over_neg: 1410.2496337890625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 992.8497924804688 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1800.3565673828125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 895.6157836914062 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7415, loss_val: nan, pos_over_neg: 971.09228515625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 824.61474609375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1766.3043212890625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1477.8800048828125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 969.9872436523438 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 871.5906982421875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 3262.6376953125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 2008.6932373046875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1282.037109375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7424, loss_val: nan, pos_over_neg: 1266.5870361328125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1011.0668334960938 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1714.8643798828125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 2684.052978515625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 3195.732421875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 829.2083740234375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1157.39794921875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1608.9732666015625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1681.706787109375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1190.617919921875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1154.493408203125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 1307.99560546875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1609.5289306640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 1762.4295654296875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1606.0806884765625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 2105.529052734375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7414, loss_val: nan, pos_over_neg: 1194.073974609375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 920.8764038085938 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1095.599853515625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7425, loss_val: nan, pos_over_neg: 1470.1231689453125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.742, loss_val: nan, pos_over_neg: 1244.658447265625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1147.2001953125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1590.514892578125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7419, loss_val: nan, pos_over_neg: 1295.6241455078125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 895.3523559570312 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 2641.19384765625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 2406.086669921875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1698.0877685546875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 642.0647583007812 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 725.9525146484375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1057.0203857421875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 738.1802978515625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1203.7325439453125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 1004.5504760742188 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1020.3235473632812 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1769.1741943359375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 766.3446044921875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 902.9290771484375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 648.4940185546875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1348.5574951171875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7439, loss_val: nan, pos_over_neg: 1700.587890625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2287.97265625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7428, loss_val: nan, pos_over_neg: 1398.5963134765625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 990.3759765625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1210.7537841796875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 1419.05810546875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1519.8951416015625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 1285.64111328125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 2649.61669921875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 2680.155517578125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7425, loss_val: nan, pos_over_neg: 1201.30712890625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 2403.39453125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1253.155517578125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1140.134765625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7378, loss_val: nan, pos_over_neg: 2355.73388671875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1094.7340087890625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 649.4490356445312 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7414, loss_val: nan, pos_over_neg: 9904.0703125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7422, loss_val: nan, pos_over_neg: -52419.0234375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1671.4661865234375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 865.8978881835938 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7434, loss_val: nan, pos_over_neg: 1744.6614990234375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1053.4493408203125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7397, loss_val: nan, pos_over_neg: 1694.340576171875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7382, loss_val: nan, pos_over_neg: 1696.0594482421875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1976.9571533203125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1341.470947265625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7423, loss_val: nan, pos_over_neg: 1957.2900390625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1038.1671142578125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 2248.049072265625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1025.621826171875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 1144.458251953125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1264.5428466796875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1084.72216796875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1124.40087890625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1983.8427734375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1105.4827880859375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2164.06884765625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1000.6470947265625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 1597.6588134765625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 931.3909912109375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 895.5348510742188 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1099.7313232421875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1678.924560546875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 1544.1573486328125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 894.81298828125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1442.7493896484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 918.4843139648438 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1557.7872314453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 856.8936157226562 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1862.7822265625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 3131.031005859375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2271.04443359375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1743.1201171875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 1585.405517578125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 725.3623657226562 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 2147.596923828125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 2264.532958984375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 769.1226806640625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1242.0125732421875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 646.2234497070312 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 823.882568359375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1985.6435546875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 584.166748046875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1241.336669921875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1146.6007080078125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7425, loss_val: nan, pos_over_neg: 1679.972900390625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 972.313232421875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 728.3534545898438 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1672.9315185546875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 1166.73583984375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1027.376953125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [2:47:38<104983:34:14, 1259.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 1765.8162841796875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1129.736572265625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1375.6771240234375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1396.7520751953125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 3747.571044921875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 2211.263916015625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1993.2218017578125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 733.7907104492188 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 760.5606689453125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7385, loss_val: nan, pos_over_neg: 1921.343017578125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1330.6051025390625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7378, loss_val: nan, pos_over_neg: 5837.18701171875 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=norm'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss  Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix  Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE  RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
