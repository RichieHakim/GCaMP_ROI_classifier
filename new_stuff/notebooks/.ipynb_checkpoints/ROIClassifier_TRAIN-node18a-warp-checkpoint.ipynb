{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')\n",
    "\n",
    "data_labeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_unbalanced.h5')\n",
    "\n",
    "masks_SYT = data_labeled['SYTmasks']\n",
    "labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])\n",
    "\n",
    "nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "labels_SYT = labels_SYT[non_nan]\n",
    "masks_SYT = masks_SYT[non_nan]\n",
    "\n",
    "X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOWklEQVR4nO3dcahe9X3H8ffHaLXMDpVcXUiyxcFlLClU5ZI5hOFmmZkdi39USGEuDEfYsGBhMJL+sdI/Av5VxmAywirLWFcJtM7g2m0hq5TBZnp1Wo0x8652ekkwt5bWyoYj2Xd/3CM83tyb5+Te5zH3+e39gss553d+5znfX37cT07O8zwnqSokSW256koXIEkaPcNdkhpkuEtSgwx3SWqQ4S5JDbr6ShcAsHHjxtq2bduVLkOSJspzzz33g6qaWm7fugj3bdu2MTs7e6XLkKSJkuQ/V9rnbRlJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQuviG6lpt2/93qz72+49+aoSVtM8/68mwlnkC5+pyrcffC6/cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qFe5Jvp/kpSQvJJnt2m5KcizJa93yxoH+B5LMJTmd5N5xFS9JWt7lXLn/alXdVlUz3fZ+4HhVTQPHu22SbAf2ADuAXcBjSTaMsGZJ0hBruS2zGzjcrR8G7h9of6Kq3quq14E5YOcaziNJukx9w72Af0zyXJJ9XdstVXUWoFve3LVvBt4cOHa+a/uAJPuSzCaZXVhYWF31kqRl9X1w2F1VdSbJzcCxJK9eom+WaauLGqoOAYcAZmZmLtovSVq9XlfuVXWmW54DnmTxNstbSTYBdMtzXfd5YOvA4VuAM6MqWJI03NBwT/JTST72/jrw68DLwFFgb9dtL/BUt34U2JPk2iS3AtPAiVEXLklaWZ/bMrcATyZ5v//fVNXfJ/kOcCTJQ8AbwAMAVXUyyRHgFeA88HBVXRhL9ZKkZQ0N96r6HvCJZdrfBu5Z4ZiDwME1VydJWhW/oSpJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa1Dvck2xI8m9Jnu62b0pyLMlr3fLGgb4HkswlOZ3k3nEULkla2eVcuT8CnBrY3g8cr6pp4Hi3TZLtwB5gB7ALeCzJhtGUK0nqo1e4J9kCfAr4i4Hm3cDhbv0wcP9A+xNV9V5VvQ7MATtHUq0kqZe+V+5/AvwR8L8DbbdU1VmAbnlz174ZeHOg33zX9gFJ9iWZTTK7sLBwuXVLki5haLgn+U3gXFU91/M1s0xbXdRQdaiqZqpqZmpqqudLS5L6uLpHn7uA30pyH3Ad8NNJ/hp4K8mmqjqbZBNwrus/D2wdOH4LcGaURUuSLm3olXtVHaiqLVW1jcU3Sv+pqn4bOArs7brtBZ7q1o8Ce5Jcm+RWYBo4MfLKJUkr6nPlvpJHgSNJHgLeAB4AqKqTSY4ArwDngYer6sKaK5Uk9XZZ4V5VzwDPdOtvA/es0O8gcHCNtUmSVslvqEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGDQ33JNclOZHkxSQnk3yxa78pybEkr3XLGweOOZBkLsnpJPeOcwCSpIv1uXJ/D/i1qvoEcBuwK8mdwH7geFVNA8e7bZJsB/YAO4BdwGNJNoyhdknSCoaGey16t9u8pvspYDdwuGs/DNzfre8Gnqiq96rqdWAO2DnKoiVJl9brnnuSDUleAM4Bx6rqWeCWqjoL0C1v7rpvBt4cOHy+a1v6mvuSzCaZXVhYWMMQJElL9Qr3qrpQVbcBW4CdST5+ie5Z7iWWec1DVTVTVTNTU1O9ipUk9XNZn5apqh8Bz7B4L/2tJJsAuuW5rts8sHXgsC3AmbUWKknqr8+nZaaS3NCtfxT4JPAqcBTY23XbCzzVrR8F9iS5NsmtwDRwYsR1S5Iu4eoefTYBh7tPvFwFHKmqp5P8C3AkyUPAG8ADAFV1MskR4BXgPPBwVV0YT/mSpOUMDfeq+i5w+zLtbwP3rHDMQeDgmquTJK2K31CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDRoa7km2JvlWklNJTiZ5pGu/KcmxJK91yxsHjjmQZC7J6ST3jnMAkqSL9blyPw/8YVX9InAn8HCS7cB+4HhVTQPHu226fXuAHcAu4LEkG8ZRvCRpeUPDvarOVtXz3fpPgFPAZmA3cLjrdhi4v1vfDTxRVe9V1evAHLBzxHVLki7hsu65J9kG3A48C9xSVWdh8S8A4Oau22bgzYHD5ru2pa+1L8lsktmFhYVVlC5JWknvcE9yPfA14HNV9c6lui7TVhc1VB2qqpmqmpmamupbhiSph17hnuQaFoP9K1X19a75rSSbuv2bgHNd+zywdeDwLcCZ0ZQrSeqjz6dlAnwZOFVVXxrYdRTY263vBZ4aaN+T5NoktwLTwInRlSxJGubqHn3uAh4EXkryQtf2eeBR4EiSh4A3gAcAqupkkiPAKyx+0ubhqrow6sIlSSsbGu5V9c8sfx8d4J4VjjkIHFxDXZKkNfAbqpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBQ8M9yeNJziV5eaDtpiTHkrzWLW8c2HcgyVyS00nuHVfhkqSV9bly/0tg15K2/cDxqpoGjnfbJNkO7AF2dMc8lmTDyKqVJPUyNNyr6tvAD5c07wYOd+uHgfsH2p+oqveq6nVgDtg5mlIlSX2t9p77LVV1FqBb3ty1bwbeHOg337VdJMm+JLNJZhcWFlZZhiRpOaN+QzXLtNVyHavqUFXNVNXM1NTUiMuQpP/fVhvubyXZBNAtz3Xt88DWgX5bgDOrL0+StBqrDfejwN5ufS/w1ED7niTXJrkVmAZOrK1ESdLlunpYhyRfBe4GNiaZB74APAocSfIQ8AbwAEBVnUxyBHgFOA88XFUXxlS7JGkFQ8O9qj6zwq57Vuh/EDi4lqIkSWvjN1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg8YW7kl2JTmdZC7J/nGdR5J0sbGEe5INwJ8BvwFsBz6TZPs4ziVJuti4rtx3AnNV9b2q+h/gCWD3mM4lSVoiVTX6F00+Deyqqt/rth8EfqmqPjvQZx+wr9v8BeD0Gk65EfjBGo5fL1oZBziW9aiVcYBjed/PVdXUcjuuXn09l5Rl2j7wt0hVHQIOjeRkyWxVzYzita6kVsYBjmU9amUc4Fj6GNdtmXlg68D2FuDMmM4lSVpiXOH+HWA6ya1JPgLsAY6O6VySpCXGclumqs4n+SzwD8AG4PGqOjmOc3VGcntnHWhlHOBY1qNWxgGOZaixvKEqSbqy/IaqJDXIcJekBk1MuA97nEEW/Wm3/7tJ7rgSdfbRYyx3J/lxkhe6nz++EnUOk+TxJOeSvLzC/kmak2FjmZQ52ZrkW0lOJTmZ5JFl+kzEvPQcy6TMy3VJTiR5sRvLF5fpM9p5qap1/8Pim7L/Afw88BHgRWD7kj73Ad9k8TP2dwLPXum61zCWu4Gnr3StPcbyK8AdwMsr7J+IOek5lkmZk03AHd36x4B/n+DflT5jmZR5CXB9t34N8Cxw5zjnZVKu3Ps8zmA38Fe16F+BG5Js+rAL7aGZRzNU1beBH16iy6TMSZ+xTISqOltVz3frPwFOAZuXdJuIeek5lonQ/Vm/221e0/0s/TTLSOdlUsJ9M/DmwPY8F09ynz7rQd86f7n7J9w3k+z4cEobuUmZk74mak6SbANuZ/EqcdDEzcslxgITMi9JNiR5ATgHHKuqsc7LuB4/MGpDH2fQs8960KfO51l8ZsS7Se4D/haYHndhYzApc9LHRM1JkuuBrwGfq6p3lu5e5pB1Oy9DxjIx81JVF4DbktwAPJnk41U1+B7PSOdlUq7c+zzOYFIeeTC0zqp65/1/wlXVN4Brkmz88EocmUmZk6EmaU6SXMNiGH6lqr6+TJeJmZdhY5mkeXlfVf0IeAbYtWTXSOdlUsK9z+MMjgK/073jfCfw46o6+2EX2sPQsST5mSTp1neyOE9vf+iVrt2kzMlQkzInXY1fBk5V1ZdW6DYR89JnLBM0L1PdFTtJPgp8Enh1SbeRzstE3JapFR5nkOT3u/1/DnyDxXeb54D/An73StV7KT3H8mngD5KcB/4b2FPd2+nrSZKvsvhphY1J5oEvsPhG0UTNCfQay0TMCXAX8CDwUnd/F+DzwM/CxM1Ln7FMyrxsAg5n8T8yugo4UlVPjzPDfPyAJDVoUm7LSJIug+EuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGvR/82ENSdA3DKAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1280\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "base_model_frozen = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.0.7.0.block.0.0.weight\n",
      "base_model.0.7.0.block.0.1.weight\n",
      "base_model.0.7.0.block.0.1.bias\n",
      "base_model.0.7.0.block.1.0.weight\n",
      "base_model.0.7.0.block.1.1.weight\n",
      "base_model.0.7.0.block.1.1.bias\n",
      "base_model.0.7.0.block.2.fc1.weight\n",
      "base_model.0.7.0.block.2.fc1.bias\n",
      "base_model.0.7.0.block.2.fc2.weight\n",
      "base_model.0.7.0.block.2.fc2.bias\n",
      "base_model.0.7.0.block.3.0.weight\n",
      "base_model.0.7.0.block.3.1.weight\n",
      "base_model.0.7.0.block.3.1.bias\n",
      "base_model.0.8.0.weight\n",
      "base_model.0.8.1.weight\n",
      "base_model.0.8.1.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    # print(name)\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[13]) < 7:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[13]) >= 7:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms    \n",
    "\n",
    "# transforms = torch.nn.Sequential(\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "#                                         scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "#                                         shear=(-15, 15, -15, 15),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         fillcolor=None, \n",
    "#                                         resample=None),\n",
    "#     augmentation.AddPoissonNoise(   #scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "#                                     scaler_bounds=(10**(4), 10**(6)),\n",
    "#                                     prob=1,\n",
    "#                                     base=1000,\n",
    "#                                     scaling='log'),\n",
    "#     augmentation.AddGaussianNoise(  mean=0, \n",
    "#                                     std=0.00015,\n",
    "#                                     prob=1),\n",
    "    \n",
    "#     augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "#     # torchvision.transforms.Resize(size=(224,224), \n",
    "#     #                               interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "#     augmentation.TileChannels(dim=0, n_channels=3),\n",
    "# )\n",
    "\n",
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    # torchvision.transforms.RandomHorizontalFlip(p=0.5),    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    augmentation.WarpPoints(r=[0, 1],\n",
    "                            cx=[-0.5, 0.5],\n",
    "                            cy=[-0.5, 0.5], \n",
    "                            dx=[-0.3, 0.0], \n",
    "                            dy=[-0.3, 0.0], \n",
    "                            n_warps=5,\n",
    "                            prob=1.0,\n",
    "                            img_size_in=[36, 36],\n",
    "                            img_size_out=[224, 224]),\n",
    "\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     torchvision.transforms.Resize(size=(224,224), \n",
    "#                                   interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    ")\n",
    "    \n",
    "\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=800,\n",
    "                                                # shuffle=True,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeiUlEQVR4nO3dW4xd133f8e9/386ZC4fkUCJFU4IlO3QQuUUcm1EcuAhcGI0dv8h5cKE8FHowoD7YQAK0D3LzEL8YSIvGfSkSQIENq0VsVUBiWCiCNIoQwHCL+ArF1iWyaFmWGMqiJN6GM+eyL/8+rHUuMxyKFOd6Nn8fYDBn9pw5e/Hwv//nv9dea21zd0REpF2SvW6AiIhsPyV3EZEWUnIXEWkhJXcRkRZSchcRaSEldxGRFtqx5G5mnzCzF8zstJk9vFP7EdlNimuZFbYT49zNLAV+Avwb4AzwPeD33P25bd+ZyC5RXMss2anK/T7gtLu/5O5D4DHg/h3al8huUVzLzNip5H4CeHXq5zNxm8gsU1zLzMh26HVtk23r+n/M7CHgIYCU7EPztrRDTRGBFT//prvfvsWXuW5cg2Jbdk/fVxl6f7O43LHkfga4a+rnO4Gz009w90eARwCWkiP+4fwTO9QUEXhy+LWfb8PLXDeuQbEtu+cfyr+55u92qlvme8BJM7vHzArgAeCJHdqXyG5RXMvM2JHK3d0rM/sc8H+AFPiKuz+7E/sS2S2Ka5klO9Utg7v/NfDXO/X6IntBcS2zQjNURURaSMldRKSFlNxFRFpIyV1EpIWU3EVEWkjJXUSkhZTcRURaSMldRKSFlNxFRFpIyV1EpIWU3EVEWkjJXUSkhZTcRURaSMldRKSFlNxFRFpIyV1EpIWU3EVEWkjJXUSkhZTcRURaSMldRKSFlNxFRFpIyV1EpIWU3EVEWkjJXUSkhZTcRURaKNvKH5vZy8AKUAOVu58ys2XgfwF3Ay8D/9bdL2ytmSK7S7Ets247Kvd/7e4fcPdT8eeHgafc/STwVPxZZBYptmVm7US3zP3Ao/Hxo8CndmAfIntBsS0zY6vJ3YG/NbMfmNlDcdsxd38NIH4/usV9iOwFxbbMtC31uQMfcfezZnYUeNLM/ulG/zAeMA8BdJnfYjNEtp1iW2balip3dz8bv58DvgHcB7xuZscB4vdz1/jbR9z9lLufyq27lWaIbDvFtsy6m07uZrZgZgdGj4HfBp4BngAejE97EPjmVhspspsU29IGW+mWOQZ8w8xGr/M1d/8bM/se8LiZfQZ4Bfj01pspsqsU2zLzbjq5u/tLwK9usv0t4GNbaZTIXlJsSxtohqqISAspuYuItJCSu4hICym5i4i0kJK7iEgLKbmLiLSQkruISAspuYuItJCSu4hICym5i4i0kJK7iEgLKbmLiLSQkruISAspuYuItJCSu4hICym5i4i0kJK7iEgLKbmLiLSQkruISAspuYuItJCSu4hICym5i4i0kJK7iEgLKbmLiLSQkruISAtdN7mb2VfM7JyZPTO1bdnMnjSzF+P3w1O/+7yZnTazF8zs4zvVcJGtUmxLm91I5f5V4BMbtj0MPOXuJ4Gn4s+Y2b3AA8D749/8qZml29Zake31VRTb0lLXTe7u/i3g/IbN9wOPxsePAp+a2v6Yuw/c/WfAaeC+7WmqyPZSbEub3Wyf+zF3fw0gfj8at58AXp163pm4TWRWKLalFbJtfj3bZJtv+kSzh4CHALrMb3MzRLadYltmys1W7q+b2XGA+P1c3H4GuGvqeXcCZzd7AXd/xN1Pufup3Lo32QyRbafYlla42eT+BPBgfPwg8M2p7Q+YWcfM7gFOAt/dWhNFdpViW1rhut0yZvZ14KPAbWZ2Bvgj4I+Bx83sM8ArwKcB3P1ZM3sceA6ogM+6e71DbRfZEsW2tJm5b9ptuKuWkiP+4XzjiDSR7fPk8Gs/cPdTu71fxbbspH8o/4bLzVubXQ/SDFURkTZSchcRaSEldxGRFtruce4iInvLm8lju3XrVyV3EWmH6aQOW0vsG19rq6+3B5Tc5Z3xZuaCXFpuuxJxfB1v1o8gtGTTwSj7npK7XN9mB89WXkMfDrIdrpXUN0vGzTWGfG9M6Bvi1BJb/5rXep19SMldru3tKqLEbizQvVlXCc1qFST7zGZdMDG2zNbH2HguzybxvLFKHyf0NN38dUZxPwNnsErusrnNKu3ExgG/LtCv8bc+Ogjia4yrIJGbdY2kPk7ESYwvM4hJ3ZoGx69O5OOXmFTnlqaTxB4/LLxuoGmwug5xP2rDPk/wSu6yuekKnVgNJZNANoB40Gw84Nad4m44tTWzSSU1IxWQ7EMbC44kCQk9SdYXIE0zScgbC43R4zQJCT1NIc8gSSfdMHWNWYNXVfiwqGdnxQkld9ncdDWUppODZrqaAaibTSuikNDz8YEz+nt3x+p68sEASvBy4zY5iyRJJtV2OknMVtd4c/VFUovPsTQFs3FStzyHPIcs3mDLHcoKH5aYN+PXGr3efu9iVHKX9TZW6mk6laBHQd9gVHhdr6uIwsEydZqc51iWQZqE7d5gdTNZBL3ehgu1cuvZrPAYxVkaY3RDhW2J4U2CpSmWZ+vjOhsl9gzvFpMz1KrGiB8S1aT4uFb3zn6j5C5XGVVAlmch8LNwMIz7MUd9j1aFpG2hirEsW/+BkBdYEQ4aIFT5w+GkCkoMZucsV/baxsJjquomy7CpbkMHcMdiInbA0lCAWJFPknqWhaRe5Hg3xzs5bmCNY0ML3TKjD4xkts4uldxlYpOKOwR/Hqvv0XCwDSMJqqm/S5PJQdMp8E4RDr54imvuof+yrIDZOL2VfWDjSJiNiT3LJvFJuCbkcZtVU9vzLBQdWQoxyXsnJPWmm9HkIZFb1ZBWzeQ1ZzBGldxlvVHf+lRi904ORY5nMcHXjg2GWC9W84Ph+qQ+qobmOjRzOZ4mWOMkPQtJPUnXVUHeOJbu4b9ZZsJ0V8xoVMs45pJkUoCYgWdY00BV45Zgo+7CPIdOEZJ6lkKW4kUWEnsnpckScEjc8TQWLeOiplk/OmyfXydScpf1FzRHXTKjyrvboVns0HQyPA/PsbIhXYuPYdLPXhThYMlCRdV0cppujmeGlQ1WNeHUeTRqZm/+tTJrpqvmJBl3/a1L7FmKj/vQ08mZolmI0VHi7xR4nsXnp5AleJ7S5Cl1ntDksRCpp0bg7IN7XtwMJXcZs9GpbpHD/BzNfJdmsaBayKm7CU1mWANWOXmekJqRmIWq3Wx80JDEA6ab0RQJTWqkgOfhALQkUWKXG3PNC/xxCOMoscdulqbIIA1nl4kZnoR4o64nMVrkeGp4muJ5QlOkIU6LBB+NBhsl9elhuzAZPDAD3TRK7hJ4A0kIfOt2aBa71AsdqgM55WJKOWc0mYFBUjpNxyjMyAinsMBVB40XCXWRhNNbhyTdMFZe1bu8E2aT4Y6jr8TwLMU7BT6X0+Sx2m4cUsPK2F0Tr/GQZ6HIyBI8S2iykNxHceoJJBVY7VCHcfKMR4XNVrQqud/q4vIAYVy6xQtMBc18QXUgZ3AoY3DQKBeMJgccsl5M8oOUZJCRDPPwUhsPmizBM8PNcIvVkMg7NK7aR90xNtUXHs80fS6n7mYxuYfRLp4lJFlCAph7qNyLjKaIcZrG+MwMEvAErA4XU60OM1KtquMM1ZjoZ4iSu8QEH05frSioF7oMD3XoH07pLycMDkE953gaRj96ZiSVkfUSsn5KMwxh1BRZrKSMJg39lz51+mqj09xYAY1mqs7CKa7sgVFcjIc9xvVjLFbnWYrnGc1cTj2fUXXTkKwTC/3mZYyvsoY69K833SwWHSE2PWEco1ZDUjlp2ZAM6/B3Vajap+81PSvxquR+iwvrvziWhnHqPtehXioYHEwZHDSGS1AuOfV8E+7bVVlM8pPkngwycPDYZ+kWDxoDPCT1pHasasbTwffDjdlln5q6wL9uFuooqceFvTxL8bmCei6n6qbhulAeug+tdvK1BqsSkjSBnNC/nk8+ANwIr+UOjZG4kwwbbNhMEntdh8l2zfoEv99HyoCS+y0vzNwjBGuR43MF5ULG8IAxPGgMDzvVkZJicUiaNtR1wvByB8jIeka+lpCtppg7db5+uVVzwoWtOh40ZR1mqGqVSLmWTVciHa0fY3FdmHARlU5BPZ9TzafUcwlVN6HqGk0K2QCSKsFqaIqUJLHQVZiHrhgIxYd56F+32rEG0kFN0q+wfokNS7wsY5/77BUjSu63uFGitTR0yVTzBcMDKYPDxuCI0xzvc+L2S9yxcJksabg4mONMfoi13iJ1N6XOLVyIcgvj4CGcCXjsu0yMZONpblPPXP+l7JHRBfh4TWg0n8KLPHTHdDPqbkI5l1AuGHXXoAFrICkgqYymCJMofHQNKFbr48TeeBjbXjYkayVJf4j1BzAYQhWX2RiNcZ+Bin1Eyf1WNepnJC6itDCPLy0wONJh7VjC2rsa0uNrvP/4OT506BXuLM5TesrPBrdTNikvzc9RF+nkNNjjBamGdQeNlfE0d1DCsITNKqEZOmBkB13rpjCjLpk4tt07OT7foV4IVXs5nzA8YJSLhmdxVQwHc8MaIx2EdWWazGjSGPdTFXtShrPKZFBhawOsN8AHQyiHeFlNltuYMUrut6LpZVJTwtjhpQMMb5tn7faU3lEnvaPHLx17k48s/5TfXHiRI0mPt5o5AF7Ij2GZj/vVQyUUX9vjAVM144lLSb+cHDRlGQ6YGayEZHeNRsRMz372bphYV8/nVHMZ5UJCuRi6EOtuiMekJPxd46TD2L+ehDPMMJJmKrEPY1If1Fh/gK328OEwrAZZ15PEPiOLhU277pFlZl8xs3Nm9szUti+Y2T+b2dPx65NTv/u8mZ02sxfM7OM71XC5eeOFwYocm+uSLC5QHzlA77aC/u3G8FjFu4+e59eXf85vLrzIrxY93p0Zh5I+udVUnuB1nNA0KrY89FkmVehfTwY1Sa8ivTIgWelhV9bwfh8flvumElJs7yNxSO7oiySuzT6aidop8LkOPt8JE+vmM8rFhMFBo3/EKA841UL4Kpecah7qTqjUPWHSHUPoLgxxGhJ70itJVmOMrq7hvT4+HOJVNRkGOYNupHL/KvDfgf+xYft/c/f/Or3BzO4FHgDeD7wL+Dsze5+7a+2/vTYagTB90HQ6oRqaHw19TBgsO3OHe9y9eJ57Om9wIr1C1zqsNSUXm3leLw9yoT+H9VPSIWEUjI8unjahYq8abFCHvsveAO/1odffj5XQV1Fs7x9TN9MI322yznqRh6Uwujn1XEa5lNE/mDBYNoZLTtMJX545Vht4QrYWEjsWR24RKvYkdsUkZRMunvYGIT7XejSDwdU35JjRs8vrttrdvwWcv8HXux94zN0H7v4z4DRw3xbaJ9thqi9zPPmjyLGiwOe7NAfnGRzOGC4Z1WLD0nyfpaxHQsNKk3OmGvBc2eWHvbt5ZuVdvHFxkXQlIe2H2XzEZD1amiApa5JhNU7svtaj6YWqfT9VQortfWjqNnajqj2M4urQzOdUBwoGh3N6RxJ6R43hIac60FAfqshu69E90oOFiqYI3YbTrHGSKl4HKmtsWIWLputidIjX9cys2f52tvKR9Dkz+1E8tT0ct50AXp16zpm4TfaLZHTDgrh2dacIF6bmEuoOofIx50rd4c1qiZ+UR/nB4ATfXv1lvnvxHl44fzvlxS75qpEOQ78lhMo92XjQ9Af4Wg/v9fCqvOrmHvuYYns7bfw/T2z9F5P5FkAY7hiT+qgAqZa6DA936B/J6S0noSvmoFMeqkmODLjt2GXeffsFjh1cIetUIbF7OLNkdJHf4zrtdUzw/XJ88XSU1Mf7H0+Wms2qHW4+uf8Z8F7gA8BrwJ/E7ZsNWt70I9DMHjKz75vZ90vv32Qz5EaN+zFHU7bjDTi8k1J3UuoCPC67Oygz3ugvcrp3lKdX383/WznJdy7czT+9dZQL5xfJLqZkq5AOIB2GmYBJ1YT+y34ZDpr+MI44KDe9w/w+ptjeThs/yKcWAht/pcm6+Q6WZ1hRYN0ufmCe6tA8w+WC/nJGb9kYLBuDw055W8nSHSv8ixOv8evHXuFXDv6C5e4qWOiaSYfh4qqNPjNiUk/61Xi4ow8GoW/d4w1n0rQ1N3K/qdEy7v766LGZ/Tnwv+OPZ4C7pp56J3D2Gq/xCPAIwFJyZPbPgfar6bVjYLLudVzLuikyqrmEJmd8GjsoM97sLTJsMs7aQVbKDq9fPsDqhTnSixmdC0Zx2clXG9JeTdqvQtXeK8M678MyVENTB01Y3mD/HzSK7Z23boXH0X11p39fFFi3E7oMFzqUSzn9QynDJWNwyBgsO7yrz3uOvcUHl1/lvd1zdJOSN6oDnO0dpBpm5H1IB6EbZjyCa9iEWO0Psf5wPI59POdin8fmO3VTyd3Mjrv7a/HH3wVGow2eAL5mZl8iXHQ6CXx3y62UbTGqlEZTt5si3KCg7oRp2xhYbQz6BecNVvodyjqlt1rgFws6FxKKS0bnvNO91JBfrshWK9JeGSYoDYZYWa07aMws3CxhRm7GodjeftPFxeQm1sn4xi4GoQumrsPPC/P44nzsiinoHcnoHQ3V+vC2isVjVzh1/FV+Y+kl/mX3VRas5Fy9yNnhYd7sLcJKHroNB3HZizg0N+1XJGsxscfZp17P3mqPN+q6yd3Mvg58FLjNzM4AfwR81Mw+QDgtfRn49wDu/qyZPQ48B1TAZzWaYJ+wuKpevNep5+HmG3UnoY6rPVoFyVpCbTmrVzJwI+knZCtGcckoVpz8SkPnUkN+pSJbLUnWwoFiVR0OmKqafN/na8gotndZMlnd0dJk3Kc+XrLCDO8UVAfnGB7uMDyY0Tti9I4a/TsqiuU+77vtPB9afoWPLT3LvfklDiQZZ+uaN4ZLPHflOGffOkh+IYndhk4ycNJ+PUnsa30Y9bGPhuXW9b65yL+drpvc3f33Ntn85bd5/heBL26lUbJNfGodl+n+9tEdZqKkgqzvcZKHUfdTSMLiYGkfistQXHaK1YZsrSG/PCRdHWK9ITaIVVDThBmoo4PFfd8fNIrtHebNpqNOxvMsRrdjzFJIE7zIqA50GBwp6B8KS2D0jjrlHUOO3XGR9xx8i1MHf85vzJ/m3rzPYjLH63WP54Z38H8vn+TZN+6gerPL/IqRrzrFlViEXBligxJb6+P9waT4GMVqS2mG6i3ApmekQkj0TZhBmg4a8rUwBt6quBZHHhJ/UkK26nQuNxQrNdmVknStxK70sEE5nuhBTOKjpH7VCnoiTA3DTSzcRzfeQcmLHM9T6oU8dMMspwyWjf4Rp7yj5NgdF3n/8i9438Lr/FLnFxxKBlxsGl6qar7X+2W+ffEkPz53nMu/OMDc6yndN5258zXFhSHZ5T7JlVitD6YS+3SM7uMCZCuU3Ftu04uYdVjrJTEjW403N6hTkiohLSfrYacDp1hpyC+Fg4TBcDwK5qrKZ2NSb+kBIzdpFINxnRgb3e803hmp6WbU3bBOTN2BuoCmcJKiJksaGoxL1Ryvlke4WM+z1nR4ZXCEH106wUtvHqF3bp65sxkLZ535czXFxSH5hdHM6JjUh8P1cdryGFVyvxWM+jTjei4+GIa7zJQVSd2QDyvSuZxsLdzz1C3etKBfk670SVZ6Ycx6WU6mZNeTlR2V0OVarlrSeTwyxcKdkFIL66x3UqqFNCzZW9h4WK5XCZf7HV5eWebCYJ6XsyMAnB/M89rlJS6fW6R4I2PpnDH/RsPcmxXF+T7J5R62sor3B1Nnl7dGUh9Rcm+76ardHa8qrK7x0sLIBHesrMKSAWUdbpVnFmaZ9sqwkNJq7+oV8jbcMPht9ysyneS9wZsmXISvmrDkLoxnEoSFvUIXYd1PWV3pUpYZb6SLAAz6OdXlgvxCyoE34+itizXd80Oyt3okl66EiXMbu2AaXx+vLY9RJfc2mwped8eaeEeZ0cY8TvOOX0nT4Gm48QbDOBFpLS6kVFabXiC7qjJr+QEj78B0LDQe71Fa40mCDcswaqZpSOqGzH28sFdSJySlkQ6NdC1011QpVB4u/uc9Y2EFOhed7oWK4mJJdqlHcmk1LPw1HJ1h1lcn9I3tajEl91tBnJ26rvsksTCuuKpCN4176KqJv/dhuADVTCX2MVs/o/BWOVhk69wdK+PoKm9gGEZcpcMwVyIddMnWMsrFlHLVqOaMOg83sKYJM6Lz1YbOxdCvnl3qhbPLtf5ktuktntRHlNxvFVPdKN445japwusmTDoa982H7pvxjTVACV22LsbbuIKPseVpig2GJP0ByZUu2VyHYi6nnsupO8n4Dl9WNeE6UK8Mfeprfbzfp4k3gfGq2vwWjrdorCq530qmulW88TBrdHRRtGkmQyWbZtK3PkOzS2VGNI4zVVmPZol6E4foViTDgqSXk8VrQBA+EMaLfa2tjc8qxwvSjW6qPQPLXOwGJfdbzMZqZtwXD+tvonGLjCiQPbLu+k09TuyUFRQl9AdYloZlf2E8GIBhSRNnl05PQLI0VULfQMn9VrJZ8DfONWfR62CRnWIJlqzvEx+v8+IOw2G48Bqrdo/j06cLkHHil00pud/qNJRR9sq1io1mcv3Hp7bLO6PkfqtTEpf9ZNwP/zbPUczeECV3Edk/lLi3jd5JEZEWUnIXEWkhJXcRkRZSchcRaSEldxGRFlJyFxFpISV3EZEWUnIXEWkhJXcRkRZSchcRaSEldxGRFlJyFxFpoesmdzO7y8z+3syeN7Nnzez34/ZlM3vSzF6M3w9P/c3nzey0mb1gZh/fyX+AyM1SbEub3UjlXgH/wd1/Bfgw8Fkzuxd4GHjK3U8CT8Wfib97AHg/8AngT810ozbZlxTb0lrXTe7u/pq7/zA+XgGeB04A9wOPxqc9CnwqPr4feMzdB+7+M+A0cN82t1tkyxTb0mbvqM/dzO4Gfg34DnDM3V+DcJAAR+PTTgCvTv3ZmbhNZN9SbEvb3PDNOsxsEfhL4A/c/fLo3oabPXWTbVfdI8vMHgIeAugyf6PNENl2im1poxuq3M0sJwT/X7j7X8XNr5vZ8fj748C5uP0McNfUn98JnN34mu7+iLufcvdTuXVvtv0iW6LYlra6kdEyBnwZeN7dvzT1qyeAB+PjB4FvTm1/wMw6ZnYPcBL47vY1WWR7KLalzW6kW+YjwL8DfmxmT8dt/wn4Y+BxM/sM8ArwaQB3f9bMHgeeI4xG+Ky7v93tbkX2imJbWuu6yd3dv83mfY0AH7vG33wR+OIW2iWy4xTb0maaoSoi0kJK7iIiLaTkLiLSQkruIiItpOQuItJCSu4iIi2k5C4i0kJK7iIiLaTkLiLSQkruIiItpOQuItJC5n7VctS73wizN4BV4M29bgtwG2rHtLa0493ufvt2NeZGmdkK8MJu73cTbfl/3C5tacc143pfJHcAM/u+u59SO9SO/dyOd2q/tFvtuPXaoW4ZEZEWUnIXEWmh/ZTcH9nrBkRqx3pqx9bsl3arHeu1vh37ps9dRES2z36q3EVEZJvseXI3s0+Y2QtmdtrMHt7lfb9sZj82s6fN7Ptx27KZPWlmL8bvh3dgv18xs3Nm9szUtmvu18w+H9+fF8zs4zvcji+Y2T/H9+RpM/vkLrTjLjP7ezN73syeNbPfj9t3/T3ZTort8bZbMrb3PK7dfc++gBT4KfAeoAD+Ebh3F/f/MnDbhm3/BXg4Pn4Y+M87sN/fAj4IPHO9/QL3xvelA9wT3690B9vxBeA/bvLcnWzHceCD8fEB4Cdxf7v+nmzj/7Fi+zr7bXts73Vc73Xlfh9w2t1fcvch8Bhw/x636X7g0fj4UeBT270Dd/8WcP4G93s/8Ji7D9z9Z8Bpwvu2U+24lp1sx2vu/sP4eAV4HjjBHrwn20ixff39tjq29zqu9zq5nwBenfr5TNy2Wxz4WzP7gZk9FLcdc/fXIPznAEd3qS3X2u9evEefM7MfxVPb0SnjrrTDzO4Gfg34DvvrPXmn9rqNiu3N7Uls70Vc73Vyt0227ebwnY+4+weB3wE+a2a/tYv7vlG7/R79GfBe4APAa8Cf7FY7zGwR+EvgD9z98ts9dafbsg32uo2K7avtSWzvVVzvdXI/A9w19fOdwNnd2rm7n43fzwHfIJwCvW5mxwHi93O71Jxr7XdX3yN3f93da3dvgD9nclq4o+0ws5xwAPyFu/9V3Lwv3pObpNie2Bf/j3sR23sZ13ud3L8HnDSze8ysAB4AntiNHZvZgpkdGD0Gfht4Ju7/wfi0B4Fv7kZ73ma/TwAPmFnHzO4BTgLf3alGjIIu+l3Ce7Kj7TAzA74MPO/uX5r61b54T26SYntiX/w/7nZs73lcb8eV6S1eUf4k4SryT4E/3MX9vodwZfofgWdH+waOAE8BL8bvyzuw768TTgtLwqf1Z95uv8AfxvfnBeB3drgd/xP4MfCjGGzHd6Ed/4pw+vkj4On49cm9eE8U24rt7WrHXse1ZqiKiLTQXnfLiIjIDlByFxFpISV3EZEWUnIXEWkhJXcRkRZSchcRaSEldxGRFlJyFxFpof8PF7xuQOBibJgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABA6UlEQVR4nO29XYxl2XXf91t773PO/ajqnmkOORpRTEgJNAIJcGRFIAIoMBQIsWQ9hPaDDOohYAABzIOExEACiIofrBcCShA7CBDEwBhSRAeOaAK2ICIwIst0DMGAZH1BlkgplGiJksaccDSc6e6quvd87L1XHvY+556qqZ5pTndV3bq9f8BF3Tp9657TVfv879rrU1SVQqFQKBwW5qYvoFAoFApPnyLuhUKhcIAUcS8UCoUDpIh7oVAoHCBF3AuFQuEAKeJeKBQKB8iVibuI/ICIfElEviwin7yq8xQK10lZ14XbglxFnruIWOAPgP8MeAX4deCHVfX3nvrJCoVroqzrwm3iqiz3jwBfVtU/UtUe+Azw0Ss6V6FwXZR1Xbg1XJW4vx/4s9n3r+RjhcJtpqzrwq3BXdH7yiXHzvl/ROQTwCcALO4/Wpu7V3QphQI8jF9/XVXf+4Rv847rGsraLlwf23hKr+1l6/LKxP0V4AOz778F+Or8Bar6MvAywF37gv7HR//5FV1KoQD/7OH//idP4W3ecV1DWduF6+NXTz/3yH+7KrfMrwMfFpEPiUgNfAx49FUUCreDsq4Lt4YrsdxV1YvIjwG/CFjgZ1T1i1dxrkLhuijrunCbuCq3DKr6T4F/elXvXyjcBGVdF24LpUK1UCgUDpAi7oVCoXCAFHEvFAqFA6SIe6FQKBwgRdwLhULhACniXigUCgdIEfdCoVA4QIq4FwqFwgFSxL1QKBQOkCLuhUKhcIAUcS8UCoUDpIh7oVAoHCBF3AuFQuEAKeJeKBQKB0gR90KhUDhAirgXCoXCAVLEvVAoFA6QIu6FQqFwgBRxLxQKhQOkiHuhUCgcIEXcC4VC4QAp4l4oFAoHSBH3QqFQOECKuBcKhcIBUsS9UCgUDhD3JD8sIl8BToAAeFX9bhG5B/wj4IPAV4C/oapvPtllFgrXS1nbhdvO07Dc/1NV/U5V/e78/SeBz6vqh4HP5+8LhdtIWduFW8tVuGU+Cnw6P/808Neu4ByFwk1Q1nbh1vCk4q7APxOR3xSRT+RjL6rqqwD56/ue8ByFwk1Q1nbhVvNEPnfge1T1qyLyPuCXROT/fdwfzDfMJwAWsn7CyygUnjplbRduNU9kuavqV/PX14CfBz4CfE1EXgLIX197xM++rKrfrarfXcviSS6jUHjqlLVduO28a3EXkbWIHI/Pgb8CfAH4HPDx/LKPA7/wpBdZKFwnZW0XDoEnccu8CPy8iIzv83+q6v8tIr8OfFZEfgT4U+CHnvwyC4Vrpaztwq3nXYu7qv4R8B9ecvzrwPc9yUUVCjdJWduFQ6BUqBYKhcIBUsS9UCgUDpAi7oVCoXCAFHEvFAqFA6SIe6FQKBwgRdwLhULhACniXigUCgdIEfdCoVA4QIq4FwqFwgFSxL1QKBQOkCLuhUKhcIAUcS8UCoUDpIh7oVAoHCBF3AuFQuEAKeJeKBQKB0gR90KhUDhAnnRAdqFQKBw+MV5+3OyvfVzEvVAoFC5jFHTV89+Pgp7GMO4tRdwLhUJhZC7oMaKjsGdkFHSRvbbaoYh7oVAoJFGfC3pU0Cz0YsBIEnZrwZidyEN+/QWrfg8o4l4oFJ5NLrPSLxN1axHnwGRxF8k/k18b9NHnuEGKuBcKhWeLi1Y67IRaDIhBrIGqSqLuHOJset1omceI+gDeA/G8X35PrPci7oVC4dngMtfLiBFE3E7Q6wqpKqgcag1qzE7AVREfEONRQEJgH233Iu6FQuGwGd0vIZx3vVx0u4yC3tRoXRHrCq0s2OxfD4qM/vUhIGRhtzZZ8DEmt82eUMS9UCgcLo+w1sU9wkqvK3RREZcVsbLEyqAGREGCIj4iQTG9x0RFQoRhAFJgVXR/bPh3dA6JyM+IyGsi8oXZsXsi8ksi8of56/Ozf/sJEfmyiHxJRL7/qi68UHhSyto+cC4Iu4gglcMsF8jxMebOMXL3GO4coXfWxLtrwvMrhueXdPca2hdq2hcq2vekR/d8xXBcEVaOuKjQyoE1YOxe5rw/juf/Z4EfuHDsk8DnVfXDwOfz94jItwMfA74j/8z/JiL7s08pFM7zs5S1fZjMK0qNQZxDFg2yXiN3juHuEXr3iHh3TbyzxN9dMjy3oHu+obtX0d6zbO8Z2ueF7jmhOxaGtRCWhrAwxNqglUWdTVk08/M+qpr1mnlHt4yq/rKIfPDC4Y8C35uffxr4l8CP5+OfUdUO+GMR+TLwEeBXntL1FvaJW1iSPaes7WcAa1Pmi3NI00BTE1cLdFkRGotWhmiF2Bj8wuCXgl8IfiWEmmT+RrB9ei4RwCABbGNhYxGTA65RkwvnRv/DO96tz/1FVX0VQFVfFZH35ePvB3519rpX8rHCIfA4FsktEfa3oaztQ2AsNLI2+dTrGl026KrBH9WElcMvDbESohVCncQ8NEKsISwgViSx9iSRV0nPFSQKsbGYyqVdgcjeZcw87YDqZR9al/6fReQTwCcAFrJ+ypdReCLerYg/zpb09op/Wdu3iHPCvmjQ1YK4TsLe33UMK8OwkizgAiYJuRqmv7RaCLUiNeggRAuiAgKiBruwyXq3Zueaifsj8e9W3L8mIi9ly+Yl4LV8/BXgA7PXfQvw1cveQFVfBl4GuGtf2J/fSOGtPEqQ50L+TlkCexhwegRlbR8CRpIrpqrQpiauavxxEvbujmVYg18K6kgf0QrGQ/1QaR5EYiWcvWTYvhdCDbFSjAUTBKIgAaq5332eArknGTPv1oz6HPDx/PzjwC/Mjn9MRBoR+RDwYeDXnuwSC9eOMecFfbTIx0cI5x7qPTrMHiGgIexVcOkboKztW46IpCpTZ1N646ohHNX0dxz9kaE/FoYjwa9JIr9KLhi3UV74rQfc+Rd/wHO/+CVe+lcnVCeCVkpYR8JK8cvksglNduG4fK+Mxovuz3p/R8tdRH6OFGB6QUReAf428FPAZ0XkR4A/BX4IQFW/KCKfBX4P8MCPqmq4omsvPE0uivDc+pj927lybXjrYpb0oSBj17zLuuftSZOlsrYPjHnpf7bcta6IC4dfWEJj8AtJ4rwAv1KiSznsToXmgSJ/8ira92AM5kt/wvIvfgfbb1a0CUQsoVNsJ0SXHupMcsvsUfHSyONky/zwI/7p+x7x+k8Bn3qSiypcA48S84v9NmBXzXcZ8455Ji3yyd8J532RY4Xg/BpuUODL2j5QjCSjwhpwlugMsTKECkIt+AX4teKPIroYP58rYkXajeY1qSEkX3wTMU0gqhArQ7TJ0o8OopNd8NYIBPamUrVUqD6LXOYrf5Sow07Y5yKev8potRi7e36R7LoBcqOlfN7b44cv3DYkuUvUClghVikjJjYk98pRhDsDy1WPtZGzesHJ/QXPfdsHkD/4Slqf3/rv8fBDUN3pQIVoFXWa3tOMgVhBrZxrAbwvFHEvTEwLtLLjgfMpZZNFlHJ7calCbwwoqZ1Z4SFOFYLiA9IJyoAEecsAhELhibm4EzWSxdcQbUp5DA3Jal8pugqs1h3Pr7esq57+6IyvNnf5SnOXoz/9iwA8/Dao/4OH1M5zumneckqV9EjnM3uXDlnE/VnEmLe4RKZ+1bLzIUp2s2CzpZ4fmgVdrQVniLVNvkfZuWCMj+AjEgJ0Hsl9NyYLfk+2roUDRLLQZstdnSQXSpVy2GMTsQvPqhm4t9zwXL1haQe+9fjrfO3FY17frIkqfKDpMKK8fromBgtBkJgLmWLy1c/vi7ckIdxwTKmI+zOKTJb4BTF3DlxK75oE3Mok3CqSBN0Z1KUy7FAb1EnKEQbMoJg+YoeIDBFjLSZGxId0vtuXQVO4jYzGiOxy2NUqOKWuPUdNx3uaM+5VZyztgJXIC80p26OKN/o19/slX9+uaPuK0FkYBPEzgdfz59k3irg/g0zBH2vzIAKXUsaq8atFa5ctciHaXU21CknUnRDrsbJPCFV+7wh2UFxrcNuIbZOQm86ds2T2rYNe4QDQC7Gi2fpSEdSmIKjUkabyLN3A0g40Ju0m21ixDRX3hyWvt2vut0tOtg3dtoLWYDqD8ezE/SJ7JvBF3J8FLks9HIV9bHW6qNGmItaW2Dj8ymarPAWMJt+ikLMFUnVfGLe6Li144yF22cKfikOSlS/zSr5C4Wkx6/44WetjwF7S+k1uGVAHYiPORowoQYUuOgKGbag4GRbc75eTsLebGt24JOz9znJHmYwTHY0lk6Y4qepe9Jcp4n7oXHCBTFa7yM5iX6QKvrCspp4bw8oQGsnVeankmvGtzJgGtrthNDdVEp+yCCQqJggSDcZb7FjJN/PLU1zuhSfhkhmoAimryznUpX7ssZJpjarRZKAo+JgEPaqhi5aNrznpF5wOdRL2bU3cOqQXTCeYABJI/vaoj2g+sT8UcX8WkTF90aCVSy6YRRb2lWFYmlTBt0gZBuqSO0ZyEGn0X0YH6jQJfP53MyTxllyijYIZUpn2aNlMlKBq4d0S3+p6EWuRykFVQ+WItUtdH13qC3Oub4wKQQ1tFvc2OE6HhtOhZtPVDL0j9hYZksVuPEhIa9qE3b3wFjRCNLudBNxYYLWI+7OMNTmVcRcY9bmKzy8FvyR1yKt1an0qmhoqqU3uGXWKVooaRYJg+pSdAILE5JqxgxAri3UpF14h+0VvbROxwk0y341m40CcO98BsqmJC0dobLbcs/VuR29NUmYfDVGFNlS0vqIdHN3g8IOF3iSrfZCZwKfdqYTxec4IC6kth4aAWPbCcCni/owi8/QtY1Ajkx891rl3xgLCUgl1Kt4YUZfF3kWwinEREYiDIXQ297ZOVrx4CK3kcWVmL3yRhVuOmbW4mNVfjKPzdFGnUXmNJeS40dj8S51iRGeF04aoMARLFyy9T8KuWdhtJ9hWsB2T1W68YjyYoMmCDzoJO6ponPncbzAdsoj7s8JlmSkiOYtAJt+kb7LVvobhKBLWERYBW0dEFBHF2IhzEWMiAhhRogpd7+htRcQRB5sCrfkDQ52cS6ksFN4tU9woW+tTPcY4A3VZE9Y1fm0Z1ga/YAr6q1XExMlyj0hau8HR9hVd5/CdQzqL3RjsVrAt2HaXJWOGJPASsu8dpmlPhJCvpTp/0RcmQ10HRdyfRbLZoiYJrpqUJhZmVrtfpE54svYsVj3LpqeyEWsilUlfR1GPKgwhbUFjMAxV6uUR81Y4+Tsv7BTifmQUFPacy0RxbOfrHDT1rjo6NwmLiwq/dvhFstpjLVnYAaMYk6x3YFq7XbDJHdM7tDMpgDqA6dMUJjPk7Bjd+dtFU3BWnUHqClkuUv0IJIHXmKYz3VDKbxH3Q+SypmBjmpi1uwnwUaeAv0oS+ThW8zUKi0CzHLi73vL8YsvCDtQ2UBuPFWWIlj5a2lDReZceNiJWUavJN5+DWKlUeyfnUlIiC5fxDgVu50bm1VWy1CuXjJQ6+9ib1AEyNCkhINQQayXWCpXiXMSZiM3J6l4Nvbd4b5I7JgdRbS+TsNshZ8fILsc9uTINsXHIapHEvuuTa0Y191EKEMb8++vtp1TE/ZB4h06PEkJan0Z2aWPzQo+xis+B1oqtI8um527T8sLilDuuw5lAJamT3jbUnIU6bWtxRE1ZCKppYo3kPHd4RGZBoTDyjqJudxb7XNgXFeoMapPIhsakeahLw7BMLsax/7o2EVMHKpsezqRzRhViNKnFgDeIz2mPfnwkFwzsesnovN5jmWTUiCCVS72UfLrXiMmVeRPLv4j7ofB2/djnzKtEp8ILmTJg1OXAUxWpas+qHrhTt9yrNxzZbrJ2hmjZAl1wbHzN2VDTDRW+T1kGU2ZBzLnBpSK18Ha8jR96Cpza7Nd2NqXwNjmNtzJoZVLP9mUadN2v00COsEztfcMyQhNwVaCpPLUJOIl4NTktUohBUrpjbjNgZmmPo9U+FvGRjRe/SNaQdQZTW2zrkN4j7ZB6KcWZ1R6ut/1/EfdDYbw5RpGfp2GJ5OKO8ebI48dyq4HRPz4vSsIpzmULR3YWzqAVPhq6mMq07/dLHnQLTtuGtq2IrUN6M2XKTOXayt6MHyvsJ5e2zb3Q/4gqW+x1hTYVYeFSGm+21v0i1Wf4laQJSw2ERqedaFUF6slyD/hg8NEQgkGz1S6zYqWRyb1oAYUo7PrWWItfGmxvsZ3DnXmsMcjgEe/ReDMuyCLut5nLOs/NBwdcHLAxWj45H3gS9joHP7OPXIXJj6IqeE2VfD5ahvx8rOZ70C1S/42uIpxWyNiDo89WT9CUURB3LqJCYc4k6vM1O8ZkxhbTo8XepDTHUdj9Kq3hMGZ5LYSwSMKeJi4pcanI0lM3A6umZ+kGauOnoKoCMZrcUiDXOWUrXUdBz8eiG/0yEJ1iKkkGuUrqqbTNRXxDQJxN7iTvU5fKa84UK+J+G3g7QbxQBSfW7Cwdyf1cRqyFsff62EsmF3mMU2VGgd91CRD64DjzqZ91Hy2nQ8PWV5x2DWdtTbfNFntrsK3JfkrZuWWmLnrFci/smET94qyAeZfFvGZ1nIc6638Umlx818xSeHPxnV/r5I6RVaBZDawXPeu6p3Ge2iYXSVQhRJOW5uhBydc3xaCMADpllY1+d1E5N2UyDvmSe4NtHLauYPBIjGiI59f/NVSvFnHfZ95O1MeFMs9AyYOBcW7KKpj3Yte8tU3+SpvaDSxsShlrcgOwKlWcYsZiDyUi9NESVdj4mtO+ofWObV/Rd1nYu2yxD0nYz/ncY04bu0zg92SeauEKuWiAzC31i1O8nJ3iQEAeBGNQa9NOc+GmiurYzIR9lXzswypPWlorcR2QJmV8rRYd63pmtZPWNSSB13HyxtTGNz+ywIPklsGz3S25Yju/Tnw6aLzBDA67rTA+pLeadazUcD271yLu+8yjBG9Ma8yvOdfC11ikrqag03zYhrrU40WdITaWIfeSmbaz9ehzV4w9nwvso6GPjq2vZsLuCJ1NPvYuVfGZPvff8Dk3eKzk8zpZMBpCCjSNQVaRvRhuULgCHmWgzLNfZkHSc+tVch1GzoYZ1220QqxT4NQvhGEl+LXQH6cpS2EV0XXALT1V7Vk2PcdNz6rqUzqvCedqNMYMr4spLdOkpbwsp5Yblsllk1w16Z6RmFtv5E6obltBUIyRtM5DzC4a3XWtvEKKuN8i5tvYsTDinH/dWGTcvrqdX12NSRaI3U1NCo0hZIs9TFWkY98YxZhcgTr24FDLECx9sLSDo+9dyozpDGaby7S3MgVQzZCGdthBMYMiIcLgU+5vCFOBR+kM+WwxrddR2MeW0+MsgTwEJr84u0LyRKUqD4axuTApV1Qnf3sW9nVEl4FqNbBc9jSV57jpWFc9K9dPNRoAPqRMmaig2ec+uWZmLa6RmYvGnn9Eq2nHWysSkuvGDMKwNVQLiwwuCfvgEWdRjbusmSt2UxZx33POZRDMCn9krO8Us+utkV0xk3+yspOlrkZyhzwzBaDGLW1Y5KyCqdAjYl2ksnHKBfbRpLTHrqZtK3xboa3FnhncJvffaJPFDtliH0YLXjFDREJMiztft4wuo4vjyaBY8YfAZVb7fIc5tpzOhUja5PV6Yc1rHkIdx2pqk56HKq3hMAr7UUTXnmbdc2fdcnfRsnZJ1NeupzJJVIMKMY8Ni6Qc90cxuVzm3VCrsU9NznVfRLSOKYVGLXYLfglhaTDegYLrfTJuYgSG87+jK1rrRdz3hUv+yG/JehmDTudelMR9GpE3WkC1e8s0pTH4FF26KYalEJZ5cPAyZRfERUTqSFX5c1V8IZrcWMnuSrTb3A61k6lMW3I1nhnTIHNzJUg7B6oaafJM1XFu61iiXTJpDprJUJkNt1Brkp99dL2MBsyYVz79MPlnssXsshsxr11/pHA8sDrqeG695T2z2aiN8dO0pahCFyu6UdyziZ46lV7uJpn71cdRfepStkysFW0i0gQ0plz5sDQpY2dh8q41TTaT1u6GaF+DG7KI+z4wilr+g593tcgu4DQWc8yZWb/qbLaCLNpYYpXKsGOVZpym4RujxcO56j2/VOJY6FF7GhdonN9Z7mrwwTKMHfPGEu0ulWgnf3sWcsnB1HFSjYFYGWRZY/wqtf0dg0oxcJMl2oVrYhSybKikZl+5z1BuXjcZL+fG45FE9Vx7jLyOFzt3TDzyrI46nl9ved/qhPcuTnmu2lJJwEqkkkBQM01dGmbiHi+I+iwTeKqyTvky7AbV2ByfqhVZBKom5bMPUQgLk65rIZghCXxyj8rlufxXRBH3m+ZR1uoo7LOA02TpzBeIyO4myQHTOAZN6/GRbojRPxlqpu1syEUecZEsEFtH6jpQO0+Vq/jGznk+zEq0xyKlYdclz3hAx+yC8YbMJdqNRWKdesI7iwweQkT7Pv034s2UaBeuiPkwjXl2zGS05Ic14My0tlVGl+NsTJ7MM1VmY/Om1tSKXQbWi57nFlve02x4b33KsW2nVhlGIkNOWB/U4sVi5isuC/zUMmNsEDarTj03aNulKm6qmIyhZiBGQ/CGWLnsRmLnTspFT9fJO+4LRORnROQ1EfnC7NhPisi/E5Hfzo8fnP3bT4jIl0XkSyLy/Vd14YfKIwNO9dinukGX9blHXNTEZZUejUsZBblqLzSzARwrYVinzAK/hiHnAvu1ElcBu/I0i55lPbCqBhbW40Y/ZTQMwRJ9Ks82+TENLRg/o85tX5N1E2phWDuGOzX+PUvC82vi3TV6tEQWzdS69br97GVt3xBzl4zJAVJnJgGc929hqgzddS8dhd3nIiVdBparjnvLDe9dnPJi85AXq4e8WD3gnjvlrt1wbFoW4qkk5BTfb8AFeC6gqlPSAQbEpfbXY7+aqWneLG1yTGZ4ZHzpingcy/1ngf8V+AcXjv/Pqvo/zQ+IyLcDHwO+A/hm4J+LyF9Q1ettqnBbmW9dZfSj5xSxupos87kFkCrokpU/+tbTV5k6441BU7+S1Ns6Wzt+max1qohbehbLJOzruqexntqmKr425N4bUdBg0oSlqe0p56yaqfcG+fgUEFN8EIy3uG0u0d70mMEj/bDztV4vP0tZ29fCVIMxrutJ4LPgCanfP0DggtWerPXJpVjPGoItI2blubNsuddseKE+5Z474649Y2EGohqCGAa1VOKxErHEqYbDXOhoN6U/zgRd4vljSB5WYxSZWginMg7h/M/u2l2TdinjeVSzIXeDRUyq+ssi8sHHfL+PAp9R1Q74YxH5MvAR4Ffe/SUeMG+3dR0t2VHgZy6Xc2QBxeagaTUO35DZ8A0IS2FYp8q9WOdGSouIbQLWBRa5NHtVDTTWs7DDOZdMiIY4E/WRqfbDpk6TyWLJPTccaXsblNCkFxqf3DQqICFirJ0VsgR0bs1cceZMWdtXzMVUv0tcMmptWi+zYOro354MFSdT2u5UibpKvnZdBZbL1Ln0uXrLke1YmY6FGbCjb0UhzKKzc0E3omk2gYkEM1rl5EHasitYMjvj5WKgV/J7RE3Pmaz1WeX3LBniuvq7P8ld82Mi8jt5a/t8PvZ+4M9mr3klHyu8G2S2KKbnXFhcF/ySZpYulhuBxTpZ7H6laWxeHsJRr3uWq471suN40XFU97N84F2hR4hmJ+7zKj4454aJVna+xuncyb8/ZJfQsE7PwzIVpGjj8tzLKlfW2t3Ag5ujrO0n4aK7YbTUYWexZ4MFt0vPjZVJBUrOEKrdXN/Q7BqCJWs9B1EXEdMEFvUw5bBXsmtJ/XbYWVcwa/IW1KbHzvWSC5YmC3z3SOs+W+7jhDIBMYpKmil87sPg0qZoV+uGfLfv/veAbwO+E3gV+Dv5+GV760s/pkTkEyLyGyLyG7227/IyDhTJUfXRJzltX3dCfylZ/MdFOeYBjxkF/igSjz32Ts/6Tsu9O2e8cHTGe9dnvLA84z2LM56rNxxXHY31U16wV0OIkgo9coBpfgXTlnUams30wRJqsksIhiPo70B/R+iODcORI6wb4mqBLpvUp7tyOyG4mV40ZW0/DS62x5i7ZOzobrRTu96xeV3MhUqx2dVihEZ23R6XsvO1LyKuDiwrz8KmdMfK+Cl9F9KM1IAhkr6GHO0PWfoMyeq2VpO/3KVHHH37jmmK09RXRkCz9Z8yNN/q3hn/8RHZldfCu8qWUdWvjc9F5O8D/1f+9hXgA7OXfgvw1Ue8x8vAywB37QslUYKZXzJ9s3tkv+RokaM63TO7LeOs2MPt0h0nS+dI0WPP4qjnaNnx3HLL3XrLyvVUEmls6rdhJOLVElTw2Xm+a66UVutFt8zki9R55d4uoJQCqyk3mAjqUt9s1xncmUvuGUB8AJ+XZJi5rK6RsrafPudTe7PVnt2OY0HSrkFXQrM7Y/Sxh2Zs35uGtseFIk2grj3LKuWyz63xqIYIOe3R0qtjUDu5GUesSYV6zgWGOiQjBoMOmrp8jM3BZHTVzP9j5/+fqdJVEJ2p+kWBj/Ha6jneleUuIi/Nvv3rwJht8DngYyLSiMiHgA8Dv/Zkl3jgXDq42uyaJ419NnK2wO41M3eNyC6IMxfUPPXdL1IOe7Puubve8p7VGS8uT/jm5QNebE54oTnlObfhuSoVfcxLtCdhHy/1kssdBX4qMLGjO0YJi+QK8kcRfxwIRzFl6KzIRVSGuHToImUEiXM7a+8GiprK2n5CHlWVOn69ZOc55ZJPvYaSiy+5+cZdoOTK0PP9j8ZCu10fJEOvjl4trVa0sZqEfVA7We4AlQnUJtVz1M7jXMBUAVycrPXodOdvH90yk6uF3aDtPM1JZ25LvSjs18w7Wu4i8nPA9wIviMgrwN8GvldEvpP03/gK8F8BqOoXReSzwO8BHvjRkk3wDTAGU/NNkIJMZifsNgm5BJiywker3e6s9qnIox6rThVZedbLjucWW15YnPFi85Dnq825POCoJucCL9JgDtlZ7tOinfqh7p7LfDEbdtV7+SaMTdpCSx1Qbwg+FaCEBQyrXBUSFGkrpOuhN9ci7GVtXwOX+ZWziGuMSDBISO0p1I7GyuylIjtRnQKVyTcuk69b8yCZJOBdrIj5AyRgsmtm13JgjjMhibwL9FUgBoMGAy7tNFWFyPiBszv36G8Xk8fo5WlO430y7W7P+S9197gGHidb5ocvOfzTb/P6TwGfepKLeia4LOgE5/3tdlaWbQ1xTKUSyNUeU9XeuQrUevRP5i3sKrBc9dxbbXnf8oT31qd8U/OAu3aLJU7+x0Etm1inIo9o6CTuyrNVdlvOmBfwuV7tTNbMNId1LM+uI2bpcVUghCTuKZUtXbMdBG1Ncj9dY6FHWdtXzGV/S03DW9QHpJPJdSAhZcYk92JKt50bDaN7I+ZgJ1aRWWO7yE7cW3VTdswo6OO/jYVMAE4itcmTmUzKVQ9VmsoUXSTWYzVtfi+XJjqpU3CKuIjJ4j5Z7pHp/kgXP94jumuzcU2UCtWb5OIfWswUeNoNKTC7iUlVEj+JY1n/rAK0Gtv25tYCOe0xrFIe8NGy4/lmw716w3uqM+7ZM47tFhi3shaLY8iVe0Z0cs3sXDI7q2TaSo/CPst3j05zoYlO5dnNYqCpPN3g2NZ2576ZNYQ6t2Uvgz1uN/nvOJXbR03dEEVSh8TcS0iCRXw8X1nt8hqf6ijk3Jg7daPFvPP4DNHSR8cQLZ1UREnB06k3ks6/NzTG401qHwzgazMZMjEKvU+uyChp+IwKyZqvIjjFVAFrI9aOvZcE7w3RG2werD32V5q6osbcCfXC7+iqKOK+D8is50TOcZ/anObipPEBu8DTzg3DZK2nnhu7EWPaRJrGs65TZ7yV6VmYIRV0MFo9YEUZFCx6zof51mtNgaWpc9/M/3i+Laqmdr5WMXkeq7ORIeRg8MWfEXbvmTtHToUehduJMelvGAGTKoE0RIQh9TUfh3OEiPjkmotLl1w1uZ5iSvEdg/bjY7Y+Yx4FGbJrZvSrjwkCFk0pkozNw5L70RJxJs1S7aNNKb8q9N7hq0gIY+1Gzl23ilRxsthNFvYY0xxWPzjobOq5NG957TWN3fMhdUWN8VrWdRH3feJRVZqjB2e0jqcy7Fy5V+VeMQuZpR6mIKpZeBb1wLrqWdqByoTcTOly8T5X7MEs3cskP6MazR86nHvATtinknHJvsm8Cxh9pIju/PO5/0ZK9eRGg6mFp8AjB8woEJJrwo+vlVTboDp9kEtlJjeGXDJYOm0gNWWxaM5QycNkfJ4WBtCYgYV4GjNQSzrhPA2yi1US/JjCJgvr6Z2nCY7OeXpr0Tp5WJL1oxgXMdkdZPLOQRWG3FAvtBbpTGqm16ZmerZXbBeQIaQ1fU1TmKCI+/5wUdhVp+HSEvMkI5MrO42ieexXzCI/ZRKMzZQahTpiq0BTjRWnIffWiFPa2CjmQWXyvY83wTiGbCzSkEncNbU7mGXmwCzgNVpZY0ZBrgC0o49UyL6d3YfEOXdMcckcFJOfOSiQ3RO5Kltq3bkaRZDGpuCqE9RomuIVZOf+m96TKRVlvk4b4zm2LUe25Tl7xkIGagkEhKA5310NZ1JjJKaGYtGytAN9tPTO0lWWobH0AsHkSzXjABs9t1Rjnr/qBwu9wbaC6cB2iusU2yqmDUjnwechNdfUQ6mI+00wWqWzUXmpdnkyRdJgCx+QPgWdVFKqWMxjx3DnI/JxrAw15I51OjU1cjmXd+5q6dXmog6ZFvwYcIqc/6AZc4GtjSlfPSoxaBqEPbOuxtSxqf9G/iCyVrHmHQo9xhmrMAWeikvmALi4A1NF8yQiEUG9B5P6QIqzSBcwNlnaamazeENe79lokGwgWJPciM5Elrbnrttyz51yz57yHnvKWtJgjIBMGTOtVixMTR0ClSyJuRUwJBfPEC0+WESUwSgxSnLDmJiyYUh++bFiO0ZD3DrsJg2uqU6hOlWq00h16rFnHdL1aD+c97mXfu4HiHlEqp/G9AghtdWFyYIHiMEiziT9tznwNE+5mrk6djfB+Vmou6wBxzAaVJjdv8XzS8KZnFHgwpTtEiNoEHQQYky+IonsuuCN1jnp+cVcYKKkD4XLDHRVzo2UL9xuLlnrMjdqIH2Yh4j4gAwe06d2BaY3SDCYi5Z7LhYa0dyTPVnuAyvTcce0PGc6VjnVNxU0pXTFhXoq3cWcOlOxtAODWra2Sk3znJ9cPjFKjvVrrkFKrThisMQgab7BxuLOkrDXD5XmYaQ+GbCnPea0hbaDGK5ldupIEfd9ImrqcY5Poj4IMqTJSiZGpHbExoFxhGimBT8vqpi3Jp1vIVPQydLGii5W2BxsipMfUiarPeguc8BJShGrbaByAe8MWgnqDXEcChw1Z+7MCj4m18uu0GPsT6NBJit9PkVeRpdMvEz1C7eWScTjLhMsHxeR9GEeQwqy9snSNoBxBttbbK9pZkDIMwQ6QzQQqjTP98zWNM5nn3s6VyWehQSqczqqRFGsKiEKgwwszJD882agM47GeGoTWDg//dQ4hi/kG001GVqhszAI0huqU6F6KNQPlMWDQH1/wD1skU0Hmy3a9bvZqddEEfeb4sKWbBp4PQ6PDjHZFd4joUqGcFSMCFrZyRef1HMW2ByLPGbCPhV5xJS/3sYqH7/QgyO7Zubl2UaU2vhcxRfwlUlDCbJfNAaBKgXAJreM2+UCG7u7jhCFELLlzrmEh8kddd25wIVrZG7F5/WvqqmALaYMLDF5x2oEU1tMH7F54lfowXaSDYg07H0YHFsb2FYVXcwtBnI/mUAqQBrvNJsfAWUhIYm7pIDryvQM1rK1NSvXE5FpxxtiNna8JZBdMkGSsA8GuzXYjVCdKfWpUj0MuJMOc7KFbYu2LTr43T1ffO7PGHnRK6R+1vPGSyHumkE6m3qxeCWGXBxxseR5tJzHt87bVj/zq1uNmGzlzBstGTQ1DItQG8/SJkvKq6Wr3LRV7UJa4FFAh5lbpsrCXkWkihgbsGaXMhaDgZCqbIlgwpgPrIgfc4HLkOyD5eLfdFz3xqSsGe9Tc7EQkah5uHpOKewF2zEF4n1j6DuHiLKpPGe+5oFfct+uOLZb7khHkEAtkUqglhS/skLaKZieHsuxaeltksLB2WkE35gtNgRLFywDNrtpDDoYpDeYTiY/e32i1A8D1YMW8+AM3WRxz9PGLv3/XyFF3PeBxwk69bvhHdK5tGXNwVUTZtWic7/kZAwLfXR0wbGeDQyocotUO3N+j0GnwViqmLJrGlMD4PP2NEaDHyyxzgGm/NkylWZXcUobcy4FckMUem8JvU1B4jx71QwpXcz0IWUTXNy6FoE/XMasmen77JLLH/DiI6YPuO1Yz5GytJIbPbUGjlS0UXhoI2/Uq2kg9sp0LGTg2LSs8TQSaMRQYYlEjAkQA2cysDA9a+2IKjRmwdIOufGY0Of0SpOt97FQiT4NiHdbcGdZ2E8i1cMBc9LCtp2EXUOcgsbXSRH3feBxg07e54CTT3MncxHTNL90DDJdWEXj9tKI4kxgZXqObcvKdKxNl4YHZ1EfM2haralCmMaSddGxsAODs7TO7YKrOuszb3NJdhb0scjDR4MGS9fWaGtxW4NrwbaK7RTbpptYBp9uhDFTpmTLPDvEuPO9B5M6hHYDDjA+IqHCBIvtDcMqB+QxeC+EIGxd5H6zZGE9x1XLC9WSs7jBoqytpxZhIQ5H8vcbPK301EQW2T3TSsXCDKxMjzeWM+rp3hmCpR8cYbDo1mLPUmaM20B1kt0xpwG7HdJkMe937sVrHow9UsR9X7g4W/GyoFMIMHjoB8QKxgqmMhhvz88xhaklQCrR1hQYNYEj23HXbXiPPeU5u+HYbLFoDqi66etZbKjFU4UlBqVzji4HrIbKMnhLjII3mgKkwlS1l4qVxp2D4L0leENoLWZrcNt0U7gtVNuI2wbMdkj/t1xUUiz2A+eybLFc6KQiSCdIiEjbIxuHdAtMX2N6h/F2GpYNybgIS8dZV/OgWvBgWPKgWqW1LZHndEvQQFDFzTTWku6NSgIL01PFOhU2mTDFo3yeHdwOjqF3xNalQqVWktW+hWoDbhMxXUiuxRBBDGINGm2Ko43Tp66RIu63gCnohEfFJPcMYESwlcX2EdtZbA/ek7JsqrR9DMEwRJPKq1WwkiyV5+yGe/aU50w/E3czfV2Zjir4qeipUzdlI/TRMmQXTW8j3qcPorHPRrrm1CEvhvQ19BbZ2mTtnI3WTpyCT7LJvslrzigo3BDjbnW2Q51aFYSIMsCQ4j3iHNYHJAQkLgDwjZtm9UYrxK1lUzd8XTSlMhrPoJaTuMTWipEHBHrW4qnEMGhkyJdiibntxrylwbjOLa13dF1F6HIF6tZgt4Ldgtsobhuxbci7z/DWdN6xffc1U8R9H7ks6KSpya/YAMOQOtW51HTJdIqrldAxlT4jQrCp0u60bbCiHFUdQ7RTJepCAivRKYMg5pzgQcHG1BvGSmRQy8r0bGzPoIaNr9jailB5ospu2E5ufToWeISQPmDUpzzg6oGhvi80byjLNwL1wwH7sMecbFLwqR+SW+aiS6pwmFzy91XVZKH0aaeqISKVQ7zHhBTor0xqlGeCYRhyFZwYBt9w0lleiYYuON5crvimxRpIRXsbe8Jd03FsUprjWOcxxpxSlXbKKjsLNRtfc9bXbLqaoXXINu88N0J1xpQdU59E3MZj2gH61DdnH1J6i7jvO/OgU+4qp7mClRAxPmL7SOwF09vUz6JNbpJYGUJn6SvH1lW0IaWLdbGizymPQ86mBKjGcwo0ElgwMIhlISkXeGX77Hv3LN0wVev12XJXFYaQMwqCIfY2FXj0BndqqE6E5r6yeBCp3+x36WKbLdp1SdgLzyZzN01OKNDRTacxDXFxPVJX2LaiOh170JhkwZs03WsIjjOzIEah9Q4fzRQgbauK99kTOt1SSZx2qT1psMcmNpyGBQ/9kgf9gofdgrOupm0rdOuw2+yK2SRhr85SFao7HbCbAelSTEwvCvsNxY6KuN82Rt+7SUEn6T12a1CB2qUBHyD4kHq/eOfosv97XS94o1lz5LqUKmba5HPUyEIiVqASSSEnEyEORDGcmY5j0xJtyoUfS7WdiVhT44zL3fQsBDvlAWuXhN1uzVTg0TxQmjc81Ztb5HSDblp0s5l6jdyEb7Kwn4gRMC4J+2yurgwB24Yp/Tc6gyiYQTDe0FGx7Q197/AhGR4nywVv1GtO6jd4r3vIQgasRIIaNtrwmr/D6/6Yr/dHvN6veaNd87BtaLc1Yeswm+RnnwR+q1O8yG4HpBuQwe8yvjTeeL1GEffbRkzbVhWD9ANyJjgfMX2FRDA+ZRT0fR6/h8EHRxeFN2rPqjrGSeTIthybFiORtfTU0lOJsMjTlypSs6+BgbV2tHY7zaPc2mpKFRsLnoZg6bG5es8QBzsJu9sI1Qk0D5T6gad6s0UenqGbDdp2qeeGkdQCtvBscqGKVaydPuzFuSnBQEKE3mMBGSyiFajDdoLfGswgEA2+S2mLb+b1eTY0bNfVNNTj2LZUuVvkSVjypl/z+nDE/WHJw37BSVcnP3tvoTNTIZXt51leit16TBZ3Bp/Sli/bgd6AwVLE/TYw29ZNQSfvkw/ee9h22E2FDAHTN0nog8393wWJhsFAu6x5s1mysANv1mvu2zMWpqc2yQtfiWEpNVYMgwaIHRsJU6rY2nRsTM3SDgQMXg2tr3YVsMEy9A7f2dRr43TXSGnxZqR5EKgf9JjTLdp2KTtGdZcqVlIfC/NAax6iDexSgdsOiRFpLdJUyBAxfcRtLGFpkGAhGozPfnjneGAXRIXKBpZ2wKCpeZikdfzAr3htOOb17ojX2zX32yVn24ahdWhrdwHUDuwWqo1SnUWqkwH7sEPOcrwoxuxvjzfub4ci7vvPI4JOEtL2L45tC+oKo4rLTcbUpuEdmidjxMowrCoe1gsaG/jz5pgj22ElYp1y13QMGvFpQCtxNkneolMecJXbBo+zVyGli/XB0nUVvnPQ2qlDntvkDnkbxW4j0uWhBTDNipXRHVMoPIqcRaUSUg+i3AfeiKTB2oMFatzCEGpNAdcu9X3xnWPrak6bhjeqFQZlk9MeB7XcH5Z8vVvz9eyKOd0s6E/rZLHPhb3N7phNpDrz2NMOk12LUwpvCOdbaNzgui7ifpuI81TDWdBJI+IqTFSMD+mPmifH2z5vVUVQqdj0hq/2jqjCiW94bXHMBxevExA6fZCzCQSL0GrySRqJ1LmS1ZKmzY+9atrgaL1j21c7S2djcGczYT9V3FnAtrkA67L5sUXgCyNv13eJ3BFSNbXkEAEfML7CWoNrTB5iA2EpxFODp2IbDH8uyhANb1Yrlm7ASKSPjtO+4cF2wbar6NsK3VrMxiYf/sAk7qbf9Wi3G49sOvRsi7ZtNlBkr3ojFXE/JMKus57dVFTO5FJtM7loBm8ZYsP/J8dsh4rTowYfLRalrWq+yd3nnrYsJNnuEdmliuUCp3HC/OnQcNo3bLqa7bZGx8DTLJe9OlOahyH1td6MqWJhZ+kUCm/H3KCBXfVyTHMFJITUbCx3k6zHEZTGEsadqxp8FLZmgfeWk2agsgEjKdOr6x1DNytQ2prUhdKnIG0amUfyt3eKbQOyHZBtR2xbGAbUjv2u94ci7reJi0EnEahc+motVC5ZwDGmLJqNST1mTKrmkwCmFyRYBl3y9c7R+dQpMiK8Ua+5X694f/Umx2ZLJSEVc2B5GBfcD2sehCVvDqucUbDiYduw3TSE0wp7mraw1amkXhunY78Njz3rkW0/lWZPfUSg+NoL78wsHXgSeY2oz/dEXSHOYiqLqwxhYbCt4lxuj2HAW0fvJeWs2zxjYEh9YtJ4PJP6HXVpSAiaRH3KjtmkFgPutMecbdGzTSq8i7rr+LFHu88i7reRMehkbfI/yi7TRGNMkXsZs2gcJtQYr1RbQ79JwSYzWIaN4WQwU9HHG6sVm1XNJjYpXcwMGCKDOu6HFa8Nd3itT4Gnr7drHmwXtNsanws8bCfnWgu4reI2OVUsC/s+pYoVbhGXGQBR0TyQVYwgPmB6jxkqbB9xbXLPMAXrDbGTNE3MkLqSekndJvtk+IwW+tjOwwya13JMvvaTAXPWQdvtjBTYK1EfKeJ+28m+alVNebYxV7LGCIPH9C7NYu0jbmGxnUPUprmUXuisY+MW/Dmpb3VtPJZUlXpkW2rxtLHiDX90Ttjvb5dst3UOoI7l2Mk3OQWdTjzupMectEjbgQ8p7TFckk2whzdHYQ94m3WhYdZF1Bro+jTYpnXYraWuUt67eBAvmCFVtsYqdzDVLO5DEvNJ2HMjPgngOsW1ittEXA6iStujPg+8nmd67dkaLuJ+W7mkk+SUReNl549UBWOwMWJ8hRqhaoRQGWIl+FYYtpatq3lgIq/XR1QS6aLjyHVTZsEb/Zo/b4+43y15sF1werbAn1Wp+nTskJd7bdSnSnUSqB722Psb5OQs3Qx5CAkhFKu98O4Z131uT5CeR8hj+qRLwXs1kFLZTZ70JWl4fMhuSs1tsn2eKzAkS930YAfF9qkhmNumoimzGdJkpX6YYkYya/C3b7yjuIvIB4B/AHwTaTjay6r6v4jIPeAfAR8EvgL8DVV9M//MTwA/Qho78V+r6i9eydU/68x98Jmpm4AJqSMdaaCH9BYdAi63CVaTMwoWkoJPUTiJwquibIaao7pjkQd1bHzNw27BSdvQdhXDtkI3FtOmTBy3ldTCt2Pawrqz7Gc/26LbbbLUL8smuMGboqztW8xsDYk1u5a6qkiMmCFiuzwkpsrD4y05NTjPGc7DYsYB3MZrSnfsUpGS2+bWAm0qVKLrkW2Hdt35Bnd7GjN6HMvdA/+tqv6WiBwDvykivwT8l8DnVfWnROSTwCeBHxeRbwc+BnwH8M3APxeRv6CqJT3iqskLXr2fJjgRNXXXE0H6hpQNDIgQrSXWqfHSEA0+VjzQFZu2pq49lQ1p6tLg6HtHaB10eUhBJ0iea2k6MF3OJOhy2uPZkIT97CwVLBmza2O8P5S1fRsZd63jSD5rc8GT5ArugHQDVhWJDrUCYpPVEyHkFtUomKAzcU8tqG2bYkVuM2BPkqUufRJ3HYbzO4Y9FXZ4DHFX1VeBV/PzExH5feD9wEeB780v+zTwL4Efz8c/o6od8Mci8mXgI8CvPO2LL1zgsoUWAupT+piKQboB01bY1uG6nFFgx4wCg5eKvrP0VYUYTTnFeaSY7XLwqRNMT745ZsUdZzk75nTAnLU5BzgFnsS5vev2WNb2ATBWsU7zDnLH1BhzI69q/uIUJA25keTololJ4O2Q2/duI+60TzvPTZvaCgy52+Pg3+pS3JP1fJFvyOcuIh8E/hLwr4EX882Bqr4qIu/LL3s/8KuzH3slHytcFe8UdFJNWTRGoK+TVTMGnU4kZwak6TamN8R67LSnOfAKtk+Wuh3A9OTJT2kr61pwrVKdBaoHA/Zhm/qzj50eZf+bgZW1fQuZGzNjzyWSUS4+gMtTlyqbUiJtNnBiFvdxLKVqMlK6mLO7/E7Yt+3ULyZlx8RrH3T9bnlscReRI+AfA39TVR++zdioy/7hLdEzEfkE8AmAhawf9zIKj8ujgk7DgLQ9pnZUVdqqGm8wwWA8+F4INcSK5MqZBZzE72aejsdsnzJjbKu4M487zX7JYUgB3nFm6x5mE4yUtX2LyWvq/KAPnwdsR8RaZAjIYFMPGoHo059RImlaWVQkaBL3swGzScI+zhhI91A433p7T9fynMcSdxGpSIv/H6rqP8mHvyYiL2XL5iXgtXz8FeADsx//FuCrF99TVV8GXga4a18oqRNXSa7aGyv7puHDnacyINGlIddigGSthwiM1s3MJ2kGTX02+uxf30SqU4/pPGYzYLK1g/f53DczheZxKWv7lnJxLCWzCtaQWgGouiTuW5u6SEYldhbGH4359VGn+0HaAWm7FCfqutTeI78vsNdr+SLv+PEjyYz5aeD3VfXvzv7pc8DH8/OPA78wO/4xEWlE5EPAh4Ffe3qXXHgsZltHETnfEztqyoHvPHbjk8W9jVTbVLBh2/w1DyawuSDJbjUPKojUDyPNm57mzQ735hZ7f4M5OcuDN/o8D3W/pyqVtX0AjDvUNLB3srJ1yK13czsO2faY0zQgxj7scQ863MMW97DFPuywJy1yltyJdH1Od7wwcGOP1/JlPI7l/j3AfwH8roj8dj723wM/BXxWRH4E+FPghwBU9Ysi8lng90jZCD9asglukLe0Tg1o309+Sa0c0lfAIvveUwVrqNhZ7rOgk8vWutt67GmPOe3OFygN/V41T3oHyto+FC5a1DnIqiEiXZ87R3rUGswo2uMHguqu/mLsezTOQL0lQn4Zj5Mt86+43NcI8H2P+JlPAZ96gusqXAV5ujww9YKXkLIJbO1Ql3PgBcaOvueCTh5sm3PYt0MS9nGw9TyTYPRJ7vmNUdb2ATAv5puttyluEgPqSa5IkZQBOfnOR5GPu5mnGm+TcfK2lArVQ2aWDwx5UYfc5Cgqak2yaIxBOodxBmfScA8d/fSqOegEpo/YdswkyMLe5lSxecDpFvklCwfAowZth+RPT5usXeT7EIT7cSjifuhcVsU6Cr4nBZTmQacQsa2d8oAZxT3ENPWmS9k2dD3atskVMx/iXSjsC2MHyW+EPd9tfiMUcX8WmAedRkLIVazJVy7GZAEPuwWuM99kiCl3ePCpf3V2w0zMrfUDukEKt5hnfB0WcX+WuCzoBLsga4ypzHrWp2P6GmOy+ENIwSeNb32fQqGwNxRxfxZ4O/GNCtEnC77rUjUpnBfvWUrYs+KvLBwIF0c6vh0HZqQUcX/G0bG73Tea0HdgN0LhGeWA13ER92edS/rCX/qaQuE28gyv3SLuhUffAKPox+JfLxRuG0Xcn3W+EZ/k+Poi8IXC3lPEvfB4FEEvFG4VRdyfdYpoFwoHiexDapuI/DlwBrx+09cCvEC5jjmHch3/vqq+92ldzOMiIifAl677vJdwKH/Hp8WhXMcj1/VeiDuAiPyGqn53uY5yHft8Hd8o+3Ld5Tqevesoe/JCoVA4QIq4FwqFwgGyT+L+8k1fQKZcx3nKdTwZ+3Ld5TrOc/DXsTc+90KhUCg8PfbJci8UCoXCU+LGxV1EfkBEviQiXxaRT17zub8iIr8rIr8tIr+Rj90TkV8SkT/MX5+/gvP+jIi8JiJfmB175HlF5Cfy7+dLIvL9V3wdPyki/y7/Tn5bRH7wGq7jAyLy/4jI74vIF0Xkv8nHr/138jQpa3s69kyu7Rtf16p6Yw/AAv8W+FagBv4N8O3XeP6vAC9cOPY/Ap/Mzz8J/A9XcN6/DHwX8IV3Oi/w7fn30gAfyr8ve4XX8ZPAf3fJa6/yOl4Cvis/Pwb+IJ/v2n8nT/FvXNb2O5z30Nf2Ta/rm7bcPwJ8WVX/SFV74DPAR2/4mj4KfDo//zTw1572CVT1l4E3HvO8HwU+o6qdqv4x8GXS7+2qruNRXOV1vKqqv5WfnwC/D7yfG/idPEXK2n7n8x702r7pdX3T4v5+4M9m37+Sj10XCvwzEflNEflEPvaiqr4K6Y8DvO+aruVR572J39GPicjv5K3tuGW8lusQkQ8Cfwn41+zX7+Qb5aavsazty7mRtX0T6/qmxV0uOXad6Tvfo6rfBfxV4EdF5C9f47kfl+v+Hf094NuA7wReBf7OdV2HiBwB/xj4m6r68O1eetXX8hS46Wssa/ut3Mjavql1fdPi/grwgdn33wJ89bpOrqpfzV9fA36etAX6moi8BJC/vnZNl/Oo817r70hVv6aqQVUj8PfZbQuv9DpEpCLdAP9QVf9JPrwXv5N3SVnbO/bi73gTa/sm1/VNi/uvAx8WkQ+JSA18DPjcdZxYRNYicjw+B/4K8IV8/o/nl30c+IXruJ63Oe/ngI+JSCMiHwI+DPzaVV3EuOgyf530O7nS6xARAX4a+H1V/buzf9qL38m7pKztHXvxd7zutX3j6/ppRKafMKL8g6Qo8r8F/tY1nvdbSZHpfwN8cTw38B7g88Af5q/3ruDcP0faFg6kT+sfebvzAn8r/36+BPzVK76O/wP4XeB38mJ76Rqu4z8hbT9/B/jt/PjBm/idlLVd1vbTuo6bXtelQrVQKBQOkJt2yxQKhULhCijiXigUCgdIEfdCoVA4QIq4FwqFwgFSxL1QKBQOkCLuhUKhcIAUcS8UCoUDpIh7oVAoHCD/P8LBt9F1Y1lkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    # print(dataset_train[ii][0][0][0].shape)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_dl(data, transforms, expand_dim=False):\n",
    "    setup_ds = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(data, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(data.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    transform=transforms,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "\n",
    "                                    temp_uncertainty=16,\n",
    "                                    expand_dim=expand_dim\n",
    "                                    )\n",
    "    setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            drop_last=False,\n",
    "                                            # pin_memory=True,\n",
    "                                            # num_workers=16,\n",
    "                                            # persistent_workers=True,\n",
    "                                            )\n",
    "\n",
    "    return setup_ds, setup_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1702, 32, 32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_labeled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "no_transforms = torch.nn.Sequential(\n",
    ")\n",
    "scale_rsz_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR)\n",
    ")\n",
    "tile_transforms = torch.nn.Sequential(\n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "\n",
    "\n",
    "scripted_all_transforms = torch.jit.script(all_transforms)\n",
    "scripted_no_transforms = torch.jit.script(no_transforms)\n",
    "scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "scripted_tile_transform = torch.jit.script(tile_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_scale_rsz_transform\n",
    "run_transform = scripted_tile_transform\n",
    "\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_train, setup_transform, expand_dim=True)\n",
    "input_xtr = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=True)\n",
    "input_xtr_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_val, setup_transform, expand_dim=True)\n",
    "input_xval = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_val_SYT, setup_transform, expand_dim=True)\n",
    "input_xval_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "# print(\"Starting!\")\n",
    "# tik = time.time()\n",
    "# ds_run = util.dataset_simCLR(\n",
    "#                             torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "#                             torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                             n_transforms=1,\n",
    "#                             class_weights=np.array([1]),\n",
    "#                             transform=run_transform,\n",
    "#                             DEVICE='cpu',\n",
    "#                             dtype_X=torch.float32,\n",
    "#                             dtype_y=torch.int64,\n",
    "\n",
    "#                             temp_uncertainty=16,\n",
    "#                             expand_dim=False\n",
    "#                             )\n",
    "# dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "#                                         batch_size=1024,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         # pin_memory=True,\n",
    "#                                         # num_workers=16,\n",
    "#                                         # persistent_workers=True,\n",
    "#                                         )\n",
    "# features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "# tok = time.time()\n",
    "\n",
    "# print(f'Tile Transformation Alone Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1702, 1, 224, 224]),\n",
       " torch.Size([54704, 1, 224, 224]),\n",
       " torch.Size([426, 1, 224, 224]),\n",
       " torch.Size([13677, 1, 224, 224]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_xtr.shape, input_xtr_SYT.shape, input_xval.shape, input_xval_SYT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_rsz_transforms = torch.nn.Sequential(\n",
    "#     augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "#     torchvision.transforms.Resize(size=(224,224),\n",
    "#                                   interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "# tile_transforms = torch.nn.Sequential(augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "\n",
    "\n",
    "# scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "# scripted_tile_transform = torch.jit.script(tile_transforms)\n",
    "\n",
    "\n",
    "# setup_transform = scripted_scale_rsz_transform\n",
    "# run_transform = scripted_tile_transform\n",
    "\n",
    "\n",
    "\n",
    "# # setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=False)\n",
    "\n",
    "# setup_ds = util.dataset_simCLR(\n",
    "#                                 torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "#                                 torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "#                                 n_transforms=1,\n",
    "#                                 transform=setup_transform,\n",
    "#                                 class_weights=np.array([1]),\n",
    "#                                 DEVICE='cpu',\n",
    "#                                 dtype_X=torch.float32,\n",
    "#                                 dtype_y=torch.int64,\n",
    "\n",
    "#                                 temp_uncertainty=16,\n",
    "#                                 expand_dim=False\n",
    "#                                 )\n",
    "# setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "#                                         batch_size=1024,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         # pin_memory=True,\n",
    "#                                         # num_workers=16,\n",
    "#                                         # persistent_workers=True,\n",
    "#                                         )\n",
    "\n",
    "\n",
    "# input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "# # setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=False)\n",
    "# # input_x_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train_lst, acc_val_lst = [], []\n",
    "# acc_train_SYT_lst, acc_val_SYT_lst = [], []\n",
    "\n",
    "# print('A')\n",
    "# ds_run, dl_run = get_ds_dl(input_xtr, run_transform, expand_dim=False)\n",
    "# features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "# ds_run, dl_run = get_ds_dl(input_xval, run_transform, expand_dim=False)\n",
    "# features_val = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "# print('B')\n",
    "# ds_run, dl_run = get_ds_dl(input_xtr_SYT, run_transform, expand_dim=False)\n",
    "# features_train_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "# ds_run, dl_run = get_ds_dl(input_xval_SYT, run_transform, expand_dim=False)\n",
    "# features_val_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "\n",
    "# acc_train, acc_val = {}, {}\n",
    "# acc_train_SYT, acc_val_SYT = {}, {}\n",
    "# C_toUse = np.array([1e1,1e0,1e-1,1e-2])\n",
    "# for C in C_toUse:\n",
    "#     print('C1')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     logreg.fit(features_train, y_labeled_train)\n",
    "#     acc_train_tmp = logreg.score(features_train, y_labeled_train)\n",
    "#     acc_train[C] = acc_train_tmp\n",
    "#     acc_val_tmp = logreg.score(features_val, y_labeled_val)\n",
    "#     acc_val[C] = acc_val_tmp\n",
    "    \n",
    "#     print('C2')\n",
    "#     logreg_SYT = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     logreg_SYT.fit(features_train_SYT, y_labeled_train_SYT)\n",
    "#     acc_train_tmp = logreg_SYT.score(features_train_SYT, y_labeled_train_SYT)\n",
    "#     acc_train_SYT[C] = acc_train_tmp\n",
    "#     acc_val_tmp = logreg_SYT.score(features_val_SYT, y_labeled_val_SYT)\n",
    "#     acc_val_SYT[C] = acc_val_tmp\n",
    "\n",
    "# acc_train_lst.append(acc_train)\n",
    "# acc_val_lst.append(acc_val)\n",
    "# acc_train_SYT_lst.append(acc_train_SYT)\n",
    "# acc_val_SYT_lst.append(acc_val_SYT)\n",
    "\n",
    "\n",
    "# losses_train_npy = np.array(losses_train)\n",
    "# losses_val_npy = np.array(losses_val)\n",
    "# val_accs_npy = np.array(val_accs)\n",
    "# acc_npy = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train_df, acc_val_df, acc_train_SYT_df, acc_val_SYT_df = pd.DataFrame(acc_train_lst), pd.DataFrame(acc_val_lst), pd.DataFrame(acc_train_SYT_lst), pd.DataFrame(acc_val_SYT_lst)\n",
    "# display(acc_train_df)\n",
    "# display(acc_val_df)\n",
    "# display(acc_train_SYT_df)\n",
    "# display(acc_val_SYT_df)\n",
    "\n",
    "# acc_train_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train.csv')\n",
    "# acc_val_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val.csv')\n",
    "# acc_train_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train_SYT.csv')\n",
    "# acc_val_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val_SYT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=efficient2-only7unfrozen'\n",
    "model.forward = model.forward_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/889, loss_train: 7.349, loss_val: nan, pos_over_neg: 1.0245325565338135 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 7.1765, loss_val: nan, pos_over_neg: 1.2428301572799683 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 6.9988, loss_val: nan, pos_over_neg: 3.1196725368499756 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 6.9009, loss_val: nan, pos_over_neg: 3.7103147506713867 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 6.7802, loss_val: nan, pos_over_neg: 4.077817916870117 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 6.7269, loss_val: nan, pos_over_neg: 5.723012447357178 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 6.6701, loss_val: nan, pos_over_neg: 9.695016860961914 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 6.6192, loss_val: nan, pos_over_neg: 13.35693073272705 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 6.5942, loss_val: nan, pos_over_neg: 12.405990600585938 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 6.5699, loss_val: nan, pos_over_neg: 13.105032920837402 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 6.5927, loss_val: nan, pos_over_neg: 11.125567436218262 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 6.5821, loss_val: nan, pos_over_neg: 12.746593475341797 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 6.5284, loss_val: nan, pos_over_neg: 14.635932922363281 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 6.5061, loss_val: nan, pos_over_neg: 17.280933380126953 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 6.4861, loss_val: nan, pos_over_neg: 24.638734817504883 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 6.4894, loss_val: nan, pos_over_neg: 35.625423431396484 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 6.4357, loss_val: nan, pos_over_neg: 36.76000213623047 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 6.4505, loss_val: nan, pos_over_neg: 29.897552490234375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 6.4243, loss_val: nan, pos_over_neg: 40.164772033691406 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 6.4171, loss_val: nan, pos_over_neg: 31.975082397460938 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 6.4169, loss_val: nan, pos_over_neg: 38.8293571472168 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 6.3966, loss_val: nan, pos_over_neg: 43.021766662597656 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 6.42, loss_val: nan, pos_over_neg: 49.94255447387695 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 6.4189, loss_val: nan, pos_over_neg: 69.38328552246094 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 6.3791, loss_val: nan, pos_over_neg: 68.40487670898438 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 6.3844, loss_val: nan, pos_over_neg: 69.47258758544922 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 6.3886, loss_val: nan, pos_over_neg: 61.37703323364258 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 6.3624, loss_val: nan, pos_over_neg: 58.32057189941406 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 6.3712, loss_val: nan, pos_over_neg: 60.382022857666016 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 6.3645, loss_val: nan, pos_over_neg: 60.4225959777832 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 6.3575, loss_val: nan, pos_over_neg: 47.780967712402344 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 6.3613, loss_val: nan, pos_over_neg: 56.664268493652344 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 6.3461, loss_val: nan, pos_over_neg: 52.18281555175781 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 6.332, loss_val: nan, pos_over_neg: 59.72954559326172 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 6.3467, loss_val: nan, pos_over_neg: 50.57052230834961 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 6.3539, loss_val: nan, pos_over_neg: 77.18146514892578 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 6.3158, loss_val: nan, pos_over_neg: 98.26416778564453 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 6.3426, loss_val: nan, pos_over_neg: 130.63702392578125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 6.2716, loss_val: nan, pos_over_neg: 572.305908203125 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 6.3388, loss_val: nan, pos_over_neg: 134.6428680419922 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 6.2854, loss_val: nan, pos_over_neg: 126.25874328613281 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 6.3007, loss_val: nan, pos_over_neg: 60.00991439819336 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 6.301, loss_val: nan, pos_over_neg: 108.29029846191406 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 6.2765, loss_val: nan, pos_over_neg: 226.2075653076172 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 6.2715, loss_val: nan, pos_over_neg: 169.67042541503906 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 6.2784, loss_val: nan, pos_over_neg: 300.9647216796875 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 6.262, loss_val: nan, pos_over_neg: 326.22735595703125 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 6.2809, loss_val: nan, pos_over_neg: 455.69189453125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 6.2647, loss_val: nan, pos_over_neg: 136.2460479736328 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 6.2584, loss_val: nan, pos_over_neg: 135.01808166503906 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 6.2663, loss_val: nan, pos_over_neg: 89.57090759277344 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 6.2566, loss_val: nan, pos_over_neg: 87.62063598632812 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 6.2692, loss_val: nan, pos_over_neg: 73.72130584716797 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 6.2493, loss_val: nan, pos_over_neg: 130.52716064453125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 6.231, loss_val: nan, pos_over_neg: 432.233642578125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 6.2642, loss_val: nan, pos_over_neg: 284.3470458984375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 6.2761, loss_val: nan, pos_over_neg: 168.20960998535156 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 6.2332, loss_val: nan, pos_over_neg: 126.30585479736328 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 6.2279, loss_val: nan, pos_over_neg: 145.0230712890625 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 6.2489, loss_val: nan, pos_over_neg: 197.8621826171875 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 6.2318, loss_val: nan, pos_over_neg: 208.46356201171875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 6.239, loss_val: nan, pos_over_neg: 204.97372436523438 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 6.2176, loss_val: nan, pos_over_neg: 263.3623962402344 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 6.2207, loss_val: nan, pos_over_neg: 150.89712524414062 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 6.2188, loss_val: nan, pos_over_neg: 172.01687622070312 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 6.2326, loss_val: nan, pos_over_neg: 220.76437377929688 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 6.2134, loss_val: nan, pos_over_neg: 548.8804931640625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 6.195, loss_val: nan, pos_over_neg: 245.45242309570312 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 6.2012, loss_val: nan, pos_over_neg: 97.57734680175781 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 6.2155, loss_val: nan, pos_over_neg: 174.06394958496094 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 6.2034, loss_val: nan, pos_over_neg: 276.4989929199219 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 6.2136, loss_val: nan, pos_over_neg: 376.9305419921875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 6.2277, loss_val: nan, pos_over_neg: 247.68959045410156 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 6.1982, loss_val: nan, pos_over_neg: 306.3572082519531 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 6.1963, loss_val: nan, pos_over_neg: 206.88906860351562 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 6.1918, loss_val: nan, pos_over_neg: 213.86781311035156 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 6.1749, loss_val: nan, pos_over_neg: 174.4286651611328 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 6.1885, loss_val: nan, pos_over_neg: 121.41209411621094 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 6.1978, loss_val: nan, pos_over_neg: 119.81388854980469 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 6.1968, loss_val: nan, pos_over_neg: 235.82254028320312 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 6.1872, loss_val: nan, pos_over_neg: 469.0918884277344 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 6.1922, loss_val: nan, pos_over_neg: 300.2934875488281 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 6.1986, loss_val: nan, pos_over_neg: 159.82041931152344 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 6.1731, loss_val: nan, pos_over_neg: 166.102783203125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 6.1749, loss_val: nan, pos_over_neg: 179.78102111816406 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 6.1772, loss_val: nan, pos_over_neg: 161.08253479003906 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 6.1752, loss_val: nan, pos_over_neg: 420.5327453613281 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 6.1623, loss_val: nan, pos_over_neg: -7677.44384765625 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 6.1704, loss_val: nan, pos_over_neg: 612.3184814453125 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 6.1709, loss_val: nan, pos_over_neg: 214.66351318359375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 6.1523, loss_val: nan, pos_over_neg: 233.5143585205078 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 6.1648, loss_val: nan, pos_over_neg: 188.58128356933594 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 6.1787, loss_val: nan, pos_over_neg: 303.2115783691406 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 6.1423, loss_val: nan, pos_over_neg: 270.2520751953125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 6.1651, loss_val: nan, pos_over_neg: 355.49554443359375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 6.1658, loss_val: nan, pos_over_neg: 332.09222412109375 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 6.1508, loss_val: nan, pos_over_neg: 126.70993041992188 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 6.1531, loss_val: nan, pos_over_neg: 138.2277069091797 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 6.1428, loss_val: nan, pos_over_neg: 307.4726257324219 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 6.1719, loss_val: nan, pos_over_neg: 447.552734375 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 6.1825, loss_val: nan, pos_over_neg: -24877.818359375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 6.1671, loss_val: nan, pos_over_neg: 589.2442626953125 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 6.1436, loss_val: nan, pos_over_neg: 138.76390075683594 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 6.1445, loss_val: nan, pos_over_neg: 113.7369613647461 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 6.1394, loss_val: nan, pos_over_neg: 136.47360229492188 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 6.1615, loss_val: nan, pos_over_neg: 154.4779510498047 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 6.1373, loss_val: nan, pos_over_neg: 596.7782592773438 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 6.151, loss_val: nan, pos_over_neg: 2300.799072265625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 6.1587, loss_val: nan, pos_over_neg: 381.8980407714844 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 6.1608, loss_val: nan, pos_over_neg: 292.99908447265625 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 6.1378, loss_val: nan, pos_over_neg: 401.57568359375 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 6.1414, loss_val: nan, pos_over_neg: 371.1463623046875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 6.1598, loss_val: nan, pos_over_neg: 252.57374572753906 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 6.1391, loss_val: nan, pos_over_neg: 330.0692443847656 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 6.1662, loss_val: nan, pos_over_neg: 169.17465209960938 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 6.1482, loss_val: nan, pos_over_neg: 105.92678833007812 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 6.1207, loss_val: nan, pos_over_neg: 131.674072265625 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 6.1526, loss_val: nan, pos_over_neg: 186.0092010498047 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 6.1384, loss_val: nan, pos_over_neg: 377.05792236328125 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 6.1486, loss_val: nan, pos_over_neg: 811.4246826171875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 6.1249, loss_val: nan, pos_over_neg: 685.2899780273438 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 6.1409, loss_val: nan, pos_over_neg: 136.18072509765625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 6.1477, loss_val: nan, pos_over_neg: 135.57937622070312 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 6.1245, loss_val: nan, pos_over_neg: 258.084228515625 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 6.1355, loss_val: nan, pos_over_neg: 563.0515747070312 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 6.1332, loss_val: nan, pos_over_neg: 508.1483154296875 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 6.1177, loss_val: nan, pos_over_neg: 997.213134765625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 6.1093, loss_val: nan, pos_over_neg: 577.0850830078125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 6.1365, loss_val: nan, pos_over_neg: 116.50235748291016 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 6.0957, loss_val: nan, pos_over_neg: 176.00088500976562 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 6.1115, loss_val: nan, pos_over_neg: 124.72626495361328 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 6.1123, loss_val: nan, pos_over_neg: 198.61288452148438 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 6.1336, loss_val: nan, pos_over_neg: 169.73446655273438 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 6.1458, loss_val: nan, pos_over_neg: 961.0975341796875 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 6.1302, loss_val: nan, pos_over_neg: 442.2905578613281 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 6.1224, loss_val: nan, pos_over_neg: 111.29237365722656 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 6.1224, loss_val: nan, pos_over_neg: 112.9844970703125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 6.1318, loss_val: nan, pos_over_neg: 228.81993103027344 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 6.1124, loss_val: nan, pos_over_neg: 233.8466796875 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 6.1161, loss_val: nan, pos_over_neg: 176.25479125976562 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 6.1206, loss_val: nan, pos_over_neg: 138.77049255371094 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 6.093, loss_val: nan, pos_over_neg: 170.2907257080078 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 6.1344, loss_val: nan, pos_over_neg: 127.20215606689453 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 6.1163, loss_val: nan, pos_over_neg: 240.2181854248047 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 6.1155, loss_val: nan, pos_over_neg: 140.7607421875 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 6.1237, loss_val: nan, pos_over_neg: 541.1458740234375 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 6.0979, loss_val: nan, pos_over_neg: 360.40264892578125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 6.1178, loss_val: nan, pos_over_neg: 784.7081298828125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 6.1056, loss_val: nan, pos_over_neg: 259.0851135253906 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 6.0839, loss_val: nan, pos_over_neg: 188.12155151367188 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 6.1094, loss_val: nan, pos_over_neg: 117.59463500976562 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 6.0784, loss_val: nan, pos_over_neg: 167.9437713623047 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 6.1269, loss_val: nan, pos_over_neg: 212.37008666992188 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 6.0913, loss_val: nan, pos_over_neg: 306.4881896972656 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 6.1034, loss_val: nan, pos_over_neg: 3306.95361328125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 6.1167, loss_val: nan, pos_over_neg: 1292.0618896484375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 6.1092, loss_val: nan, pos_over_neg: 547.3026733398438 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 6.0777, loss_val: nan, pos_over_neg: 234.54412841796875 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 6.1067, loss_val: nan, pos_over_neg: 341.1305847167969 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 6.1058, loss_val: nan, pos_over_neg: 326.1900329589844 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 6.1171, loss_val: nan, pos_over_neg: 478.8206481933594 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 6.0999, loss_val: nan, pos_over_neg: 931.221435546875 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 6.0887, loss_val: nan, pos_over_neg: 560.9856567382812 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 6.099, loss_val: nan, pos_over_neg: 2972.880859375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 6.0833, loss_val: nan, pos_over_neg: 1021.8251342773438 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 6.0966, loss_val: nan, pos_over_neg: 181.4689483642578 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 6.0894, loss_val: nan, pos_over_neg: 184.38783264160156 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 6.087, loss_val: nan, pos_over_neg: 191.08847045898438 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 6.1122, loss_val: nan, pos_over_neg: 251.79798889160156 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 6.0964, loss_val: nan, pos_over_neg: 287.64434814453125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 6.0989, loss_val: nan, pos_over_neg: 278.6950988769531 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 6.0924, loss_val: nan, pos_over_neg: 323.1756591796875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 6.0872, loss_val: nan, pos_over_neg: 425.6821594238281 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 6.1037, loss_val: nan, pos_over_neg: 112.01836395263672 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 6.1048, loss_val: nan, pos_over_neg: 179.1822967529297 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 6.1053, loss_val: nan, pos_over_neg: 158.2784423828125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 6.0868, loss_val: nan, pos_over_neg: 194.80972290039062 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 6.0734, loss_val: nan, pos_over_neg: 774.9964599609375 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 6.1018, loss_val: nan, pos_over_neg: 685.9425659179688 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 6.0799, loss_val: nan, pos_over_neg: 379.6163635253906 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 6.0596, loss_val: nan, pos_over_neg: 477.7045593261719 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 6.1063, loss_val: nan, pos_over_neg: 603.1556396484375 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 6.0711, loss_val: nan, pos_over_neg: 242.29188537597656 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 6.0956, loss_val: nan, pos_over_neg: 407.2132873535156 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 6.0911, loss_val: nan, pos_over_neg: 309.9689025878906 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 6.0869, loss_val: nan, pos_over_neg: 526.7890625 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 6.0658, loss_val: nan, pos_over_neg: 235.78839111328125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 6.0771, loss_val: nan, pos_over_neg: 179.64590454101562 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 6.0818, loss_val: nan, pos_over_neg: 222.45541381835938 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 6.0803, loss_val: nan, pos_over_neg: 339.2373962402344 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 6.0691, loss_val: nan, pos_over_neg: 765.577392578125 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 6.1007, loss_val: nan, pos_over_neg: 504.1806335449219 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 6.0949, loss_val: nan, pos_over_neg: 383.8205871582031 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 6.08, loss_val: nan, pos_over_neg: 177.63140869140625 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 6.089, loss_val: nan, pos_over_neg: 277.6163635253906 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 6.1073, loss_val: nan, pos_over_neg: 569.2347412109375 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 6.0773, loss_val: nan, pos_over_neg: 415.7864074707031 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 6.06, loss_val: nan, pos_over_neg: 485.172119140625 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 6.1141, loss_val: nan, pos_over_neg: 391.3511047363281 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 6.0639, loss_val: nan, pos_over_neg: 635.400146484375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 6.0695, loss_val: nan, pos_over_neg: 484.0053405761719 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 6.0605, loss_val: nan, pos_over_neg: 343.4300537109375 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 6.0831, loss_val: nan, pos_over_neg: 175.85365295410156 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 6.0685, loss_val: nan, pos_over_neg: 437.9359130859375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 6.068, loss_val: nan, pos_over_neg: 551.6668090820312 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 6.078, loss_val: nan, pos_over_neg: 290.13336181640625 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 6.0858, loss_val: nan, pos_over_neg: 219.14889526367188 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 6.07, loss_val: nan, pos_over_neg: 354.9859619140625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 6.0863, loss_val: nan, pos_over_neg: 434.8337707519531 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 6.0774, loss_val: nan, pos_over_neg: 928.058349609375 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 6.0528, loss_val: nan, pos_over_neg: 440.9391784667969 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 6.0455, loss_val: nan, pos_over_neg: 481.46856689453125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 6.0805, loss_val: nan, pos_over_neg: 228.8953094482422 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 6.0829, loss_val: nan, pos_over_neg: 142.84400939941406 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 6.0563, loss_val: nan, pos_over_neg: 275.2860107421875 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 6.0842, loss_val: nan, pos_over_neg: 1596.584228515625 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 6.0653, loss_val: nan, pos_over_neg: 919.218017578125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 6.0694, loss_val: nan, pos_over_neg: 193.86285400390625 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 6.0773, loss_val: nan, pos_over_neg: 196.5309600830078 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 6.0561, loss_val: nan, pos_over_neg: 257.108154296875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 6.0743, loss_val: nan, pos_over_neg: 122.29447174072266 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 6.0569, loss_val: nan, pos_over_neg: 264.14404296875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 6.0709, loss_val: nan, pos_over_neg: 477.197265625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 6.0836, loss_val: nan, pos_over_neg: 265.72210693359375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 6.0769, loss_val: nan, pos_over_neg: 196.02281188964844 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 6.0784, loss_val: nan, pos_over_neg: 189.0275421142578 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 6.0748, loss_val: nan, pos_over_neg: 185.75411987304688 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 6.0757, loss_val: nan, pos_over_neg: 191.92628479003906 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 6.0383, loss_val: nan, pos_over_neg: 165.88064575195312 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 6.0737, loss_val: nan, pos_over_neg: 130.7822723388672 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 6.0447, loss_val: nan, pos_over_neg: 562.9055786132812 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 6.0731, loss_val: nan, pos_over_neg: 366.1249694824219 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 6.0734, loss_val: nan, pos_over_neg: 352.9368896484375 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 6.0704, loss_val: nan, pos_over_neg: 267.0198974609375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 6.0673, loss_val: nan, pos_over_neg: 231.6077117919922 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 6.069, loss_val: nan, pos_over_neg: 109.19654846191406 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 6.0591, loss_val: nan, pos_over_neg: 82.6478042602539 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 6.0481, loss_val: nan, pos_over_neg: 289.3094787597656 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 6.0495, loss_val: nan, pos_over_neg: 783.5059204101562 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 6.0331, loss_val: nan, pos_over_neg: 1499.990234375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 6.0445, loss_val: nan, pos_over_neg: 890.4927368164062 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 6.0674, loss_val: nan, pos_over_neg: 308.8248596191406 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 6.0691, loss_val: nan, pos_over_neg: 183.21322631835938 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 6.0502, loss_val: nan, pos_over_neg: 148.223876953125 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 6.0517, loss_val: nan, pos_over_neg: 132.1959686279297 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 6.034, loss_val: nan, pos_over_neg: 265.7971496582031 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 6.0604, loss_val: nan, pos_over_neg: 402.277099609375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 6.0478, loss_val: nan, pos_over_neg: 1323.299560546875 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 6.0471, loss_val: nan, pos_over_neg: 403.595703125 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 6.0511, loss_val: nan, pos_over_neg: 417.18902587890625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 6.0374, loss_val: nan, pos_over_neg: 149.13075256347656 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 6.0624, loss_val: nan, pos_over_neg: 154.74053955078125 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 6.0735, loss_val: nan, pos_over_neg: 159.3502655029297 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 6.058, loss_val: nan, pos_over_neg: 1003.171630859375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 6.0532, loss_val: nan, pos_over_neg: 3285.30126953125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 6.0612, loss_val: nan, pos_over_neg: 3656.6171875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 6.0401, loss_val: nan, pos_over_neg: 992.57470703125 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 6.0486, loss_val: nan, pos_over_neg: 144.74978637695312 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 6.0663, loss_val: nan, pos_over_neg: 148.45652770996094 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 6.0391, loss_val: nan, pos_over_neg: 220.4055938720703 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 6.0531, loss_val: nan, pos_over_neg: 278.8292541503906 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 6.065, loss_val: nan, pos_over_neg: 1070.564208984375 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 6.0587, loss_val: nan, pos_over_neg: 325.5976867675781 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 6.054, loss_val: nan, pos_over_neg: 1095.5150146484375 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 6.0402, loss_val: nan, pos_over_neg: 251.8214569091797 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 6.0398, loss_val: nan, pos_over_neg: 239.9314422607422 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 6.0434, loss_val: nan, pos_over_neg: 627.7245483398438 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 6.0573, loss_val: nan, pos_over_neg: 340.116943359375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 6.0205, loss_val: nan, pos_over_neg: 417.4338684082031 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 6.0402, loss_val: nan, pos_over_neg: 404.34503173828125 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 6.0459, loss_val: nan, pos_over_neg: 276.26336669921875 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 6.045, loss_val: nan, pos_over_neg: 294.5837707519531 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 6.0522, loss_val: nan, pos_over_neg: 454.353271484375 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 6.0437, loss_val: nan, pos_over_neg: 517.5831298828125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 6.0454, loss_val: nan, pos_over_neg: 668.3501586914062 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 6.0286, loss_val: nan, pos_over_neg: 10620.67578125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 6.0426, loss_val: nan, pos_over_neg: 406.9490661621094 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 6.0364, loss_val: nan, pos_over_neg: 286.02972412109375 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 6.0378, loss_val: nan, pos_over_neg: 212.52296447753906 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 6.047, loss_val: nan, pos_over_neg: 218.9250946044922 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 6.0371, loss_val: nan, pos_over_neg: 3385.96923828125 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 6.0197, loss_val: nan, pos_over_neg: 2239.483642578125 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 6.0517, loss_val: nan, pos_over_neg: 2624.38525390625 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 6.0369, loss_val: nan, pos_over_neg: 572.6892700195312 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 6.0159, loss_val: nan, pos_over_neg: 343.184814453125 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 6.0411, loss_val: nan, pos_over_neg: 137.8662567138672 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 6.0511, loss_val: nan, pos_over_neg: 208.94766235351562 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 6.0384, loss_val: nan, pos_over_neg: 362.5743713378906 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 6.0473, loss_val: nan, pos_over_neg: 460.682861328125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 6.0332, loss_val: nan, pos_over_neg: -5406.7041015625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 6.0382, loss_val: nan, pos_over_neg: 1661.519775390625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 6.0477, loss_val: nan, pos_over_neg: 808.9295043945312 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 6.0359, loss_val: nan, pos_over_neg: 251.299072265625 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 6.0427, loss_val: nan, pos_over_neg: 314.2055358886719 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 6.0469, loss_val: nan, pos_over_neg: 238.5451202392578 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 6.0356, loss_val: nan, pos_over_neg: 298.510009765625 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 6.0318, loss_val: nan, pos_over_neg: 332.7234191894531 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 6.0412, loss_val: nan, pos_over_neg: 2256.974853515625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 6.0306, loss_val: nan, pos_over_neg: 421.6792907714844 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 6.0462, loss_val: nan, pos_over_neg: 562.04736328125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 6.0493, loss_val: nan, pos_over_neg: 200.7292022705078 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 6.0159, loss_val: nan, pos_over_neg: 183.5385284423828 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 6.0512, loss_val: nan, pos_over_neg: 139.75416564941406 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 6.0267, loss_val: nan, pos_over_neg: 249.395263671875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 6.0164, loss_val: nan, pos_over_neg: 750.8545532226562 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 6.0421, loss_val: nan, pos_over_neg: 348.6832275390625 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 6.0253, loss_val: nan, pos_over_neg: 1713.6400146484375 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 6.0403, loss_val: nan, pos_over_neg: 425.55230712890625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 6.0484, loss_val: nan, pos_over_neg: 239.84854125976562 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 6.0451, loss_val: nan, pos_over_neg: 202.922119140625 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 6.0185, loss_val: nan, pos_over_neg: 1088.005859375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 6.0367, loss_val: nan, pos_over_neg: 717.2081909179688 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 6.02, loss_val: nan, pos_over_neg: 2610.604248046875 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.9974, loss_val: nan, pos_over_neg: 423.4955139160156 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 6.0349, loss_val: nan, pos_over_neg: 406.9923095703125 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 6.0422, loss_val: nan, pos_over_neg: 330.0997314453125 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 6.026, loss_val: nan, pos_over_neg: 255.5793914794922 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 6.0155, loss_val: nan, pos_over_neg: 266.0823669433594 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 6.013, loss_val: nan, pos_over_neg: 536.6521606445312 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 6.0251, loss_val: nan, pos_over_neg: -17510.72265625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 6.0414, loss_val: nan, pos_over_neg: 1538.0009765625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 6.0099, loss_val: nan, pos_over_neg: 2605.526611328125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 6.0343, loss_val: nan, pos_over_neg: 948.7951049804688 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 6.0277, loss_val: nan, pos_over_neg: 304.4674072265625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 6.0295, loss_val: nan, pos_over_neg: 282.7124938964844 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 6.0125, loss_val: nan, pos_over_neg: 225.94476318359375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 6.0041, loss_val: nan, pos_over_neg: 3702.162841796875 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 6.0303, loss_val: nan, pos_over_neg: 325.5401306152344 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 6.0305, loss_val: nan, pos_over_neg: 310.80108642578125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 6.0444, loss_val: nan, pos_over_neg: 476.4184265136719 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 6.0144, loss_val: nan, pos_over_neg: 307.77667236328125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 6.004, loss_val: nan, pos_over_neg: 263.8039855957031 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 6.0403, loss_val: nan, pos_over_neg: 262.9934387207031 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 6.0029, loss_val: nan, pos_over_neg: 463.28497314453125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 6.0302, loss_val: nan, pos_over_neg: 401.8743591308594 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 6.0353, loss_val: nan, pos_over_neg: 234.64244079589844 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 6.028, loss_val: nan, pos_over_neg: 264.1387023925781 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 6.0232, loss_val: nan, pos_over_neg: 660.7567749023438 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 6.0325, loss_val: nan, pos_over_neg: 364.371337890625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 6.0016, loss_val: nan, pos_over_neg: 824.596435546875 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 6.0407, loss_val: nan, pos_over_neg: 175.90689086914062 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 6.034, loss_val: nan, pos_over_neg: 734.5546264648438 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 6.0315, loss_val: nan, pos_over_neg: 629.5333251953125 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.9954, loss_val: nan, pos_over_neg: 850.3240966796875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 6.0045, loss_val: nan, pos_over_neg: 1383.4215087890625 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 6.0152, loss_val: nan, pos_over_neg: 543.6287841796875 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 6.0244, loss_val: nan, pos_over_neg: 292.333984375 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 6.0074, loss_val: nan, pos_over_neg: 197.77818298339844 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 6.0271, loss_val: nan, pos_over_neg: 490.3291931152344 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 6.0062, loss_val: nan, pos_over_neg: 2648.751953125 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 6.0025, loss_val: nan, pos_over_neg: 550.0111083984375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 6.0233, loss_val: nan, pos_over_neg: 668.4303588867188 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 6.0178, loss_val: nan, pos_over_neg: 1420.9293212890625 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 6.0189, loss_val: nan, pos_over_neg: 1780.7869873046875 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 6.0141, loss_val: nan, pos_over_neg: 576.0408935546875 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.9984, loss_val: nan, pos_over_neg: 333.4681701660156 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.989, loss_val: nan, pos_over_neg: 1367.8792724609375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 6.0256, loss_val: nan, pos_over_neg: 287.0678405761719 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 6.0024, loss_val: nan, pos_over_neg: 1098.151611328125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 6.0079, loss_val: nan, pos_over_neg: 527.3541870117188 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 6.0261, loss_val: nan, pos_over_neg: 2296.5322265625 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 6.0209, loss_val: nan, pos_over_neg: 569.3574829101562 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 6.021, loss_val: nan, pos_over_neg: 1283.038330078125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 6.0161, loss_val: nan, pos_over_neg: 230.01858520507812 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 6.0049, loss_val: nan, pos_over_neg: 417.79486083984375 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 6.0383, loss_val: nan, pos_over_neg: 602.71630859375 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 6.0006, loss_val: nan, pos_over_neg: 1266.50537109375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 677.104248046875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 6.0062, loss_val: nan, pos_over_neg: 631.6318969726562 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 6.0053, loss_val: nan, pos_over_neg: 194.23731994628906 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 6.0257, loss_val: nan, pos_over_neg: 126.38853454589844 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 6.0124, loss_val: nan, pos_over_neg: 299.2698059082031 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 6.0339, loss_val: nan, pos_over_neg: -2675.741943359375 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 6.0095, loss_val: nan, pos_over_neg: 2942.415771484375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 6.0028, loss_val: nan, pos_over_neg: 743.8992919921875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 6.0237, loss_val: nan, pos_over_neg: 717.0974731445312 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 6.0047, loss_val: nan, pos_over_neg: 254.90487670898438 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 6.0131, loss_val: nan, pos_over_neg: 171.81838989257812 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 6.0287, loss_val: nan, pos_over_neg: 210.20828247070312 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 6.0045, loss_val: nan, pos_over_neg: 326.94256591796875 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 352.2470703125 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.9939, loss_val: nan, pos_over_neg: 1449.8052978515625 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 6.0144, loss_val: nan, pos_over_neg: 353.131103515625 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 6.0276, loss_val: nan, pos_over_neg: 901.6201782226562 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 6.009, loss_val: nan, pos_over_neg: 505.0825500488281 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 6.0269, loss_val: nan, pos_over_neg: 119.44227600097656 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 6.0052, loss_val: nan, pos_over_neg: 154.17788696289062 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 6.0264, loss_val: nan, pos_over_neg: 249.44752502441406 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 6.0186, loss_val: nan, pos_over_neg: 809.7978515625 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 6.0214, loss_val: nan, pos_over_neg: 2051.627685546875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 6.0385, loss_val: nan, pos_over_neg: 555.631591796875 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 6.0129, loss_val: nan, pos_over_neg: 1447.7130126953125 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 6.0075, loss_val: nan, pos_over_neg: 468.21697998046875 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 6.011, loss_val: nan, pos_over_neg: 212.7982940673828 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 6.0308, loss_val: nan, pos_over_neg: 110.22721862792969 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 6.0076, loss_val: nan, pos_over_neg: 162.865478515625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 6.0134, loss_val: nan, pos_over_neg: 348.755615234375 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 6.0316, loss_val: nan, pos_over_neg: 866.540771484375 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 6.003, loss_val: nan, pos_over_neg: 492.404296875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 6.0032, loss_val: nan, pos_over_neg: 541.9801635742188 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 6.0177, loss_val: nan, pos_over_neg: 150.16270446777344 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.9985, loss_val: nan, pos_over_neg: 304.7993469238281 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 6.008, loss_val: nan, pos_over_neg: 202.5272674560547 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 6.0173, loss_val: nan, pos_over_neg: 288.7264404296875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 6.0087, loss_val: nan, pos_over_neg: 783.4584350585938 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 6.026, loss_val: nan, pos_over_neg: 452.4351806640625 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 6.0208, loss_val: nan, pos_over_neg: 788.1705932617188 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.9959, loss_val: nan, pos_over_neg: 535.8552856445312 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.9873, loss_val: nan, pos_over_neg: 348.8756408691406 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.9991, loss_val: nan, pos_over_neg: 457.10784912109375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 6.0097, loss_val: nan, pos_over_neg: 140.25148010253906 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 6.0238, loss_val: nan, pos_over_neg: 300.07666015625 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.9834, loss_val: nan, pos_over_neg: 687.7653198242188 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 6.0084, loss_val: nan, pos_over_neg: 742.4940185546875 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.9698, loss_val: nan, pos_over_neg: 582.76513671875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.995, loss_val: nan, pos_over_neg: 924.9945068359375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 6.0168, loss_val: nan, pos_over_neg: 635.1290283203125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 6.0131, loss_val: nan, pos_over_neg: 259.8522644042969 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 6.0179, loss_val: nan, pos_over_neg: 303.0720520019531 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 6.0119, loss_val: nan, pos_over_neg: 241.08299255371094 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.9918, loss_val: nan, pos_over_neg: 2609.267822265625 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.9979, loss_val: nan, pos_over_neg: 2458.138671875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 1404.26318359375 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.9844, loss_val: nan, pos_over_neg: 484.1462097167969 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.9817, loss_val: nan, pos_over_neg: 639.3779907226562 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.9844, loss_val: nan, pos_over_neg: 399.2692565917969 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.9951, loss_val: nan, pos_over_neg: 366.7940368652344 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.9964, loss_val: nan, pos_over_neg: 325.4020690917969 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.9768, loss_val: nan, pos_over_neg: 4981.40185546875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.9969, loss_val: nan, pos_over_neg: 2033.0452880859375 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.9987, loss_val: nan, pos_over_neg: 6055.62646484375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.9924, loss_val: nan, pos_over_neg: 416.8900146484375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.9996, loss_val: nan, pos_over_neg: 754.4828491210938 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.9954, loss_val: nan, pos_over_neg: 387.2734069824219 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.9909, loss_val: nan, pos_over_neg: 456.023681640625 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.9952, loss_val: nan, pos_over_neg: 223.94259643554688 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 6.0089, loss_val: nan, pos_over_neg: 259.28082275390625 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.9988, loss_val: nan, pos_over_neg: 3554.64306640625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.9857, loss_val: nan, pos_over_neg: 930.5309448242188 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.997, loss_val: nan, pos_over_neg: 1062.7816162109375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.9846, loss_val: nan, pos_over_neg: 1371.844482421875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 6.0202, loss_val: nan, pos_over_neg: 337.51214599609375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 222.5097198486328 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 210.78636169433594 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 6.0119, loss_val: nan, pos_over_neg: 342.0547790527344 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.9928, loss_val: nan, pos_over_neg: 330.0772399902344 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.99, loss_val: nan, pos_over_neg: 1904.2625732421875 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.9894, loss_val: nan, pos_over_neg: 726.1651611328125 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.9863, loss_val: nan, pos_over_neg: 337.3924865722656 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1045.682861328125 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 6.0032, loss_val: nan, pos_over_neg: 581.6473388671875 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.9964, loss_val: nan, pos_over_neg: 216.3961181640625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.9924, loss_val: nan, pos_over_neg: 309.2660217285156 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.9708, loss_val: nan, pos_over_neg: 446.2805480957031 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.9785, loss_val: nan, pos_over_neg: 441.1813049316406 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 6.0125, loss_val: nan, pos_over_neg: 210.8646240234375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.9825, loss_val: nan, pos_over_neg: 776.7752685546875 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 6.0143, loss_val: nan, pos_over_neg: 258.15533447265625 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 6.0081, loss_val: nan, pos_over_neg: 411.97845458984375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.992, loss_val: nan, pos_over_neg: 205.8431854248047 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.998, loss_val: nan, pos_over_neg: 364.6636047363281 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.9931, loss_val: nan, pos_over_neg: 622.0582885742188 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.9845, loss_val: nan, pos_over_neg: 228.09947204589844 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.9941, loss_val: nan, pos_over_neg: 955.7363891601562 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.9978, loss_val: nan, pos_over_neg: 4910.35791015625 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 6.0018, loss_val: nan, pos_over_neg: 853.6832885742188 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.9787, loss_val: nan, pos_over_neg: 474.74884033203125 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 428.73687744140625 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.9752, loss_val: nan, pos_over_neg: 1508.2191162109375 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 6.012, loss_val: nan, pos_over_neg: 501.4278259277344 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 6.0074, loss_val: nan, pos_over_neg: 702.370361328125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.9905, loss_val: nan, pos_over_neg: 566.3084716796875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.9749, loss_val: nan, pos_over_neg: 512.3134155273438 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.9884, loss_val: nan, pos_over_neg: 475.42694091796875 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.9791, loss_val: nan, pos_over_neg: 458.7783508300781 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.9946, loss_val: nan, pos_over_neg: 366.9732971191406 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 6.0085, loss_val: nan, pos_over_neg: 544.922607421875 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.9837, loss_val: nan, pos_over_neg: 329.3006896972656 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.9756, loss_val: nan, pos_over_neg: 343.7438049316406 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.9853, loss_val: nan, pos_over_neg: 261.52691650390625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.9924, loss_val: nan, pos_over_neg: 332.1285705566406 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.9707, loss_val: nan, pos_over_neg: 399.348388671875 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.9908, loss_val: nan, pos_over_neg: 669.4721069335938 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 6.0115, loss_val: nan, pos_over_neg: 450.01190185546875 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 6.0094, loss_val: nan, pos_over_neg: 264.3487854003906 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.9826, loss_val: nan, pos_over_neg: 273.2353820800781 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.9933, loss_val: nan, pos_over_neg: 441.5589904785156 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.9988, loss_val: nan, pos_over_neg: 533.7532958984375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.9694, loss_val: nan, pos_over_neg: 303.0691223144531 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.9884, loss_val: nan, pos_over_neg: 197.36729431152344 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.9849, loss_val: nan, pos_over_neg: 289.8023986816406 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.9751, loss_val: nan, pos_over_neg: 1893.3836669921875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.9972, loss_val: nan, pos_over_neg: 537.3340454101562 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.9831, loss_val: nan, pos_over_neg: 292.003662109375 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.9972, loss_val: nan, pos_over_neg: 555.7028198242188 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 6.013, loss_val: nan, pos_over_neg: 328.61346435546875 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 6.0029, loss_val: nan, pos_over_neg: 185.38438415527344 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.9826, loss_val: nan, pos_over_neg: 364.0512390136719 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1631.6043701171875 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.9828, loss_val: nan, pos_over_neg: 911.6468505859375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.9933, loss_val: nan, pos_over_neg: 27287.67578125 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.9894, loss_val: nan, pos_over_neg: 2510.309326171875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.9841, loss_val: nan, pos_over_neg: -3624.9453125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.9822, loss_val: nan, pos_over_neg: 335.00103759765625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.9994, loss_val: nan, pos_over_neg: 180.3646697998047 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.9739, loss_val: nan, pos_over_neg: 610.9769287109375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.9711, loss_val: nan, pos_over_neg: 433.873291015625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 6.0011, loss_val: nan, pos_over_neg: 296.5339660644531 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 1824.940673828125 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 673.2130126953125 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.9781, loss_val: nan, pos_over_neg: 1658.387451171875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.9857, loss_val: nan, pos_over_neg: 928.4471435546875 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.9637, loss_val: nan, pos_over_neg: 303.27349853515625 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.9658, loss_val: nan, pos_over_neg: 567.9112548828125 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.9844, loss_val: nan, pos_over_neg: 196.99053955078125 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.9571, loss_val: nan, pos_over_neg: 747.0789794921875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.9812, loss_val: nan, pos_over_neg: 346.44366455078125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 6.0017, loss_val: nan, pos_over_neg: 2954.9150390625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.9738, loss_val: nan, pos_over_neg: 1188.111083984375 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.9839, loss_val: nan, pos_over_neg: 1311.12890625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.9921, loss_val: nan, pos_over_neg: 355.8170166015625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.9832, loss_val: nan, pos_over_neg: 141.0496826171875 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.9853, loss_val: nan, pos_over_neg: 243.82443237304688 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.9922, loss_val: nan, pos_over_neg: 588.7001342773438 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.9766, loss_val: nan, pos_over_neg: 868.017822265625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.9736, loss_val: nan, pos_over_neg: 2634.92138671875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.9885, loss_val: nan, pos_over_neg: 324.9642639160156 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.9558, loss_val: nan, pos_over_neg: 785.1195678710938 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.9851, loss_val: nan, pos_over_neg: 209.2281951904297 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.9511, loss_val: nan, pos_over_neg: 228.50389099121094 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.9768, loss_val: nan, pos_over_neg: 372.5640869140625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.9939, loss_val: nan, pos_over_neg: 513.28515625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.9736, loss_val: nan, pos_over_neg: 848.488037109375 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.977, loss_val: nan, pos_over_neg: -9711.42578125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.9718, loss_val: nan, pos_over_neg: 1426.5048828125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 746.3258666992188 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.9592, loss_val: nan, pos_over_neg: 489.2165222167969 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.9866, loss_val: nan, pos_over_neg: 178.40525817871094 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.9723, loss_val: nan, pos_over_neg: 268.8999328613281 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.9815, loss_val: nan, pos_over_neg: 505.87457275390625 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.972, loss_val: nan, pos_over_neg: 1147.2213134765625 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.9661, loss_val: nan, pos_over_neg: 826.0988159179688 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.9686, loss_val: nan, pos_over_neg: 1448.1021728515625 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.9628, loss_val: nan, pos_over_neg: 1361.262939453125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.9758, loss_val: nan, pos_over_neg: 1960.646728515625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.9554, loss_val: nan, pos_over_neg: 509.6680908203125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.9756, loss_val: nan, pos_over_neg: 473.6911926269531 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.9738, loss_val: nan, pos_over_neg: 211.9881591796875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.9526, loss_val: nan, pos_over_neg: 672.2615966796875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.9772, loss_val: nan, pos_over_neg: -14315.3359375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 6.0019, loss_val: nan, pos_over_neg: 398.5762939453125 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.981, loss_val: nan, pos_over_neg: 370.3598937988281 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.9819, loss_val: nan, pos_over_neg: 306.5924072265625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.9803, loss_val: nan, pos_over_neg: 420.14599609375 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.9742, loss_val: nan, pos_over_neg: 4016.10888671875 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.976, loss_val: nan, pos_over_neg: 305.87335205078125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.9725, loss_val: nan, pos_over_neg: 334.7818298339844 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.9788, loss_val: nan, pos_over_neg: 698.751953125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.9749, loss_val: nan, pos_over_neg: 257.2479553222656 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.9779, loss_val: nan, pos_over_neg: 245.78921508789062 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.973, loss_val: nan, pos_over_neg: 378.21002197265625 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.9984, loss_val: nan, pos_over_neg: 286.1874084472656 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.9756, loss_val: nan, pos_over_neg: 536.7828369140625 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.9758, loss_val: nan, pos_over_neg: 294.1189880371094 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.9633, loss_val: nan, pos_over_neg: 3187.660400390625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.956, loss_val: nan, pos_over_neg: 958.70849609375 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.9843, loss_val: nan, pos_over_neg: 725.524169921875 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.9847, loss_val: nan, pos_over_neg: 233.95433044433594 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.9769, loss_val: nan, pos_over_neg: 196.53610229492188 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.9869, loss_val: nan, pos_over_neg: 395.5359802246094 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.9772, loss_val: nan, pos_over_neg: 648.5795288085938 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.9899, loss_val: nan, pos_over_neg: 1624.5760498046875 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.969, loss_val: nan, pos_over_neg: 875.0302124023438 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.9799, loss_val: nan, pos_over_neg: -32054.265625 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: 1028.510498046875 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.9717, loss_val: nan, pos_over_neg: 222.69606018066406 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.9839, loss_val: nan, pos_over_neg: 191.39210510253906 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.9843, loss_val: nan, pos_over_neg: 413.8588562011719 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.9561, loss_val: nan, pos_over_neg: 1313.13134765625 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.9716, loss_val: nan, pos_over_neg: 3854.513671875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.9577, loss_val: nan, pos_over_neg: 3896.53076171875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.9877, loss_val: nan, pos_over_neg: 342.9325256347656 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.9803, loss_val: nan, pos_over_neg: 631.974853515625 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.9504, loss_val: nan, pos_over_neg: 543.4007568359375 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.9816, loss_val: nan, pos_over_neg: 248.52639770507812 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.9637, loss_val: nan, pos_over_neg: 408.4450988769531 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.9954, loss_val: nan, pos_over_neg: 449.8481750488281 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.9754, loss_val: nan, pos_over_neg: 1422.9212646484375 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.9878, loss_val: nan, pos_over_neg: 794.1521606445312 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.9624, loss_val: nan, pos_over_neg: 606.2186889648438 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.9358, loss_val: nan, pos_over_neg: 354.6705017089844 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.9624, loss_val: nan, pos_over_neg: 229.8301544189453 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.9695, loss_val: nan, pos_over_neg: 156.2989044189453 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.9707, loss_val: nan, pos_over_neg: 449.3573913574219 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.944, loss_val: nan, pos_over_neg: 3996.75439453125 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.9355, loss_val: nan, pos_over_neg: 678.0845336914062 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.9544, loss_val: nan, pos_over_neg: 1417.15576171875 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.9677, loss_val: nan, pos_over_neg: 222.07052612304688 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.9597, loss_val: nan, pos_over_neg: 603.0778198242188 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.9616, loss_val: nan, pos_over_neg: 265.3634948730469 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.9824, loss_val: nan, pos_over_neg: 347.11431884765625 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.984, loss_val: nan, pos_over_neg: 370.1916809082031 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.9763, loss_val: nan, pos_over_neg: 576.8291015625 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.9403, loss_val: nan, pos_over_neg: -3547.289306640625 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.9751, loss_val: nan, pos_over_neg: 485.4084167480469 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.9615, loss_val: nan, pos_over_neg: 320.4238586425781 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.9596, loss_val: nan, pos_over_neg: 634.4950561523438 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.9431, loss_val: nan, pos_over_neg: 519.5990600585938 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.9455, loss_val: nan, pos_over_neg: 465.63885498046875 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.9646, loss_val: nan, pos_over_neg: 833.5431518554688 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.9486, loss_val: nan, pos_over_neg: 985.4978637695312 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.9747, loss_val: nan, pos_over_neg: 898.3540649414062 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.9773, loss_val: nan, pos_over_neg: 404.51165771484375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.9398, loss_val: nan, pos_over_neg: 441.4505920410156 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.9564, loss_val: nan, pos_over_neg: 577.0260620117188 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.987, loss_val: nan, pos_over_neg: 254.43324279785156 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.9922, loss_val: nan, pos_over_neg: 182.5548553466797 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.9605, loss_val: nan, pos_over_neg: 320.8453674316406 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.9583, loss_val: nan, pos_over_neg: 630.9618530273438 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.9659, loss_val: nan, pos_over_neg: 316.8423767089844 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.9913, loss_val: nan, pos_over_neg: 2148.609130859375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.947, loss_val: nan, pos_over_neg: 971.89453125 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.9733, loss_val: nan, pos_over_neg: 424.2788391113281 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.9661, loss_val: nan, pos_over_neg: 708.4546508789062 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.9583, loss_val: nan, pos_over_neg: 615.819091796875 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.9684, loss_val: nan, pos_over_neg: 362.97283935546875 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.9563, loss_val: nan, pos_over_neg: 620.5018310546875 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.9725, loss_val: nan, pos_over_neg: 399.40789794921875 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.9577, loss_val: nan, pos_over_neg: 2310.072998046875 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.974, loss_val: nan, pos_over_neg: 3406.220947265625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.9452, loss_val: nan, pos_over_neg: 1450.5042724609375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.9649, loss_val: nan, pos_over_neg: 1022.3977661132812 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.9622, loss_val: nan, pos_over_neg: 313.35498046875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.9919, loss_val: nan, pos_over_neg: 201.4144287109375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.9743, loss_val: nan, pos_over_neg: 665.330810546875 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.9551, loss_val: nan, pos_over_neg: 593.214599609375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.9476, loss_val: nan, pos_over_neg: 637.4708862304688 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.9647, loss_val: nan, pos_over_neg: 1710.826416015625 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.967, loss_val: nan, pos_over_neg: 2650.199462890625 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.9584, loss_val: nan, pos_over_neg: 777.09375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1241.6331787109375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.9698, loss_val: nan, pos_over_neg: 760.531005859375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.9749, loss_val: nan, pos_over_neg: 306.3925476074219 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.971, loss_val: nan, pos_over_neg: 320.2483825683594 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.9354, loss_val: nan, pos_over_neg: 329.2418518066406 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.9413, loss_val: nan, pos_over_neg: 364.5213928222656 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.9581, loss_val: nan, pos_over_neg: 397.1905212402344 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.9494, loss_val: nan, pos_over_neg: 1041.8492431640625 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.9729, loss_val: nan, pos_over_neg: 503.7575378417969 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.9666, loss_val: nan, pos_over_neg: 172.5717315673828 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.9496, loss_val: nan, pos_over_neg: 767.2417602539062 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.9365, loss_val: nan, pos_over_neg: 1145.9832763671875 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.9699, loss_val: nan, pos_over_neg: 350.16802978515625 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.9604, loss_val: nan, pos_over_neg: 466.8750305175781 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.958, loss_val: nan, pos_over_neg: 287.72735595703125 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.9587, loss_val: nan, pos_over_neg: 860.3345947265625 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.9588, loss_val: nan, pos_over_neg: 2023.935546875 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.975, loss_val: nan, pos_over_neg: 472.71099853515625 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.9501, loss_val: nan, pos_over_neg: 457.0325012207031 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.9362, loss_val: nan, pos_over_neg: 508.1440124511719 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.9634, loss_val: nan, pos_over_neg: 255.64297485351562 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.968, loss_val: nan, pos_over_neg: 329.9924011230469 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.9542, loss_val: nan, pos_over_neg: 266.3347473144531 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.9636, loss_val: nan, pos_over_neg: 251.2538299560547 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.9784, loss_val: nan, pos_over_neg: 427.4071044921875 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.9859, loss_val: nan, pos_over_neg: 801.3709716796875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.9619, loss_val: nan, pos_over_neg: 509.20819091796875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.9865, loss_val: nan, pos_over_neg: 332.13201904296875 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.9719, loss_val: nan, pos_over_neg: 357.98358154296875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.9526, loss_val: nan, pos_over_neg: 263.75421142578125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.965, loss_val: nan, pos_over_neg: 196.659912109375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.9585, loss_val: nan, pos_over_neg: 571.0667724609375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.9468, loss_val: nan, pos_over_neg: 722.080322265625 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.9482, loss_val: nan, pos_over_neg: 412.7255554199219 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: -73472.7265625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.9608, loss_val: nan, pos_over_neg: 1812.963623046875 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.9417, loss_val: nan, pos_over_neg: 707.7296752929688 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.9547, loss_val: nan, pos_over_neg: 515.2927856445312 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.9543, loss_val: nan, pos_over_neg: 464.07476806640625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: 344.91754150390625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.9594, loss_val: nan, pos_over_neg: 363.43267822265625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.9707, loss_val: nan, pos_over_neg: 403.4909973144531 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.9435, loss_val: nan, pos_over_neg: 424.0624084472656 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.9624, loss_val: nan, pos_over_neg: 40080.60546875 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.9556, loss_val: nan, pos_over_neg: 2189.838623046875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.9517, loss_val: nan, pos_over_neg: 1710.2567138671875 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.9503, loss_val: nan, pos_over_neg: 625.9295043945312 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.9515, loss_val: nan, pos_over_neg: 564.5918579101562 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.9475, loss_val: nan, pos_over_neg: 477.0509338378906 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.9501, loss_val: nan, pos_over_neg: 257.14959716796875 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.9669, loss_val: nan, pos_over_neg: 528.7546997070312 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.9806, loss_val: nan, pos_over_neg: 574.2048950195312 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.9459, loss_val: nan, pos_over_neg: 1559.72705078125 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.9449, loss_val: nan, pos_over_neg: 507.4721984863281 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.9592, loss_val: nan, pos_over_neg: 417.0815734863281 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.9379, loss_val: nan, pos_over_neg: 394.46917724609375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.9469, loss_val: nan, pos_over_neg: 2654.621337890625 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.9477, loss_val: nan, pos_over_neg: 2310.497802734375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.9361, loss_val: nan, pos_over_neg: 12143.54296875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.953, loss_val: nan, pos_over_neg: 1002.6043090820312 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.9486, loss_val: nan, pos_over_neg: 4368.833984375 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.9617, loss_val: nan, pos_over_neg: 290.7593994140625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.9617, loss_val: nan, pos_over_neg: 495.2429504394531 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.9354, loss_val: nan, pos_over_neg: 561.4925537109375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.9686, loss_val: nan, pos_over_neg: 543.8638305664062 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.9581, loss_val: nan, pos_over_neg: 580.509521484375 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.9644, loss_val: nan, pos_over_neg: 563.949462890625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.9521, loss_val: nan, pos_over_neg: 3723.449951171875 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.948, loss_val: nan, pos_over_neg: 1515.534912109375 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.9606, loss_val: nan, pos_over_neg: 364.584716796875 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.9597, loss_val: nan, pos_over_neg: 487.6386413574219 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.9733, loss_val: nan, pos_over_neg: 249.94363403320312 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1337.097412109375 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1964.9063720703125 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.9474, loss_val: nan, pos_over_neg: 1135.0655517578125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.961, loss_val: nan, pos_over_neg: 1501.4339599609375 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.9655, loss_val: nan, pos_over_neg: 880.9076538085938 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.9472, loss_val: nan, pos_over_neg: 339.7194519042969 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.9809, loss_val: nan, pos_over_neg: 318.3359375 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.9441, loss_val: nan, pos_over_neg: 620.979736328125 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.9441, loss_val: nan, pos_over_neg: 1476.41748046875 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.9455, loss_val: nan, pos_over_neg: 8080.62353515625 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.9441, loss_val: nan, pos_over_neg: 1189.607421875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.9503, loss_val: nan, pos_over_neg: -96353.1796875 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.9626, loss_val: nan, pos_over_neg: 5356.9345703125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.9851, loss_val: nan, pos_over_neg: 207.588623046875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.9666, loss_val: nan, pos_over_neg: 185.9300994873047 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.9357, loss_val: nan, pos_over_neg: 548.4971923828125 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.9397, loss_val: nan, pos_over_neg: 628.1309204101562 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.961, loss_val: nan, pos_over_neg: 437.188720703125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.9511, loss_val: nan, pos_over_neg: 1012.8915405273438 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.9385, loss_val: nan, pos_over_neg: 1508.66943359375 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.9682, loss_val: nan, pos_over_neg: 707.5682373046875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.9514, loss_val: nan, pos_over_neg: 2308.309814453125 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.9552, loss_val: nan, pos_over_neg: 646.00927734375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.9484, loss_val: nan, pos_over_neg: 264.76849365234375 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.9734, loss_val: nan, pos_over_neg: 225.53656005859375 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.9534, loss_val: nan, pos_over_neg: 336.7052917480469 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.9545, loss_val: nan, pos_over_neg: 451.4533386230469 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.9529, loss_val: nan, pos_over_neg: 1289.8048095703125 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.9826, loss_val: nan, pos_over_neg: 391.23565673828125 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.9428, loss_val: nan, pos_over_neg: 755.0454711914062 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.9838, loss_val: nan, pos_over_neg: 214.05239868164062 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.9376, loss_val: nan, pos_over_neg: 233.96868896484375 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.9555, loss_val: nan, pos_over_neg: 281.789794921875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.9342, loss_val: nan, pos_over_neg: 351.9046630859375 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.945, loss_val: nan, pos_over_neg: 444.179931640625 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 735.59423828125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.9547, loss_val: nan, pos_over_neg: 246.84422302246094 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: 442.0066833496094 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.9337, loss_val: nan, pos_over_neg: 600.1708374023438 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.9505, loss_val: nan, pos_over_neg: 269.0769348144531 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.945, loss_val: nan, pos_over_neg: 387.5591735839844 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.9487, loss_val: nan, pos_over_neg: 639.8821411132812 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.9511, loss_val: nan, pos_over_neg: 557.4862670898438 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.949, loss_val: nan, pos_over_neg: 620.270263671875 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.9595, loss_val: nan, pos_over_neg: -12579.5341796875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.9239, loss_val: nan, pos_over_neg: 3054.812255859375 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.9499, loss_val: nan, pos_over_neg: 602.6878051757812 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.9285, loss_val: nan, pos_over_neg: 819.5792846679688 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.9297, loss_val: nan, pos_over_neg: 2057.0302734375 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.9458, loss_val: nan, pos_over_neg: 376.9891662597656 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.9383, loss_val: nan, pos_over_neg: 658.758056640625 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.9218, loss_val: nan, pos_over_neg: 292.41094970703125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.9532, loss_val: nan, pos_over_neg: 402.3589782714844 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.9448, loss_val: nan, pos_over_neg: 536.2177124023438 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.935, loss_val: nan, pos_over_neg: 2286.681640625 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.9174, loss_val: nan, pos_over_neg: 716.9614868164062 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.9403, loss_val: nan, pos_over_neg: 1432.234375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.9296, loss_val: nan, pos_over_neg: 1960.6727294921875 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.9451, loss_val: nan, pos_over_neg: 421.1434631347656 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.932, loss_val: nan, pos_over_neg: 262.03729248046875 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.9401, loss_val: nan, pos_over_neg: 237.3811492919922 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.9475, loss_val: nan, pos_over_neg: 310.3262634277344 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.9392, loss_val: nan, pos_over_neg: 660.9075317382812 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.9653, loss_val: nan, pos_over_neg: 1733.354248046875 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.9292, loss_val: nan, pos_over_neg: 539.4072875976562 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.9601, loss_val: nan, pos_over_neg: 172.11282348632812 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.9712, loss_val: nan, pos_over_neg: 229.63034057617188 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.9291, loss_val: nan, pos_over_neg: 434.18426513671875 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.9447, loss_val: nan, pos_over_neg: 328.008056640625 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.9444, loss_val: nan, pos_over_neg: 256.05792236328125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.9517, loss_val: nan, pos_over_neg: 543.5482788085938 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.9348, loss_val: nan, pos_over_neg: 315.7367858886719 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.9644, loss_val: nan, pos_over_neg: 968.1500244140625 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.9645, loss_val: nan, pos_over_neg: 188.84832763671875 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.9715, loss_val: nan, pos_over_neg: 670.6665649414062 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.9315, loss_val: nan, pos_over_neg: 461.7056884765625 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.9479, loss_val: nan, pos_over_neg: 481.9519348144531 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.9402, loss_val: nan, pos_over_neg: 605.6566162109375 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.9369, loss_val: nan, pos_over_neg: 395.59332275390625 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.9447, loss_val: nan, pos_over_neg: 712.3467407226562 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.9525, loss_val: nan, pos_over_neg: 644.8046875 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.9352, loss_val: nan, pos_over_neg: 471.3186340332031 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.9287, loss_val: nan, pos_over_neg: 288.3128967285156 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.9584, loss_val: nan, pos_over_neg: 233.4298095703125 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.9408, loss_val: nan, pos_over_neg: 531.8684692382812 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.9173, loss_val: nan, pos_over_neg: 589.423583984375 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.9565, loss_val: nan, pos_over_neg: 668.7005004882812 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.9328, loss_val: nan, pos_over_neg: 838.3190307617188 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.9445, loss_val: nan, pos_over_neg: 1047.892333984375 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.9623, loss_val: nan, pos_over_neg: 216.04815673828125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.9393, loss_val: nan, pos_over_neg: 475.6846008300781 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.939, loss_val: nan, pos_over_neg: 439.0572814941406 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.934, loss_val: nan, pos_over_neg: 410.89947509765625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.9563, loss_val: nan, pos_over_neg: 696.6547241210938 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.9409, loss_val: nan, pos_over_neg: 15494.80078125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.9281, loss_val: nan, pos_over_neg: 436.80267333984375 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.9311, loss_val: nan, pos_over_neg: 317.0371398925781 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.9312, loss_val: nan, pos_over_neg: 745.5546875 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 657.3705444335938 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.9394, loss_val: nan, pos_over_neg: 652.5260009765625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1071.5513916015625 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.9364, loss_val: nan, pos_over_neg: 1638.923583984375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.9281, loss_val: nan, pos_over_neg: -5159.50927734375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.9394, loss_val: nan, pos_over_neg: 431.57452392578125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.9488, loss_val: nan, pos_over_neg: 849.8182373046875 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.9488, loss_val: nan, pos_over_neg: 868.846923828125 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.9343, loss_val: nan, pos_over_neg: 384.78228759765625 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.9603, loss_val: nan, pos_over_neg: 278.7498474121094 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.9353, loss_val: nan, pos_over_neg: 797.469482421875 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1205.3511962890625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.9493, loss_val: nan, pos_over_neg: 281.2925109863281 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.9475, loss_val: nan, pos_over_neg: 379.2812805175781 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.9428, loss_val: nan, pos_over_neg: 895.4895629882812 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.908, loss_val: nan, pos_over_neg: 482.017822265625 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.9148, loss_val: nan, pos_over_neg: 311.84002685546875 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.9499, loss_val: nan, pos_over_neg: 307.79266357421875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.955, loss_val: nan, pos_over_neg: 455.188232421875 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.9398, loss_val: nan, pos_over_neg: -369848.3125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.9383, loss_val: nan, pos_over_neg: 5292.7890625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.9547, loss_val: nan, pos_over_neg: 1357.6771240234375 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.942, loss_val: nan, pos_over_neg: 2766.57666015625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.9285, loss_val: nan, pos_over_neg: 816.49609375 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.9273, loss_val: nan, pos_over_neg: 539.4733276367188 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.961, loss_val: nan, pos_over_neg: 163.15707397460938 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.9566, loss_val: nan, pos_over_neg: 232.56114196777344 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.9379, loss_val: nan, pos_over_neg: 1083.809814453125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.9346, loss_val: nan, pos_over_neg: 2254.384521484375 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.9485, loss_val: nan, pos_over_neg: 973.31494140625 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 526.4390869140625 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.9456, loss_val: nan, pos_over_neg: 346.5412902832031 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.9254, loss_val: nan, pos_over_neg: 769.4743041992188 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.9369, loss_val: nan, pos_over_neg: 534.52587890625 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.9297, loss_val: nan, pos_over_neg: -5804.0556640625 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.9439, loss_val: nan, pos_over_neg: 1032.957275390625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.9405, loss_val: nan, pos_over_neg: 379.6251220703125 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.9467, loss_val: nan, pos_over_neg: 633.7762451171875 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.9513, loss_val: nan, pos_over_neg: 277.4427185058594 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 2757.127685546875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.9451, loss_val: nan, pos_over_neg: 605.3677368164062 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.9527, loss_val: nan, pos_over_neg: 920.5164794921875 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.9413, loss_val: nan, pos_over_neg: 426.78387451171875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2337.00390625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.9111, loss_val: nan, pos_over_neg: 578.88525390625 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.9468, loss_val: nan, pos_over_neg: 694.9198608398438 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.9265, loss_val: nan, pos_over_neg: 381.28387451171875 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.934, loss_val: nan, pos_over_neg: 581.3336181640625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.9285, loss_val: nan, pos_over_neg: 990.1636352539062 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.9248, loss_val: nan, pos_over_neg: 1203.2001953125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2354.39697265625 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.9057, loss_val: nan, pos_over_neg: -140202.40625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.9419, loss_val: nan, pos_over_neg: 945.7930297851562 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 979.222900390625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 859.8779907226562 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.9319, loss_val: nan, pos_over_neg: 414.7882385253906 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.9219, loss_val: nan, pos_over_neg: 462.45562744140625 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.9446, loss_val: nan, pos_over_neg: 622.695068359375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.9357, loss_val: nan, pos_over_neg: 683.0396728515625 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.9352, loss_val: nan, pos_over_neg: 34126.95703125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.9556, loss_val: nan, pos_over_neg: 1697.1766357421875 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2470.022705078125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1285.5323486328125 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.9456, loss_val: nan, pos_over_neg: 310.6751403808594 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.9431, loss_val: nan, pos_over_neg: 561.8411254882812 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.9383, loss_val: nan, pos_over_neg: 902.3755493164062 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.9328, loss_val: nan, pos_over_neg: 302.3341979980469 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.9469, loss_val: nan, pos_over_neg: 343.3909606933594 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.9444, loss_val: nan, pos_over_neg: 718.5056762695312 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.9246, loss_val: nan, pos_over_neg: 3088.381103515625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.9355, loss_val: nan, pos_over_neg: 1511.33935546875 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.9353, loss_val: nan, pos_over_neg: 281.54998779296875 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 266.06707763671875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.9283, loss_val: nan, pos_over_neg: 536.0421752929688 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.938, loss_val: nan, pos_over_neg: 309.01861572265625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.9394, loss_val: nan, pos_over_neg: 546.9005737304688 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.9366, loss_val: nan, pos_over_neg: 422.80517578125 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.9309, loss_val: nan, pos_over_neg: 1195.3099365234375 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1058.77001953125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [25:06<125525:33:55, 1506.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/889, loss_train: 5.9379, loss_val: nan, pos_over_neg: 836.7720947265625 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.9342, loss_val: nan, pos_over_neg: 302.8044738769531 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.9302, loss_val: nan, pos_over_neg: 2447.04052734375 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.9191, loss_val: nan, pos_over_neg: 1052.4263916015625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.9281, loss_val: nan, pos_over_neg: 342.2099609375 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.9343, loss_val: nan, pos_over_neg: 391.80157470703125 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.9256, loss_val: nan, pos_over_neg: 349.9445495605469 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.936, loss_val: nan, pos_over_neg: 673.9456176757812 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.9228, loss_val: nan, pos_over_neg: 675.8674926757812 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.9235, loss_val: nan, pos_over_neg: 692.7626953125 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.9405, loss_val: nan, pos_over_neg: 328.53765869140625 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.9349, loss_val: nan, pos_over_neg: 524.168701171875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.9315, loss_val: nan, pos_over_neg: 642.7512817382812 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.9287, loss_val: nan, pos_over_neg: 391.97607421875 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1047.9857177734375 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.9155, loss_val: nan, pos_over_neg: 716.3146362304688 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.9434, loss_val: nan, pos_over_neg: 585.4293823242188 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.9167, loss_val: nan, pos_over_neg: 804.0676879882812 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.9321, loss_val: nan, pos_over_neg: 439.32403564453125 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.9157, loss_val: nan, pos_over_neg: 1403.9527587890625 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.9283, loss_val: nan, pos_over_neg: 6646.0654296875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.928, loss_val: nan, pos_over_neg: 1356.577392578125 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.9222, loss_val: nan, pos_over_neg: 867.9805297851562 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.9224, loss_val: nan, pos_over_neg: 382.5021667480469 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.9397, loss_val: nan, pos_over_neg: 643.0014038085938 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.9233, loss_val: nan, pos_over_neg: 852.8610229492188 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.9387, loss_val: nan, pos_over_neg: 708.6339721679688 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.9396, loss_val: nan, pos_over_neg: 519.7952880859375 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.9259, loss_val: nan, pos_over_neg: 657.2423706054688 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.9419, loss_val: nan, pos_over_neg: 15739.5419921875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.9452, loss_val: nan, pos_over_neg: 434.9681091308594 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.9436, loss_val: nan, pos_over_neg: 896.3618774414062 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.9219, loss_val: nan, pos_over_neg: 514.2257080078125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.9448, loss_val: nan, pos_over_neg: 276.66357421875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1417.314697265625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 922.880615234375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.9381, loss_val: nan, pos_over_neg: 480.6160888671875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.9145, loss_val: nan, pos_over_neg: 2447.803955078125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.9382, loss_val: nan, pos_over_neg: 616.6158447265625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.9363, loss_val: nan, pos_over_neg: 1373.2706298828125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.9386, loss_val: nan, pos_over_neg: 1007.4410400390625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1002.2719116210938 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.9197, loss_val: nan, pos_over_neg: 573.6642456054688 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.925, loss_val: nan, pos_over_neg: 350.3515625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.9302, loss_val: nan, pos_over_neg: 587.5128784179688 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.9318, loss_val: nan, pos_over_neg: 529.5972900390625 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.9286, loss_val: nan, pos_over_neg: 465.07537841796875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.9252, loss_val: nan, pos_over_neg: 347.78741455078125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.9471, loss_val: nan, pos_over_neg: 312.86993408203125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1922.1448974609375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.9254, loss_val: nan, pos_over_neg: 961.8570556640625 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.9396, loss_val: nan, pos_over_neg: 736.1995239257812 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.9405, loss_val: nan, pos_over_neg: 778.7427978515625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.9282, loss_val: nan, pos_over_neg: 656.5799560546875 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.9145, loss_val: nan, pos_over_neg: 568.0379638671875 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.906, loss_val: nan, pos_over_neg: 265.7946472167969 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 851.4569702148438 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 635.6089477539062 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.9375, loss_val: nan, pos_over_neg: 370.6193542480469 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.9023, loss_val: nan, pos_over_neg: 449.1189270019531 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.9197, loss_val: nan, pos_over_neg: 338.3833923339844 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.9236, loss_val: nan, pos_over_neg: 488.46832275390625 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.9382, loss_val: nan, pos_over_neg: 440.984130859375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.9106, loss_val: nan, pos_over_neg: 481.4092712402344 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.9325, loss_val: nan, pos_over_neg: 397.1817626953125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.9469, loss_val: nan, pos_over_neg: 348.9867858886719 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.9235, loss_val: nan, pos_over_neg: 748.935791015625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.9093, loss_val: nan, pos_over_neg: 602.3949584960938 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.9197, loss_val: nan, pos_over_neg: 503.21563720703125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.93, loss_val: nan, pos_over_neg: 362.3328857421875 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.9199, loss_val: nan, pos_over_neg: -8576.021484375 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8955, loss_val: nan, pos_over_neg: 678.69775390625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 628.1722412109375 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.9358, loss_val: nan, pos_over_neg: 434.1410827636719 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.9271, loss_val: nan, pos_over_neg: 534.5394897460938 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.9185, loss_val: nan, pos_over_neg: 625.1991577148438 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.9138, loss_val: nan, pos_over_neg: 678.7293701171875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 369.8231506347656 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.9373, loss_val: nan, pos_over_neg: 990.7454833984375 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 1818.6282958984375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.9305, loss_val: nan, pos_over_neg: 474.88446044921875 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 446.0697326660156 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1164.074951171875 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1810.885986328125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1182.030029296875 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.9118, loss_val: nan, pos_over_neg: 855.3295288085938 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.9185, loss_val: nan, pos_over_neg: 14242.3720703125 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.9106, loss_val: nan, pos_over_neg: 16662.546875 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.9183, loss_val: nan, pos_over_neg: 236.33984375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 507.2308044433594 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.9098, loss_val: nan, pos_over_neg: 528.10888671875 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.9298, loss_val: nan, pos_over_neg: 459.5431213378906 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.9058, loss_val: nan, pos_over_neg: 483.2398681640625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.9284, loss_val: nan, pos_over_neg: 258.1907043457031 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 381.2937316894531 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.9307, loss_val: nan, pos_over_neg: 498.35235595703125 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.9532, loss_val: nan, pos_over_neg: 246.59312438964844 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 265.37188720703125 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1099.0328369140625 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.9188, loss_val: nan, pos_over_neg: 430.0424499511719 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8999, loss_val: nan, pos_over_neg: 454.2769470214844 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8986, loss_val: nan, pos_over_neg: 750.8400268554688 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.9265, loss_val: nan, pos_over_neg: 242.1654510498047 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.9274, loss_val: nan, pos_over_neg: 573.848388671875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.936, loss_val: nan, pos_over_neg: 386.7127685546875 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.911, loss_val: nan, pos_over_neg: 276.1266174316406 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.9333, loss_val: nan, pos_over_neg: 570.3038330078125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.9203, loss_val: nan, pos_over_neg: 234.22250366210938 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.914, loss_val: nan, pos_over_neg: 221.9892120361328 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.9171, loss_val: nan, pos_over_neg: 301.6824951171875 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.9224, loss_val: nan, pos_over_neg: 308.85809326171875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.9303, loss_val: nan, pos_over_neg: 485.88543701171875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 2584.116455078125 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8955, loss_val: nan, pos_over_neg: 550.90234375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.9179, loss_val: nan, pos_over_neg: 488.98443603515625 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.93, loss_val: nan, pos_over_neg: 724.1250610351562 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.936, loss_val: nan, pos_over_neg: 366.17291259765625 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.914, loss_val: nan, pos_over_neg: 732.6065063476562 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.9294, loss_val: nan, pos_over_neg: 305.2129821777344 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.9028, loss_val: nan, pos_over_neg: 846.3099365234375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 464.6542053222656 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 2730.417236328125 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.9246, loss_val: nan, pos_over_neg: 348.8503112792969 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.915, loss_val: nan, pos_over_neg: 3450.462158203125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.9317, loss_val: nan, pos_over_neg: 2321.423583984375 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.9032, loss_val: nan, pos_over_neg: 3584.247802734375 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.9073, loss_val: nan, pos_over_neg: 757.8285522460938 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.9078, loss_val: nan, pos_over_neg: 928.8526611328125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.9236, loss_val: nan, pos_over_neg: 653.1839599609375 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.9352, loss_val: nan, pos_over_neg: 265.913330078125 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.9022, loss_val: nan, pos_over_neg: 643.4158935546875 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.9231, loss_val: nan, pos_over_neg: 555.0843505859375 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.923, loss_val: nan, pos_over_neg: 909.6328735351562 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.9346, loss_val: nan, pos_over_neg: 257.1886901855469 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.9127, loss_val: nan, pos_over_neg: 3040.913330078125 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.9204, loss_val: nan, pos_over_neg: -36498.01953125 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.916, loss_val: nan, pos_over_neg: 1076.2364501953125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.9118, loss_val: nan, pos_over_neg: 3265.083251953125 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 435.6319274902344 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1012.1414794921875 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8979, loss_val: nan, pos_over_neg: 527.3817749023438 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.916, loss_val: nan, pos_over_neg: 753.5291137695312 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.9325, loss_val: nan, pos_over_neg: 372.06884765625 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.9203, loss_val: nan, pos_over_neg: 6173.318359375 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.9167, loss_val: nan, pos_over_neg: 4062.07958984375 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.9266, loss_val: nan, pos_over_neg: 880.4725341796875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.915, loss_val: nan, pos_over_neg: 428.5052490234375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 3362.160888671875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.9099, loss_val: nan, pos_over_neg: 341.9124755859375 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 657.66943359375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 587.4418334960938 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.9127, loss_val: nan, pos_over_neg: 464.57061767578125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.915, loss_val: nan, pos_over_neg: 1386.825927734375 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.9215, loss_val: nan, pos_over_neg: 838.4158325195312 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.9268, loss_val: nan, pos_over_neg: 907.239013671875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 266.9720764160156 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.9009, loss_val: nan, pos_over_neg: 1685.8013916015625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.9147, loss_val: nan, pos_over_neg: 553.1577758789062 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 2369.632568359375 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1993.2061767578125 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.9323, loss_val: nan, pos_over_neg: 1227.0330810546875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.9071, loss_val: nan, pos_over_neg: 800.1154174804688 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.9057, loss_val: nan, pos_over_neg: 2332.80859375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.923, loss_val: nan, pos_over_neg: 3327.6591796875 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.926, loss_val: nan, pos_over_neg: 370.9113464355469 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 728.7630615234375 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.9296, loss_val: nan, pos_over_neg: 603.7725830078125 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.9088, loss_val: nan, pos_over_neg: 350.8837585449219 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1959.49658203125 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.9001, loss_val: nan, pos_over_neg: 700.6461181640625 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 824.3500366210938 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 1264.021484375 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.9213, loss_val: nan, pos_over_neg: 703.3492431640625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8974, loss_val: nan, pos_over_neg: 698.569091796875 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.9186, loss_val: nan, pos_over_neg: 243.4152374267578 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.9172, loss_val: nan, pos_over_neg: 297.2059020996094 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.9315, loss_val: nan, pos_over_neg: 1432.5318603515625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.9193, loss_val: nan, pos_over_neg: 597.5227661132812 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.9282, loss_val: nan, pos_over_neg: -4245.95947265625 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.911, loss_val: nan, pos_over_neg: 2991.484375 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.9175, loss_val: nan, pos_over_neg: 581.7600708007812 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.9161, loss_val: nan, pos_over_neg: 332.427490234375 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8995, loss_val: nan, pos_over_neg: 266.0779724121094 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.9009, loss_val: nan, pos_over_neg: 318.3259582519531 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.9269, loss_val: nan, pos_over_neg: 572.08935546875 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.9078, loss_val: nan, pos_over_neg: 1301.0682373046875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.9043, loss_val: nan, pos_over_neg: 608.990478515625 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.9016, loss_val: nan, pos_over_neg: 5503.77392578125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.922, loss_val: nan, pos_over_neg: 713.4677124023438 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 1007.9620361328125 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.9076, loss_val: nan, pos_over_neg: 581.1928100585938 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1062.998779296875 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.9282, loss_val: nan, pos_over_neg: 351.93170166015625 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.9217, loss_val: nan, pos_over_neg: 2362.716552734375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.9395, loss_val: nan, pos_over_neg: 830.4714965820312 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1130.898193359375 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8994, loss_val: nan, pos_over_neg: 791.8644409179688 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.9093, loss_val: nan, pos_over_neg: 575.8211059570312 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8985, loss_val: nan, pos_over_neg: 563.4288330078125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.9215, loss_val: nan, pos_over_neg: 481.2333984375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.9091, loss_val: nan, pos_over_neg: 301.97723388671875 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 563.1827392578125 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.9137, loss_val: nan, pos_over_neg: 859.6358642578125 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.9097, loss_val: nan, pos_over_neg: 531.98388671875 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1310.9434814453125 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.9202, loss_val: nan, pos_over_neg: 451.3404846191406 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.9129, loss_val: nan, pos_over_neg: 300.8221435546875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.9203, loss_val: nan, pos_over_neg: 294.6104736328125 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.9035, loss_val: nan, pos_over_neg: 599.0208740234375 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.9177, loss_val: nan, pos_over_neg: 526.5089111328125 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.9172, loss_val: nan, pos_over_neg: 288.1439208984375 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.921, loss_val: nan, pos_over_neg: 582.3744506835938 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1135.7994384765625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1162.361328125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.921, loss_val: nan, pos_over_neg: 388.9736328125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.9082, loss_val: nan, pos_over_neg: 333.5173034667969 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8989, loss_val: nan, pos_over_neg: 668.428955078125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 1140.5013427734375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.9225, loss_val: nan, pos_over_neg: 441.86273193359375 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.9005, loss_val: nan, pos_over_neg: 298.1125793457031 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.9166, loss_val: nan, pos_over_neg: 368.89013671875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8998, loss_val: nan, pos_over_neg: 427.68310546875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.9083, loss_val: nan, pos_over_neg: 433.9444885253906 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.9088, loss_val: nan, pos_over_neg: 465.1457824707031 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 416.1618347167969 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.9184, loss_val: nan, pos_over_neg: 167.95101928710938 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.9094, loss_val: nan, pos_over_neg: 805.5147705078125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.9134, loss_val: nan, pos_over_neg: 724.4593505859375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8962, loss_val: nan, pos_over_neg: 503.0556945800781 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.9073, loss_val: nan, pos_over_neg: 648.3631591796875 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.9084, loss_val: nan, pos_over_neg: -13926.732421875 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.9343, loss_val: nan, pos_over_neg: 361.7447204589844 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.9303, loss_val: nan, pos_over_neg: 250.5220947265625 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8893, loss_val: nan, pos_over_neg: 759.2198486328125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.9426, loss_val: nan, pos_over_neg: 259.2240295410156 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.9233, loss_val: nan, pos_over_neg: 472.1707458496094 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.9148, loss_val: nan, pos_over_neg: 452.5403137207031 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.9141, loss_val: nan, pos_over_neg: 902.1175537109375 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.924, loss_val: nan, pos_over_neg: 762.87255859375 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.9048, loss_val: nan, pos_over_neg: 789.8384399414062 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.9022, loss_val: nan, pos_over_neg: 455.28948974609375 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 300.75006103515625 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.9243, loss_val: nan, pos_over_neg: 298.64666748046875 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.9145, loss_val: nan, pos_over_neg: 653.007568359375 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.9251, loss_val: nan, pos_over_neg: 341.9557800292969 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1007.8546142578125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.924, loss_val: nan, pos_over_neg: 1989.4256591796875 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8981, loss_val: nan, pos_over_neg: 803.5055541992188 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.9264, loss_val: nan, pos_over_neg: 622.5687255859375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8959, loss_val: nan, pos_over_neg: 440.09857177734375 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8966, loss_val: nan, pos_over_neg: 600.9822998046875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.9226, loss_val: nan, pos_over_neg: 355.56890869140625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.9153, loss_val: nan, pos_over_neg: 806.2166748046875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.903, loss_val: nan, pos_over_neg: 497.19970703125 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.911, loss_val: nan, pos_over_neg: 494.87969970703125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.9115, loss_val: nan, pos_over_neg: 458.4769592285156 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.9131, loss_val: nan, pos_over_neg: 943.2338256835938 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.9187, loss_val: nan, pos_over_neg: 758.5281982421875 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.9233, loss_val: nan, pos_over_neg: 794.793212890625 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.9263, loss_val: nan, pos_over_neg: 364.1551818847656 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8872, loss_val: nan, pos_over_neg: 2993.190185546875 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.9115, loss_val: nan, pos_over_neg: 705.3141479492188 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.9025, loss_val: nan, pos_over_neg: 384.8189392089844 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8927, loss_val: nan, pos_over_neg: 339.48638916015625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8879, loss_val: nan, pos_over_neg: 856.8070068359375 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 2142.439453125 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.9165, loss_val: nan, pos_over_neg: 369.1758728027344 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.891, loss_val: nan, pos_over_neg: 917.0775756835938 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1852.9022216796875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.9131, loss_val: nan, pos_over_neg: 441.1456298828125 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.9195, loss_val: nan, pos_over_neg: 378.017578125 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.9288, loss_val: nan, pos_over_neg: 349.5747375488281 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.9007, loss_val: nan, pos_over_neg: 1965.099365234375 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.9134, loss_val: nan, pos_over_neg: 482.1929016113281 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 642.3475341796875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1247.8370361328125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.9091, loss_val: nan, pos_over_neg: 543.9309692382812 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1262.5943603515625 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 689.5642700195312 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.9204, loss_val: nan, pos_over_neg: 335.2655944824219 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.9139, loss_val: nan, pos_over_neg: 395.0549011230469 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.9058, loss_val: nan, pos_over_neg: 623.8757934570312 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.9015, loss_val: nan, pos_over_neg: 476.7777404785156 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.9027, loss_val: nan, pos_over_neg: 1595.641845703125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.9269, loss_val: nan, pos_over_neg: 246.2244873046875 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 317.9728698730469 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.9105, loss_val: nan, pos_over_neg: 711.1519165039062 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 439.5408630371094 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.9089, loss_val: nan, pos_over_neg: 324.7552490234375 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8931, loss_val: nan, pos_over_neg: 433.92620849609375 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.9123, loss_val: nan, pos_over_neg: 383.4340515136719 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 705.2322387695312 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8996, loss_val: nan, pos_over_neg: 1614.858154296875 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.9192, loss_val: nan, pos_over_neg: 662.3483276367188 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8943, loss_val: nan, pos_over_neg: 966.185791015625 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8965, loss_val: nan, pos_over_neg: 493.8673095703125 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.9301, loss_val: nan, pos_over_neg: 345.3692932128906 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.9017, loss_val: nan, pos_over_neg: 212.27833557128906 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8858, loss_val: nan, pos_over_neg: 459.727294921875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8972, loss_val: nan, pos_over_neg: 1488.4923095703125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.9235, loss_val: nan, pos_over_neg: -1805.9552001953125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2488.4599609375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 1133.6903076171875 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8875, loss_val: nan, pos_over_neg: 424.1064758300781 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.9307, loss_val: nan, pos_over_neg: 214.7949981689453 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 472.8448181152344 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 601.9361572265625 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8926, loss_val: nan, pos_over_neg: 617.9676513671875 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.912, loss_val: nan, pos_over_neg: 501.7853698730469 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.9025, loss_val: nan, pos_over_neg: -3926.016357421875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 408.6362609863281 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.9104, loss_val: nan, pos_over_neg: 368.0096130371094 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8983, loss_val: nan, pos_over_neg: 355.65966796875 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.9018, loss_val: nan, pos_over_neg: 3245.97216796875 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.9162, loss_val: nan, pos_over_neg: 472.7054748535156 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.9155, loss_val: nan, pos_over_neg: 228.1803436279297 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.9084, loss_val: nan, pos_over_neg: 368.1419982910156 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.9122, loss_val: nan, pos_over_neg: 315.02886962890625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8981, loss_val: nan, pos_over_neg: 675.4744873046875 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 764.31884765625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.9139, loss_val: nan, pos_over_neg: 264.2423400878906 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1647.3074951171875 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.9329, loss_val: nan, pos_over_neg: 443.44085693359375 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1314.28369140625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.9347, loss_val: nan, pos_over_neg: 441.1961975097656 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 468.6865539550781 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 294.3618469238281 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 3025.01611328125 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 251.67755126953125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.9091, loss_val: nan, pos_over_neg: 289.49774169921875 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.9114, loss_val: nan, pos_over_neg: 343.4283447265625 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.9093, loss_val: nan, pos_over_neg: 538.138427734375 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.9107, loss_val: nan, pos_over_neg: 665.6597290039062 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8906, loss_val: nan, pos_over_neg: 3084.208251953125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 3777.653564453125 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.921, loss_val: nan, pos_over_neg: 790.4403076171875 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.9169, loss_val: nan, pos_over_neg: 1091.9410400390625 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.902, loss_val: nan, pos_over_neg: 1200.080322265625 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 909.4475708007812 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.9171, loss_val: nan, pos_over_neg: 214.6783905029297 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.9078, loss_val: nan, pos_over_neg: 530.8419189453125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.923, loss_val: nan, pos_over_neg: 250.23513793945312 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 333.6680603027344 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.9031, loss_val: nan, pos_over_neg: 651.7206420898438 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1214.919677734375 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.9259, loss_val: nan, pos_over_neg: 542.5956420898438 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 340.1953430175781 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8981, loss_val: nan, pos_over_neg: 459.4508361816406 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 611.3436889648438 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8994, loss_val: nan, pos_over_neg: 719.6273803710938 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8949, loss_val: nan, pos_over_neg: 457.0719299316406 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8889, loss_val: nan, pos_over_neg: 1477.845947265625 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 281.46826171875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8853, loss_val: nan, pos_over_neg: 3291.576416015625 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.905, loss_val: nan, pos_over_neg: 1248.7017822265625 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8948, loss_val: nan, pos_over_neg: 432.6325988769531 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 562.90478515625 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8928, loss_val: nan, pos_over_neg: 365.4876403808594 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8966, loss_val: nan, pos_over_neg: 462.6136779785156 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8965, loss_val: nan, pos_over_neg: 289.41998291015625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 5110.21630859375 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.9098, loss_val: nan, pos_over_neg: 873.326171875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.9099, loss_val: nan, pos_over_neg: 445.5585632324219 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1832.4661865234375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.9055, loss_val: nan, pos_over_neg: 357.7665100097656 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.9054, loss_val: nan, pos_over_neg: 451.162353515625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8917, loss_val: nan, pos_over_neg: 1179.101806640625 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8934, loss_val: nan, pos_over_neg: 672.4605712890625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8844, loss_val: nan, pos_over_neg: 649.1009521484375 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.9044, loss_val: nan, pos_over_neg: 705.7579345703125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.9089, loss_val: nan, pos_over_neg: 469.4029541015625 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8896, loss_val: nan, pos_over_neg: 1788.9083251953125 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8985, loss_val: nan, pos_over_neg: 637.358642578125 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 740.7902221679688 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.9014, loss_val: nan, pos_over_neg: 648.873779296875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8866, loss_val: nan, pos_over_neg: 1219.7626953125 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.9121, loss_val: nan, pos_over_neg: 348.27703857421875 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.899, loss_val: nan, pos_over_neg: 586.7098999023438 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8916, loss_val: nan, pos_over_neg: 688.9286499023438 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 473.3155212402344 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 407.1054992675781 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 842.805419921875 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8894, loss_val: nan, pos_over_neg: 363.6236267089844 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 336.0245666503906 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.9306, loss_val: nan, pos_over_neg: 568.8128662109375 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.904, loss_val: nan, pos_over_neg: 415.7455139160156 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8971, loss_val: nan, pos_over_neg: -7068.02587890625 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8995, loss_val: nan, pos_over_neg: 4348.74609375 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 513.4353637695312 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.9074, loss_val: nan, pos_over_neg: 869.7265625 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 2071.564697265625 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.9027, loss_val: nan, pos_over_neg: 479.8277282714844 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.9057, loss_val: nan, pos_over_neg: 299.1687316894531 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.9118, loss_val: nan, pos_over_neg: 408.4020690917969 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.9035, loss_val: nan, pos_over_neg: 890.6370849609375 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.9274, loss_val: nan, pos_over_neg: 253.2266845703125 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.9124, loss_val: nan, pos_over_neg: 362.6310729980469 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8997, loss_val: nan, pos_over_neg: 2986.9716796875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8806, loss_val: nan, pos_over_neg: 717.8519897460938 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8988, loss_val: nan, pos_over_neg: 818.8295288085938 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.9081, loss_val: nan, pos_over_neg: 273.3731994628906 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.9065, loss_val: nan, pos_over_neg: 424.61810302734375 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8932, loss_val: nan, pos_over_neg: 1055.495361328125 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.9133, loss_val: nan, pos_over_neg: 259.5829772949219 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8937, loss_val: nan, pos_over_neg: 406.7304992675781 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8971, loss_val: nan, pos_over_neg: 570.2938232421875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8885, loss_val: nan, pos_over_neg: 319.1236267089844 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.9098, loss_val: nan, pos_over_neg: 2183.00927734375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 2255.489013671875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.9074, loss_val: nan, pos_over_neg: 469.9479064941406 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8976, loss_val: nan, pos_over_neg: 266.5321350097656 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 322.86474609375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8978, loss_val: nan, pos_over_neg: 544.290771484375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 832.5512084960938 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8909, loss_val: nan, pos_over_neg: 336.097412109375 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2612.646484375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8947, loss_val: nan, pos_over_neg: 871.0689697265625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8942, loss_val: nan, pos_over_neg: 646.1151733398438 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8988, loss_val: nan, pos_over_neg: 301.6468200683594 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.9146, loss_val: nan, pos_over_neg: 239.0704345703125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 450.3094482421875 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8934, loss_val: nan, pos_over_neg: 785.4794921875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 811.147705078125 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.9141, loss_val: nan, pos_over_neg: 349.08551025390625 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 1614.6036376953125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 685.7630615234375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 368.4942321777344 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 750.0967407226562 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.9015, loss_val: nan, pos_over_neg: 252.50306701660156 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 495.2933654785156 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.9084, loss_val: nan, pos_over_neg: 2630.3134765625 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 1098.581787109375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1551.5762939453125 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8878, loss_val: nan, pos_over_neg: 288.0241394042969 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.914, loss_val: nan, pos_over_neg: 355.2582702636719 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.9049, loss_val: nan, pos_over_neg: 244.57888793945312 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.9211, loss_val: nan, pos_over_neg: 297.5679626464844 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8963, loss_val: nan, pos_over_neg: 708.0805053710938 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8919, loss_val: nan, pos_over_neg: 865.46875 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 260.3616638183594 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.9021, loss_val: nan, pos_over_neg: 846.401123046875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8923, loss_val: nan, pos_over_neg: 720.33154296875 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8945, loss_val: nan, pos_over_neg: 1048.3790283203125 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8975, loss_val: nan, pos_over_neg: 312.6240234375 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8938, loss_val: nan, pos_over_neg: 314.41290283203125 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8891, loss_val: nan, pos_over_neg: 521.9918823242188 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8923, loss_val: nan, pos_over_neg: 588.972412109375 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 2210.014404296875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8927, loss_val: nan, pos_over_neg: 579.4403686523438 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.889, loss_val: nan, pos_over_neg: 834.2207641601562 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8986, loss_val: nan, pos_over_neg: 2207.25634765625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 513.156494140625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 1269.5634765625 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8884, loss_val: nan, pos_over_neg: 356.4333190917969 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.9179, loss_val: nan, pos_over_neg: 237.98411560058594 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.9154, loss_val: nan, pos_over_neg: 324.4672546386719 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8987, loss_val: nan, pos_over_neg: 480.8013000488281 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1135.7733154296875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 433.6571350097656 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8723, loss_val: nan, pos_over_neg: 2438.426513671875 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 283.6044006347656 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.9036, loss_val: nan, pos_over_neg: 264.21697998046875 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.9059, loss_val: nan, pos_over_neg: 414.82379150390625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.9117, loss_val: nan, pos_over_neg: 608.3895874023438 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3082.764892578125 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 833.8212890625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 385.780517578125 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.9109, loss_val: nan, pos_over_neg: 1005.5333862304688 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.9028, loss_val: nan, pos_over_neg: 647.837890625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.9138, loss_val: nan, pos_over_neg: 500.0395202636719 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8869, loss_val: nan, pos_over_neg: 458.4137268066406 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8921, loss_val: nan, pos_over_neg: 315.55706787109375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.904, loss_val: nan, pos_over_neg: 390.30303955078125 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8989, loss_val: nan, pos_over_neg: 358.6590270996094 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8842, loss_val: nan, pos_over_neg: 929.069580078125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8979, loss_val: nan, pos_over_neg: 2693.594482421875 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.9023, loss_val: nan, pos_over_neg: 441.9612731933594 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.9059, loss_val: nan, pos_over_neg: 205.0383758544922 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.897, loss_val: nan, pos_over_neg: 403.0819396972656 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8935, loss_val: nan, pos_over_neg: 594.8404541015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.9226, loss_val: nan, pos_over_neg: 446.7851867675781 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8817, loss_val: nan, pos_over_neg: 642.7399291992188 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.9046, loss_val: nan, pos_over_neg: 385.2395935058594 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8775, loss_val: nan, pos_over_neg: 3116.760498046875 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.9046, loss_val: nan, pos_over_neg: 681.8577270507812 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8933, loss_val: nan, pos_over_neg: 557.9224853515625 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8842, loss_val: nan, pos_over_neg: 815.8115234375 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8941, loss_val: nan, pos_over_neg: 1435.7008056640625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: -3572.52880859375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8986, loss_val: nan, pos_over_neg: 948.9005126953125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.908, loss_val: nan, pos_over_neg: 495.39227294921875 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 327.6160583496094 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 240.2061767578125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 1182.4661865234375 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8908, loss_val: nan, pos_over_neg: 393.3223876953125 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.903, loss_val: nan, pos_over_neg: 473.62921142578125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 548.7803955078125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8903, loss_val: nan, pos_over_neg: 912.80712890625 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8865, loss_val: nan, pos_over_neg: 741.574462890625 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1045.1566162109375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.9072, loss_val: nan, pos_over_neg: 996.9354858398438 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8936, loss_val: nan, pos_over_neg: 714.3323364257812 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 4443.3203125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8881, loss_val: nan, pos_over_neg: 1672.697265625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: -4186.8828125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8992, loss_val: nan, pos_over_neg: 720.3392333984375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8942, loss_val: nan, pos_over_neg: 380.22381591796875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 304.23712158203125 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 584.4807739257812 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.9053, loss_val: nan, pos_over_neg: 357.32904052734375 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8892, loss_val: nan, pos_over_neg: 427.36572265625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: 581.3705444335938 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8988, loss_val: nan, pos_over_neg: 775.3858032226562 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8949, loss_val: nan, pos_over_neg: 3319.923828125 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 786.3154907226562 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8834, loss_val: nan, pos_over_neg: 1110.768310546875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.9044, loss_val: nan, pos_over_neg: 559.1914672851562 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 1414.2291259765625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 622.9440307617188 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.9101, loss_val: nan, pos_over_neg: 371.6031494140625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8801, loss_val: nan, pos_over_neg: 2084.5986328125 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.9218, loss_val: nan, pos_over_neg: 539.111328125 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.8858, loss_val: nan, pos_over_neg: 528.6455078125 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1113.7921142578125 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 2485.893798828125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 2238.60107421875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: -78314.8203125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8907, loss_val: nan, pos_over_neg: 361.4979248046875 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8956, loss_val: nan, pos_over_neg: 1065.4598388671875 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8958, loss_val: nan, pos_over_neg: 395.3732604980469 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 480.07452392578125 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.887, loss_val: nan, pos_over_neg: 1355.6312255859375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1013.1441650390625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.9027, loss_val: nan, pos_over_neg: 631.55078125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8843, loss_val: nan, pos_over_neg: 2411.853271484375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.9052, loss_val: nan, pos_over_neg: 683.43017578125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 6179.0068359375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 1073.6466064453125 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 666.1695556640625 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.8521, loss_val: nan, pos_over_neg: -4215.62646484375 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8894, loss_val: nan, pos_over_neg: 719.5487060546875 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8908, loss_val: nan, pos_over_neg: 1202.126220703125 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 835.7968139648438 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 1273.21826171875 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8902, loss_val: nan, pos_over_neg: 834.9902954101562 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8772, loss_val: nan, pos_over_neg: 1057.2138671875 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.9017, loss_val: nan, pos_over_neg: 937.3350219726562 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 621.3346557617188 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 330.17047119140625 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8998, loss_val: nan, pos_over_neg: 442.3929138183594 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.9137, loss_val: nan, pos_over_neg: 538.919677734375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.8848, loss_val: nan, pos_over_neg: 3538.275146484375 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8827, loss_val: nan, pos_over_neg: 593.3997192382812 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8995, loss_val: nan, pos_over_neg: 389.2201843261719 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8833, loss_val: nan, pos_over_neg: 1455.8563232421875 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8809, loss_val: nan, pos_over_neg: 1060.9171142578125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.9007, loss_val: nan, pos_over_neg: 456.54168701171875 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 523.7120361328125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8979, loss_val: nan, pos_over_neg: 579.14208984375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 1767.9774169921875 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8975, loss_val: nan, pos_over_neg: 761.630859375 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8873, loss_val: nan, pos_over_neg: 316.27978515625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 407.7854919433594 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8814, loss_val: nan, pos_over_neg: 9328.041015625 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8891, loss_val: nan, pos_over_neg: 1091.346923828125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 371.0654296875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 426.6227111816406 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8963, loss_val: nan, pos_over_neg: 515.1836547851562 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 423.2566833496094 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.889, loss_val: nan, pos_over_neg: 977.5902709960938 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 703.000244140625 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 333.4101867675781 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 356.87066650390625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8766, loss_val: nan, pos_over_neg: 7111.833984375 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8868, loss_val: nan, pos_over_neg: 656.5270385742188 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8813, loss_val: nan, pos_over_neg: 576.936767578125 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8885, loss_val: nan, pos_over_neg: 2044.09716796875 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8891, loss_val: nan, pos_over_neg: 545.208984375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8724, loss_val: nan, pos_over_neg: 477.7451171875 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8855, loss_val: nan, pos_over_neg: 329.15045166015625 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8714, loss_val: nan, pos_over_neg: 603.1397705078125 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.9005, loss_val: nan, pos_over_neg: 340.0184631347656 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8762, loss_val: nan, pos_over_neg: 5127.49169921875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 464.58184814453125 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 15717.849609375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.9013, loss_val: nan, pos_over_neg: 596.1564331054688 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8906, loss_val: nan, pos_over_neg: 2840.1728515625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: -10099.3466796875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.9067, loss_val: nan, pos_over_neg: 2444.637939453125 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8914, loss_val: nan, pos_over_neg: 729.0769653320312 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 1164.853271484375 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8753, loss_val: nan, pos_over_neg: 735.99169921875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 1101.5916748046875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8756, loss_val: nan, pos_over_neg: 1886.3458251953125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 379.5143127441406 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 245.16830444335938 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8935, loss_val: nan, pos_over_neg: 320.3105773925781 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8925, loss_val: nan, pos_over_neg: 388.953369140625 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 624.9525756835938 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8963, loss_val: nan, pos_over_neg: 1972.4825439453125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.9067, loss_val: nan, pos_over_neg: 615.9689331054688 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.9024, loss_val: nan, pos_over_neg: 7975.8974609375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.891, loss_val: nan, pos_over_neg: 581.0065307617188 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 3847.69482421875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8857, loss_val: nan, pos_over_neg: 2575.20751953125 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8955, loss_val: nan, pos_over_neg: 1230.100830078125 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8967, loss_val: nan, pos_over_neg: 498.73468017578125 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8842, loss_val: nan, pos_over_neg: 512.35986328125 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8812, loss_val: nan, pos_over_neg: 1124.7130126953125 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8848, loss_val: nan, pos_over_neg: 1650.504638671875 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8876, loss_val: nan, pos_over_neg: 473.10028076171875 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8788, loss_val: nan, pos_over_neg: 587.9468994140625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8833, loss_val: nan, pos_over_neg: 337.086669921875 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8823, loss_val: nan, pos_over_neg: 1082.449951171875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.889, loss_val: nan, pos_over_neg: 614.8693237304688 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8999, loss_val: nan, pos_over_neg: 951.01953125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8768, loss_val: nan, pos_over_neg: 561.9002685546875 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.9017, loss_val: nan, pos_over_neg: 759.2863159179688 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 403.1372985839844 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 543.1517944335938 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 555.5492553710938 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8871, loss_val: nan, pos_over_neg: 346.3279113769531 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 345.873046875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 493.41546630859375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8838, loss_val: nan, pos_over_neg: 337.18487548828125 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.88, loss_val: nan, pos_over_neg: 1545.266845703125 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8939, loss_val: nan, pos_over_neg: 1547.3912353515625 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 531.322021484375 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8928, loss_val: nan, pos_over_neg: 357.82159423828125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8782, loss_val: nan, pos_over_neg: 272.5589294433594 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8741, loss_val: nan, pos_over_neg: 492.1314392089844 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8974, loss_val: nan, pos_over_neg: 326.48065185546875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 1216.85791015625 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.9297, loss_val: nan, pos_over_neg: 436.80645751953125 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8865, loss_val: nan, pos_over_neg: 893.5662231445312 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.892, loss_val: nan, pos_over_neg: 261.33734130859375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8888, loss_val: nan, pos_over_neg: 427.0809631347656 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8939, loss_val: nan, pos_over_neg: 831.1725463867188 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8951, loss_val: nan, pos_over_neg: 334.2983703613281 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.8839, loss_val: nan, pos_over_neg: 531.466064453125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 981.5607299804688 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8915, loss_val: nan, pos_over_neg: 382.7576599121094 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8875, loss_val: nan, pos_over_neg: 445.4129943847656 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 487.9630432128906 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8662, loss_val: nan, pos_over_neg: 5944.11474609375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8924, loss_val: nan, pos_over_neg: 415.8723449707031 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 248.1579132080078 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8945, loss_val: nan, pos_over_neg: 443.0042419433594 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 499.30230712890625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8821, loss_val: nan, pos_over_neg: 927.263671875 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 362.8209533691406 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 395.5877685546875 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8908, loss_val: nan, pos_over_neg: 5053.0546875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 953.9458618164062 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.899, loss_val: nan, pos_over_neg: 763.0831909179688 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8931, loss_val: nan, pos_over_neg: 349.7425231933594 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 991.4747924804688 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.9032, loss_val: nan, pos_over_neg: 477.1410217285156 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 347.8028564453125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 1368.3323974609375 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.893, loss_val: nan, pos_over_neg: 1188.4520263671875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 343.6553649902344 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.893, loss_val: nan, pos_over_neg: 276.8127136230469 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 338.57196044921875 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 369.64404296875 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8983, loss_val: nan, pos_over_neg: 215.98313903808594 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 461.5781555175781 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 21808.455078125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.9069, loss_val: nan, pos_over_neg: 536.5988159179688 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.879, loss_val: nan, pos_over_neg: 849.07080078125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 1079.9603271484375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8793, loss_val: nan, pos_over_neg: 358.15484619140625 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8932, loss_val: nan, pos_over_neg: 401.9375915527344 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8737, loss_val: nan, pos_over_neg: 2590.065673828125 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.885, loss_val: nan, pos_over_neg: 234.39645385742188 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8857, loss_val: nan, pos_over_neg: 798.9495239257812 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8892, loss_val: nan, pos_over_neg: 652.3558959960938 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: -2335.291259765625 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8872, loss_val: nan, pos_over_neg: 722.4969482421875 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8953, loss_val: nan, pos_over_neg: 522.1619262695312 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 1327.9239501953125 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8834, loss_val: nan, pos_over_neg: 717.2515869140625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 241.59291076660156 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8789, loss_val: nan, pos_over_neg: 570.6428833007812 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: 529.8562622070312 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 1039.93896484375 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8998, loss_val: nan, pos_over_neg: 391.42041015625 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8853, loss_val: nan, pos_over_neg: 331.86016845703125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8777, loss_val: nan, pos_over_neg: 1220.574462890625 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8893, loss_val: nan, pos_over_neg: 420.43743896484375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1553.9638671875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 522.7280883789062 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8734, loss_val: nan, pos_over_neg: 325.99017333984375 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8993, loss_val: nan, pos_over_neg: 7282.27392578125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 830.2398071289062 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.9006, loss_val: nan, pos_over_neg: 9316.8876953125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8874, loss_val: nan, pos_over_neg: 776.4791259765625 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 2068.126220703125 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8886, loss_val: nan, pos_over_neg: 2426.76904296875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8959, loss_val: nan, pos_over_neg: 343.0646667480469 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8984, loss_val: nan, pos_over_neg: 446.21490478515625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 236.67648315429688 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 425.5094299316406 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 311.9241027832031 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 671.2755737304688 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8802, loss_val: nan, pos_over_neg: 663.5509033203125 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 142149.0625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8824, loss_val: nan, pos_over_neg: 24404.373046875 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8935, loss_val: nan, pos_over_neg: 508.4715881347656 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8962, loss_val: nan, pos_over_neg: 491.8982849121094 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8805, loss_val: nan, pos_over_neg: 483.77252197265625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8923, loss_val: nan, pos_over_neg: 548.2720947265625 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 800.8330078125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 387.18310546875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8959, loss_val: nan, pos_over_neg: 373.8369140625 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 554.834228515625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 653.4716186523438 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8851, loss_val: nan, pos_over_neg: 242.2792205810547 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 1122.3299560546875 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8937, loss_val: nan, pos_over_neg: -2656.113525390625 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8826, loss_val: nan, pos_over_neg: 290.979248046875 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8758, loss_val: nan, pos_over_neg: 243.91746520996094 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.896, loss_val: nan, pos_over_neg: 346.87835693359375 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 766.84228515625 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8964, loss_val: nan, pos_over_neg: 332.3446044921875 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8706, loss_val: nan, pos_over_neg: 809.9148559570312 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8689, loss_val: nan, pos_over_neg: 219.88645935058594 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.885, loss_val: nan, pos_over_neg: 1326.450927734375 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 70374.8046875 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 791.6876831054688 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8866, loss_val: nan, pos_over_neg: 1581.553466796875 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8929, loss_val: nan, pos_over_neg: 578.4685668945312 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8929, loss_val: nan, pos_over_neg: 571.5304565429688 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 1340.9154052734375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 3018.515380859375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 815.8659057617188 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8866, loss_val: nan, pos_over_neg: 1592.0966796875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.882, loss_val: nan, pos_over_neg: 2422.795166015625 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 988.4677124023438 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 799.4517211914062 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8873, loss_val: nan, pos_over_neg: 341.09466552734375 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8907, loss_val: nan, pos_over_neg: 1195.013671875 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: -9023.33984375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1670.12158203125 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8719, loss_val: nan, pos_over_neg: 922.445556640625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8799, loss_val: nan, pos_over_neg: 644.0513305664062 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8916, loss_val: nan, pos_over_neg: 446.95928955078125 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 784.9315795898438 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 1533.5712890625 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8778, loss_val: nan, pos_over_neg: -40567.41015625 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: -3958.721923828125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.882, loss_val: nan, pos_over_neg: 994.6589965820312 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8706, loss_val: nan, pos_over_neg: 539.5186767578125 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 673.7045288085938 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 5179.3525390625 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 329.97857666015625 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 294.71075439453125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 525.23583984375 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 291.6466064453125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8805, loss_val: nan, pos_over_neg: 457.9380187988281 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 384.7767639160156 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 887.9707641601562 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8763, loss_val: nan, pos_over_neg: 645.5285034179688 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 1213.0953369140625 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 1031.745361328125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.8674, loss_val: nan, pos_over_neg: 285.5088195800781 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8964, loss_val: nan, pos_over_neg: 202.79306030273438 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 265.3450927734375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 334.2889099121094 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 398.3818664550781 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.9035, loss_val: nan, pos_over_neg: 274.4458923339844 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8907, loss_val: nan, pos_over_neg: 442.0044860839844 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.898, loss_val: nan, pos_over_neg: 3885.30712890625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8773, loss_val: nan, pos_over_neg: 506.64117431640625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8841, loss_val: nan, pos_over_neg: 822.8489379882812 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: 319.5180969238281 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8812, loss_val: nan, pos_over_neg: 205.345703125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8874, loss_val: nan, pos_over_neg: 296.01361083984375 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 435.3320617675781 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8859, loss_val: nan, pos_over_neg: 910.041259765625 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 1252.499755859375 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 519.13671875 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8816, loss_val: nan, pos_over_neg: 390.9224548339844 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.8889, loss_val: nan, pos_over_neg: 394.8148498535156 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.892, loss_val: nan, pos_over_neg: 345.1260070800781 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 888.9567260742188 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8618, loss_val: nan, pos_over_neg: 715.395263671875 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8775, loss_val: nan, pos_over_neg: 379.1897277832031 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.8879, loss_val: nan, pos_over_neg: 277.3395690917969 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 564.8689575195312 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 498.3711853027344 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8936, loss_val: nan, pos_over_neg: 398.169189453125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8841, loss_val: nan, pos_over_neg: 331.9107971191406 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 1419.0880126953125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 53185.87890625 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 684.1962280273438 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8735, loss_val: nan, pos_over_neg: 730.8394775390625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8754, loss_val: nan, pos_over_neg: 226.2207489013672 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8816, loss_val: nan, pos_over_neg: 415.3807678222656 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 657.4954833984375 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8592, loss_val: nan, pos_over_neg: 368.73236083984375 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8824, loss_val: nan, pos_over_neg: 472.2039794921875 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.9097, loss_val: nan, pos_over_neg: 333.66717529296875 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 803.5322875976562 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8853, loss_val: nan, pos_over_neg: -32693.578125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8886, loss_val: nan, pos_over_neg: 3492.388427734375 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8864, loss_val: nan, pos_over_neg: 622.8615112304688 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8809, loss_val: nan, pos_over_neg: 933.62451171875 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8849, loss_val: nan, pos_over_neg: 4846.38671875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8933, loss_val: nan, pos_over_neg: 624.554443359375 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8909, loss_val: nan, pos_over_neg: 555.3707275390625 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8915, loss_val: nan, pos_over_neg: 305.4637145996094 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8719, loss_val: nan, pos_over_neg: 250.62066650390625 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 765.9854125976562 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.881, loss_val: nan, pos_over_neg: 587.4918823242188 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.895, loss_val: nan, pos_over_neg: 589.5160522460938 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.8937, loss_val: nan, pos_over_neg: -36412.11328125 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 4664.31201171875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8949, loss_val: nan, pos_over_neg: 4779.28076171875 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8822, loss_val: nan, pos_over_neg: 524.1141357421875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8718, loss_val: nan, pos_over_neg: 913.1514892578125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8795, loss_val: nan, pos_over_neg: 662.965087890625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 2740.090576171875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.8931, loss_val: nan, pos_over_neg: 554.7077026367188 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 1029.3900146484375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8743, loss_val: nan, pos_over_neg: 305.09326171875 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 274.51763916015625 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8722, loss_val: nan, pos_over_neg: 1308.03955078125 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.9028, loss_val: nan, pos_over_neg: 377.28900146484375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8797, loss_val: nan, pos_over_neg: 429.542724609375 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8769, loss_val: nan, pos_over_neg: 682.3121337890625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8718, loss_val: nan, pos_over_neg: 896.4800415039062 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8848, loss_val: nan, pos_over_neg: 805.6010131835938 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 524.964111328125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 468.4932556152344 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8805, loss_val: nan, pos_over_neg: 499.4515380859375 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8775, loss_val: nan, pos_over_neg: 925.756103515625 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8724, loss_val: nan, pos_over_neg: 1139.93798828125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8803, loss_val: nan, pos_over_neg: 7139.84619140625 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 106878.46875 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8627, loss_val: nan, pos_over_neg: 969.404052734375 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8696, loss_val: nan, pos_over_neg: 439.95660400390625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 378.12255859375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.8875, loss_val: nan, pos_over_neg: 295.49029541015625 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.895, loss_val: nan, pos_over_neg: 325.7441711425781 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8916, loss_val: nan, pos_over_neg: 735.8640747070312 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 1098.8482666015625 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.871, loss_val: nan, pos_over_neg: 10962.294921875 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: -9137.341796875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8933, loss_val: nan, pos_over_neg: 1206.1697998046875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8922, loss_val: nan, pos_over_neg: 976.2044067382812 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 1930.3194580078125 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 437.67852783203125 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 1759.302734375 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8765, loss_val: nan, pos_over_neg: 789.0890502929688 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8784, loss_val: nan, pos_over_neg: 318.4605712890625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 421.6560974121094 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8902, loss_val: nan, pos_over_neg: 341.195068359375 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 300.76959228515625 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 452.689697265625 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8731, loss_val: nan, pos_over_neg: 572.9445190429688 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.8595, loss_val: nan, pos_over_neg: 429.3912658691406 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 606.3817749023438 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8837, loss_val: nan, pos_over_neg: 713.945556640625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8757, loss_val: nan, pos_over_neg: 566.6756591796875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8779, loss_val: nan, pos_over_neg: 1806.934814453125 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 6084.78369140625 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8884, loss_val: nan, pos_over_neg: 2026.9635009765625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 1771.9432373046875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 791.0905151367188 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8569, loss_val: nan, pos_over_neg: 3551.06689453125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8912, loss_val: nan, pos_over_neg: 670.3289794921875 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.881, loss_val: nan, pos_over_neg: 268.8786926269531 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 399.42620849609375 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 381.9920349121094 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8841, loss_val: nan, pos_over_neg: 2272.904296875 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.891, loss_val: nan, pos_over_neg: -8961.0556640625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8849, loss_val: nan, pos_over_neg: 639.0338745117188 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 1582.318603515625 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8836, loss_val: nan, pos_over_neg: 910.5545043945312 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 1984.9639892578125 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 535.238037109375 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 820.094482421875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 528.517578125 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.893, loss_val: nan, pos_over_neg: 292.9237976074219 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8687, loss_val: nan, pos_over_neg: 773.1793823242188 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 656.4672241210938 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8762, loss_val: nan, pos_over_neg: 1130.901611328125 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 1215.0367431640625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [51:15<128593:49:30, 1543.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 1075.176513671875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 6245.3291015625 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8873, loss_val: nan, pos_over_neg: 355.1353454589844 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 1739.677978515625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8695, loss_val: nan, pos_over_neg: 1063.34521484375 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 4997.0244140625 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 4643.34716796875 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8731, loss_val: nan, pos_over_neg: 2999.383544921875 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 543.8554077148438 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.871, loss_val: nan, pos_over_neg: 452.1960754394531 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8793, loss_val: nan, pos_over_neg: 191.77870178222656 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 462.4013977050781 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8832, loss_val: nan, pos_over_neg: 876.7869262695312 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 593.9888305664062 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8832, loss_val: nan, pos_over_neg: 667.9849853515625 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 323.50323486328125 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 1888.3741455078125 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8932, loss_val: nan, pos_over_neg: 585.3035888671875 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 3394.8193359375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.8719, loss_val: nan, pos_over_neg: 887.2806396484375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 432.22039794921875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 270.9685974121094 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 545.8938598632812 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8822, loss_val: nan, pos_over_neg: 1583.386962890625 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 1960.891357421875 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8919, loss_val: nan, pos_over_neg: 432.4559020996094 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8696, loss_val: nan, pos_over_neg: 717.6377563476562 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 960.196044921875 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.864, loss_val: nan, pos_over_neg: 2022.40673828125 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 5089.40380859375 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 602.0731811523438 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8851, loss_val: nan, pos_over_neg: 326.2461242675781 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.862, loss_val: nan, pos_over_neg: 367.5066223144531 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 2383.094970703125 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8612, loss_val: nan, pos_over_neg: 567.879150390625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8662, loss_val: nan, pos_over_neg: 2275.517822265625 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 524.3797607421875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.886, loss_val: nan, pos_over_neg: 612.2474365234375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8785, loss_val: nan, pos_over_neg: 3342.60400390625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 1648.2421875 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 4595.8603515625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.9106, loss_val: nan, pos_over_neg: 311.8813781738281 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 443.65875244140625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 1162.101318359375 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 873.6826171875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1738.14697265625 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: -6518.93994140625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 490.4224853515625 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8765, loss_val: nan, pos_over_neg: 558.9411010742188 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8867, loss_val: nan, pos_over_neg: 799.1810302734375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8693, loss_val: nan, pos_over_neg: 557.5020141601562 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 402.30548095703125 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 591.1005249023438 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 641.1077880859375 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1161.3863525390625 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 3311.521728515625 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 1091.151123046875 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 331.8443298339844 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 8991.48828125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8799, loss_val: nan, pos_over_neg: 442.14501953125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 614.81640625 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.8615, loss_val: nan, pos_over_neg: 376.1037292480469 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 895.2864990234375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.858, loss_val: nan, pos_over_neg: 468.9333801269531 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8756, loss_val: nan, pos_over_neg: 601.141845703125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8741, loss_val: nan, pos_over_neg: 507.6368408203125 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8884, loss_val: nan, pos_over_neg: 887.4764404296875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 572.3271484375 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 513.8126220703125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.887, loss_val: nan, pos_over_neg: 933.9344482421875 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 1297.7496337890625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 9654.0556640625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.879, loss_val: nan, pos_over_neg: 816.9501342773438 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 831.2824096679688 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 923.8306884765625 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: 933.5872802734375 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8656, loss_val: nan, pos_over_neg: 628.0035400390625 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.868, loss_val: nan, pos_over_neg: 461.08941650390625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.877, loss_val: nan, pos_over_neg: 730.2784423828125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1391.01171875 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 403.7232666015625 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.876, loss_val: nan, pos_over_neg: 693.6778564453125 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.867, loss_val: nan, pos_over_neg: 694.7367553710938 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 1107.1572265625 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 1257.7288818359375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8593, loss_val: nan, pos_over_neg: 397.6473388671875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 542.8385009765625 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.88, loss_val: nan, pos_over_neg: 2310.9765625 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8688, loss_val: nan, pos_over_neg: 616.1572875976562 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8659, loss_val: nan, pos_over_neg: 426.4395446777344 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 817.4014282226562 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8862, loss_val: nan, pos_over_neg: 625.2652587890625 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 1489.786865234375 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 2902.106689453125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 560.3138427734375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 2646.97998046875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8754, loss_val: nan, pos_over_neg: 2284.910888671875 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8799, loss_val: nan, pos_over_neg: 3584.594482421875 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.8708, loss_val: nan, pos_over_neg: 602.47412109375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 374.5241394042969 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 273.1947937011719 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 934.3021240234375 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 573.803466796875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8697, loss_val: nan, pos_over_neg: 284.7366638183594 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 652.1111450195312 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 750.2005004882812 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 529.9183959960938 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 585.62060546875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 1765.7197265625 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 577.4762573242188 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8652, loss_val: nan, pos_over_neg: 2297.88671875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 943.4608154296875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8742, loss_val: nan, pos_over_neg: 1442.147705078125 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 348.2008056640625 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 391.04168701171875 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8877, loss_val: nan, pos_over_neg: 251.00904846191406 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8608, loss_val: nan, pos_over_neg: 765.5843505859375 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 1255.4815673828125 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8929, loss_val: nan, pos_over_neg: 365.936279296875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 1356.2216796875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 805.7055053710938 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 980.322509765625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 504.180908203125 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8564, loss_val: nan, pos_over_neg: 1714.7630615234375 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 274.7056884765625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 205.72996520996094 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: 369.5574645996094 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 563.0384521484375 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 3629.805908203125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 513.1337280273438 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8652, loss_val: nan, pos_over_neg: 358.1010437011719 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 796.3998413085938 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 396.4903259277344 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 1003.0144653320312 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 714.5008544921875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 338.38897705078125 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 144.02392578125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 596.3695068359375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8887, loss_val: nan, pos_over_neg: 664.2579956054688 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 1022.3934936523438 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8772, loss_val: nan, pos_over_neg: 351.6241149902344 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 481.7894287109375 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8672, loss_val: nan, pos_over_neg: 1125.246826171875 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: 583.0029907226562 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 311.0412902832031 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 658.407470703125 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: 490.03009033203125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 370.50830078125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 584.31103515625 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8648, loss_val: nan, pos_over_neg: 248.98089599609375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8804, loss_val: nan, pos_over_neg: 648.3477172851562 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8862, loss_val: nan, pos_over_neg: 539.1656494140625 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 1903.995849609375 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 470.1190490722656 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 903.1780395507812 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8814, loss_val: nan, pos_over_neg: 268.8365173339844 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 712.3817749023438 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8619, loss_val: nan, pos_over_neg: 940.0736694335938 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 639.8726196289062 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 364.759765625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.877, loss_val: nan, pos_over_neg: 1295.3055419921875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8914, loss_val: nan, pos_over_neg: 952.1818237304688 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: -7391.7548828125 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 719.8208618164062 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.8714, loss_val: nan, pos_over_neg: 322.4218444824219 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8925, loss_val: nan, pos_over_neg: 1211.4786376953125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8797, loss_val: nan, pos_over_neg: 510.75146484375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1396.8419189453125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8666, loss_val: nan, pos_over_neg: 399.9798889160156 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8628, loss_val: nan, pos_over_neg: 414.46978759765625 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8615, loss_val: nan, pos_over_neg: 436.56689453125 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8795, loss_val: nan, pos_over_neg: 402.2867736816406 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8778, loss_val: nan, pos_over_neg: 298.0601806640625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 218.41915893554688 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1075.7552490234375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 508.6595458984375 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.9042, loss_val: nan, pos_over_neg: 581.7366943359375 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 540.5795288085938 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 1083.8953857421875 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8625, loss_val: nan, pos_over_neg: 425.75897216796875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: 598.8929443359375 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 474.13671875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8731, loss_val: nan, pos_over_neg: 282.3034362792969 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 539.6220703125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8773, loss_val: nan, pos_over_neg: 1666.28076171875 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8761, loss_val: nan, pos_over_neg: 1210.2955322265625 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 749.868896484375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 1133.2200927734375 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8709, loss_val: nan, pos_over_neg: 511.7240295410156 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 1096.329833984375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8844, loss_val: nan, pos_over_neg: 290.720947265625 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.877, loss_val: nan, pos_over_neg: 556.4549560546875 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 400.2999267578125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8734, loss_val: nan, pos_over_neg: 476.8496398925781 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.8753, loss_val: nan, pos_over_neg: 886.701904296875 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8966, loss_val: nan, pos_over_neg: 310.0042419433594 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8766, loss_val: nan, pos_over_neg: 521.3740844726562 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 1471.786865234375 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 341.41680908203125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 2794.212890625 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.89, loss_val: nan, pos_over_neg: 1592.7725830078125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 633.0692749023438 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: 233.9544677734375 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 309.1622314453125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 297.6532897949219 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 392.19610595703125 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 372.8125305175781 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8825, loss_val: nan, pos_over_neg: 1170.86474609375 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8871, loss_val: nan, pos_over_neg: 689.5963745117188 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8855, loss_val: nan, pos_over_neg: 918.1259765625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 887.2156372070312 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8709, loss_val: nan, pos_over_neg: 372.27044677734375 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: 299.1202087402344 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8606, loss_val: nan, pos_over_neg: 1054.76953125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8594, loss_val: nan, pos_over_neg: 439.40771484375 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 302.4642639160156 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8655, loss_val: nan, pos_over_neg: 339.68701171875 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 780.3733520507812 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8722, loss_val: nan, pos_over_neg: 2231.85009765625 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8828, loss_val: nan, pos_over_neg: 914.815185546875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8823, loss_val: nan, pos_over_neg: 514.0079956054688 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 344.5271911621094 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 762.9468994140625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8773, loss_val: nan, pos_over_neg: 173.73849487304688 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.883, loss_val: nan, pos_over_neg: 284.9039001464844 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8844, loss_val: nan, pos_over_neg: 331.09716796875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8743, loss_val: nan, pos_over_neg: 725.8280639648438 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8651, loss_val: nan, pos_over_neg: 590.068359375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 534.75146484375 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 323.7666931152344 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8618, loss_val: nan, pos_over_neg: 838.58642578125 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8676, loss_val: nan, pos_over_neg: 1209.07763671875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 508.37548828125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 446.8233642578125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 531.4090576171875 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 357.5298156738281 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8666, loss_val: nan, pos_over_neg: 735.4611206054688 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 399.1441650390625 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 538.1569213867188 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -4625.30224609375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 837.5424194335938 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 472.31817626953125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 722.05908203125 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 718.9353637695312 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 644.680908203125 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 1279.848876953125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 526.2493896484375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1630.385498046875 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1041.918701171875 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: 558.5006713867188 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: 357.6961669921875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.876, loss_val: nan, pos_over_neg: 433.65484619140625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1319.66015625 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 696.2008056640625 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 1533.298583984375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 545.6492919921875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8747, loss_val: nan, pos_over_neg: 521.754150390625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 1020.3488159179688 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 1938.970703125 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 384.027099609375 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 623.4685668945312 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 501.5295104980469 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 695.5584716796875 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8661, loss_val: nan, pos_over_neg: 2061.42822265625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 770.4571533203125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 2253.27783203125 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8735, loss_val: nan, pos_over_neg: 546.16064453125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 412.6590881347656 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 994.4027099609375 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 1317.000732421875 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.8737, loss_val: nan, pos_over_neg: 1429.2412109375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 336.7294921875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8621, loss_val: nan, pos_over_neg: 650.4860229492188 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.8726, loss_val: nan, pos_over_neg: 534.0308227539062 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 968.074462890625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8816, loss_val: nan, pos_over_neg: 429.2969055175781 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 281.4535217285156 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: 220.11976623535156 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.8694, loss_val: nan, pos_over_neg: 213.42129516601562 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 315.1135559082031 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.856, loss_val: nan, pos_over_neg: 393.583984375 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 226.6142578125 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8596, loss_val: nan, pos_over_neg: 589.6156616210938 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 789.3516845703125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 3459.49462890625 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8722, loss_val: nan, pos_over_neg: 854.5340576171875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 1013.1727905273438 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 221.9559326171875 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.857, loss_val: nan, pos_over_neg: 362.2867736816406 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8671, loss_val: nan, pos_over_neg: 608.814453125 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 703.6651000976562 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 1449.37646484375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8925, loss_val: nan, pos_over_neg: 187.80938720703125 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 605.8714599609375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.854, loss_val: nan, pos_over_neg: 590.1094360351562 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 1017.3925170898438 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8714, loss_val: nan, pos_over_neg: -3564.129638671875 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 461.349853515625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 547.3106689453125 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8793, loss_val: nan, pos_over_neg: 909.7786865234375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 361.75067138671875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 863.6995849609375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 917.5386962890625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8713, loss_val: nan, pos_over_neg: 695.3216552734375 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 1690.034912109375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 2559.393310546875 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8832, loss_val: nan, pos_over_neg: 556.2385864257812 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 1304.87646484375 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8694, loss_val: nan, pos_over_neg: 960.646728515625 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 644.548828125 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8669, loss_val: nan, pos_over_neg: 434.55841064453125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 333.318115234375 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 401.7045593261719 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8576, loss_val: nan, pos_over_neg: 482.4705810546875 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 1051.75341796875 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 1006.8344116210938 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 1565.5108642578125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.8814, loss_val: nan, pos_over_neg: 384.39727783203125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 1029.7974853515625 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 360.1948547363281 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8819, loss_val: nan, pos_over_neg: 646.34130859375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 898.9514770507812 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 1403.008056640625 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 1766.3531494140625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8518, loss_val: nan, pos_over_neg: -4241.66943359375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8548, loss_val: nan, pos_over_neg: -4191.185546875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 621.191162109375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 408.08746337890625 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8563, loss_val: nan, pos_over_neg: 514.2671508789062 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8766, loss_val: nan, pos_over_neg: 790.348388671875 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 484.1797180175781 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 557.0962524414062 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 1910.7216796875 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 857.7520141601562 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: -2547094.0 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8839, loss_val: nan, pos_over_neg: 2844.94091796875 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 1227.699951171875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 2043.2186279296875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8656, loss_val: nan, pos_over_neg: 1020.6309814453125 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 683.3217163085938 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8843, loss_val: nan, pos_over_neg: 608.4768676757812 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.867, loss_val: nan, pos_over_neg: 653.2766723632812 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 469.05657958984375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8655, loss_val: nan, pos_over_neg: 321.680908203125 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8749, loss_val: nan, pos_over_neg: 602.4171752929688 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 597.5897827148438 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: 1155.3753662109375 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 426.7418518066406 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8992, loss_val: nan, pos_over_neg: 165.90049743652344 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 516.0029296875 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 406.4738464355469 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 453.05389404296875 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 905.6323852539062 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 549.3851928710938 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.8564, loss_val: nan, pos_over_neg: 632.2247314453125 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 303.576416015625 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8606, loss_val: nan, pos_over_neg: 720.5989379882812 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 531.1820068359375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8809, loss_val: nan, pos_over_neg: 596.673095703125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8858, loss_val: nan, pos_over_neg: 418.83978271484375 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 255.2589569091797 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8597, loss_val: nan, pos_over_neg: 193.5805206298828 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 1614.203369140625 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 394.97265625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8672, loss_val: nan, pos_over_neg: 276.6116943359375 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 683.8222045898438 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 266.56292724609375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 217.8087615966797 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1331.6214599609375 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 2289.9921875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 546.5974731445312 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 301.5432434082031 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 171.99488830566406 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8739, loss_val: nan, pos_over_neg: 291.4035339355469 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 3963.844482421875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8735, loss_val: nan, pos_over_neg: 877.3892211914062 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 568.612548828125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 325.6177062988281 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 3438.592041015625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 627.5899047851562 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8681, loss_val: nan, pos_over_neg: 320.5360107421875 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: 404.7117004394531 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 400.2588806152344 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 435.5526123046875 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.8572, loss_val: nan, pos_over_neg: -39020.57421875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8674, loss_val: nan, pos_over_neg: 1962.2811279296875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8642, loss_val: nan, pos_over_neg: 884.6886596679688 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8648, loss_val: nan, pos_over_neg: 679.3472900390625 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 1209.349365234375 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 468.0185546875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 31836.431640625 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 619.6639404296875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.87, loss_val: nan, pos_over_neg: 252.29800415039062 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8558, loss_val: nan, pos_over_neg: 485.5860595703125 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.8548, loss_val: nan, pos_over_neg: 1487.2535400390625 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8801, loss_val: nan, pos_over_neg: 494.8489074707031 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 4776.67529296875 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 945.213623046875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 937.6642456054688 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 897.7469482421875 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 575.764892578125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.87, loss_val: nan, pos_over_neg: 362.3784484863281 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8696, loss_val: nan, pos_over_neg: 263.7637634277344 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 329.08087158203125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 579.288330078125 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 450.0419921875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 519.9652709960938 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8749, loss_val: nan, pos_over_neg: 4639.177734375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8537, loss_val: nan, pos_over_neg: 676.9046020507812 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.8691, loss_val: nan, pos_over_neg: 610.6914672851562 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 827.241943359375 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 1691.2491455078125 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8767, loss_val: nan, pos_over_neg: 325.5877990722656 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 534.7273559570312 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8466, loss_val: nan, pos_over_neg: 1018.2996826171875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 656.4041748046875 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8888, loss_val: nan, pos_over_neg: 335.8367919921875 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 763.5714111328125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8558, loss_val: nan, pos_over_neg: 8946.2255859375 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8561, loss_val: nan, pos_over_neg: 1713.7073974609375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8687, loss_val: nan, pos_over_neg: 673.1533203125 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8658, loss_val: nan, pos_over_neg: 772.338623046875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8855, loss_val: nan, pos_over_neg: 291.40399169921875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 1137.8673095703125 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 1777.3609619140625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 1531.0958251953125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 7404.6552734375 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.856, loss_val: nan, pos_over_neg: 5691.376953125 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.841, loss_val: nan, pos_over_neg: 1306.958740234375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 2171.59033203125 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8772, loss_val: nan, pos_over_neg: 473.74273681640625 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8658, loss_val: nan, pos_over_neg: 864.4778442382812 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 510.8460388183594 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 258.12060546875 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 473.5105895996094 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1335.9295654296875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 759.4105224609375 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: -23276.330078125 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: 2882.6455078125 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 2012.438232421875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 2527.00048828125 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: -10330.49609375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 372.5998840332031 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8687, loss_val: nan, pos_over_neg: 378.6512451171875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8715, loss_val: nan, pos_over_neg: 682.2481079101562 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 888.1846923828125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 825.2175903320312 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 849.0145874023438 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8695, loss_val: nan, pos_over_neg: 1135.860107421875 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1576.049072265625 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8693, loss_val: nan, pos_over_neg: 785.7926025390625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 359.757568359375 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 749.1768798828125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8827, loss_val: nan, pos_over_neg: 339.3416442871094 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 1674.0111083984375 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 991.2174072265625 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8656, loss_val: nan, pos_over_neg: -5276.25634765625 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 641.1976928710938 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8638, loss_val: nan, pos_over_neg: 587.2557373046875 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 3479.39794921875 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 8065.04052734375 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -2652.373291015625 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8661, loss_val: nan, pos_over_neg: 6887.51123046875 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 1067.0196533203125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8527, loss_val: nan, pos_over_neg: 1235.5003662109375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 712.2490234375 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 786.2131958007812 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8393, loss_val: nan, pos_over_neg: 646.0137939453125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8554, loss_val: nan, pos_over_neg: 934.6567993164062 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8395, loss_val: nan, pos_over_neg: -6584.259765625 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 9565.5517578125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: -17760.73046875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1658.3154296875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 706.1901245117188 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 1191.3656005859375 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 393.9706115722656 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8602, loss_val: nan, pos_over_neg: 966.177001953125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: -12948.791015625 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 1049.5029296875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 955.2474365234375 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 669.9185791015625 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8522, loss_val: nan, pos_over_neg: 2085.2392578125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 727.03955078125 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 600.527587890625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 1055.9547119140625 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 490.3638610839844 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 409.5103454589844 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 306.95367431640625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8471, loss_val: nan, pos_over_neg: 1253.83984375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 13123.6376953125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 782.2640991210938 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1847.6956787109375 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8691, loss_val: nan, pos_over_neg: 517.2401123046875 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 1141.8946533203125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2101.15771484375 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 630.656982421875 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8812, loss_val: nan, pos_over_neg: 572.68115234375 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 721.4280395507812 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 995.5783081054688 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 2778.945068359375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8651, loss_val: nan, pos_over_neg: 413.7323913574219 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8695, loss_val: nan, pos_over_neg: 363.4141540527344 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 1324.236083984375 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 2322.294677734375 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: -6988.37548828125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 1339.4754638671875 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 2587.89306640625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -3477.9404296875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 485.7971496582031 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 715.3729858398438 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8599, loss_val: nan, pos_over_neg: 904.8485717773438 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 661.2586059570312 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 705.7615966796875 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 2202.97607421875 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8738, loss_val: nan, pos_over_neg: 346.07318115234375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 4252.22265625 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 748.9725341796875 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 267.291259765625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 486.227294921875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 291.2694396972656 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1163.99560546875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 508.2518615722656 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.862, loss_val: nan, pos_over_neg: 344.2172546386719 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8628, loss_val: nan, pos_over_neg: 700.358642578125 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8566, loss_val: nan, pos_over_neg: 1142.7076416015625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 23885.443359375 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 3242.742431640625 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1926.61572265625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 2134.08740234375 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 688.7275390625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.868, loss_val: nan, pos_over_neg: 410.5346984863281 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 602.9505615234375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8644, loss_val: nan, pos_over_neg: 471.8742370605469 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 761.359375 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 422.06683349609375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 803.9366455078125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 1189.305908203125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8592, loss_val: nan, pos_over_neg: 2384.121826171875 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8667, loss_val: nan, pos_over_neg: 3130.070556640625 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 1340.395263671875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 739.5156860351562 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8594, loss_val: nan, pos_over_neg: 787.5612182617188 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 474.64581298828125 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 338.9555969238281 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 292.4483642578125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 1267.7471923828125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 744.4164428710938 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 4232.38525390625 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: 487.2979431152344 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 964.0324096679688 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 750.3941650390625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 463.85296630859375 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8441, loss_val: nan, pos_over_neg: 527.4739379882812 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 782.6657104492188 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 12037.712890625 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 95061.203125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 1736.31494140625 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 392.7061767578125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8599, loss_val: nan, pos_over_neg: 2195.28271484375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8551, loss_val: nan, pos_over_neg: 1448.6512451171875 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8603, loss_val: nan, pos_over_neg: -17139.326171875 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8803, loss_val: nan, pos_over_neg: 288.756103515625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.869, loss_val: nan, pos_over_neg: 440.304443359375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 3650.73828125 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 2671.7265625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 944.5056762695312 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8625, loss_val: nan, pos_over_neg: 403.895263671875 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8681, loss_val: nan, pos_over_neg: 1745.553955078125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 545.7738037109375 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 851.486083984375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8628, loss_val: nan, pos_over_neg: 872.2369384765625 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8759, loss_val: nan, pos_over_neg: 354.57208251953125 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 678.7432861328125 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 368.88201904296875 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1097.1959228515625 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 628.8080444335938 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 1100.11962890625 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 1318.5677490234375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 937.3278198242188 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1028.36865234375 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 513.65625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 816.1243896484375 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1601.462646484375 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8795, loss_val: nan, pos_over_neg: 406.99822998046875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1543.7552490234375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 409.19921875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8624, loss_val: nan, pos_over_neg: 332.855712890625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 340.5652770996094 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8509, loss_val: nan, pos_over_neg: 673.401123046875 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 488.64288330078125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 474.0411071777344 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 683.2166137695312 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1117.888916015625 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 750.571044921875 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 1000.20166015625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8428, loss_val: nan, pos_over_neg: 868.0068969726562 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 416.4661865234375 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8683, loss_val: nan, pos_over_neg: 240.49478149414062 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 403.9413146972656 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 616.9020385742188 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 1171.4473876953125 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 473.29150390625 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1506.303466796875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 2033.2044677734375 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8625, loss_val: nan, pos_over_neg: 571.8095092773438 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8428, loss_val: nan, pos_over_neg: 468.31488037109375 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8346, loss_val: nan, pos_over_neg: 449.4470520019531 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: 290.7755126953125 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 516.5377197265625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 516.1143798828125 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8595, loss_val: nan, pos_over_neg: 1392.5423583984375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 491.71295166015625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8666, loss_val: nan, pos_over_neg: 1802.03466796875 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: -3984.944091796875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: -5052.12451171875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 2600.091064453125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8494, loss_val: nan, pos_over_neg: 931.1320190429688 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 716.1036376953125 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 227.76443481445312 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 492.6982727050781 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 417.78277587890625 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 439.3492126464844 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 484.4576416015625 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1572.0009765625 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8608, loss_val: nan, pos_over_neg: 458.2521057128906 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8724, loss_val: nan, pos_over_neg: 291.6383361816406 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 442.6464538574219 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 901.731689453125 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1178.8233642578125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 834.4254760742188 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 2110.198974609375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: -6886.248046875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 459.7488708496094 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 1252.6463623046875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 577.9113159179688 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8561, loss_val: nan, pos_over_neg: 554.026123046875 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 688.0755004882812 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8535, loss_val: nan, pos_over_neg: 629.7623901367188 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 698.0081787109375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 413.25567626953125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 1214.0413818359375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8521, loss_val: nan, pos_over_neg: 617.390380859375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 627.9589233398438 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 788.986083984375 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 1044.9739990234375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 286.29913330078125 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.854, loss_val: nan, pos_over_neg: 540.586669921875 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 507.27252197265625 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.8644, loss_val: nan, pos_over_neg: 436.36810302734375 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 975.802490234375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 1041.6104736328125 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 567.90283203125 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 402.948486328125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 565.4600830078125 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 907.9468994140625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8647, loss_val: nan, pos_over_neg: 200.0150146484375 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 502.3478698730469 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 1441.0572509765625 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 331.79388427734375 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 916.0939331054688 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1371.5533447265625 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1321.8155517578125 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.8509, loss_val: nan, pos_over_neg: 581.592041015625 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 5875.29736328125 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8693, loss_val: nan, pos_over_neg: 986.5808715820312 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 1755.831298828125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 489.9143981933594 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 525.4821166992188 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 930.517822265625 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8638, loss_val: nan, pos_over_neg: 1393.100830078125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 430.4989013671875 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8603, loss_val: nan, pos_over_neg: 627.0699462890625 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8615, loss_val: nan, pos_over_neg: 533.424072265625 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 3316.7216796875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 1780.536865234375 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 3547.778564453125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 520.5115356445312 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8681, loss_val: nan, pos_over_neg: 600.45654296875 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 605.0275268554688 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 1279.73876953125 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8347, loss_val: nan, pos_over_neg: 765.7999877929688 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1320.394775390625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 5097.068359375 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 2572.69677734375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 2088.02392578125 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 690.2446899414062 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8648, loss_val: nan, pos_over_neg: 466.8162841796875 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8806, loss_val: nan, pos_over_neg: 371.36981201171875 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.862, loss_val: nan, pos_over_neg: 957.4724731445312 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 521.1004638671875 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 771.1326293945312 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: -6475.9345703125 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 4073.7275390625 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8597, loss_val: nan, pos_over_neg: 541.0107421875 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 584.0819091796875 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 672.967041015625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8683, loss_val: nan, pos_over_neg: 418.9327087402344 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 587.7807006835938 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8569, loss_val: nan, pos_over_neg: 882.8037109375 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1376.7900390625 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 16379.1376953125 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 3197.150390625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: -4083.77294921875 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: 7642.314453125 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 863.5057983398438 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 1105.347412109375 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.8555, loss_val: nan, pos_over_neg: 263.05712890625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1946.9718017578125 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 2868.06982421875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 860.8370361328125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1536.236572265625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 624.3844604492188 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 318.0038757324219 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1338.1160888671875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 570.1878051757812 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 2222.86181640625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 767.2025146484375 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1654.583984375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 2041.14306640625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 490.2674560546875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.858, loss_val: nan, pos_over_neg: 1136.31103515625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 504.21478271484375 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 689.6544189453125 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8519, loss_val: nan, pos_over_neg: 1861.0638427734375 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8506, loss_val: nan, pos_over_neg: -33460.58203125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1999.5877685546875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 136.09275817871094 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 823.255615234375 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8578, loss_val: nan, pos_over_neg: 544.1290283203125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 476.7257080078125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 544.3353881835938 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 400.8596496582031 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 508.796630859375 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8658, loss_val: nan, pos_over_neg: 2349.414794921875 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 938.8298950195312 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8593, loss_val: nan, pos_over_neg: 471.5934143066406 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 1149.3248291015625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 515.778564453125 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 796.0274658203125 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: -1224.8035888671875 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 1246.9814453125 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 795.2605590820312 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 2910.092041015625 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 594.3202514648438 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8683, loss_val: nan, pos_over_neg: 1655.443115234375 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8476, loss_val: nan, pos_over_neg: 325.9455261230469 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 377.5336608886719 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 305.984619140625 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 887.0179443359375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 1313.824951171875 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 873.5665283203125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8608, loss_val: nan, pos_over_neg: 761.4998168945312 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: -7494.98779296875 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8501, loss_val: nan, pos_over_neg: 20331.470703125 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 2481.849609375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1695.014892578125 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 560.004638671875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 332.9964294433594 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 441.9856872558594 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 833.6871948242188 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 1520.2452392578125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 716.6887817382812 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8691, loss_val: nan, pos_over_neg: 1508.1102294921875 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 780.6260986328125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 604.5587768554688 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.859, loss_val: nan, pos_over_neg: 1159.70849609375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8739, loss_val: nan, pos_over_neg: 374.8650817871094 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 725.9254150390625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8569, loss_val: nan, pos_over_neg: 5862.5771484375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1763.5347900390625 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 2045.86865234375 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 743.9234619140625 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 452.31304931640625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8522, loss_val: nan, pos_over_neg: 677.28564453125 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8429, loss_val: nan, pos_over_neg: -4964.22998046875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.864, loss_val: nan, pos_over_neg: 1072.6767578125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 950.5066528320312 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 3949.888671875 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 388.2855224609375 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 2158.423095703125 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 578.9176635742188 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 881.18701171875 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 2132.6708984375 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1810.9583740234375 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 542.3095703125 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 367.8229064941406 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 797.0142822265625 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8547, loss_val: nan, pos_over_neg: 755.6251831054688 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 1583.283447265625 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8391, loss_val: nan, pos_over_neg: 814.164794921875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 2079.4951171875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 443.7566223144531 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 6416.62890625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 1532.1329345703125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 8342.8251953125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 12060.2197265625 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 4934.2822265625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.867, loss_val: nan, pos_over_neg: 317.6824951171875 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 518.3070068359375 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 659.1494750976562 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: -4527.74462890625 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8537, loss_val: nan, pos_over_neg: 375.1302795410156 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 462.4061584472656 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 202.4540557861328 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8518, loss_val: nan, pos_over_neg: 2731.347900390625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 950.0594482421875 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 2009.57470703125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 646.1814575195312 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 678.7001953125 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 448.60223388671875 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1022.8273315429688 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 784.4420166015625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 486.7146911621094 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 731.7244262695312 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 565.64794921875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 1618.5443115234375 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.8551, loss_val: nan, pos_over_neg: 844.4874877929688 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 468.38763427734375 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 2690.471435546875 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.859, loss_val: nan, pos_over_neg: 483.33905029296875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: -2812.01025390625 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8577, loss_val: nan, pos_over_neg: 552.3569946289062 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 389.1024169921875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1803.05908203125 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8423, loss_val: nan, pos_over_neg: 2103.60888671875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 733.1332397460938 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8635, loss_val: nan, pos_over_neg: 422.6601257324219 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8547, loss_val: nan, pos_over_neg: 1146.8255615234375 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8612, loss_val: nan, pos_over_neg: 2255.81005859375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 884.72216796875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 711.2398681640625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 415.0723876953125 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 386.3736572265625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 486.0013122558594 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 702.3699340820312 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 924.27392578125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 626.1954956054688 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8593, loss_val: nan, pos_over_neg: 677.1514892578125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 788.25341796875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 1450.8192138671875 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8675, loss_val: nan, pos_over_neg: 5844.9560546875 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8443, loss_val: nan, pos_over_neg: 863.898193359375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8568, loss_val: nan, pos_over_neg: 873.5923461914062 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.8514, loss_val: nan, pos_over_neg: 528.235107421875 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8395, loss_val: nan, pos_over_neg: 458.3009033203125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8527, loss_val: nan, pos_over_neg: 989.3134155273438 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.859, loss_val: nan, pos_over_neg: 1727.3070068359375 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 577.0971069335938 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8743, loss_val: nan, pos_over_neg: 487.6715087890625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 1120.6617431640625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8566, loss_val: nan, pos_over_neg: 396.3429260253906 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 2207.652587890625 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 350.8941955566406 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 679.8090209960938 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8565, loss_val: nan, pos_over_neg: 469.50927734375 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: -3071.916015625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1030.5872802734375 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 659.72412109375 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 868.6398315429688 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 263.8551025390625 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 2648.52392578125 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.868, loss_val: nan, pos_over_neg: 875.9598388671875 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 22878.89453125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8503, loss_val: nan, pos_over_neg: 2114.8251953125 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 697.412841796875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8572, loss_val: nan, pos_over_neg: 583.2543334960938 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 345.40576171875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 791.8844604492188 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1251.1646728515625 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 363.63507080078125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 1373.658447265625 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1632.10009765625 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 926.3179931640625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 1184.937744140625 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 503.44232177734375 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8474, loss_val: nan, pos_over_neg: -3979.78076171875 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 341.95489501953125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 2046.746826171875 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 505.71331787109375 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 421.24017333984375 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8518, loss_val: nan, pos_over_neg: 439.4966735839844 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 1152.4095458984375 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 272.582275390625 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8533, loss_val: nan, pos_over_neg: 905.1065063476562 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1414.36279296875 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8565, loss_val: nan, pos_over_neg: 607.9937133789062 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 401.8050842285156 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8558, loss_val: nan, pos_over_neg: 3524.43994140625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1396.581298828125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:16:46<128159:55:32, 1537.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/889, loss_train: 5.8443, loss_val: nan, pos_over_neg: 2466.065673828125 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1065.8143310546875 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 981.7886962890625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 303.48638916015625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 666.6537475585938 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.8663, loss_val: nan, pos_over_neg: 701.0667114257812 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1000.0113525390625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1011.1612548828125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 821.1480102539062 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 809.2012329101562 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 340.6093444824219 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 722.5845336914062 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8471, loss_val: nan, pos_over_neg: 716.59765625 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 583.3740234375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8548, loss_val: nan, pos_over_neg: 575.225341796875 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 1146.1351318359375 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 643.8851928710938 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8577, loss_val: nan, pos_over_neg: 910.6577758789062 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 1495.2288818359375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 892.4231567382812 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 13522.478515625 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 627.9716186523438 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 375.6837463378906 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8501, loss_val: nan, pos_over_neg: 302.9676208496094 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1079.217041015625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 1847.1988525390625 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 475.89935302734375 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8672, loss_val: nan, pos_over_neg: 450.49029541015625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 996.3467407226562 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 768.3350830078125 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.8624, loss_val: nan, pos_over_neg: 288.3725891113281 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8555, loss_val: nan, pos_over_neg: 636.4075927734375 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 563.66796875 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 318.2464294433594 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 236.73912048339844 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 332.60443115234375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1043.7447509765625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: 706.9942626953125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 1165.4049072265625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 444.47125244140625 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 1335.633056640625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8578, loss_val: nan, pos_over_neg: 347.1168212890625 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8556, loss_val: nan, pos_over_neg: 663.4349975585938 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 339.3305969238281 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 388.19244384765625 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: 323.9368591308594 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 484.319580078125 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8391, loss_val: nan, pos_over_neg: 659.2789916992188 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8539, loss_val: nan, pos_over_neg: 377.6083679199219 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 231.83258056640625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 343.3299255371094 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 439.2105407714844 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 615.531005859375 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 631.3292236328125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 675.8602294921875 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8503, loss_val: nan, pos_over_neg: 411.11376953125 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1011.9521484375 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 453.3864440917969 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 693.8598022460938 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 274.2830810546875 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 570.2306518554688 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 374.52685546875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.846, loss_val: nan, pos_over_neg: 408.7663879394531 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 571.11474609375 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1940.18359375 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 285.1263427734375 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 482.9710693359375 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 654.22998046875 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8445, loss_val: nan, pos_over_neg: 521.6522216796875 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8641, loss_val: nan, pos_over_neg: 378.4000244140625 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8527, loss_val: nan, pos_over_neg: 489.22686767578125 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 1469.46435546875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.8514, loss_val: nan, pos_over_neg: 324.0608825683594 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8547, loss_val: nan, pos_over_neg: 549.0965576171875 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8426, loss_val: nan, pos_over_neg: 685.40771484375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 526.5121459960938 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 685.4268798828125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8612, loss_val: nan, pos_over_neg: 329.27557373046875 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 646.4334106445312 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 837.9232788085938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8675, loss_val: nan, pos_over_neg: 1767.0443115234375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 248.678466796875 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.8566, loss_val: nan, pos_over_neg: 385.500244140625 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8717, loss_val: nan, pos_over_neg: 290.01434326171875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8535, loss_val: nan, pos_over_neg: 684.43115234375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 630.366455078125 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 1153.9716796875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 2017.5330810546875 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1944.6859130859375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1333.1693115234375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8416, loss_val: nan, pos_over_neg: 1624.154541015625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 417.08831787109375 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 724.916015625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1827.6571044921875 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 553.3336181640625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 891.5404663085938 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 501.3624572753906 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 502.1820068359375 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 753.1337280273438 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 305.1260070800781 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 781.4602661132812 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 770.9437866210938 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1542.9141845703125 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 810.179443359375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1420.693603515625 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 552.921142578125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 330.7018127441406 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 921.185546875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 379.52838134765625 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 2345.016357421875 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.86, loss_val: nan, pos_over_neg: 649.2520141601562 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 485.4549255371094 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 710.1654663085938 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: -1195.8045654296875 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 538.5342407226562 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8534, loss_val: nan, pos_over_neg: 575.2193603515625 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 789.5206909179688 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8342, loss_val: nan, pos_over_neg: 595.0471801757812 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8521, loss_val: nan, pos_over_neg: 6681.05419921875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 1058.447021484375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1674.7215576171875 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8413, loss_val: nan, pos_over_neg: 841.6788330078125 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 488.98590087890625 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: 674.2462768554688 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 1516.1168212890625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: 976.5150756835938 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1155.6490478515625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 652.5096435546875 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: 4547.23291015625 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8474, loss_val: nan, pos_over_neg: 1617.8409423828125 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 1627.3470458984375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 1441.416748046875 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 430.79925537109375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 418.6505126953125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 888.1944580078125 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 806.5783081054688 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 1402.408203125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1850.4818115234375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 2477.26171875 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 782.3867797851562 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.841, loss_val: nan, pos_over_neg: 1059.1868896484375 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1108.6300048828125 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 18421.58984375 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 7327.00244140625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 7809.33642578125 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 560.8322143554688 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8426, loss_val: nan, pos_over_neg: 550.2835693359375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 477.3525390625 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 606.0150146484375 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 526.7847900390625 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 497.5501708984375 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 324.8565979003906 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 470.3383483886719 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.8585, loss_val: nan, pos_over_neg: 1313.53125 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 1844.1109619140625 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 931.9253540039062 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1279.5902099609375 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 862.0852661132812 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 647.5396728515625 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1041.5531005859375 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 1833.994384765625 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 2458.228271484375 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1709.4061279296875 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 785.3495483398438 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 499.63385009765625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 499.8020324707031 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 448.64361572265625 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8616, loss_val: nan, pos_over_neg: 537.8912963867188 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 1046.84375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 3684.382080078125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8669, loss_val: nan, pos_over_neg: 947.9563598632812 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8445, loss_val: nan, pos_over_neg: 3581.1962890625 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8498, loss_val: nan, pos_over_neg: 1307.2242431640625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 1381.4508056640625 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 8697.7568359375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 330.56243896484375 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1454.9625244140625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 787.3121337890625 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.844, loss_val: nan, pos_over_neg: 586.7781982421875 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8568, loss_val: nan, pos_over_neg: 286.914306640625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 273.7283630371094 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 408.0048828125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 626.2324829101562 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 2482.665283203125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 399.8953552246094 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 502.36248779296875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 736.0884399414062 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8506, loss_val: nan, pos_over_neg: 576.7042846679688 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 247.2111053466797 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 623.8096923828125 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 465.56390380859375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.8468, loss_val: nan, pos_over_neg: 884.849609375 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 309.28387451171875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 566.1668701171875 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 533.5142822265625 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 1054.186767578125 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 325.82598876953125 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8416, loss_val: nan, pos_over_neg: 463.7735900878906 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: 326.084716796875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1143.609130859375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 852.2100830078125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 487.670654296875 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 552.83984375 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 1188.759765625 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 3179.70751953125 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 432.0575256347656 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 434.7867126464844 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8463, loss_val: nan, pos_over_neg: 402.88824462890625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8655, loss_val: nan, pos_over_neg: 378.0176086425781 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 89334.609375 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8388, loss_val: nan, pos_over_neg: 530.9576416015625 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8471, loss_val: nan, pos_over_neg: -16102.4345703125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1014.0775146484375 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 11359.234375 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8448, loss_val: nan, pos_over_neg: 2220.265869140625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 713.1948852539062 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 629.9130249023438 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 3585.678955078125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 739.8639526367188 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 810.7025146484375 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.87, loss_val: nan, pos_over_neg: 1062.8092041015625 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 612.255615234375 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1247.734619140625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: -4710.86767578125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 6036.40966796875 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1733.173828125 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.845, loss_val: nan, pos_over_neg: -5322.30322265625 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 16969.662109375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 2198.506591796875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 490.8798522949219 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 2473.53271484375 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8568, loss_val: nan, pos_over_neg: 690.4813232421875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 544.22802734375 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 3285.8193359375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 2066.093017578125 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 802.565185546875 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 464.7518615722656 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 1453.5965576171875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 678.2716674804688 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1106.312255859375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: -8482.4560546875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 1309.1710205078125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 693.0930786132812 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: -6642.55322265625 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 1153.153076171875 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1564.833740234375 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 520.2141723632812 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 359.7380676269531 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 650.6002807617188 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 2243.603515625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 492.8252258300781 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -2264.5830078125 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 2500.97265625 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 667.2232055664062 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: -35938.42578125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 2091.744873046875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 798.3357543945312 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: -6324.39599609375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 784.411865234375 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 535.0762939453125 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8537, loss_val: nan, pos_over_neg: 549.2537231445312 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8456, loss_val: nan, pos_over_neg: 867.857666015625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 713.4089965820312 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 1035.400634765625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8358, loss_val: nan, pos_over_neg: 1136.5467529296875 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1576.58544921875 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1020.9102172851562 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 2125.3818359375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1124.5675048828125 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 628.8384399414062 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 589.2125244140625 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 901.6432495117188 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 961.0066528320312 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.8448, loss_val: nan, pos_over_neg: 943.8526611328125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1437.9273681640625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 391.1870422363281 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 4655.23046875 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 438.8931579589844 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1326.46728515625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 466.6582946777344 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8439, loss_val: nan, pos_over_neg: 548.6790161132812 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1615.2908935546875 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8417, loss_val: nan, pos_over_neg: 1042.8602294921875 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 7160.52294921875 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 590.4283447265625 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 309.65228271484375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8429, loss_val: nan, pos_over_neg: 458.7528076171875 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1620.9801025390625 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1264.638916015625 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 716.5562133789062 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 934.6000366210938 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 767.5760498046875 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 596.284912109375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 744.9610595703125 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8577, loss_val: nan, pos_over_neg: 534.3975830078125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8342, loss_val: nan, pos_over_neg: 644.2820434570312 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.844, loss_val: nan, pos_over_neg: 3469.015380859375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 1115.7762451171875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 328.0389404296875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 754.116455078125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 415.804443359375 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 774.8496704101562 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 421.8898010253906 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8644, loss_val: nan, pos_over_neg: 910.5714721679688 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 711.8775634765625 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 324.9044494628906 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8417, loss_val: nan, pos_over_neg: 993.8720092773438 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 622.6137084960938 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1099.7025146484375 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 438.8740234375 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 756.6986083984375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 684.1202392578125 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 495.3412780761719 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.853, loss_val: nan, pos_over_neg: 399.3384704589844 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 447.5902404785156 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 500.2681579589844 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 547.5894775390625 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 568.5001220703125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1019.30908203125 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 708.7605590820312 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 444.4467468261719 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 629.7454833984375 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 537.4309692382812 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.8463, loss_val: nan, pos_over_neg: 1212.77734375 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: -3735.404541015625 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8375, loss_val: nan, pos_over_neg: 558.4554443359375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 665.031005859375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 523.00146484375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 550.6627197265625 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 622.0020141601562 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 1464.08642578125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 1707.5489501953125 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8355, loss_val: nan, pos_over_neg: 476.94219970703125 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 600.1363525390625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8424, loss_val: nan, pos_over_neg: 913.9561157226562 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 805.003173828125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1594.533203125 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 432.34259033203125 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 315.8262023925781 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 575.0591430664062 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8476, loss_val: nan, pos_over_neg: 364.2408447265625 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 675.6857299804688 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: -3162.071044921875 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1986.5408935546875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 857.8855590820312 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1858.892578125 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 470.7485656738281 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8499, loss_val: nan, pos_over_neg: -3577.987060546875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 3988.211669921875 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 275.4056396484375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 147.75738525390625 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1148.3934326171875 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 306.4760437011719 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 351.9121398925781 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 808.6481323242188 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8494, loss_val: nan, pos_over_neg: 303.1787109375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 1033.8880615234375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 667.7832641601562 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1791.3602294921875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 466.7815246582031 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8499, loss_val: nan, pos_over_neg: 792.0918579101562 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 491.3172302246094 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 626.5946655273438 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1204.974853515625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 497.3778381347656 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 530.962890625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 1023.0457763671875 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 336.5376892089844 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8342, loss_val: nan, pos_over_neg: 424.84893798828125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 464.3945007324219 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 839.7589721679688 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 2096.83837890625 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 357.0068664550781 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 424.1230773925781 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 1779.2225341796875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1219.69287109375 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 405.77911376953125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 1189.2669677734375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 311.71832275390625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 347.0991516113281 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 507.1873474121094 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 621.3656616210938 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8534, loss_val: nan, pos_over_neg: 1581.539306640625 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 535.4490356445312 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 612.4412841796875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 1113.627685546875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 6439.58203125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 495.595458984375 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 528.7750854492188 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 441.1551513671875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 658.77490234375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 711.541015625 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1140.2374267578125 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 548.9352416992188 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 720.7538452148438 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 317.7551574707031 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 314.1240539550781 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 2179.70947265625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 522.3953857421875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 526.9987182617188 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 452.9750061035156 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: -81467.328125 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 1627.657958984375 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8572, loss_val: nan, pos_over_neg: 5564.8720703125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 4940.0537109375 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 414.8348693847656 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 1668.6590576171875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 2856.72607421875 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1594.701416015625 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 2070.817138671875 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 1286.407470703125 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 721.400390625 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8443, loss_val: nan, pos_over_neg: 341.47552490234375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 528.67041015625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 851.7413940429688 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 659.3936767578125 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 779.270751953125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 889.4264526367188 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 2217.010986328125 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 10295.9208984375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 3292.642333984375 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1622.749267578125 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: 2195.85205078125 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 982.9451904296875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8448, loss_val: nan, pos_over_neg: 559.9746704101562 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 402.71234130859375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1292.1544189453125 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 2265.04345703125 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1111.1605224609375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8428, loss_val: nan, pos_over_neg: 529.8400268554688 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 896.9290771484375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 461.690185546875 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 2152.5859375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 806.2136840820312 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1090.3818359375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 998.4944458007812 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 545.6782836914062 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1100.720947265625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 2795.94140625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8429, loss_val: nan, pos_over_neg: 7370.236328125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8442, loss_val: nan, pos_over_neg: 789.4108276367188 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 389.9306640625 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 374.3068542480469 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8358, loss_val: nan, pos_over_neg: 359.81805419921875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 415.0178527832031 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8494, loss_val: nan, pos_over_neg: 578.8458251953125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 708.3109741210938 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 445.3155212402344 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 424.2566833496094 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 1339.904052734375 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: -11575.1044921875 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 4821.4228515625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 923.3274536132812 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8393, loss_val: nan, pos_over_neg: 851.358154296875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1286.8878173828125 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 988.4202270507812 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 12277.55859375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 3477.5185546875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 300.2544860839844 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 274.83203125 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8506, loss_val: nan, pos_over_neg: 709.7845458984375 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.822, loss_val: nan, pos_over_neg: -3669.516845703125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8595, loss_val: nan, pos_over_neg: 1733.7965087890625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 2426.72314453125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 2334.940185546875 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.849, loss_val: nan, pos_over_neg: -3554.90087890625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: -2324.300537109375 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 397.2493591308594 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 807.6107788085938 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 809.52099609375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 689.3311157226562 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 351.9349365234375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 732.9874267578125 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 676.471923828125 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 911.7964477539062 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: 457.6138916015625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 922.5213012695312 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8439, loss_val: nan, pos_over_neg: 44806.5078125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1797.6190185546875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 1982.554931640625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.853, loss_val: nan, pos_over_neg: 564.9769287109375 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 1305.364013671875 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 401.23651123046875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1618.1026611328125 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 623.4459838867188 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 732.1924438476562 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 1207.6453857421875 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 3174.325439453125 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 679346.375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8391, loss_val: nan, pos_over_neg: 779.3519287109375 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 388.2063903808594 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 255.00186157226562 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1162.5164794921875 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8638, loss_val: nan, pos_over_neg: 351.86590576171875 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1619.19189453125 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8616, loss_val: nan, pos_over_neg: 301.7975769042969 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 425.9989013671875 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 984.8508911132812 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 565.9537353515625 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 745.7958984375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 803.2741088867188 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 650.1586303710938 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 679.8803100585938 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8346, loss_val: nan, pos_over_neg: 2445.9765625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 1597.0623779296875 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 582.2315063476562 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 416.11444091796875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 828.3270263671875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 2186.32568359375 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 787.950927734375 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 734.278076171875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8219, loss_val: nan, pos_over_neg: 1077.8970947265625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 790.9722290039062 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 543.7063598632812 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 386.8588562011719 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 457.2557373046875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 649.0579833984375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 1932.40625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 373.6579284667969 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 484.4259033203125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1454.6822509765625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 495.0986633300781 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1082.1192626953125 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8244, loss_val: nan, pos_over_neg: 932.945068359375 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 1399.9481201171875 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 253.8992919921875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 929.3449096679688 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1283.1146240234375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: -2817.801025390625 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 768.496337890625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1310.1851806640625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1262.6724853515625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 1188.521728515625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 555.585693359375 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8427, loss_val: nan, pos_over_neg: 749.79248046875 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 476.011962890625 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8357, loss_val: nan, pos_over_neg: 1753.5433349609375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 1120.702880859375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 3431.432861328125 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 476.10028076171875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8465, loss_val: nan, pos_over_neg: 1017.336669921875 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.843, loss_val: nan, pos_over_neg: 735.6171875 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: 500.6279296875 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8556, loss_val: nan, pos_over_neg: 217.67420959472656 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 4178.767578125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 8150.6279296875 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 333.9430847167969 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 1290.340576171875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 5617.1064453125 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8564, loss_val: nan, pos_over_neg: 388.009765625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 622.8070068359375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 2761.533447265625 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: -14254.8837890625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 814.2296752929688 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 549.0142211914062 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 765.8764038085938 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 612.96923828125 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 5503.49267578125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 859.7573852539062 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 2205.130126953125 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 388.5873107910156 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 711.0162963867188 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 486.5859680175781 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8465, loss_val: nan, pos_over_neg: 376.1567687988281 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 708.6525268554688 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 367.11541748046875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 1179.21484375 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 399.55682373046875 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 853.6284790039062 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 547.6143188476562 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 1713.5648193359375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 855.8114624023438 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: -28195.521484375 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 1553.647705078125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 909.2193603515625 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 577.0303344726562 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 409.7355041503906 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 365.6449890136719 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 358.3708801269531 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 590.06787109375 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1485.899169921875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 525.2886352539062 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8375, loss_val: nan, pos_over_neg: 490.284423828125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8345, loss_val: nan, pos_over_neg: 570.7391357421875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 1057.3126220703125 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 3117.6796875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 96734.8515625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 467.5924987792969 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 374.1377258300781 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8481, loss_val: nan, pos_over_neg: 527.8285522460938 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 290.3365478515625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: 806.6060180664062 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8423, loss_val: nan, pos_over_neg: 445.2655029296875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 764.4059448242188 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 706.302978515625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 14599.9443359375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.843, loss_val: nan, pos_over_neg: 1091.40673828125 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1748.5369873046875 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 1426.468017578125 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1978.681884765625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1372.9102783203125 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 307.84814453125 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 383.53533935546875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 494.9018859863281 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 1155.4132080078125 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 610.3407592773438 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 509.1431884765625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 392.7252197265625 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 2553.58837890625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: 4579.5478515625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1528.7767333984375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 389.9655456542969 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 438.1468200683594 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 554.9794311523438 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 368.5992126464844 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 473.1827392578125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8599, loss_val: nan, pos_over_neg: 303.03704833984375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 594.1932373046875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 4435.63232421875 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 3515.16259765625 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 3708.280029296875 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 886.4738159179688 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8519, loss_val: nan, pos_over_neg: 554.1129760742188 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 9009.4521484375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 2896.758056640625 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8466, loss_val: nan, pos_over_neg: 1667.5611572265625 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 2363.19580078125 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 684.3338012695312 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8356, loss_val: nan, pos_over_neg: 3254.8173828125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1119.3282470703125 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 518.7921142578125 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 551.565185546875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8498, loss_val: nan, pos_over_neg: 855.5878295898438 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 717.83544921875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 645.0936279296875 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 6003.28662109375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 456.9342956542969 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8383, loss_val: nan, pos_over_neg: 627.532470703125 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 3383.049560546875 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 4892.3125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 969.0841674804688 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8424, loss_val: nan, pos_over_neg: 283.5102844238281 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 283.6214904785156 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8463, loss_val: nan, pos_over_neg: 2775.30126953125 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 493.46673583984375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 490.3393859863281 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1266.3743896484375 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 769.5452880859375 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 513.1185302734375 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 2256.309326171875 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1893.877685546875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 1067.7047119140625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 412.15362548828125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 463.7403259277344 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8345, loss_val: nan, pos_over_neg: 644.994140625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1727.7161865234375 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 9532.4853515625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 875.346923828125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 605.5092163085938 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 473.0350646972656 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1296.9822998046875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 1100.82763671875 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 353.7183837890625 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 595.9369506835938 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 444.6109619140625 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 981.930908203125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 22397.51953125 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 757.7805786132812 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 402.51971435546875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8552, loss_val: nan, pos_over_neg: 540.29833984375 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 1197.019287109375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 681.7850952148438 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 619.8715209960938 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 667.62841796875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 1307.4527587890625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1153.9754638671875 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 2238.455322265625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 908.8714599609375 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: 527.1109619140625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 2099.566650390625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 440.9059753417969 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 260.4726257324219 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 502.53131103515625 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 732.3170776367188 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 1941.491455078125 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 396.1759033203125 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 717.8726196289062 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 430.54168701171875 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 369.6706237792969 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 551.618896484375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 1724.8726806640625 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 562.6636352539062 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 552.2647094726562 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1835.9793701171875 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 933.912841796875 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1389.685791015625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 1036.8740234375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 618.9827270507812 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 451.8885498046875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 559.10107421875 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 300.4645080566406 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 381.7170104980469 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 259.7263488769531 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 234.59390258789062 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1161.0045166015625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1838.2353515625 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1169.3392333984375 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 339.1943359375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 641.6795654296875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1130.834716796875 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 298.11212158203125 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 471.1530456542969 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 327.6316223144531 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8347, loss_val: nan, pos_over_neg: 375.0145263671875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 519.331298828125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 963.9797973632812 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 880.903564453125 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1987.6580810546875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 3115.99609375 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 439.205810546875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 555.9591674804688 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 233.52857971191406 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 874.135498046875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 838.9825439453125 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 1178.896484375 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 3748.817138671875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1591.871826171875 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1955.5614013671875 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 2790.80908203125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 860.1641845703125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 1909.942626953125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8439, loss_val: nan, pos_over_neg: 403.9610900878906 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 735.1278076171875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 880.1467895507812 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 763.2576293945312 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 420.1824951171875 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1331.27685546875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: -6540.8740234375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 438.09600830078125 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: -24612.259765625 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: -6548.05419921875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 12108.78515625 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 952.3846435546875 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 1996.7025146484375 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8393, loss_val: nan, pos_over_neg: 417.44512939453125 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 314.4626770019531 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 363.3477478027344 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 768.4861450195312 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 3058.799560546875 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 867.5380249023438 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8481, loss_val: nan, pos_over_neg: 595.100830078125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8501, loss_val: nan, pos_over_neg: 492.9255065917969 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 674.5592651367188 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1729.6712646484375 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 882.2268676757812 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 502.1742248535156 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1044.401611328125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 1624.058349609375 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1652.8438720703125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 2202.600830078125 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 600.2238159179688 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 465.08807373046875 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8424, loss_val: nan, pos_over_neg: 317.76690673828125 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: -3702.45556640625 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1042.788818359375 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 777.604736328125 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 1550.507568359375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 560.4097900390625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 536.1901245117188 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 432.7978820800781 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 336.25933837890625 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 2076.32861328125 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 1698.605224609375 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 1196.750732421875 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 476.4013977050781 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 628.350830078125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 2629.58447265625 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 1704.42529296875 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 3938.37060546875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 1200.8004150390625 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 820.3158569335938 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 2147.270263671875 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 316.7895202636719 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 442.4298095703125 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 343.6575012207031 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 572.017822265625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 625.3546752929688 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 602.9475708007812 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 554.7772827148438 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 588.4009399414062 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 4262.4365234375 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 1846.4801025390625 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 1033.1083984375 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 939.0433349609375 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.853, loss_val: nan, pos_over_neg: 242.0231475830078 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 564.666259765625 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 584.539306640625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8431, loss_val: nan, pos_over_neg: 251.10121154785156 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 375.4867858886719 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 1375.061767578125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 1326.239990234375 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 355.15374755859375 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 394.46832275390625 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 2354.37890625 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 453.7239074707031 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 16159.6689453125 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 545.3340454101562 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 283.58624267578125 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 508.02880859375 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 628.903564453125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 2590.46923828125 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 1538.30419921875 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 652.0650634765625 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 231.41859436035156 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 482.5071105957031 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 2474.676025390625 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 912.9873046875 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8354, loss_val: nan, pos_over_neg: 481.2782897949219 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8472, loss_val: nan, pos_over_neg: 404.90130615234375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 650.966796875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 590.3845825195312 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 566.7587280273438 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 384.2112731933594 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.834, loss_val: nan, pos_over_neg: 511.9076843261719 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 539.928955078125 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1999.1993408203125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: -47961.13671875 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8354, loss_val: nan, pos_over_neg: 12861.1796875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 366.8192138671875 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 1500.4141845703125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 865.7409057617188 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: -11289.5966796875 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1787.4931640625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 142093.65625 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 10851.6123046875 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 1283.1925048828125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1002.498291015625 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 681.068115234375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 6602.8154296875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 562.9443969726562 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 505.9974365234375 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8357, loss_val: nan, pos_over_neg: 342.5184631347656 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 660.445068359375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 21920.7734375 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: 789.9207153320312 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 1622.00537109375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: -2688.64501953125 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1013.42333984375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 384.68310546875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 512.465576171875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 446.8092956542969 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 427.5144958496094 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 9132.5771484375 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2756.40478515625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 8327.1982421875 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 2612.109130859375 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1082.7640380859375 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 726.0943603515625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 778.6622314453125 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 3136.812744140625 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 734.7959594726562 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 605.6522216796875 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8498, loss_val: nan, pos_over_neg: 968.0166625976562 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 514.6896362304688 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 5997.0654296875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 630.8977661132812 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1675.73779296875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 567.569580078125 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 287.4620666503906 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 645.3057861328125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 426.6215515136719 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 420.9722595214844 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 8603.6416015625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: -5819.16650390625 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1052.7271728515625 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1019.291015625 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 785.3504028320312 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 650.895751953125 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 467.1831970214844 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 326.357421875 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 1046.7144775390625 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8442, loss_val: nan, pos_over_neg: 546.5318603515625 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1856.2225341796875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 1462.39599609375 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 601.7301025390625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8383, loss_val: nan, pos_over_neg: 7238.6162109375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: -4811.86279296875 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 9197.0556640625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 1050.4422607421875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:42:51<129040:50:08, 1548.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 1332.977294921875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 1484.685791015625 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1532.5133056640625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 791.9417724609375 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1258.7066650390625 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1033.0068359375 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 681.4507446289062 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 3315.596923828125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8535, loss_val: nan, pos_over_neg: 401.41900634765625 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 909.55322265625 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: -2270.199951171875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 352.6422119140625 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 989.9049072265625 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 437.8656921386719 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 347.5080871582031 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 5131.91748046875 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 561.5304565429688 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 892.8121948242188 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 83607.25 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1025.3372802734375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 5518.82958984375 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8565, loss_val: nan, pos_over_neg: 957.1326293945312 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 3253.087890625 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8416, loss_val: nan, pos_over_neg: 741.8074340820312 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 846.715087890625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1336.360107421875 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 799.9405517578125 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8472, loss_val: nan, pos_over_neg: 218.47418212890625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 307.6674499511719 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 1878.96875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 605.0100708007812 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: 800.904296875 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 504.6259765625 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 4428.6435546875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1490.435791015625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 582.2889404296875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 15329.2119140625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 1115.164794921875 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8357, loss_val: nan, pos_over_neg: 311.1371154785156 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8204, loss_val: nan, pos_over_neg: 17176.9296875 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 2041.363525390625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: 1719.0323486328125 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1201.6900634765625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 2437.6591796875 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 576.1251220703125 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 370.4081115722656 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1212.996337890625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 569.0320434570312 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8253, loss_val: nan, pos_over_neg: 1389.311767578125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 2821.35791015625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: -1686.302734375 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.836, loss_val: nan, pos_over_neg: 1600.299072265625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 799.7322387695312 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 494.7958679199219 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 3665.700927734375 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 606.4779052734375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 459.72216796875 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 474.4653625488281 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 211.4830780029297 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8388, loss_val: nan, pos_over_neg: 487.33892822265625 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: 1742.9578857421875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 504.836669921875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 813.88916015625 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 3885.6220703125 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 9105.9267578125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 2568.827880859375 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 412.53759765625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 896.703857421875 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 630.588134765625 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: -3077.18798828125 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 710.553955078125 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 459.3966064453125 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.8323, loss_val: nan, pos_over_neg: 508.8385314941406 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 594.2413940429688 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1304.4012451171875 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: -3950.345458984375 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 620.1224975585938 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8323, loss_val: nan, pos_over_neg: 316.8851013183594 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 588.585205078125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 595.7777709960938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 1864.3427734375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 457.7340393066406 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 401.5248718261719 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 564.1952514648438 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 470.8522033691406 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 833.9461669921875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 469.420166015625 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 319.4328918457031 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1319.4583740234375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 565.3174438476562 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1065.6973876953125 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 499.2113952636719 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 576.6680908203125 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 456.38653564453125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 1950.5179443359375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 489.10064697265625 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: -3969.060546875 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 3785.95849609375 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1472.166015625 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8456, loss_val: nan, pos_over_neg: 897.1627197265625 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 397.0415954589844 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 732.1874389648438 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1009.7817993164062 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 2334.243896484375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8187, loss_val: nan, pos_over_neg: 624.2557983398438 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 515.2042236328125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 746.38916015625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 4882.1298828125 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 898.7767944335938 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1390.61572265625 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 7728.29443359375 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 1164.8193359375 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 708.223388671875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 1693.4833984375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 544.0813598632812 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 463.2973327636719 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 927.7211303710938 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 300.08941650390625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 543.7174682617188 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 340.3421630859375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 332.3893127441406 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 697.1359252929688 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1523.2572021484375 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 404.6497497558594 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 254.81692504882812 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 631.4634399414062 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 571.6429443359375 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 784.202392578125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 873.0725708007812 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 687.8368530273438 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 330.37249755859375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 381.4262390136719 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 816.7776489257812 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1240.5089111328125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8356, loss_val: nan, pos_over_neg: 669.846435546875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 378.4947509765625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 313.7902526855469 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8363, loss_val: nan, pos_over_neg: 267.8889465332031 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 326.2873229980469 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 378.5701599121094 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 385.5609436035156 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 653.7935791015625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 923.7322387695312 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 661.9517822265625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1131.3475341796875 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 225.22218322753906 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 372.5697937011719 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1299.2449951171875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 361.9891052246094 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 372.64276123046875 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 517.1666870117188 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8413, loss_val: nan, pos_over_neg: 316.275634765625 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 532.7857055664062 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.818, loss_val: nan, pos_over_neg: -11127.8623046875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: -9591.6982421875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1750.571044921875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 739.1272583007812 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 347.0636901855469 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 1202.305908203125 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 3752.77001953125 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 938.3859252929688 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 2831.962158203125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 577.5415649414062 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 544.5472412109375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 2621.414306640625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 1901.5020751953125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 415.2477111816406 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 845.399169921875 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 557.46630859375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 835.154052734375 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 401.537353515625 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 751.5904541015625 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 212.2986602783203 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1441.261474609375 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 471.0311279296875 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 789.8221435546875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 501.2950439453125 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 631.1616821289062 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 1643.36376953125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 2113.071044921875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1493.432861328125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 327.859130859375 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 286.7003479003906 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 637.7052612304688 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 921.6363525390625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 496.11883544921875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 417.8884582519531 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.829, loss_val: nan, pos_over_neg: 478.02783203125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 1286.2841796875 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 877.2488403320312 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 1277.7572021484375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 796.6677856445312 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 566.4200439453125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 1302.802734375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 3927.8544921875 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: -68023.6953125 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 1066.2122802734375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: -28340.412109375 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: -6726.63134765625 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 540.6618041992188 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 426.5282897949219 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 247.16477966308594 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1319.8985595703125 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 2015.4466552734375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1276.0423583984375 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 986.5070190429688 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1057.2901611328125 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 595.474365234375 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 3254.940185546875 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: -1307.309814453125 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 5404.859375 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 779.8331298828125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1159.2725830078125 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 1136.4420166015625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 701.583251953125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 435.496826171875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 930.9888916015625 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1902.067626953125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 791.2398071289062 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 901.8023071289062 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 632.0923461914062 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 606.8876953125 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 747.881591796875 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 2199.23974609375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 1121.7476806640625 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 630.2175903320312 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 648.1704711914062 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 849.5804443359375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 752.3195190429688 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 355.8598937988281 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 1141.11181640625 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1028.32861328125 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 854.9646606445312 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 739.0265502929688 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 343.7023620605469 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 657.5521850585938 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1222.4049072265625 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: -5756.69677734375 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 7403.9794921875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 419.2969665527344 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 378.849365234375 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 259.0028381347656 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1280.659912109375 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 7152.5751953125 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 877.4642333984375 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: 837.91845703125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1906.8060302734375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1240.8394775390625 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8307, loss_val: nan, pos_over_neg: 1351.5032958984375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 891.2208251953125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 679.0038452148438 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 1625.0465087890625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8307, loss_val: nan, pos_over_neg: 502.2060546875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1259.7958984375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 1296.81201171875 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 2101.664794921875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 915.6483764648438 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 480.78277587890625 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 303.39862060546875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 354.5715026855469 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1321.5576171875 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1007.8419799804688 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 388.9189758300781 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 385.59063720703125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: -12114.3408203125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 2699.5947265625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8267, loss_val: nan, pos_over_neg: 1173.9462890625 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1062.7423095703125 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 953.7328491210938 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 279.8558044433594 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: 398.865966796875 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 1002.6803588867188 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 888.5496826171875 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 1108.00537109375 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 712.3785400390625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 362.43780517578125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 658.3473510742188 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: -16483.34375 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.826, loss_val: nan, pos_over_neg: -1588.6041259765625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 10929.3740234375 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: -2304.967529296875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1100.7484130859375 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8355, loss_val: nan, pos_over_neg: 814.3383178710938 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 422.5269775390625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 339.8081359863281 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 546.6654052734375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 303.2270812988281 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 821.5541381835938 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 1553.032470703125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 9717.5009765625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: -7948.88134765625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 6319.552734375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: -2283.120361328125 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 599.8659057617188 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8442, loss_val: nan, pos_over_neg: 401.37738037109375 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 978.6744995117188 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: -4063.55224609375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 8283.8232421875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 3989.233154296875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 2094.40771484375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 680.2874755859375 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 9410.626953125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 1668.599609375 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 2670.482177734375 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 506.2670593261719 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 403.44903564453125 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 302.38116455078125 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 4065.39306640625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: -27553.623046875 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 814.8827514648438 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 782.4811401367188 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 3073.23974609375 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 1893.6221923828125 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 498.51214599609375 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 458.96722412109375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 476.3731689453125 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 627.7702026367188 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 941.4517211914062 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8244, loss_val: nan, pos_over_neg: 604.1649169921875 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 4814.6865234375 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1764.650634765625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 780.093505859375 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 589.0476684570312 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 3231.92724609375 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 11459.15234375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 60174.46484375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 1013.8057250976562 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 1045.774658203125 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 862.708251953125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 718.5961303710938 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8307, loss_val: nan, pos_over_neg: 382.5260314941406 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8267, loss_val: nan, pos_over_neg: 1415.4755859375 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8341, loss_val: nan, pos_over_neg: 313.9555969238281 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: -2822.6357421875 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 458038.875 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 613.0665283203125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 1035.779541015625 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 490.7485046386719 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 640.5882568359375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 645.2023315429688 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 554.1783447265625 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 576.4287719726562 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1363.1292724609375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 500.84228515625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 936.8641967773438 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1686.1806640625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 651.2060546875 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 2390.88232421875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1319.705322265625 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 308.09906005859375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 506.1529541015625 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 531.0397338867188 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 803.3409423828125 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 525.9942626953125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 1032.3323974609375 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 595.1182250976562 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 2564.89404296875 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 2595.46923828125 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 47113.73046875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 4172.24267578125 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 494.31256103515625 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 326.4202880859375 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 487.2355041503906 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 579.195068359375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 797.814697265625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 843.0720825195312 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 443.0742492675781 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 3575.9970703125 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 3330.836669921875 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 11410.5888671875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 377.5028991699219 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 2220.286865234375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 928.0565185546875 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 2600.8798828125 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.83, loss_val: nan, pos_over_neg: 502.9117126464844 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 12454.291015625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 560.6082763671875 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 465.0947265625 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 837.607177734375 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1502.166015625 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: 253.49305725097656 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 1280.677978515625 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 499.69989013671875 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 726.1085815429688 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 362.8819885253906 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 584.115966796875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 867.0152587890625 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 713.7159423828125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.829, loss_val: nan, pos_over_neg: 347.0951843261719 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 714.865966796875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 283.6891174316406 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 968.9070434570312 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 2049.837890625 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 1689.0792236328125 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.834, loss_val: nan, pos_over_neg: 799.9360961914062 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1178.43310546875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 992.7079467773438 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1629.3170166015625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 531.9442138671875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1467.8265380859375 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1176.0802001953125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.8038, loss_val: nan, pos_over_neg: 639.2720947265625 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 539.3677978515625 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 1043.039794921875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1194.6895751953125 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 1231.6241455078125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 680.9986572265625 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 803.8125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1877.201904296875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 979.7880859375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8288, loss_val: nan, pos_over_neg: 1440.1942138671875 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 416.0003662109375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1018.6922607421875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1123.80029296875 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 491.3175964355469 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 386.236328125 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 421.0166320800781 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1292.3651123046875 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.836, loss_val: nan, pos_over_neg: 1894.5389404296875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 1336.094482421875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 636.9359130859375 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 1034.3328857421875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 402.6874694824219 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 814.5682373046875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 680.26611328125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 2562.435546875 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 716.2886352539062 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 951.3850708007812 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1049.9688720703125 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 531.50830078125 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1112.760498046875 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 2978.051025390625 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 559.7365112304688 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 659.80126953125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 1736.0706787109375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 764.88134765625 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 254.31153869628906 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 777.1689453125 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 513.7797241210938 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 882.8681030273438 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 845.223388671875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 352.8528747558594 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1227.42724609375 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 466.5658264160156 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 1158.400634765625 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 402.0037536621094 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 224.29251098632812 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 318.07489013671875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 833.8655395507812 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.83, loss_val: nan, pos_over_neg: 980.285888671875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 569.3095092773438 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 3018.128662109375 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 2475.52734375 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 452.05303955078125 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8356, loss_val: nan, pos_over_neg: 548.4697265625 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 445.1709899902344 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 1081.7984619140625 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 1179.7601318359375 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 679.058349609375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 863.1838989257812 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 495.3688659667969 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 749.0232543945312 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1547.3509521484375 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 547.1207885742188 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 560.7462768554688 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 618.8585205078125 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 951.8281860351562 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 2383.976806640625 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 13491.0087890625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8233, loss_val: nan, pos_over_neg: 727.2293701171875 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 898.4384155273438 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 480.94891357421875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1965.9822998046875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1541.4703369140625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 388.7671813964844 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 644.2405395507812 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.834, loss_val: nan, pos_over_neg: 366.14111328125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 529.2120361328125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1450.700927734375 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1612.4420166015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 585.9899291992188 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8363, loss_val: nan, pos_over_neg: 1210.927001953125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 631.8440551757812 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 380.18609619140625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 883.8700561523438 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8297, loss_val: nan, pos_over_neg: 494.0263366699219 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.823, loss_val: nan, pos_over_neg: 759.343017578125 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 627.7665405273438 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 1592.46337890625 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 751.2334594726562 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 6453.40087890625 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 1515.7825927734375 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 1196.05615234375 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 620.2299194335938 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1053.162841796875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 946.5365600585938 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 486.86553955078125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 845.4725341796875 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1349.424560546875 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 721.1851806640625 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 1416.4140625 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 1533.6082763671875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 685.8843994140625 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 457.4696350097656 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 741.773193359375 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 721.0914916992188 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 824.1072387695312 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 195.5948944091797 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 482.1943664550781 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 542.8978881835938 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 656.5103759765625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 1117.93505859375 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 477.54827880859375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 752.3211059570312 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 549.9222412109375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: -2703.631103515625 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1459.6956787109375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 4371.1572265625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 467.76422119140625 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 338.7753601074219 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 348.56390380859375 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 744.920166015625 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 324.9574279785156 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 382.504638671875 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: 409.9117431640625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 602.6578979492188 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 405.940673828125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 3874.65234375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 496.09393310546875 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 302.9256896972656 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 559.2955322265625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 936.3646240234375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 889.7716674804688 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 517.2539672851562 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 518.5484619140625 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 325.6572570800781 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 2511.072265625 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 1090.8873291015625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 765.6608276367188 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 226.51593017578125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 355.09698486328125 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 419.7475280761719 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 486.6337890625 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 724.6478881835938 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1246.854736328125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1338.7808837890625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 6115.4755859375 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1676.28662109375 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: -3626.93408203125 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 261.805908203125 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 397.7799987792969 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 488.2709655761719 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 1341.5465087890625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 337.1330261230469 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 604.7180786132812 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 1045.193115234375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 350.6950378417969 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 908.7991333007812 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 2545.064208984375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 482.57269287109375 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 209.14430236816406 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 694.9153442382812 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 1917.8463134765625 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1239.0733642578125 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 1243.1544189453125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8361, loss_val: nan, pos_over_neg: 366.3664855957031 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 714.1109008789062 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 398.4012145996094 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1081.8419189453125 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 3145.56201171875 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 1602.341064453125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 719.291015625 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 547.235107421875 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 7101.02294921875 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1474.9754638671875 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 258.9355773925781 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 1207.8402099609375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 726.1583251953125 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 573.9296264648438 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 546.7186889648438 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 437.2215881347656 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 795.926025390625 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1249.3272705078125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 2381.31103515625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 548.3674926757812 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 965.8153076171875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1255.9437255859375 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1082.0987548828125 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8204, loss_val: nan, pos_over_neg: 1709.52490234375 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: -6785.328125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 1032.308837890625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 931.8612060546875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1003.1881103515625 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8347, loss_val: nan, pos_over_neg: 807.4564819335938 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 808.9000854492188 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1267.2177734375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1500.719970703125 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 476.7751159667969 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 565.9683837890625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 508.9958190917969 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 920.8079833984375 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 3640.538330078125 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 716.5108642578125 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8413, loss_val: nan, pos_over_neg: 1131.8572998046875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1278.05322265625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 4701.22900390625 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 2395.8095703125 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 1335.98193359375 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 952.525146484375 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 475.32769775390625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 1254.8372802734375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 859.349365234375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 32169.06640625 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1057.35107421875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 442.54779052734375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 670.5680541992188 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: -7778.9970703125 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 1547.025390625 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 1343.5230712890625 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 853.6646118164062 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 50528.828125 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1733.6746826171875 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: 2310.287841796875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 329.6921691894531 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1273.511474609375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 488.6058044433594 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 982.937255859375 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 997.4935913085938 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 675.2134399414062 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 467.65399169921875 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 731.4187622070312 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 666.2911376953125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 34672.17578125 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 3862.48779296875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: -2921.830322265625 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 420.83856201171875 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 325.919189453125 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 330.2157897949219 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 2042.6505126953125 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1090.47021484375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 3764.365966796875 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 372.6037292480469 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8253, loss_val: nan, pos_over_neg: 1232.249267578125 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 3684.033447265625 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 821.157958984375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 5613.544921875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 314.56494140625 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 525.1023559570312 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 452.8927307128906 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 383.921142578125 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8256, loss_val: nan, pos_over_neg: 506.7235412597656 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 450.0738830566406 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 1785.3173828125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 3559.0224609375 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1391.57763671875 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 409.33843994140625 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1126.043212890625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 1954.8409423828125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 826.6649780273438 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 2649.703125 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.8288, loss_val: nan, pos_over_neg: 1484.2552490234375 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 493.2529602050781 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1690.2982177734375 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 5908.67724609375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 749.0816040039062 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 918.1444091796875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 254.39796447753906 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 546.6922607421875 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 1084.2186279296875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 433.2651062011719 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 980.2667236328125 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 8323.0654296875 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 971.7371826171875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 1436.3021240234375 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 1059.8087158203125 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 731.1739501953125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 606.4662475585938 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 790.1185302734375 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: -1502.8934326171875 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1094.00439453125 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 10075.6650390625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 731.3272705078125 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 2806.200439453125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 453.24163818359375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 209.1885528564453 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 279.67230224609375 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 1221.248291015625 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 604.142578125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 831.346923828125 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 1365.7718505859375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 4080.95751953125 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 713.7479248046875 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 761.8110961914062 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 380.55743408203125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 534.9170532226562 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 357.8876953125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 715.7265014648438 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: -7877.53271484375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 2076.7158203125 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 2522.14990234375 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 685.4041748046875 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1024.259033203125 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 493.9466247558594 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 1280.528076171875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 971.5358276367188 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: -9400.3564453125 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 926.1844482421875 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 1805.360595703125 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 407.2812805175781 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 345.639892578125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 1016.3150634765625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: 2793.54638671875 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1003.8060302734375 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1154.865234375 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 770.142333984375 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 2673.192626953125 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 788.3577270507812 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 1147.5419921875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 557.9237670898438 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 442.4327392578125 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 704.52197265625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8383, loss_val: nan, pos_over_neg: 409.6075439453125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 810.2393798828125 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 998.1534423828125 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 286.4176025390625 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1333.9254150390625 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 872.1337280273438 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 2080.51513671875 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: -5753.6064453125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8388, loss_val: nan, pos_over_neg: 815.810791015625 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 807.3743896484375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 2471.053955078125 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 777.4789428710938 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1248.837646484375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 718.5150146484375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 339.1204528808594 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1233.649169921875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 7672.80322265625 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 578.0138549804688 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 873.5293579101562 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8219, loss_val: nan, pos_over_neg: 392.9130554199219 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 490.6878356933594 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 1336.8072509765625 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 2429.114501953125 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 698.3017578125 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 899.4212646484375 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 471.9472961425781 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 468.79620361328125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 813.2880859375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: -3084.616943359375 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1338.36376953125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 572.1549682617188 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 803.8450317382812 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 1785.6363525390625 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 1580.8192138671875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 867.9439086914062 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 1437.771484375 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1298.640625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1360.4356689453125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 3096.537353515625 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: -35339.4453125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1306.58837890625 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 525.2317504882812 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 851.48193359375 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1511.3214111328125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 2414.15576171875 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1478.142333984375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 470.63824462890625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 1272.480712890625 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: 825.371826171875 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 393.428955078125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 517.967041015625 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1819.80517578125 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 3761.033203125 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 561.3451538085938 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 1127.1248779296875 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.809, loss_val: nan, pos_over_neg: -12340.3388671875 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 2022.857666015625 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 791.08935546875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 443.3926086425781 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 416.7926940917969 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 575.5101318359375 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 280.31878662109375 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 7608.77294921875 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 762.8257446289062 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 596.711669921875 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1033.37109375 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1161.166259765625 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.806, loss_val: nan, pos_over_neg: -2876.95361328125 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 742.2205810546875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 1310.3504638671875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 493.163330078125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1574.25244140625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 419.4698791503906 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 743.2050170898438 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 377.2100524902344 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 547.3235473632812 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 718.1007690429688 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2448.676513671875 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 1447.86767578125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 1219.9705810546875 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8375, loss_val: nan, pos_over_neg: 259.4275817871094 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 712.9760131835938 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 495.9917907714844 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 792.7701416015625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 1053.4498291015625 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 727.1722412109375 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 583.0320434570312 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 591.0133666992188 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 513.263916015625 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 609.3976440429688 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1022.3169555664062 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 723.693359375 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 549.00634765625 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.8341, loss_val: nan, pos_over_neg: 525.310546875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 519.880126953125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 334.1871643066406 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 733.1582641601562 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 3406.626953125 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1514.104736328125 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 659.0872192382812 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 832.5717163085938 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1846.41162109375 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 863.5179443359375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 399.843505859375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 728.6557006835938 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 1283.4254150390625 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1752.5843505859375 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 359.6402893066406 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 743.2840576171875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 328.158935546875 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: -3044.572265625 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 2116.5693359375 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8233, loss_val: nan, pos_over_neg: 974.1813354492188 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1145.23876953125 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 480.22698974609375 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: 2529.034423828125 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 681.5668334960938 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1462.25341796875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1113.8033447265625 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 956.2608032226562 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 428.37744140625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 580.15771484375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1339.3927001953125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1953.729736328125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 491.23297119140625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 356.0769958496094 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 2283.753662109375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 1208.8043212890625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 627.02197265625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1059.1715087890625 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 2910.15478515625 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 1015.304931640625 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 3688.19580078125 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 737.9661865234375 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 3803.105224609375 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 634.0650634765625 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 503.62359619140625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 475.3443908691406 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 959.2413940429688 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 717.6209716796875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 518.14697265625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 522.8428344726562 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 10528.8603515625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 940.0108642578125 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 10926.9501953125 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 2137.220703125 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: -2514.683837890625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: 515.2659912109375 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 3050.4599609375 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 4118.826171875 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 2677.645751953125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 5134.279296875 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 517.522705078125 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 545.656494140625 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 389.4539489746094 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 557.54345703125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 347.1061096191406 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 798.2303466796875 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 1637.0140380859375 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 1770.296142578125 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 879.233642578125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 3178.623046875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 572.02001953125 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 373.94287109375 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 370.78778076171875 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 402.81195068359375 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 2250.693603515625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.8354, loss_val: nan, pos_over_neg: 689.7123413085938 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [2:08:51<129366:41:38, 1552.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 989.0838623046875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 1221.1290283203125 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 845.54150390625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 856.6835327148438 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 1451.4873046875 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 725.66845703125 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1577.31640625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 379.6065979003906 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 728.5592041015625 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 473.2219543457031 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 6219.79833984375 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: -1939.3387451171875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 902.352294921875 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 3025.76708984375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 667.1117553710938 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1438.897216796875 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 393.3723449707031 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1006.2974243164062 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 645.5481567382812 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 1037.7734375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 1036.6669921875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 721.7974853515625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1226.8751220703125 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 428.1856384277344 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: -2745.848388671875 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: -4366.19189453125 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 746.9970092773438 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 617.6188354492188 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 2038.8172607421875 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 5960.6044921875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 33598.578125 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 359.9823303222656 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 514.6065673828125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 269.8260803222656 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 630.8137817382812 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 298.63720703125 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 742.7550048828125 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 2744.17138671875 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 633.1292114257812 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 2028.5001220703125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 2521.55859375 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 721.40478515625 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 1253.45263671875 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 900.5717163085938 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 1471.5789794921875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 591.1007690429688 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 1118.99609375 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 596.9722900390625 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 695.9600830078125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 446.3424072265625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 769.5116577148438 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 704.1669311523438 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 4838.81494140625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 373.81207275390625 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 1945.6944580078125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 6126.73486328125 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 555.9190063476562 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 688.4104614257812 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: -61502.5078125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: -10040.1611328125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 737.0696411132812 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 873.8475341796875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 487.5276794433594 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 933.648193359375 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 606.934326171875 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: -6118.1728515625 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 533.809326171875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: 556.9136352539062 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 885.8910522460938 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 1192.863525390625 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 5838.81787109375 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 698.15673828125 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1839.8804931640625 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 8469.21484375 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8244, loss_val: nan, pos_over_neg: 769.3206787109375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 958.5166625976562 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 1149.531982421875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 219.64019775390625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 815.3682861328125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 3472.9609375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 2328.48583984375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 2532.648193359375 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 6622.4580078125 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 517.2139892578125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: -21550.728515625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 2159.064697265625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 957.1785278320312 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 1204.307861328125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 1892.5426025390625 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 402.7742614746094 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1491.9019775390625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: -8181.9453125 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: -9846.703125 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: -3188.872802734375 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: -3075.926513671875 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 767.5653686523438 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 692.1511840820312 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 295.2706604003906 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1153.834228515625 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1916.5980224609375 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1518.50537109375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: -3841.612548828125 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.802, loss_val: nan, pos_over_neg: -8326.583984375 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 920.7249145507812 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1089.9783935546875 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 1074.1956787109375 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 2924.68115234375 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 6430.12744140625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1436.8076171875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 328.5530090332031 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 601.1483154296875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 907.4114379882812 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 1833.2265625 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1001.1336059570312 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 7316.6865234375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 1048.370849609375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 613.882080078125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 1580.9652099609375 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: -11855.9716796875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 573.5512084960938 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 3105.189208984375 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1750.41552734375 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 675.9606323242188 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 647.955078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 808.9297485351562 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 668.4891967773438 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 941.5262451171875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1132.1966552734375 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 871.5074462890625 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 615.7044677734375 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1001.192626953125 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 613.576416015625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 870.52880859375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 282.1803894042969 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 605.514404296875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 348.60308837890625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 575.6675415039062 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8256, loss_val: nan, pos_over_neg: 813.82568359375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 947.0191650390625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 494.40093994140625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 1342.822265625 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 235.46902465820312 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1477.4390869140625 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 4115.97900390625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8267, loss_val: nan, pos_over_neg: 708.1360473632812 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1600.1307373046875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 422.4645080566406 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 407.4665832519531 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1054.983154296875 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1395.5731201171875 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 622.7532958984375 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: -8203.056640625 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 1011.4118041992188 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 472.635498046875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: -3278.173583984375 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 5061.2333984375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1141.1544189453125 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 511.05194091796875 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 458.4585266113281 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 3020.554931640625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: 369.2686462402344 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 619.0753173828125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: 2340.090576171875 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 703.989990234375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 763.6962890625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 540.1134643554688 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 947.9881591796875 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 1420.829345703125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 913.5267944335938 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 624.5708618164062 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 831.8857421875 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1275.4666748046875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1984.084716796875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 698.4678955078125 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 526.9450073242188 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 842.3514404296875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 754.8310546875 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: -4780.2802734375 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 3140.106689453125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 702.9301147460938 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 571.289794921875 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 1079.470947265625 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 522.232177734375 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 346.78466796875 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 1044490.8125 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 763.9404296875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: -6302.93408203125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 8952.9541015625 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: -5456.05712890625 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 5631.83740234375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 15906.96875 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 4404.63427734375 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 1038.915283203125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 5586.08642578125 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2070.622802734375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1086.4285888671875 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 1320.5150146484375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 893.3076171875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 979.0860595703125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 398.8391418457031 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 415.7236022949219 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 763.0564575195312 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 814.6751098632812 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 1983.533203125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 1153.6746826171875 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 2970.417724609375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 697.7621459960938 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 550.1764526367188 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 1259.14892578125 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 933.90576171875 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 373.3641662597656 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 464.2696533203125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 8741.1337890625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 1290.6024169921875 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 1842.1715087890625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 878.77587890625 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 510.6976623535156 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 2816.7255859375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1045.8779296875 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1562.9930419921875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1634.4482421875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 550.4178466796875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8358, loss_val: nan, pos_over_neg: 232.38778686523438 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 392.75030517578125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 517.3832397460938 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 330.51763916015625 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 610.9219970703125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 416.8913879394531 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8187, loss_val: nan, pos_over_neg: 472.3199768066406 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 876.02197265625 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 730.3228149414062 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 2746.151611328125 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 394.1680908203125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 492.0835876464844 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1192.31787109375 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 1102.0919189453125 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 2447.51513671875 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 399.6695861816406 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 740.656494140625 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 351.8692321777344 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 4350.54248046875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 795.9613037109375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 683.093994140625 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 519.0671997070312 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 602.498779296875 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: -6188.74658203125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 566.5847778320312 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 6499.0263671875 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 1541.2086181640625 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1537.9073486328125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 267.85321044921875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 1046.7523193359375 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 449.7471923828125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 752.7919921875 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 849.641845703125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 484.8435974121094 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 801.2799072265625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8233, loss_val: nan, pos_over_neg: 475.2546081542969 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 793.8125610351562 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: -4941.22900390625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 571.7847900390625 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 1143.84326171875 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1109.5457763671875 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 4258.8125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 819.0072021484375 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1004.7711791992188 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 446.5078125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 782.5926513671875 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 487.06744384765625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 514.9410400390625 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1139.679931640625 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 860.5619506835938 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1862.9254150390625 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 742.805908203125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 7637.7060546875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 959.9682006835938 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 772.1672973632812 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 525.9911499023438 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 4502.02978515625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 429.7242736816406 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 2352.127685546875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 664.6831665039062 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 11749.9892578125 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 2752.31494140625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1446.08251953125 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 3045.39990234375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 393.0926818847656 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 759.8732299804688 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 2717.077392578125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 580.5446166992188 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 616.1874389648438 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 264.5180358886719 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1312.36865234375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 545.5335083007812 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 288.67169189453125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 960.1719970703125 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 984.6744384765625 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 2072.802978515625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 437.6721496582031 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: -8274.1787109375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: -24457.98046875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: -8639.673828125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: -32695.900390625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: -2226.66796875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 668.988037109375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 494.0677185058594 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 504.84130859375 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 344.0785217285156 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 782.6946411132812 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 626.8689575195312 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 457.0069580078125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 358.7001647949219 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 453.82220458984375 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 563.6161499023438 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 777.8683471679688 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 716.8230590820312 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1223.997314453125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1079.70947265625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 712.2247924804688 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 1043.2615966796875 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 2466.78369140625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1751.818359375 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: -5468.5 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 543.9286499023438 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 920.7670288085938 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 750.187255859375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 1102.91796875 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 664.554443359375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 189022.796875 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 1908.3350830078125 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 435.2919616699219 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 861.5009155273438 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 1066.3016357421875 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 1192.3594970703125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 526.9826049804688 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 367.1149597167969 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 959.9210205078125 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 426.16192626953125 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 756.305419921875 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 663.0230712890625 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 1159.021484375 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 27298.796875 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 4075.52490234375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 268.86199951171875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 819.5276489257812 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 696.24658203125 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 1139.919189453125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 2335.444091796875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 3160.25927734375 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 445.9900817871094 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 402.0126953125 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 409.17578125 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 401.4203796386719 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8345, loss_val: nan, pos_over_neg: 343.34033203125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 518.2886352539062 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 1332.2764892578125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: -3264.57177734375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.792, loss_val: nan, pos_over_neg: -5294.25390625 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 5456.5263671875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 2149.199462890625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: 283.80731201171875 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 911.2183227539062 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 867.2531127929688 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 400.1981201171875 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 657.1654663085938 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2032.0531005859375 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 776.1989135742188 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: -3288.656005859375 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: -4618.47119140625 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1533.218017578125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1531.49267578125 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 6250.4521484375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 488.9037780761719 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 531.93701171875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 363.1971740722656 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 269.5426025390625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 624.8916015625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 2928.243896484375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1163.73046875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 842.5173950195312 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: -39939.203125 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 34318.85546875 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 544.1770629882812 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 612.152099609375 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1618.37158203125 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 834.5174560546875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1196.6971435546875 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 2250.756103515625 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 2814.8154296875 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 951.8792724609375 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 560.840576171875 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 477.95123291015625 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.829, loss_val: nan, pos_over_neg: 405.9033508300781 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 1209.0057373046875 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 1099.379638671875 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 2632.77099609375 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 492.83282470703125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 5125.751953125 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 1924.7099609375 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 2155.036376953125 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: -16556.828125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 656.9439086914062 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 394.4543151855469 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 1171.0994873046875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 986.7650146484375 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 425.4579162597656 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 1452.149169921875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 725.3106689453125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 849.0238037109375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1121.33984375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 2485.110595703125 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1208.1182861328125 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 519.7550048828125 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 538.4800415039062 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1321.7791748046875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1446.2086181640625 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 812.723876953125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1054.0931396484375 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 2018.68994140625 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 872.8599243164062 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1412.3538818359375 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 487.0447692871094 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 998.0599365234375 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 521.2340698242188 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 558.584716796875 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: -22426.9609375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 866.8582153320312 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 652.5545043945312 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 327.6346740722656 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1735.41943359375 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.823, loss_val: nan, pos_over_neg: 2140.690673828125 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 698.630615234375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 790.93359375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 11209.30859375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 743.9518432617188 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 501.45574951171875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 890.6032104492188 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 2173.403076171875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1200.4388427734375 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8063, loss_val: nan, pos_over_neg: 27890.89453125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 1057.709228515625 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 790.4782104492188 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1362.3701171875 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8038, loss_val: nan, pos_over_neg: 893.6851806640625 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 3837.7607421875 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1644.451416015625 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1283.8111572265625 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1763.264892578125 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 552.1909790039062 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1619.0477294921875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 685.036865234375 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 566.498046875 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 532.7372436523438 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: -2737.662353515625 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 674.5321655273438 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 943.02490234375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 34862.5859375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 1978.1109619140625 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1971.5628662109375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 1793.9898681640625 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 2565.711669921875 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1097.9249267578125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 359.8708801269531 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 3358.6611328125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 835.593505859375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 754.477783203125 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 462.0625 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 803.0433959960938 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: -2838.40087890625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1007.6670532226562 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: -8960.771484375 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 631.9276123046875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1703.5810546875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 836.2304077148438 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1524.1309814453125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 1001.7554321289062 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 868.4405517578125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 3016.395751953125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 954.399658203125 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 815.857666015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 708.5703735351562 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 663.5621948242188 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 579.053955078125 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 726.0764770507812 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 815.40380859375 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1762.272216796875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 791.80908203125 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 729.1907958984375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 455.0909118652344 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 281.1910095214844 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 487.6465148925781 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 614.9044799804688 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 544.4749755859375 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 274.4103698730469 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 531.6182861328125 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 432.9576416015625 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 19540.34765625 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 740.0353393554688 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 701.748291015625 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 626.9752807617188 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 1012.873291015625 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 617.541259765625 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 1053.378173828125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 407.46051025390625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 374.93194580078125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 576.7437133789062 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 3339.076171875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 604.8191528320312 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 2395.2041015625 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8288, loss_val: nan, pos_over_neg: 502.1728820800781 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 733.6921997070312 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 726.47509765625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 696.3665771484375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1729.435791015625 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1237.19677734375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 815.1459350585938 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1271.6162109375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 2722.3828125 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: -19527.2578125 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 2991.54736328125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1059.1051025390625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1971.1630859375 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 2093.420166015625 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1899.4200439453125 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 1410.0521240234375 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: -3974.259521484375 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 636.8259887695312 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 770.55224609375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 730.2357177734375 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: -12744.697265625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1628.566162109375 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 3342.928466796875 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 2488.260009765625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 470.32196044921875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 4427.46240234375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 2434.118896484375 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 569.25048828125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 1255.9381103515625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1177.6683349609375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 7272.1240234375 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: -197274.296875 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 10372.9326171875 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 757.7459716796875 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 830.8389892578125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 933.2930297851562 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 725.6134033203125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 497.6959228515625 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 383.8552551269531 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 455.8186950683594 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 3160.534912109375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 3061.375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 665.2655639648438 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 2090.069580078125 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 1039.237548828125 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 436.93743896484375 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 890.0452880859375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 663.5007934570312 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 926.9168090820312 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 2257.828125 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1207.5426025390625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1405.6483154296875 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 364.9641418457031 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 625.7122802734375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 734.2770385742188 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 2012.214111328125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 620.745361328125 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 580.5598754882812 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 645.889892578125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1376.0115966796875 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 558.9572143554688 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: -14873.5078125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2539.953857421875 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 772.8643798828125 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 613.1735229492188 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 597.281982421875 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 777.1474609375 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: -23346.798828125 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1126.6175537109375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1116.247314453125 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 1605.6923828125 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 2281.96484375 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 776.1629028320312 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 694.19482421875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 635.7605590820312 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 10239.4853515625 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 1514.57568359375 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 485.9275207519531 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 5756.06201171875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2332.788818359375 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 762.2400512695312 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 691.7085571289062 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 709.1434326171875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 626.3883666992188 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1278.208251953125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 401.60205078125 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 557.0281372070312 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 5744.66845703125 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.815, loss_val: nan, pos_over_neg: -4156.41259765625 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 533.3236083984375 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 717.9639892578125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 289.4179382324219 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 851.9334106445312 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 1029.574951171875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 2975.542236328125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 1116.433837890625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 1986.61962890625 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 2010.102294921875 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 871.3245239257812 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 464.37481689453125 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: -46369.34375 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 536.26904296875 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 619.6314697265625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8063, loss_val: nan, pos_over_neg: 503.070068359375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 397.0839538574219 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 594.532958984375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 1666.2305908203125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 7156.5439453125 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1175.934326171875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: -28339.966796875 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1010.4156494140625 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1890.658203125 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 9646.0751953125 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 430.01812744140625 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1805.4990234375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 560.109375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 723.7644653320312 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 380.5801086425781 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 502.8731994628906 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 502.1993103027344 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 689.369384765625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 492.1241149902344 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.831, loss_val: nan, pos_over_neg: -2589.49609375 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 2935.1904296875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 2039.52734375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 2387.474365234375 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 8045.869140625 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 1126.4154052734375 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 449.5064697265625 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 587.3463134765625 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 3915.50830078125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1744.441162109375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 3155.6318359375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 493.842041015625 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1970.5775146484375 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 533.4970703125 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 900.6524658203125 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 729.169677734375 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 758.6554565429688 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 632.5350341796875 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 758.5281372070312 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1710.213623046875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8323, loss_val: nan, pos_over_neg: 2086.177978515625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 2388.03076171875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 960.8485717773438 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: -2708.377197265625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 643.9366455078125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 594.1307983398438 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 1103.0538330078125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 686.2318725585938 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1804.457763671875 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: -10575.740234375 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 407.03070068359375 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 869.8771362304688 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 819.4110107421875 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 259.4411315917969 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 983.0987548828125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 484.352294921875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 459.8370361328125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 580.8311767578125 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 355.9336242675781 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 829.3580322265625 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 667.089599609375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 464.75701904296875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1215.3516845703125 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1491.651123046875 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: -38087.71484375 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 2540.42724609375 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 730.8328247070312 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 1010.2291870117188 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 588.65087890625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 2210.6591796875 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 1368.845947265625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 667.2794799804688 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: -9867.955078125 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 1771.4261474609375 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1385.861328125 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1763.489990234375 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 740.0537109375 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 3574.099365234375 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: 2603.84423828125 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 429.02935791015625 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 754.4320068359375 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1887.0374755859375 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 501.6921081542969 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 2672.47021484375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1439.0081787109375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1824.828857421875 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 681.7651977539062 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: -8929.6318359375 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 583.4266967773438 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 303.5416564941406 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 774.4972534179688 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 575.3872680664062 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 5106.79248046875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1114.0977783203125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 697.4171752929688 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1375.1156005859375 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: -2390.177490234375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1081.122802734375 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 335.034423828125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1028.61279296875 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1440.0985107421875 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: -4549.3330078125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 487.725830078125 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1078.606689453125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2264.98583984375 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 18240.58203125 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 3268.240966796875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 8420.2353515625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 583.5051879882812 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1362.439697265625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 1551.830078125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 3728.397216796875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 734.4082641601562 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 665.7411499023438 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 285.79833984375 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 4854.22900390625 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 456.6615295410156 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 570.9395751953125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 335.8350524902344 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 5533.82470703125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1215.31103515625 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 2543.143310546875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1135.98193359375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 569.00146484375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: -3957.360595703125 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: -4815.84375 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: -13200.6708984375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 576.6307373046875 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 456.93463134765625 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 724.4415893554688 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 395.3525695800781 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1760.052734375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 507.9880065917969 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1447.968505859375 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 384.0812072753906 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 948.8369140625 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 618.4566650390625 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 1205.6412353515625 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1648.4385986328125 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 887.1535034179688 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 1356.9613037109375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 3169.8115234375 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 808.9480590820312 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1064.7669677734375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 11607.8984375 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 1517.557373046875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 2002.6834716796875 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 372.890380859375 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1484.2117919921875 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 3725.77099609375 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 447.615234375 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 490.040771484375 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 30675.06640625 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 2322.7001953125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 379.8511657714844 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 409.9866638183594 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8187, loss_val: nan, pos_over_neg: 2526.408935546875 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: -2115.37353515625 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1750.7880859375 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 695.1790161132812 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 750.36279296875 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 2025.041259765625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 672.623779296875 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: -6797.2294921875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 1437.9180908203125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1802.9931640625 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1240.6964111328125 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1564.6732177734375 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 825.7591552734375 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1030.050048828125 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 237.64691162109375 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1152.6400146484375 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 551.5109252929688 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 506.0106201171875 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1354.660400390625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 2164.59423828125 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 993.8792114257812 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 9325.6357421875 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 572.1201782226562 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 1008.5089111328125 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1288.40771484375 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 466.74267578125 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: -1913.1505126953125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 1523.4580078125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 347.8162536621094 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 1384.3834228515625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: -12146.8798828125 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 828.3737182617188 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 900.9503173828125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 602.5916137695312 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 838.0308227539062 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 376.11920166015625 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 425.5256652832031 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 592.5563354492188 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: -7726.43896484375 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 1854.9525146484375 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 2186.9814453125 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 1013.1942138671875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 723.4630126953125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 2935.57177734375 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 717.93212890625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 295.162841796875 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 2980.05859375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 3282.092529296875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1250.2159423828125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 5792.12890625 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1149.9254150390625 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 2867.792724609375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 1250.1064453125 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 2798.9111328125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 491.6922912597656 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 437.7845153808594 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 507.1268005371094 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1134.4930419921875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1401.178955078125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 53184.08203125 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 1592.207763671875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 523.0501708984375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 527.623779296875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 839.9202880859375 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 739.9036254882812 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: -77431.5234375 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 1589.4307861328125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 825.5154418945312 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: -2371.8251953125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 3726.345947265625 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 1777.417724609375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1008.2501220703125 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 536.8624877929688 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 601.7926635742188 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 4183.14599609375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1759.8658447265625 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 5407.87158203125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: -19817.970703125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.798, loss_val: nan, pos_over_neg: -5268.755859375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 3767.807373046875 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 8663.1259765625 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 771.703369140625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 314.2196044921875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 481.7801818847656 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 528.3541259765625 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 810.2987670898438 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 531.7193603515625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 796.2882690429688 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 2176.011962890625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 488.4314270019531 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1260.7237548828125 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 2602.62451171875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 7204.9814453125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 866.37255859375 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 4382.18359375 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 406.0932922363281 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 699.0784301757812 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 962.165771484375 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 613.1130981445312 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 561.7879638671875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 705.447021484375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 1178.883056640625 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 380.66748046875 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 3858.39306640625 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 500.7049255371094 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 2891.6064453125 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.802, loss_val: nan, pos_over_neg: -2154.655517578125 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 751.4204711914062 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 496.177001953125 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1722.3212890625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 665.0659790039062 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 425.7177429199219 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1330.94189453125 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 418.6142883300781 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 596.8331909179688 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 690.2052001953125 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: -3521.959716796875 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: -6390.9375 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 564.5350341796875 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1177.7896728515625 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: -4362.50927734375 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 975.410888671875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:34:07<128353:43:49, 1540.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: -63498.40234375 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: -1404.64404296875 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 778.8506469726562 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 431.81549072265625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.823, loss_val: nan, pos_over_neg: 2049.84912109375 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 669.4655151367188 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 955.7408447265625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 400.49609375 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8063, loss_val: nan, pos_over_neg: 3292.92236328125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 795.2947387695312 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: -3135.500244140625 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: -4443.685546875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2673.175537109375 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 433.3121337890625 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 314.2413330078125 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 7659.306640625 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1056.6776123046875 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 3514.2490234375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 8905.005859375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 21763.220703125 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 986.9786376953125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 969.1070556640625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 837.5093383789062 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 175.03005981445312 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 344.68414306640625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 986.8167114257812 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1522.5943603515625 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 368.8453674316406 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 522.6987915039062 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 602.3378295898438 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 3696.603271484375 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1034.20947265625 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 535.6829833984375 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 361.4577941894531 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 327.8190612792969 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 752.0994262695312 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 490.8089904785156 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 789.1947021484375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 477.7741394042969 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 4837.41552734375 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 13019.80859375 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 642.1928100585938 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 297.7781677246094 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1646.5860595703125 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 613.0484619140625 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 843.032958984375 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1274.6502685546875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 594.1945190429688 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 567.4609375 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 894.8012084960938 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 6177.12158203125 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 834.978515625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 500.0151062011719 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 956.7249145507812 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1651.908203125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 589.8341674804688 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 644.89306640625 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 825.6864624023438 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 919.1619873046875 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 679.3697509765625 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 2354.1923828125 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2054.32177734375 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1026.1640625 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 3256.171630859375 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 690.1441650390625 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1097.2044677734375 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 771.5187377929688 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 2318.27734375 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 728.4010620117188 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 482.4165344238281 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 500.2852478027344 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 2342.86962890625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 3502.05810546875 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 889.9651489257812 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 2476.060546875 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1543.55908203125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: -5054.220703125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 8188.79052734375 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1227.62255859375 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 546.9125366210938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 4936.55615234375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 545.0326538085938 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 275.7101745605469 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1955.202392578125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 610.1934204101562 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 570.9957275390625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 508.02655029296875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 856.9032592773438 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 1180.514404296875 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 808.3939208984375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 932.9619140625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 927.6397094726562 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 502.5204772949219 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 1504.6876220703125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 782.722900390625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1334.5184326171875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.793, loss_val: nan, pos_over_neg: -33255.0859375 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 2466.36962890625 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 396.7500305175781 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 229.98016357421875 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 323.9969177246094 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1161.4857177734375 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 66007.90625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 15195.6357421875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 6034.3779296875 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 30852.10546875 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1074.2105712890625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 775.174072265625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1271718.75 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 362.1174011230469 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 828.5946655273438 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 389.8658447265625 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 450.5089416503906 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 734.4364624023438 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 998.560302734375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 700.1484375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1857.8173828125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 819.9573364257812 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1158.5223388671875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 3885.04931640625 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 848.8702392578125 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 395.37310791015625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 589.8599243164062 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 335.6944580078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 411.1377258300781 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 1384.515380859375 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 2289.143310546875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 852.7817993164062 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 296.4571838378906 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 543.2518310546875 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 397.6554260253906 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 627.78759765625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 975.158203125 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 769.6995239257812 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 318.01409912109375 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1198.5057373046875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 2261.6103515625 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 436.81280517578125 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 443.4547119140625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 477.98052978515625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1090.1102294921875 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 1545.3961181640625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 945.1915893554688 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 862.65625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 509.18939208984375 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 66345.296875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1123.2803955078125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1573.069580078125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 564.0637817382812 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 772.4330444335938 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 370.7440490722656 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 446.2478942871094 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 1404.16796875 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1401.9129638671875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: -1787.317626953125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 428.2508544921875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 71613.5625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 6772.77001953125 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 635.1737060546875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1355.0281982421875 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 626.7200927734375 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 2251.3486328125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 18647.607421875 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 615.5497436523438 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 624.041015625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 385.3411560058594 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 645.4514770507812 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 474.4189453125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 794.4808349609375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 559.95263671875 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 46387.59375 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 1076.6697998046875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 594.71630859375 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1320.42626953125 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 770.8413696289062 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: -15059.7041015625 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1782.0509033203125 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 514.9129028320312 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 611.9830322265625 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1365.000732421875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 540.1697387695312 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 1355.299072265625 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 699.5899658203125 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 976.2774047851562 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 701.0338745117188 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 923.2644653320312 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 1829.689453125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.805, loss_val: nan, pos_over_neg: -109610.7421875 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 764.6834106445312 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 308.6045837402344 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 383.5429992675781 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 426.5934753417969 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 2173.305419921875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 957.2387084960938 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 874.745849609375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 542.9815063476562 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 657.271240234375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 580.8350219726562 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 1751.392578125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 647.4856567382812 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 1067.75048828125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 841.9443969726562 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1141.2164306640625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 602.6083374023438 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 554.4554443359375 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 821.479248046875 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 557.0206909179688 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 2124.082275390625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1008.0726928710938 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: -1569.28759765625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: -7024.76416015625 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: -5476.3056640625 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 2922.562744140625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 409.5931396484375 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 1361.8626708984375 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 1339.951171875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1245.224365234375 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 814.1017456054688 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 422.3680725097656 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 3019.611328125 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 565.1367797851562 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 1162.681884765625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 436.7347412109375 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: -8957.9365234375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 570.22705078125 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1826.3426513671875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 683.8556518554688 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1802.29296875 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1970.7918701171875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 1690.4202880859375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 377.8922424316406 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 804.94921875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1112.5986328125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 10492.3486328125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1136.4002685546875 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 350.0618896484375 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 335.9877014160156 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 4977.697265625 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 741.0404663085938 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 2560.99951171875 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 2477.67822265625 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1290.3646240234375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 330.02178955078125 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 457.3935852050781 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 7053.30029296875 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 3005.173828125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 8876.5458984375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 641.5742797851562 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 440.0751953125 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 682.5281982421875 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 451.3333740234375 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1781.6910400390625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 909.5783081054688 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 6702.25634765625 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 7496.71337890625 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 1009.8355712890625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 3391.54931640625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: -3640.755859375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 691.9104614257812 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 896.2503051757812 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 1075.78466796875 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 987.2369384765625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1261.477783203125 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 3910.02197265625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1025.741455078125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 759.9296264648438 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 3585.173583984375 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1292.9847412109375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 2096.53466796875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1154.648681640625 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1131.4691162109375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 455.58612060546875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 537.3103637695312 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 665.1536865234375 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 799.5388793945312 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 668.581298828125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 511.65460205078125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 2092.059326171875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 991.4310913085938 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 2267.2509765625 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 1236.62109375 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 2138.695556640625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1716.574951171875 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1314.3165283203125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 442.5426330566406 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 454.0780029296875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 646.5370483398438 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 772.8195190429688 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 2100.334228515625 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1017.0369873046875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 449.43951416015625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1434.7083740234375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 792.5833740234375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 678.6654052734375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 458.33758544921875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 898.6173706054688 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: -3266.9287109375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1482.7069091796875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 4211.78466796875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 856.9585571289062 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 533.3529052734375 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 951.2367553710938 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 2156.69189453125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 3002.432373046875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 690.951904296875 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 727.4552612304688 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 559.5714721679688 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 730.1099853515625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 513.123291015625 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 537.9579467773438 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: -2901.7958984375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 837.4459838867188 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 885.4942016601562 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 1736.7890625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 3261.197998046875 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 1079.3392333984375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 3364.860107421875 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 483.61798095703125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 467.3896179199219 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 626.209228515625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 784.8660888671875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 363.7149353027344 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 245.8863067626953 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 353.7322692871094 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 1004.8829956054688 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 978.402099609375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1548.8348388671875 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 2046.43896484375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 7050.88134765625 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1715.014892578125 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 852.7529296875 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 805.6820678710938 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 661.08349609375 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 513.146728515625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 686.1314697265625 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 298.9138488769531 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1894.7020263671875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 858.5812377929688 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 629.7376708984375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 828.5289306640625 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: -4577.22607421875 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 254766.34375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 674.4103393554688 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 846.69140625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1022.12451171875 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 2390.629638671875 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 809.4371948242188 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 970.8272705078125 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1547.991943359375 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 829.3427734375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 614.5259399414062 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1829.554443359375 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1852.558837890625 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 2336.687744140625 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 749.7560424804688 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1109.2320556640625 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: -8500.072265625 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 28792.02734375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 2097.7470703125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8038, loss_val: nan, pos_over_neg: 598.8587646484375 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2183.456787109375 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 374.5457763671875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2319.73828125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 459.6758117675781 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1374.0767822265625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 820.22119140625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 7301.765625 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 2271.572998046875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 577.9366455078125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 759.5474243164062 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 2591.30078125 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 510.37200927734375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: -131802.65625 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 11707.1484375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 330.88226318359375 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1370.9085693359375 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.794, loss_val: nan, pos_over_neg: -2877.7041015625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 2172.7890625 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 750.858154296875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 767.4074096679688 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 505.8196716308594 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 571.7809448242188 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 8018.91943359375 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 917.4183959960938 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: -5262.7509765625 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 740.373046875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 432.48529052734375 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: -18777.599609375 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 4764.18798828125 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: -2695.98583984375 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 354.2461242675781 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 4428.7705078125 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1185.583984375 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 542.7811279296875 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: -3717.813232421875 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 1123.4168701171875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1406.032470703125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1665.03955078125 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 3001.246826171875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 918.7979125976562 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 557.0603637695312 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 533.2225341796875 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 921.9916381835938 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 748.8779296875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 610.3912353515625 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1422.93017578125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 1032.373046875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 6142.32861328125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 592.9151000976562 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1696.3623046875 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 792.5807495117188 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 406.8873596191406 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 1148.4752197265625 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 614.239501953125 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 624.7342529296875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 572.8217163085938 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 532.748779296875 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1151.20361328125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 1198.190673828125 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 289.9647216796875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 394.7234802246094 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 645.1820678710938 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 751.9295654296875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 2891.845703125 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 7055.19970703125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 2051.553955078125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1453.8302001953125 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 2243.802734375 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 321.03363037109375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 459.6149597167969 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 518.8468627929688 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1613.62451171875 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 814.6253662109375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 655.5267944335938 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 867.70654296875 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 17000.279296875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 2055.490478515625 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 4160.91357421875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1249.201416015625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 986.7255859375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 12593.162109375 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1195.603271484375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 386.1304931640625 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1403.2662353515625 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 859.9807739257812 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 933.9893798828125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 1390.8570556640625 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1366.340576171875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 38826.2265625 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 489.67547607421875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 853.2550659179688 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1606.6942138671875 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 586.46142578125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 505.93572998046875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 494.5060729980469 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 2298.389404296875 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 3320.38232421875 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 759.9004516601562 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 2171.25537109375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 361.7827453613281 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 534.2008666992188 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 1639.098388671875 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1077.38525390625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1758.748046875 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 664.0082397460938 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 962.6096801757812 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1150.0067138671875 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1549.5462646484375 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 868.1378173828125 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 592.3863525390625 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1858.0831298828125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 392.9716491699219 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1201.75 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 1129.94775390625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 836.1619262695312 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1791.099853515625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 5645.607421875 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 530.710205078125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 562.8409423828125 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 4378.34521484375 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 512.8820190429688 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 2604.318359375 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 723.5054931640625 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1033.51318359375 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 816.5736083984375 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 10818.5283203125 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 783.2006225585938 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 845.482177734375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 691.052490234375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: -6368.7431640625 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 525.3049926757812 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 426.94488525390625 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 726.5780639648438 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 762.234375 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: -2739.56201171875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 6835.173828125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 593.3592529296875 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 606.7504272460938 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 916.6737670898438 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 4459.5693359375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 987.4756469726562 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 452.6397705078125 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1364.4432373046875 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 482.0149230957031 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 948.973388671875 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 854.1087036132812 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1610.6510009765625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 803.3443603515625 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 907.0459594726562 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 733.392578125 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 1269.9219970703125 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1568.229736328125 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1802.36474609375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 490.0534973144531 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 2484.24951171875 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 602.3790893554688 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 9597.939453125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 301.9348449707031 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 306.8106689453125 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1058.095703125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 979.5873413085938 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 458.46563720703125 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 668.3240966796875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 976.5011596679688 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 1727.754638671875 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: -10456.7470703125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 699.001953125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 1678.176025390625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 6074.2265625 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1528.131591796875 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1007.0841064453125 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 508.3394470214844 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 980.5523681640625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1774.9588623046875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 558.9774169921875 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 487.21026611328125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 1670.16748046875 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 3068.245849609375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 19259.580078125 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 3270.048828125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 2764.908447265625 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1310.6614990234375 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 1233.3333740234375 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 604.0872802734375 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 638.7528686523438 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 4642.20458984375 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1268.2403564453125 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1338.9716796875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 575.092529296875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 6754.8857421875 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 2008.11962890625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.793, loss_val: nan, pos_over_neg: -3756.110107421875 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: -17429.625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 356.93365478515625 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 513.8682861328125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1204.990234375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 767.279052734375 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 1186.288818359375 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1323.49609375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 743.5001831054688 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 311.86328125 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1532.587890625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 7476.2177734375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 600.1199951171875 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 1605.2462158203125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 425.22515869140625 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 484.6129150390625 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1920.95703125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 3082.03955078125 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 3429.383056640625 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1875.0966796875 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1006.7883911132812 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: -5818.66357421875 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 36208.0703125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 803.4931030273438 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 282.3153381347656 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1621.066162109375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 4055.978759765625 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 3282.948974609375 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1137.7889404296875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1328.923583984375 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 620.6174926757812 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 887.0232543945312 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 811.566650390625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 803.5430908203125 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 993.8070068359375 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 2861.668701171875 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 973.3035888671875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: -2495.940185546875 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1351.11279296875 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1719.6636962890625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1242.2799072265625 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 1061.1875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 903.077392578125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 468.3442077636719 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 432.63037109375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 820.14404296875 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 875.0997314453125 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 1755.028076171875 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 706.664306640625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: -19287.419921875 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 2375.81005859375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 2187.46240234375 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 512.824951171875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 792.62646484375 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 631.9173583984375 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 449.45025634765625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 872.989990234375 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 847.7382202148438 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 527.450439453125 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 994.3861694335938 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1400.280029296875 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 3001.210693359375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1025.3270263671875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 673.4222412109375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1622.7994384765625 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 878.1315307617188 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1364.8138427734375 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 641.70458984375 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 15440.3623046875 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 2260.721435546875 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 922.5271606445312 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 671.0785522460938 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1111.274658203125 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 620.4762573242188 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: 955.1898193359375 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 800.2273559570312 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 738.2505493164062 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 647.98388671875 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1738.6575927734375 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 559.2777099609375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 474.6025695800781 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 757.6766967773438 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 2117.593505859375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1024.153564453125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 446.3888244628906 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 751.158935546875 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 614.2450561523438 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1023.9210815429688 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 608.3289184570312 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 366.9979553222656 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 616.4905395507812 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 1740.9176025390625 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 665.5348510742188 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 753.06005859375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 576.676513671875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 665.609619140625 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 622.9171142578125 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 552.5098876953125 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 544.1439208984375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 393.9538269042969 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 465.9170837402344 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 798.0965576171875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1690.6651611328125 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 6191.83642578125 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 470.3009338378906 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.794, loss_val: nan, pos_over_neg: -4910.5517578125 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: -6316.48193359375 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1119.3155517578125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 747.5202026367188 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 686.8466796875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1466.72705078125 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 574.6102905273438 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 465.63958740234375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 816.8513793945312 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1014.0184326171875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: -8887.685546875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 6134.0693359375 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 531.3616943359375 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: -11295.068359375 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1047.58935546875 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: -2374.389404296875 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1046.3956298828125 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 816.7796630859375 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1725.5653076171875 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 2939.059814453125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 52850.203125 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1923.245361328125 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 751.2413940429688 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 1006.40625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 828.8707275390625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 2294.338134765625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: -7332.9970703125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1238.376708984375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 4489.60546875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 868.9010620117188 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 820.6192626953125 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1133.1461181640625 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 4681.51123046875 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 361.4461364746094 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 737.4475708007812 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1240.77197265625 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 575.0955200195312 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 493.2822570800781 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 310.0205383300781 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 784.193359375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 994.8886108398438 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1741.574951171875 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 1338.97265625 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2246.092529296875 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 4567.68994140625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1453.8636474609375 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 1381.121826171875 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 8225.3271484375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: -7804.58447265625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 575.8727416992188 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 1703.7696533203125 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 313.4212646484375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 544.9563598632812 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 660.0844116210938 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1060.7681884765625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 2062.4814453125 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 395.7106018066406 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 448.0409240722656 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 696.9360961914062 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1272.4185791015625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 835.239501953125 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 691.7456665039062 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 517.0181274414062 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 1140.1607666015625 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 486.67437744140625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1434.550048828125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 629.0292358398438 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 397.1302185058594 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1625.5811767578125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 997.7932739257812 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 501.8439025878906 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 647.5287475585938 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1479.3365478515625 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1767.5965576171875 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1632.59130859375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 608.7339477539062 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 659.6034545898438 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 365.6700439453125 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 4275.078125 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 496.3498840332031 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1814.7469482421875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 922.0151977539062 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1234.7479248046875 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1345.40966796875 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 4095.68701171875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 1433.103759765625 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 787.6798706054688 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 788.225341796875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 687.22509765625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 527.0403442382812 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 257.7713928222656 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 884.0192260742188 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 568.7479248046875 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 562.1410522460938 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 1104.09619140625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 531.97802734375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: -9103.060546875 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1547.0057373046875 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1317.6787109375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 3611.9853515625 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 443.3130187988281 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 1212.90283203125 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 856.2432250976562 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 664.853759765625 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 824.3702392578125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 811.4064331054688 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1001.9569702148438 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1369.954345703125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1133.918212890625 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 2235.712158203125 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1004.308349609375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 716.4976806640625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 2572.965576171875 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 1643.1007080078125 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 283.6566467285156 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 502.9090881347656 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 3588.712158203125 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 672.0611572265625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 413.3063659667969 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 627.9054565429688 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 2355.552001953125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1465.416015625 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 781.7328491210938 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1193.858154296875 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 998.5091552734375 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 761.2626953125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 588.9302978515625 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: -119158.015625 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 846.7883911132812 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 712.54150390625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 684.87158203125 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 793.5741577148438 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 966.4264526367188 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 966.3330688476562 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1289.2564697265625 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 2331.548583984375 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 619.3016357421875 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1356.515625 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 906.8331909179688 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 535.7337036132812 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1381.6104736328125 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2022.869873046875 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 301.3951110839844 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 347.5727233886719 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 867.8706665039062 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1322.774658203125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 449.409423828125 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 761.7055053710938 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 606.7384033203125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 727.0296020507812 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1361.9483642578125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 2198.949951171875 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1104.6585693359375 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: -12815.4892578125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 440.9384460449219 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 963.2166137695312 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 936.3597412109375 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 792.7920532226562 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1128.07177734375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: -3845.621826171875 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 2522.64697265625 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 10509.884765625 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: -1938.308349609375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 713.2323608398438 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 805.439697265625 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 437.2855529785156 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 626.0494995117188 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 584.2904052734375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 475.8515319824219 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 501.5838928222656 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: -4838.79541015625 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 19364.912109375 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: -64386.7734375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 392.3672790527344 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2301.289306640625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 14649.4267578125 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1314.8482666015625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 545.5765991210938 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 903.9563598632812 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1299.1239013671875 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 868.5971069335938 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 435.499267578125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 475.9422302246094 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 321.62042236328125 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1740.5782470703125 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: -6930.9580078125 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 2111.82861328125 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 270.9315185546875 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 641.5280151367188 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 2164.36474609375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2441.27001953125 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 555.2803955078125 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 807.4930419921875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1179.82373046875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 605.2902221679688 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 606.2072143554688 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 774.9041748046875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1533.8118896484375 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: -4061.68017578125 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 1575635.5 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1326.8994140625 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1770.8631591796875 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 583.8974609375 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 401.96990966796875 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 888.6469116210938 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1067.8624267578125 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1599.1925048828125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1090.7177734375 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 643.86279296875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 842.5670166015625 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 801.8231201171875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 2853.75146484375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 7558.4921875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 611.513916015625 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 673.5589599609375 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.799, loss_val: nan, pos_over_neg: -3855.38427734375 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: -3189.50146484375 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 10405.4921875 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 876.9636840820312 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 348.2359619140625 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 822.7430419921875 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1652.9315185546875 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 1014.23193359375 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 39972.44140625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 768.8846435546875 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 3315.307861328125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: -6004.58935546875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 941.2816162109375 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 1165.6787109375 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 671.3544921875 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 430.5423278808594 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2255.256103515625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 622.8724365234375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [3:00:07<128873:59:37, 1546.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 428.9358825683594 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 697.8335571289062 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1150.2855224609375 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 913.9742431640625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 772.8807373046875 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2086.922607421875 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 1660.8465576171875 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 436.9815368652344 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 978.3173828125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 526.50732421875 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2694.52001953125 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 537.5466918945312 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: -7479.37841796875 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 635.6077880859375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 690.6222534179688 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 563.780517578125 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 381.62872314453125 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 503.6488037109375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 428.4697570800781 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 917.7058715820312 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 612.5421142578125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 883.3790283203125 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1138.4781494140625 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: -6110.99560546875 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 887.5757446289062 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1328.2447509765625 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 771.7894287109375 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 1888.6058349609375 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1584.2965087890625 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.8204, loss_val: nan, pos_over_neg: 579.3897705078125 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 972.3845825195312 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1616.6739501953125 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1543.902099609375 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1948.293212890625 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1665.3843994140625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1276.6629638671875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 527.681884765625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 653.396728515625 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 360.1602783203125 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 397.4815673828125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1400.1624755859375 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 820.457763671875 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 752.024658203125 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 1922.426025390625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 3063.03271484375 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 2424.543212890625 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 520.7303466796875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1135.3470458984375 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1952.3404541015625 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 594.1839599609375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1405.9984130859375 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 439.0948181152344 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 352.40777587890625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 37152.86328125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1449.5152587890625 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 469.5511169433594 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 739.2529907226562 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 604.7349853515625 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 528.3257446289062 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 329.6050720214844 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 722.1385498046875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 930.3552856445312 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1119.329833984375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 535.0650634765625 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 616.567138671875 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1683.496826171875 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1784.888916015625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 730.9036254882812 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 831.4407958984375 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1078.921142578125 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 632.2325439453125 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1408.2044677734375 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 717.071044921875 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 631.8228149414062 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 3694.9912109375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1464.51171875 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1083.5924072265625 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.804, loss_val: nan, pos_over_neg: -1773.3355712890625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 2370.790283203125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1696.8955078125 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1846.906494140625 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 429.0865783691406 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 694.5978393554688 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1102.4539794921875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: -4083.661865234375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 458.9219665527344 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 657.19482421875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 2273.007568359375 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1147.3109130859375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 795.0779418945312 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1427.0699462890625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 1373.8673095703125 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1836.293212890625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 960.2999267578125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 552.9671630859375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.796, loss_val: nan, pos_over_neg: -6391.87646484375 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 731.1671142578125 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 896.9277954101562 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 602.88818359375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 705.3189086914062 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 686.193359375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1422.95556640625 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 721.16357421875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 501.11395263671875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 2906.50146484375 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 549.36474609375 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 483.41973876953125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 911.9810180664062 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 540.1123046875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 66713.828125 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: -6137.42041015625 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 456.30010986328125 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 844.8178100585938 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 1665.5413818359375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 4963.1142578125 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 528.2459716796875 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1452.23095703125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1614.896484375 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 364.45068359375 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 370.260498046875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 786.4751586914062 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 565.1906127929688 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 784.0635375976562 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1042.645263671875 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: -2566.99853515625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 12851.939453125 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 752.3949584960938 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: -6062.107421875 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 2050.845703125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 10519.1748046875 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1493.0867919921875 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 1201.5826416015625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1934.3111572265625 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 767.5126342773438 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 936.364990234375 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2383.99072265625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1618.658203125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 546.4389038085938 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 740.718994140625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: -83907.328125 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 985.8588256835938 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 446.6208801269531 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 552.083984375 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 6888.2744140625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 7888.6904296875 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 3429.420166015625 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 2321.193115234375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1269.43115234375 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1569.4462890625 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 647.0580444335938 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 379.95294189453125 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 3709.048828125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 937.6557006835938 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: -1576.2484130859375 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 515.7042846679688 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 487.8143005371094 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 977.7310180664062 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 797.2882690429688 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 2649.50244140625 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 476.7662048339844 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 965.9124145507812 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1181.5313720703125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2079.99853515625 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: -3816.63525390625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: -1894.2913818359375 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1884.459228515625 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: -3350.841552734375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1991.629638671875 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 2555.37646484375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 637.829833984375 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: -3673.357421875 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 377.8650207519531 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 922.77685546875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 529.1988525390625 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 899.443115234375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1532.7294921875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: -1647.5322265625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 829.6309814453125 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 432.7832336425781 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 740.9352416992188 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 3806.761474609375 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 565.2111206054688 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 576.7904052734375 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 517.6797485351562 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 785.6031494140625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1131.0595703125 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1219.19775390625 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1099.1915283203125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 488.833251953125 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 767.6224975585938 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 680.2831420898438 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1348.8056640625 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1259.6773681640625 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: -4745.80859375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 5461.07568359375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 11720.4765625 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 774.0596313476562 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1294.8817138671875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 724.52294921875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1110.458251953125 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 603.7266845703125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 605.8621826171875 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1046.336181640625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: -8823.2021484375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1244.0343017578125 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 12788.654296875 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1719.4315185546875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 592.818603515625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: -1913.859130859375 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: -4722.68994140625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 522.5916137695312 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 690.798828125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1843.544921875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1105.508056640625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 12835.9736328125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 1811.6866455078125 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1218.5728759765625 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 11018.765625 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: -3427.509765625 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1476.613525390625 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1313.4266357421875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 538.670166015625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 364.5287170410156 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1552.743408203125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 422.23583984375 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.778, loss_val: nan, pos_over_neg: -7776.11181640625 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 460.7677917480469 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1037.984375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 471.42449951171875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 718.0189208984375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1582.586181640625 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 5158.13427734375 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 394.41473388671875 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 427.8536682128906 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 599.8798217773438 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 2287.007568359375 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: -2119.53271484375 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 6673.99951171875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 761.7661743164062 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 658.4805908203125 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 673.258544921875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 313.3067626953125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 980.4190063476562 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 883.4021606445312 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 396.6631164550781 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 704.9381713867188 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1368.80322265625 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 678.2012329101562 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: -7014.08740234375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1227.4615478515625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1575.49609375 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1010.3299560546875 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 393.0118103027344 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 926.4903564453125 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 465.0843200683594 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1712.1986083984375 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1597.8729248046875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 624.1519165039062 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 933.4010009765625 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 974.5264282226562 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1205.128173828125 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 9147.5869140625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 9475.1328125 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 2051.7705078125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1509.77197265625 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 2625.31640625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 711.1364135742188 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 580.8555297851562 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 780.76904296875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 875.2508544921875 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 445.3579406738281 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 488.5940246582031 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 428.31988525390625 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 1573.429931640625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 1796.359619140625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 3017.851806640625 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 2968.292724609375 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1796.0906982421875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 539.0194702148438 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 767.386962890625 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 2223.392822265625 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 3985.248291015625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 3584.44677734375 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 4385.32080078125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 1094.7176513671875 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 566.288818359375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 559.4588623046875 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 676.59033203125 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: -10575.0966796875 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 1426.5076904296875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 4538.88134765625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 1513.7890625 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 823.6185913085938 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 558.0458984375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 1071.451171875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 534.5881958007812 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 2961.960205078125 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 414.68328857421875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 668.7031860351562 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 433.7723388671875 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1465.3232421875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 4843.8359375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1107.41259765625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 4191.64111328125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: -9412.3251953125 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 571.4972534179688 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 468.9420471191406 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 609.752197265625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 394.3424987792969 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 3638.52685546875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 753.8265380859375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 781.2509155273438 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1911.822509765625 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 369.447509765625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1185.222412109375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 412.6603698730469 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 1527.4268798828125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: -2817.8740234375 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 728.3587646484375 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: -11911.3564453125 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1324.6512451171875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 955.4996337890625 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1202.02099609375 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 756.0938110351562 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 737.55224609375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 5872.43798828125 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 4817.99658203125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1396.5516357421875 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 912.0610961914062 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 3374.256103515625 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1003.866455078125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 2428.65869140625 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 2939.701416015625 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 2724.62353515625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 7491.28271484375 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 668.3619995117188 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 4195.955078125 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 3039.54638671875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 308.14727783203125 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 840.6764526367188 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 2625.63330078125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: -2745.31689453125 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 2804.6953125 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 2691.818603515625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1671.82177734375 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 644.3760986328125 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: -2287.344482421875 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: -20943.083984375 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 347.6964111328125 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1393.4532470703125 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 764.9035034179688 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: -24780.52734375 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 3789.2685546875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1496.8157958984375 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1363.8138427734375 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: -4111.2705078125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 1276.9873046875 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 805.3074951171875 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: 474.0218200683594 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 6983.0185546875 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 738.7198486328125 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: -12129.65625 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 616.255126953125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1924.2666015625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 452.36163330078125 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1730.7940673828125 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1089.67333984375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 421.09869384765625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 617.9453125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2117.635009765625 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 852.4578247070312 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: -16992.03125 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 357.0045471191406 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 3698.367919921875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 2238.535400390625 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 752.00732421875 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 5238.2080078125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 5820.07373046875 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 6015.05517578125 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 521.9140625 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1866.7630615234375 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 545.6673583984375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 491.5013122558594 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 448.3458251953125 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 742.8543090820312 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1179.1024169921875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 1093.7686767578125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1259.8868408203125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 809.4418334960938 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 2410.3447265625 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 745.2150268554688 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 626.1849365234375 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 800.22412109375 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 747.7406616210938 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1323.8482666015625 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 622.9674072265625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 426.49200439453125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 361.19256591796875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 385.49285888671875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 1052.20068359375 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 662.35302734375 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1878.501220703125 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 608.7166137695312 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 678.0479736328125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1173.6617431640625 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 474.1816101074219 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 502.66180419921875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 580.0399169921875 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 840.6097412109375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 3846.734375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 746.0570678710938 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 665.8095092773438 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 506.88092041015625 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: -2432.431640625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1222.33056640625 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 2609.641357421875 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1392.4781494140625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 1041.1395263671875 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1184.4716796875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: -28173.23828125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 485.9824523925781 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 278.99029541015625 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 644.5386352539062 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 6062.1083984375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 970.5446166992188 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1763.4986572265625 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 820.8494262695312 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1200.04345703125 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1141.322998046875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 3083.2236328125 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 2552.2998046875 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 4209.416015625 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1324.57763671875 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 663.5140380859375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 726.4305419921875 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1295.2330322265625 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1015.5042724609375 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 801.7958984375 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 715.336181640625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 645.594482421875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 728.6096801757812 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 8423.6201171875 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 908.8882446289062 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 678.6666259765625 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1337.5234375 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 478.3893127441406 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 305.4659729003906 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 369.3839416503906 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 611.5647583007812 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 28139.322265625 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: -23636.158203125 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 511.7979736328125 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 488.51007080078125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 905.6531982421875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 911.4586181640625 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 982.7443237304688 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 856.9407958984375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1126.565185546875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 5295.412109375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 2287.43505859375 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 630.584228515625 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 748.6232299804688 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 862.0775146484375 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 3509.78759765625 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: -12133.607421875 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 756.2989501953125 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 390.10845947265625 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 781.8380737304688 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1841.4539794921875 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 856.7907104492188 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1003.4679565429688 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 4693.34814453125 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 3470.660888671875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: -5669.931640625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2063.74072265625 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1055.0738525390625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 920.77978515625 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: -3044.81640625 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 637.724853515625 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 2969.688720703125 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 343.1136474609375 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 437.0045166015625 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1453.9031982421875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 988.6363525390625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1468.8133544921875 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 3120.544189453125 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 594.1304321289062 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 3163.64453125 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 26648.23828125 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 530.4164428710938 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 653.61376953125 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 4176.9560546875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 964.1487426757812 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 426.72442626953125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 2710.89794921875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1317.6871337890625 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 749.3316650390625 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 486.4423522949219 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 759.6563720703125 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 5131.107421875 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 643.6033325195312 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1197.5244140625 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 897.130859375 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 966.3455810546875 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 393.6354064941406 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 693.9427490234375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 450.16058349609375 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1076.294921875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 106919.8984375 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: -3397.315673828125 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: -5197.123046875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 3934.0126953125 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 946.2369384765625 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 766.0114135742188 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1807.034912109375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 12769.0576171875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1674.4249267578125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 731.5914916992188 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 467.43280029296875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1002.1162719726562 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 8935.2607421875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 572.7791748046875 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 2614.563232421875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 419.3586730957031 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 2327.079345703125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1043.0584716796875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: -2085.707763671875 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 650.7530517578125 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 619.3650512695312 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1717.6756591796875 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1620.150146484375 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 9542.1181640625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1029.22314453125 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 2185.114013671875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 425.0171813964844 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1944.7720947265625 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 1649.0714111328125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 724.064208984375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 476.6556396484375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: -2422.6826171875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 741.0624389648438 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 466.5634460449219 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 30195.052734375 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 1487.59326171875 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 4326.4765625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 2362.31640625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 812.2713623046875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 825.9065551757812 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 615.0906982421875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: -1879.7386474609375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1843.1119384765625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1716.8936767578125 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1090.2593994140625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 12748.916015625 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1136.0177001953125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 676.0687255859375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 434.2704162597656 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 612.4511108398438 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 2180.54443359375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 346.45355224609375 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1080.908203125 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 920.37109375 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 562.7703247070312 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 492989.15625 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 338.01220703125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: -16869.29296875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 902.6575927734375 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1750.488525390625 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 804.6975708007812 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1690.6673583984375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 850.6165771484375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 279.65325927734375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: -9493.49609375 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1462.52001953125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 584.7715454101562 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: -44395.796875 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1340.50439453125 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 381.44903564453125 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 29656.103515625 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1506.8211669921875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1661.3121337890625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 874.22802734375 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 459.77276611328125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1152.4139404296875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1173.9412841796875 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 775.2019653320312 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 755.7506713867188 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 982.4976196289062 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1751.183837890625 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 742.9842529296875 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1132.0208740234375 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 708.4740600585938 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1403.37646484375 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 2365.86279296875 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 12226.8056640625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1175.0821533203125 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "l2_alpha = 0.000\n",
    "n_epochs=300000\n",
    "\n",
    "acc_train_lst, acc_val_lst = [], []\n",
    "acc_train_SYT_lst, acc_val_SYT_lst = [], []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    ds_run, dl_run = get_ds_dl(input_xtr, run_transform, expand_dim=False)\n",
    "    features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    ds_run, dl_run = get_ds_dl(input_xval, run_transform, expand_dim=False)\n",
    "    features_val = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    \n",
    "    ds_run, dl_run = get_ds_dl(input_xtr_SYT, run_transform, expand_dim=False)\n",
    "    features_train_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    ds_run, dl_run = get_ds_dl(input_xval_SYT, run_transform, expand_dim=False)\n",
    "    features_val_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "\n",
    "    acc_train, acc_val = {}, {}\n",
    "    acc_train_SYT, acc_val_SYT = {}, {}\n",
    "    C_toUse = np.array([1e1,1e0,1e-1,1e-2])\n",
    "    for C in C_toUse:\n",
    "        logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "        logreg.fit(features_train, y_labeled_train)\n",
    "        acc_train_tmp = logreg.score(features_train, y_labeled_train)\n",
    "        acc_train[C] = acc_train_tmp\n",
    "        acc_val_tmp = logreg.score(features_val, y_labeled_val)\n",
    "        acc_val[C] = acc_val_tmp\n",
    "        \n",
    "        logreg_SYT = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "        logreg_SYT.fit(features_train_SYT, y_labeled_train_SYT)\n",
    "        acc_train_tmp = logreg_SYT.score(features_train_SYT, y_labeled_train_SYT)\n",
    "        acc_train_SYT[C] = acc_train_tmp\n",
    "        acc_val_tmp = logreg_SYT.score(features_val_SYT, y_labeled_val_SYT)\n",
    "        acc_val_SYT[C] = acc_val_tmp\n",
    "\n",
    "    acc_train_lst.append(acc_train)\n",
    "    acc_val_lst.append(acc_val)\n",
    "    acc_train_SYT_lst.append(acc_train_SYT)\n",
    "    acc_val_SYT_lst.append(acc_val_SYT)\n",
    "\n",
    "    acc_train_df, acc_val_df, acc_train_SYT_df, acc_val_SYT_df = pd.DataFrame(acc_train_lst), pd.DataFrame(acc_val_lst), pd.DataFrame(acc_train_SYT_lst), pd.DataFrame(acc_val_SYT_lst)\n",
    "    \n",
    "    acc_train_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train.csv')\n",
    "    acc_train_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train_SYT.csv')\n",
    "    acc_val_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val.csv')\n",
    "    acc_val_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val_SYT.csv')\n",
    "    \n",
    "    \n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "   \n",
    "    \n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "# model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=efficient'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc.estimate_size_of_float_array(input_shape=(80000,3,224,224), bitsize=32)/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape, features_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train_SYT)\n",
    "    # logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "    # acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "    # acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(1)).fit(features_train, y_labeled_train_SYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "#                                     torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "#                                     torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                                     n_transforms=1,\n",
    "#                                     class_weights=np.array([1]),\n",
    "#                                     # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "#                                     transform=scripted_transforms_validation,\n",
    "#                                     # DEVICE='cpu',\n",
    "#                                     DEVICE='cpu',\n",
    "#                                     dtype_X=torch.float32,\n",
    "#                                     dtype_y=torch.int64,\n",
    "                                    \n",
    "#                                     temp_uncertainty=16\n",
    "#                                     )\n",
    "# dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "# #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "#                                                 batch_size=1024,\n",
    "#                                                 shuffle=False,\n",
    "#                                                 drop_last=False,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 num_workers=32,\n",
    "#                                                 persistent_workers=True,\n",
    "#                                                 # prefetch_factor=0\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_labe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(images_dup[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(images_dup[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "#                                     torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "#                                     torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                                     n_transforms=1,\n",
    "#                                     class_weights=np.array([1]),\n",
    "#                                     # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "#                                     transform=scripted_transforms_validation,\n",
    "#                                     # DEVICE='cpu',\n",
    "#                                     DEVICE='cpu',\n",
    "#                                     dtype_X=torch.float32,\n",
    "#                                     dtype_y=torch.int64,\n",
    "                                    \n",
    "#                                     temp_uncertainty=16\n",
    "#                                     )\n",
    "# dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "# #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "#                                                 batch_size=1024,\n",
    "#                                                 shuffle=False,\n",
    "#                                                 drop_last=False,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 num_workers=32,\n",
    "#                                                 persistent_workers=True,\n",
    "#                                                 # prefetch_factor=0\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedded.shape, labels_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_dup, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(evr[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "\n",
    "# # mpl.rcParams['image.cmap'] = 'Set1'\n",
    "# %matplotlib notebook\n",
    "# plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# # plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# # plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Precomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "no_transforms = torch.nn.Sequential()\n",
    "scale_rsz_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "tile_transforms = torch.nn.Sequential(augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "\n",
    "\n",
    "scripted_all_transforms = torch.jit.script(all_transforms)\n",
    "scripted_no_transforms = torch.jit.script(no_transforms)\n",
    "scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "scripted_tile_transform = torch.jit.script(tile_transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_no_transforms\n",
    "run_transform = scripted_all_transforms\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16,\n",
    "                                expand_dim=False\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'All Transformations Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_all_transforms\n",
    "run_transform = scripted_no_transforms\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16,\n",
    "                                expand_dim=False\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'No Transformations Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup_transform = scripted_scale_rsz_transform\n",
    "run_transform = scripted_tile_transform\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16,\n",
    "                            expand_dim=False\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'Tile Transformation Alone Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
