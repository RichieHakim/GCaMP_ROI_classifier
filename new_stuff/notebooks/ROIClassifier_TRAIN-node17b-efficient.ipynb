{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')\n",
    "\n",
    "data_labeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_unbalanced.h5')\n",
    "\n",
    "masks_SYT = data_labeled['SYTmasks']\n",
    "labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])\n",
    "\n",
    "nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "labels_SYT = labels_SYT[non_nan]\n",
    "masks_SYT = masks_SYT[non_nan]\n",
    "\n",
    "X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAADnCAYAAACjZ7WjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABZvElEQVR4nO29aZAc533f/+me7rnvY+97sbu470PgARK8SZGOJeosWYot20qcSpVjv8gbVexKOalU5UVS+SuJK6mKneiILEuMSJEUKR7gARA3QAALYLEL7H3Mzu7M7NzHzvV/AU+blHDtcqYXAp5PFaooEZz5TvfT3+fs31eqVCoIBAKB4LMhr7UAgUAguBcQZioQCAQ1QJipQCAQ1ABhpgKBQFADhJkKBAJBDVBu9S8lSarZVn+lUpFW+98KHUKH0PHbqeV+0iFGpgKBQFADhJkKBAJBDRBmKhAIBDVAmKlAIBDUgHvGTCVJQpZlJOkzrdt/Zg2KoqAoyprquFtQFAWTyYQsr30zk2UZg8GwphpUVcXtdmOxWO6K9iFJ0prrcDgc2O32NdXwST5LW73lbv4df4iiUCqV0Ps9f1mWsdlseDwe7HY7RqORTCbDxMQEy8vLumoxGo2YzWYURaFcLlMoFMhms5TLZV113C34fD56e3vxer1Eo1HGxsYIh8O661BVlUAgQH9/P3a7naNHjxKNRnXX4XQ6OXjwIM3NzaTTaQYHB7ly5Qq5XE53LbIsY7VacbvdVCoVYrEY6XRadx0DAwM8++yzmEwmXnrpJcbHxymVSrrrgOvXxO124/f7SafThMNh8vn8ij5j1WZqNBpxOBy43W4CgQCZTIbh4eEVC/gsmM1m9u3bR29vL4lEglwuh8lkoqmpibNnz5JKpXTRYTKZaG9vp7u7m+XlZebn50kkEqiqSiqVolgs6qIDwOv10tTUhKqqjI6O6nYNPonFYmHPnj18/vOfZ/v27cRiMX75y1/ywx/+kGQyqZsOm83GAw88wIYNG9i6dSsulwuPx8OhQ4eYm5vTtfPfs2cPe/fuxWg0srCwQEdHBx6Phw8++EDXDldVVVwuF01NTbS0tGCz2ZienmZoaEg3QzUajWzcuJEXX3yRz33uc/T09PDggw/yox/9iPfee4+FhQVddBgMBjZs2IDdbqdSqaAoivbPIyMjTE5OrqiNrMpM/X4/27Ztw+/343K5NBP56U9/yuXLl3VrHG63m23bthEIBLh8+TKxWIxSqcTGjRsplUqcPHmSQqFQVw2SJBEIBHj22Wf51re+xbFjx3jttdeYmJigUqlQKBR0HbUrikJ3dzdOpxOLxcLMzAzBYFDXHt9ut7NhwwZaWlpQFIUtW7bQ1NTE4cOHuXjxom46PB4Pe/fuJZPJcOXKFZqamujv76dYLPLuu+/q9tACtLe3I8sylUoFo9GI1WrFarXq9v1w3cQ8Hg8ej4dAIEBnZyddXV00NjaSyWRYWFggk8mQyWTqqkOWZfx+P4lEgmPHjjEyMsLAwADf+ta3CAQCvPLKK0xPT9dVA1yfPe3cuZPl5WVCoRCFQoF8Po/b7WbDhg3k83kWFhbu+Nm5rZmqqoosy5TLZWRZxmw2s337dnp7ezGbzfj9fvr7+2lubiYcDjM1NUUikfjMP/R2SJKEx+Nhw4YNNDQ0EI1GiUQiJBIJLBYLgUAAm81GLBaru5ZSqUQoFOL8+fMEg0HMZjMWi4VUKqX7mpTT6eTBBx/kkUce4cSJE5w9e5bBwUEGBwd16+RMJhMmk4mRkREuX75MV1cX69atw+/3I0mSLh2LJEl0dXXR0tLCzMwMAJVKhUAgwMaNG/n44491NVNJknA6nRSLRaxWK9FolHw+r2v7UFUVRVEwGAw4HA5aWlo4ePAgXq8XSZJ4++23626kAIVCgWg0yvj4OKqq4nQ6icVi2Gw2/H4/69evJxwOk81m66rDaDTicrmoVCrk83ntt9tsNjo7O8nlciwvLxOJRO7o825rpr8+RS2VSszMzODxeHC73RQKBYxGI21tbXR0dOByuXQx06oWWZbZu3cvhUKBZDLJ1NQU2WyW+fl53abX2WyW8+fPE4lEMBgM2kNSqVR0X0tubW3FbreTyWTw+Xx4vV7cbrfWIepBPp/XRsTZbJaFhQWmpqZ0X3JYXFxkeXmZffv2kUgkcDgceDweksmkrssNAAsLC2zatIlCocD7779PPB7XfcaQy+Uol8uUSiWy2SzRaJRUKsXy8jKzs7PMz8/rYqalUon5+Xk6OjoolUoYDAbtXsViMQwGA36/n9nZ2bq22cXFRebn53niiSfYtm2bdk2KxaI2Kl3J+vptzfTXjaBYLDI9Pc26des09w6FQly6dIkTJ04Qj8dX/qtWQaVSYW5ujldeeQVVVens7OTAgQMcP36cI0eOcOnSJV0W9yuVCrlcjkQiweTkJIpy/ZJWpwx6rpcCnD17litXruByubDb7czPz5NKpXR/aKemprDZbBgMBhYWFkilUszPz+vWsVQqFYaHh/nZz37Gv/yX/5J9+/ZhNpu5fPkyH3zwAaFQSBcdVYaGhojH47S3t2M2m4lEIszNzemqoVQqEYvFMBqNpNNp5ufn+fnPf87c3BzHjx/XtbOLxWJMT0/T3t6OxWIhn8+TTqfJZrNkMhnNZOtppvl8nuPHj9PU1MSLL75IT08PV65cYXR0lKNHjxIOh1fUXqVb/eVbvc+6fv16uru7KRaLeL1eisUi77///k2HxPV4r1aSJMxmMy6Xix07dtDX18eZM2c4derUTXfz66GjeqrAbDZrF79UKpHL5cjn8zdsEHf7e8afRYeiKPT39+P3+7HZbNjtdmKxGB988IGu9wX+cZPhW9/6Fk1NTfziF7/g9ddfv+kUsl46VFXld3/3d3n++eeZmZnhBz/4AVeuXLnpZ9Xz3XyTyaSdgKnO6OLx+E073Hrem+oOevXIWCqVYm5ujlAo9Bt66qXDZDKxZcsWHn74YbLZLENDQ5w5c+amncvNdKzaTD95prN6Xq1Sqdx0JFZv8zAYDJjN5tseR6qXDlmWUVWVSqWiTe/XQsdKqZcOp9OJ2WzGYDAgSRL5fP6Wa0/1vh7V866FQuGWs4V66ujt7eWBBx4gGAxy5MiRW86c6l3opHomWpIkCoXCLUdgerTVT/pJuVy+oZ5661BVFUmSKBaLq3p2V22mK+VeNw+hQ+i4nQ5FUTCbzRSLxdsuQYmqUb99OmpyaF8gENyeYrG4Jud+Bfqw9u/5CQQCwT2AMFOBQCCoAcJMBQKBoAbccgNKIBAIBHeGGJkKBAJBDRBmKhAIBDVApJMKHULHPabjbtJyP+kQI1OBQCCoAcJMBQKBoAYIMxUIBPc1kiRhMpk+8+fcU2Z6NwSECT6NuB+Cu51KpbL2ZlrNf1rr1Ee4XjWqubkZt9u9ZhokScJgMNwVaZx3Aw6HA5fLJa7HJ6hW0QL9OxqDwaDVDxWd3Kepxeh01YVOPB4PTzzxBP39/bz99ttcuHBhTZIWPxnB0NnZydzcHMlkUreizJIkaUWQm5ubaW9vJx6PMzMzw+LiYt0zqG6kR1VVAK0k4lq8mOHxeNi9ezd2u52xsTGuXr2qSxX3GyHLspY0sFZpsR6Ph4aGBjo7O4lGoywuLlIsFgkGg7ppMhgMPProo7S2tnL27FmmpqaYm5vTLRnj1zGZTNjtdkwmE+VymaWlJV0DOavY7XY6OztRVZWxsTHi8fiq7smqzLSaA/X1r3+dxx57jI6ODv76r/+a4eHhuue2fBKLxUJfXx8+nw+3243D4aBYLDI3N8fi4qJuOvx+P08//TQ7duygsbERk8nE3Nwcr7/+Om+++aZu16TaOKt1RCuVCplMhnQ6rWtH5/f7+dznPqfVl92yZQtdXV28++67ulZNqna0FotFCzfMZrNafIdeqKrKgQMH6OnpQVEU8vk8lUqFM2fOMD8/r5sOSZLw+Xz84R/+Id/5zncYHx9ncnKSN998k3feeUfXCGxJkuju7ub5559n//79zMzM8Ld/+7dcunRJ9wGIx+OhubmZ5uZment7uXjxIiMjIysekK3KTB0OB+vXryebzXLu3Dnsdjs7duwgkUgwMTGhS0O12Wxs27aN3bt3azn1JpMJg8FAIpEglUrpYmKSJNHS0sL69evJ5/OMjIzQ29vLI488QmNjI5FIhFOnTpHL5eo+QjQajbjdbnw+Hw6Hg3K5TDQaJR6PE41GyWazdW+oVquVzZs343a7tcgUVVXZuHEj4+PjDA4O6jJSlmUZj8dDT08PTU1NzM7OagF6BoOBbDar2+zF4/HQ3t5OLpcjEomQzWbp6OigsbERWZZ1i5QpFouMj48TiURQFIVUKkVPTw//7t/9O3p7e/lP/+k/6TYyNJlMrFu3jl27dtHT00NHRwcXL15kenr6jgPsaoHRaMRkMpHP50mlUjQ2NvLUU0/hcDj4+OOPb5oMcSNWbKaSJOH1eqlUKhw7dgy4fpOWl5dpbm4mEonokgNVLBaRZZne3l78fj+RSIR8Pk8sFiMajXLt2jVdzNRisWC32zl//rw2xVYUhc2bN3PgwAGi0SjlclnLpLpZjMlnpZryuHHjRjZv3kyxWGR+fp5yuUyxWMRgMFAqlZiYmKj5d1cxGAw0NDTQ1tbG8vIyJpOJUqmk5fu0t7dz+fJlXUxMlmXsdjsvvPACzzzzDN/73vdIJpMsLy9TLBZRVVW3sEOPx8PGjRuJRCKEw2EKhQK5XA6r1arr2mWpVOLq1avMzMyQzWYJBoMYDAbS6TRbtmzB5XLplthaXUcPh8McO3YMu92Oy+XC7XYTjUZ1W5qqxg0Vi0VKpRJGo5HHHnuMnTt38t/+23/j0qVLdzybWrGZVqeO8/Pz2gNaNQebzYbH49ElwC2fz3P16lUWFhZ4+OGH8Xg8ZDIZLZtdj00xSZJobGzEYrEQj8dxuVwYjUaMRiOFQoGPP/6YN998kwsXLpBKpTAYDKiqWpfe32q10tnZyde//nUefvhhfvaznzEyMkIsFtM6uHqPOkqlEslkElVVaW9vp1AoEIvFKJVKWtvw+XxEIpG6G1mxWCQej3Pt2jWGhoa0EYYsyxgMBm0jSK/Y6Q0bNpBIJJiZmdFihZeWlnRfww0Gg5w7dw63262Z6szMjJZiq5eZlstlwuEws7Oz2Gw2xsbGmJmZ0TX4Ef4xfbm6LFYdfOTz+RU/M6ua5kciES3uuRodqygKsViMRCKhWwMJh8McOXKE9evX89BDD2mL2LOzs8Risbp/v9Fo1DLHP5lbYzKZGBoa4qWXXuK1114jnU4D1PW6FItFMpkM77//PleuXGFkZITJyUmi0aiW+qgHS0tLjI6O8tRTT7F582ZCoRAXL15kcnKSy5cvk0wmkSQJo9Go6a6XocXjcQ4dOsTw8LA2eyqVShSLRQqFgm7ttBotvX37dqamprBarSQSCYaGhnQ306WlJc6fP89XvvIV5ufntVGgw+HAarXqpiOfzzM9Pc2RI0dwu91UKhVCoRCRSETXDdPq9a+a59zcHK+99hpjY2NMTEysaFlsVWaayWQYHh4mEAhoD0UulyMYDOq6G1cqlThz5gySJHHx4kVUVeX48eNcunRpRWsdq8FgMGC1WgmHw2SzWWw2G5lMBpvNRjKZJBaLceTIEc1I6006nWZkZISpqSnsdjuKomgmquemYLlc5uOPP+bll1+mXC5jsVi4fPkyb731FlNTU58yD1mWMRqNdZtFlMtlgsEgiUQCk8mkGWomk9HVxCKRCIcPH6a1tZWBgQEcDgeHDx9mbm5uTU4XnDhxgq6uLpxOJy0tLRiNRqLRqK6bg5VKhWAwSC6Xw263k8vliMfjup8IMhqNLC0tkUqlsNlsLC4ukkwmCYfDK9bymQL1qlOmtU44NJvN+Hw+fD4fY2Njt2wUtdJhNpu10U31ZYFPnqdUFOWWm071vB6fTIv9h++66d+tlw6j0aidMV1aWrpl5/YPI/u6FrGork3ebtRTr+uxbt06Hn/8cVRV5dq1a1y4cIFgMFiX9nE7LdW1fbfbTUNDAwAzMzM3nVXWu8DInS631FqHJElYLBbtdMXt0lFvp+OeSietruGutY47Qei4v3TIsozX6yWTyZDNZutq6rfTslLu9XtTKx33VDqp3ovXAsGdUt1wEdy7iPf8BAKBoAYIMxUIBIIaIMxUIBAIaoBIJxUIBIIaIEamAoFAUAOEmQoEAkENEGYqEAgENUBEPQsdQsc9puNu0nI/6RAjU4FAIKgBwkwFAoGgBggzFdQdkRp796Io99Qb5WvKZzJTo9FIS0sLbW1tWCyWWmlaFdXCy2uZhOn3++ns7BSJnP9ANa21mtG11lSrJd0N9+Zu6VzW+r6oqorH48Hr9a6pjk9WwFstq+qWDAYDXq+Xnp4e1q1bh9vtZm5ujnfeeYdkMrlqMavV4nQ6aW1tJRAIMDs7y8jIiK4a4HqjrNaGdLlcpFIp5ubmdK/PqCgKfr8fs9kMXC9MHI/Hdcs7gusPSDWCohrwFw6HWVpa0k1DlUAggMvlwmaz4fV6aWho4Ny5c4yMjOie2irLMi6XS4tz0TP08UZYLJaa5MWvltbWVvbu3Yvb7SYYDHLmzJk1uSZ2ux2/34/dbtcKRK+maNKqzNRmszEwMEBLS4vW2z/00EO4XC5+8Ytf6JZy6Ha78fv92oPS3t6Ow+EgGAzqauqKotDQ0ICqqhQKBYxGI93d3TQ3N/Pxxx/rFnGsKArbt29n//79eDwe1q1bhyRJHDp0iEOHDjE1NaWLgVitVnbt2oXL5SKZTJLJZHC5XCiKopupy7LM1q1b+b3f+z18Ph92u522tjYSiQQ//OEPmZyc1K2jMxqNtLe309raqkXaxONx3UIffx2TyURzczOtra3k83ld46artLa28swzz9DW1sbCwgIOh4Pe3l5isZiu6aRGo5Gmpia6urro6ekhnU7z8ccf65NOKkkSLpeL1tZWLBYL2WyWdDqN3W5n165dvP/++7qYqdlspq2tjZ6eHpxOJ4qikM1mtTjbVCqlayiXw+FAVVVt2tLV1UVjYyPxeJyLFy/qoqOjo4M9e/ZQLBaZnZ3Fbrfz/PPPs3XrVjKZDKFQSBcDURRFS9+cmZlhaWkJi8WCx+PRbeQhyzJdXV1IkkQwGKS9vR2Px0NLSwsdHR26TvWbm5v5yle+QiKR4OLFi2SzWex2O16vl9nZWd10VPF6vXR2dvLUU09RKBQIh8N1DVr8daxWKzt27GD9+vXE43EWFxeJRCI0NTXR09Oj26zBYrHQ3NxMQ0MDW7Zs4Stf+QrhcBiLxUImk2FycnJFOlZlpjabTZsiSJKE0+mkv79f13qNkiQRCAR4/PHHGRgYYGxsjFOnTpHNZmlqaiIUCunW6yuKoq3FWSwWmpqa2LlzJzt37mR4eJiRkZG6x6ioqsrmzZsxmUzMz8+TzWbJZDIUi0W2bNnC8vKybqMPo9HIxo0b2bhxI8eOHaNcLrO8vKxrxlC1ino8HiebzdLQ0IDD4cBsNiNJkq7LHna7nSeffFLLwqq2S7/fvyajQpvNRk9PDwcOHCAcDjMwMMD8/LxuI/VisUggEMDn85HL5TCbzVQqFfL5vK5Bh6qqYrVacTqd2O12VFXVOlu/37/ia7KqdNLqD60ahNvtprW1lYmJCV1inqvfXSgU2LZtG83NzVy9ehVVVbWHqDpq1gNJkjRDtVgs2pSloaEBs9msS1KqyWTCbDZrM4VisUi5XGZ4eJjp6WlOnjxZd0OvYrFY6O3txWq14vP5tJwsv9/P9PS0buZRKBRIJpOUSiXi8TjpdJp0Os3MzIyuBlZNsVVVlaGhIWKxmJaeuhaFhhYXF5mfn+fVV18lmUxSLpexWq26mWmlUqFYLOL1egkEAvT29vLWW29pSbp63ZtUKkU8Hsfv9zM8PMyPfvQjzGazltq60ud2VWa6tLREqVTC6/ViMpno6OhAURRGRkZ0C5ArlUpMTk7y0UcfsX//fkwmE1arVYtS1vNhyWazWg56VUcoFOLs2bMcO3ZMl0aayWQYGxvTRqd2ux2fz0c+n+e9994jGAzWXUOVUqmE1WrF5XJpi/upVIpAIIAsy7rdm2oWerlcJp1OMz4+zsLCAseOHdNtZFodBR8+fJhNmzbR3t6upYImEok1MdNkMsng4CDBYBBFUTQtelEqlTh16hSNjY3s27cPl8tFX18f4+Pjuj631fQDk8lELpdjYWEBRVHIZDKk0+kV35tVZUApisLOnTt59NFH6e3tpa2tjdnZWb7//e9z7NixG+6E1eNVMFVV6e3tZceOHbjdbmKxGJOTk8zPzzMzM3PDkVi9Xklrb29n06ZNtLa24na7iUQiHDlyhLGxMd1CyqxWKx0dHRiNRoxGI1arlampqVuuh9VDh8lk4tFHH2X37t1kMhnC4TBTU1OMjIwwPz9/w0Zaax2yLNPf38+BAwcIBAI0NTXR1tbG97//fX7+85/f9LPqEdpmtVqx2Wxs27YNr9fLxMQEExMTRCKRm5q6Xq+T2mw2crncLXev6/XMtLS08OCDD7J//35UVeX111/n0KFDN51B1UuHxWKhoaFBO95ZKBSIRqMkk8kb3p+aB+oZjUY6OjrYsmULZrOZCxcuMDY2dtOpdT2Dymw2G1arlXK5TCKRuGUmer10VHf0A4EA6XSaSCRCLBbTPZ30k+fk/iH185afVc/7oiiKtv5VXXbQU0d1M/KZZ55hw4YNXL16lZdeeumWJz3q+f539SxjuVyua/DjnWhZCfW8JiaTifb2dtra2hgbG2NqampNdMiy/KkE29W01c+cTmo2m5Fl+baJi3d7kYJa6biThFQ9dNwp94OO6siwUqncdpP0Xrged5OWO9VhNBqpVCq3PBZ1t9+beyrqWegQOoSOu0vL/aRj7d+rEwgEgnsAYaYCgUBQA4SZCgQCQQ0Q6aQCgUBQA8TIVCAQCGqAMFOBQCCoAcJMBQKBoAaIdFKho+4vMaiqetv6BPfL9dBDx92k5W7VUX3jaTVFoMU5U8GaITY5BXcbgUAAp9NZ088UZiqoK3dD3pJA8OtIkkRHR0dNs7hENOE9jJ7l7m6GwWAQhiq46wiFQgBavd1a8JlauSzLmM1mHA6HLgWQ73aq4X5+vx+v17um16SaiLDWfLIaz63+Tq359epZwtB/E0VRMBqN9+W1qVQqhEIhyuUyHo+nJp+5qpGp0WjUco/sdjuyLLOwsMDCwkJNRP02YrFY6O7uZtu2bfh8PsxmM8ePH79pfdd6YrVa6evrQ1VVzp8/r2tA2a9jNptvW4i51p2OLMu43W7tujc2NmK1WllcXCQajeoaYmexWPD7/TQ0NGCz2bSCxOPj4+Tzed10fBJZlnE4HBiNRi2xIp/P695OAZxOJw0NDeTzeUKhkG5pEPCPhe537drF4uLiZw6cXJWZWq1Went7tdFXNSAsEAiwuLhIIpHQLQLB6/WyYcMG4vE4Y2NjdU0CNZlMN6yV6nK52Lt3L21tbaxbtw6r1UpraytWq5WLFy8Si8XqpumTOJ1OHnzwQXbt2sXWrVtZXFzk7//+7zl+/PiaPLjVgsjpdJrBwcEbmlg98uPL5TK5XI5169axadMm+vv7KRaLRCIRLQFBr6RWSZL4xje+wWOPPcbg4KBW2f3q1aucOXOGkZERXZdiFEXB7Xbjdru10pmpVApJkshms7pq2bx5M//iX/wLent7OX78OKdOneLUqVPaFFwPKpUKmzZtYseOHRw7dozx8XHGx8dXFfy4YjNVVZWGhgb6+vqw2+2kUingutFURwN6xiwfPHiQ7373u4yPj/M3f/M3vP/++3WNTqnmLMH1G6EoChs2bGD//v1YLBacTidWq5VKpUJzczO9vb18/PHHdW+ksiyzYcMG/vAP/xCn06kVIn7hhRdQFIX3339f15FHQ0MDn//85/niF79IJBLhJz/5CYcPH9baiyRJWnZWPSJE8vk8VquVL3zhC1gsFj788EPS6TRNTU0cOHCA119/XZcU3eXlZYLBIJcuXeLEiRMkk0kt+8jlctX9+6tUR+ttbW00NjYiyzLhcJhyuUyhUNDW1/UYtauqyt69e/nLv/xLWltbOXfunBaEuXHjRpaWlnQbodrtdq1AtdPp5MqVK+zatYuPPvqIiYkJbDYbJpOJ2dlZLQLnZqwqndRqteL1evH7/cRiMS1dMJvNUiwWdR2qJ5NJzTxbWlro6+tjaGioLiOxfD6PoiiYTCZtatra2srTTz/Nhg0baG9vx2q1Eo/HtZH59u3bGR8fr/uDK0kSnZ2dbNq0iVgsxpUrV0gmk3g8Hj73uc9x+vRp3cIOHQ4HX/3qV/mn//Sfap3sH//xH+N0Ojly5Ig2ui+VShSLxbqNEC0WCxs2bGBhYYFUKkU0GiUcDmO1WmlubiYej9e9gykWi5w5c4aWlhZMJhOZTEb7oyj67f/KskxPTw/f/e53KZVKHD58WGsT1Y5Nr/y0xsZGfv/3f5/du3dz6dIlpqamtAFYa2srLS0tukVPOxwO8vk8V65cwefzYTQaeeqpp/ja177Gz3/+c06fPs3k5CQWi4VisXjLme+K72a5XKZcLtPS0sLevXtZXFzk2rVrJBIJJiYmSKVSuk4VPv74Y37605/i8Xi0B8Vut9dtWlssFikWi1oKavX7/H4/TU1N+Hw+5ufntaTWRCKh20ZUuVwmFoshSRLRaJRQKITNZiMajerWwRkMBvbu3csXvvAFGhsbKZVKTE1NYTAYePTRR5menmZkZIRMJkMul6NSqdRtqj87O8uvfvUrXC4XpVJJi1CRZRm73X7HqQiflVgspi2FVbO5mpqamJ2drctvvxHVtdp3330Xi8WixRgvLy+zvLystWs9lj7C4TCXL19mx44dhEIhJEnSlh0qlQoXL16su4Yqy8vLxGIxpqenSaVSGI1G4vE4hUKBkydPcuLEiTtuIys201KpRCQSIZVKsW7dOnp7eymVSly+fJmZmZlb5h7Vg0gkwi9/+Ut6eno009DjAalUKmQyGYaGhvjJT36CyWTC6XSiKAqFQoHFxUUOHz7Mhx9+SCQS0UXP9PQ07733HpIksbCwQCwWY2lpiWvXrulippIk4XQ6eeqpp1i3bh3BYJBEIkEoFCKbzTI2Nsa1a9d+4yhKPdpLpVJhYWGBoaEh1q1bh8FgwGazkU6nSaVShEIh3TbmQqEQk5OTPPbYY8iyjN/vJ5FIrGpdbrVUO5ef//znNDc3YzKZyOfzmokWCgUKhYIuz24ul+Pv/u7vANiyZQuNjY10dHRgMBi06bVeZLNZEokETqeTUqmE0+nk0qVLzMzMMDg4uCIvWVXU8/z8PG+99RYmk4nW1lYmJiYYHBxckx3KcrnM6Oio1sMVCgVdd2vz+TynT5+mUqlQKpXYuHEj586d45VXXuHChQvaGmG9KZfLjI+P8/HHH+P3+8lkMsTjcVKpFCMjI3XtYKpHj1RVxWazYbFYKJVKpFIpIpEIyWSSaDTK4cOHa3am706IxWLaOmU1X2h+fp7R0VFdO/1CocDg4CBf+9rX2L9/P+l0mv/wH/4D586d021kWtWxuLhIpVLR9jdKpRL5fF63DeMqwWCQv/7rv2bTpk089thjbNy4kUwmw6FDh3RbjoLrEemhUAiHw0G5XMblcmEymRgeHl7xs7vqDCiHw4HT6USSJPL5POl0+pahevfi+72fpLom9eSTTzI+Ps6HH354y/WVeqVxWiwWvF4viqKwvLzM0tKSNp3WQ4fFYuGb3/wmf/qnf0oikWBqaorJyUmOHDnCu+++e9PNwXrdF1VVcTgc2v9OpVK3HKXXS4fVauXJJ5/k2WefZXh4mB/84Ae37Fjq+W6+2WzG4/FgMBhIJBK3XZqr97NrMpkwGo3aBpjeycJms5mmpib8fj/Nzc34fD7eeecdZmZmVqRDBOrVUIeiKOzbtw+z2cypU6dIJBJromMl1COvfv/+/fz5n/85Xq+X48eP89Zbb3HmzJn78np8EpPJRFtbG5FIhEQiUTcDuxMt1Rc6MpnMbUfo98O9gesdryzLWK1WAJaWllakQ5hpjXU4nU56enoIBoO3PC93r14PRVHw+Xxs2LCBQqHAyMgI0Wi0rjnxd/P1WAsdd6Kl+tbTnWwW3wvXRA8d4t38GpNIJJicnNQ2o+pxhvJuplwus7CwQC6XI5PJrOnbV4Kbs9Y1G+5FhJnWgVgsRqlUwm63k06n7ytDqT6k1eM2AsH9gjDTOlCpVEgkEiiKcl/W8qye37wff7vg/kWkkwoEAkENuP9qbwkEAkEdEGYqEAgENUCYqUAgENQAkU4qdAgd95iOu0nL/aRDjEwFAoGgBggzFQgEghogzFQguI8wGo26FqW+n/hMV7Va9LhSqazZ62mSJGE0GqlUKhSLxbvmNblqIeL7EVmWsdlsWjzG5OTkmoXHwfUCI3a7nUKhcMtiK/XEaDTi9XqRJIlwOLxmb8UZDAa8Xi+xWEy7J/drO601qzJTSZIwmUzYbDYURSGXy5HNZnWNK6lSLTlntVq10LRkMrkmpmq323G73ZjNZhRFYWJiQrc6kZIkYbfbtSiQtbgXcN1I+/r66OjowGQy0djYSF9fH+fPn79pSbN6oqoqfr8fj8eD2WxmampK9xRdo9HI/v37+frXv47f7+fVV1/ljTfeWLM037a2Nvbv38/CwgKLi4sEg0Fdc9s+iaqquFwu8vn8mmmoRtZXnxu73U6lUllxltyqzNRsNtPR0cG+ffsIBAKcPXuWS5cuEY1GdX0f22Aw4HK58Hq9tLa20tbWRiaT4aOPPmJ6elq3ADmTyURvby979+6lqakJj8dDJpPhRz/6EaOjo7r0/Iqi8KUvfYmNGzfy4YcfMj09rT0seo6CrFYrPT09WkZYT08PfX199Pf38+677zI8PKzbKFWWZbxeLw6HA6vVis/nw+Vycfz48bqGLv46DoeDL37xi3z5y19mfn4egKamJl5//XUuX76sezrp5s2b+bM/+zPK5TJHjhzh//2//3fbEom1pjogCwQCPPvss7S0tPCzn/2Ma9eu6V6o2mAw4PF4sNlseL1ezGYzwWCQycnJFWlZsZlW83Meeughvv3tb9PU1MQvfvELCoUC58+f17V3MRqNuFwunE4nZrOZlpYWdu/ejd/v5wc/+IEu6ZMGg4F169bxwgsvEAgECIVCzM/PYzKZ2L9/P5lMhvn5+bo/MMVikWAwyJe+9CXS6TR2u53Ozk7Gx8cZGhrSzVC9Xi92ux2Hw8GWLVvo6elhamqKfD5Pb28v8XicyclJXbRU48eNRqO2Vuj1evH5fLqaabFYRJZlRkdHeffdd5mfn8dsNtPX18fIyIiuswij0aglMZjNZnp6evjc5z7H0tISw8PDuqVU2Gw2bDYbTqeT9vZ2HnzwQS5duqTFcevZwRiNRgKBAOvWrcPr9ZJMJonFYivObluVmZrNZnw+H4VCgZmZGdxuNwMDA0xOTpLJZHQZEUqShMPhwO1243A4MJvNWhql2Wzm7bffJplM1tVEqnk+/f392vqg1WrVYnRNJhNbt25leXm57nEdlUqF48ePazlMkUgEt9vN5z//efr6+jh27BihUKju90ZVVaxWK4FAgO3btxMIBJAkSetk9Zy52O12mpubkWVZM5GmpiZaWlqYmZnR7YEtFovMzs5y+PBhLl26RDqdxu1209HRQVNTE1NTU7rogOvGoaoq58+f13KfDAYDra2tLCws6JJOCmjPZbFY5MMPP+TixYsEg0GMRiM2m41UKlX3GV11qbKvr4+DBw+yZcsWwuEwFy9eZHl5ecXXYVWBesVikcXFRYaHh5FlmVwuR3NzMy6Xi4WFBV2n1xaLhUqlokV0zM7OEo/HUVVVC7erB2azGVmWMZlMOBwOGhoa6OrqYnJykunpaeD6lLcavKcH8XicCxcuaOuDZrOZxsZGnn76aR5++GFef/11Pvzww7pOszOZDMViUcuGd7vdbNq0iWQyic1m0y2pVZZlGhoa2L59Ow0NDSQSCYrFIoqi6PKgfpJCocDc3BzxeJzZ2VkKhQKqqtLQ0IDFYtFNh6qqOJ1OYrEYsixrI+ZsNkupVEJVVYxGoy7T7GpmWqFQYGFhgUgkQi6Xo1QqYTAY6r6B+8nIeqfTSWdnJw899BCnT5/m2LFjJBKJFT8nqwrUSyQSDA8P43a78fv92tRO7119g8GAoigYDAYMBoO2wXDu3DktrrVeVBvc7Owsx48fp6enhz179miRtZlMhsnJSUZHR3WbOlUqFaLRKLIsa2Xwzp8/z/Hjx7lw4QKhUKju65Xz8/OcOHECh8PBlStX6OjoIJ1Ok0wmCQQC2lS/3qFp5XKZVCpFQ0MDX/ziF7WI6aGhIS1UTi+Wl5eZnp6mv7+fcrlMsVgkl8sRi8V0C1yE69dkamqKt99+m/b2dtxutxaLvrCwQCqV0m29sto+qzNZSZIol8vkcjldlqQqlQrJZFL73r//+79naGiIRCLB1atXicVi9R+ZwvXRx/DwMOVyme7ublwuF6FQSPcNqHQ6TSgU0gysuj44PDysm5ZSqcTIyAj/9b/+V4aGhnjqqafI5/OcO3eO06dPs7CwoOuDm81mWVxc/FScsR5R01UqlQrXrl1jaWmJs2fPsmvXLgwGgxYj3N/fD8Dx48fr3slUp9ZtbW34fD4tk/1m2T71ZHx8nK1bt2oxzwaDgVAopGvcc6lUIpvNMj4+TiwWw+12Y7VateNaeu535PN5bWRaKBSQJEnrZPQy9GKxSDQaJZVKEQwGOX36NAaDAVmWV7WOveoMqOpunMlk0s543upITj3eq1UUBUmSUFVVM05ZlrVp/41+Wz3f71VVlZaWFmw2G/Pz8ywtLd23aa1VquuUbrdbmz0sLy9z4cKFT5l8vXRUj2dVj89dunTplqPieiZgPv/882zbto10Ok04HObIkSMMDw/XvJ3eTss//HvtGa7+s97JsZ/U8onvuunf0+uZud0SgwjUEzrWTIfBYMBkMlX/G62z++TMQQ8dNpsNVVWJxWI3/Tv11OF2u+nt7aVQKDA/P08kErnp/oIodPLbp0OYqdBxX+m4XcjhvXA97iYt95MO8W6+4L5ChPwJ6oUwU4FAIKgBwkwFAoGgBoh0UoFAIKgBYmQqEAgENUCYqUAgENQAYaYCgUBQA+6pdNLW1lYWFxdv+SrY3X5WTei4d3SoqookSat6NVGcM/3t03HPjExVVUWW75mfI7gHKJVKIhLkPuKecR9VVTEYDKLxCm6Jw+HQ7bsqlcqn3jsX3NvcM2ZaraYuuHuolgK8W6gG6+lJtTyk4N7nnsh8lSQJl8uFqqprOjJVVRWz2UyhUNA9xwauXwdFUbTSasFgcM2C9VRVxefzoSgKiURCKxq9VsiyTFtbGwaDgWAwqMt3VioVrRB0uVy+a2ZN1WpRsixTKpVuWmFNb03V2qJrhaqqlEqlVWv4zGaqqirAmkXXAjidTnp6eshms2uyblqNWvD5fFitVnK5HKFQiEQioVsjlSSJnp4eDhw4wLZt23C5XHz00Ue8/vrruplHFZvNRm9vL+3t7VpibDgcJhqN6h61bDAYaGhoYOvWrezfv59Tp04xMjKi2/cXi0UMBgMWi4VsNnvT9vBZH+SV0N/fz/PPP4/D4WB2dpZjx45x8eLFun/vzagmVZjNZiYnJ3UtmA3Xi990dnayfft2ZmdnGRkZWVV+3KrN1OfzsWfPHtrb27l27Rrnzp0jFovp2sPJsozb7Wbz5s10dXWRSCS4du1a3fOWPvn9TqcTn8+H2+3Wenu3200gEGBycpJgMKhLjIvT6eSZZ56ho6NDM/EvfOELbNy4kf/yX/6LbiF2Ho+HJ554gp07dxKJRJiamkKWZWw2G7IsUygUdA1t2717N/v372fLli3s2bOH3t5ezp07x+zsrC4a4HohZIvFgs1mI51OU6lUUBQFWZYpl8tIkqSbkTY0NPDcc88xMDBAKpXS0jKqkT96Y7PZ2LhxIx0dHSiKQkdHB4ODg7rGgvf29vLCCy+wbds2JiYmaG1t5ejRo8zPz6/Iz1Zlpk6nk29/+9t84xvfIJfL8d577wHoGhcryzLt7e309fXhdru1XJuOjg4ikYgupl4102r8gyzLKIqiJaW2tLRw6NAhXSrdBwIBnE4nExMTZLNZHA4HFouFAwcOEAqF+M//+T/Xfcqvqirbt2/nn/2zf4bRaOSDDz741Hqhw+HQpvt6zGRaW1t5/PHHMZvNJJNJ4vE4u3fvZv369czNzena8edyOS3gLxAI0NLSQrlc1jocPSruGwwGtm3bRnd3t1bIPZfLaWm2elJ9Vjdt2kRvby8+n49cLsfo6KhuU31Jkmhvb+fgwYPs2rWLYrGI2WymubmZBx54gBMnTqzI1FdspkajkZ6eHp577jna29v58MMPSSQSGI1GLBaLbmbqcrnYvHkz7e3t5HI5lpaWKBaLeDweLBaLbiF25XKZUqlEqVRCURQtpKs6Wg4Ggxw7dqzuo9NEIsHS0pJWfDmXy7G4uEipVOKRRx7hBz/4AXNzc3XVoCgKXV1dDAwMaNOkaiHoUqn0qZyweDxe1zwqWZYJBALE43GCwSA+nw9Zltm9ezeBQEBbL9QLSZIwm8088sgjfPWrX8XhcDA3N8fIyAjvvfcemUym7tNbRVFYt24dLS0tOBwOJiYmGB0dJZlM0t3dzdjYmC6dnCzLbN++nccff5zW1lYaGhqwWq1MTU1x9uxZFhYW6q4Brt8Tj8dDS0sLxWKRdDqNqqr4/X5UVWV8fLy+ZlooFJicnOSXv/wlMzMzXL58mYmJCVKplK6N0+l00tvbS3Nzs/ZwptNppqentXXcelMqlYjFYpjNZu1YlizLuFwuHA4He/bsYefOnVy6dKnuSyDxeJxIJMLAwAANDQ2YTCZcLpdm8OvWrSMajdZ1Y6xUKjE3N8fp06exWCzkcjkqlYp2eL1QKGjhevUeJVcqFVKpFDMzMyQSCaanp0mlUgQCAYxGo+5Hlqr3fs+ePTz88MNMTk5qkeDVyPLqEkA9MZvNtLe34/P5gOthf9Uk30AgUPcOF67PYB544AH27duH0WjE5/MhSRKjo6OMjo7qtlFZHXRU26WiKNqzUygU8Pv9mEymO+70V5VOurS0xP/+3/+bzZs343a7SSQSBINBXYPKJEnC5/OxadMmPB4PANPT07zxxhtaREa9qT6wc3NzFItFnE4npVIJq9XK1atXmZ+f58iRI7oF+y0tLWG1Wlm/fj1ms5lAIIDf7+e9994jnU5rm2P11HDhwgX+v//v/2PXrl1ks1kURcHlcpFKpZifnycYDOqyZlqpVAgGg3R2dlIul0mn01po2uLiou4bldUHd3p6mqmpKdLpNNFolLm5OdLptC7Hp0qlEkNDQ5w7d45t27Zp+WnVuHa9YqeXl5c5ffo0TzzxBFu3bsXpdHLs2DHeeust3db24fo9icfjjI+PUywW6erqYu/evQwODjI1NaWFQN4pq96AWlxc5IMPPsDr9VIqlUgkErqOTKs9SkdHB729vaiqSj6fp1Ao6KoDrqe1hkIhAC13/Ny5cwwNDTE+Pq7L2lzVyBYWFti3bx8PPvgguVyOv/mbv+Hll18mEonUfTRYKpUIBoMsLi4SDAbp7u7G4XCQz+eZnZ1lampKt+UXgGg0SjKZpLOzk2QySTgc5ty5c7pNI29EdY20ml8/MzNDLperewQ3oEV/V6eulUqFfD5PLpcjm82yvLysbYrVk0qlwvnz53nttddwu92cP3+ev/3bv+XSpUu6nwpaWlriyJEjdHV1MTExQTgcJpFIMDQ0xKVLl1b0zPzWZkBZrVYef/xxXnjhBfr7+0kmk7z11lu8/fbbjI6O3vSm1PP9XoPBgNVqxel0ArCwsLAmOlRVxel0YrFYCIfDtxyN1kuHqqp4vV4sFgv5fJ5EInHT9Mt66mhoaKC5uVlbArqdadW7fWzbto0XX3wRm81GNBrV4smvXLnyqTXTer2bX13eWEkHX69r4vf72blzJ2fOnLmjTdp63htFUbBYLJhMJm3JcqXP7m+tmUqSRGdnJ21tbdpwvRqvfKuRqR7FEu7kAPLdXrRB6KiPDrvdTmdnJw6HA7vdTi6XY3x8nGAw+Kn2cj8UOjGbzZhMpltGb+uhY6Xcc2YK13t6VVVZXl6+46nJ3X5DhI57W0d186u6MVcul8nn87/Rfu8HM73XdPxWv05aPZIkEPy2YDAYKBQK2jlPwb3DPVPoRCC426nWTljL988F9UOYqUCgI8Vicc2Ligjqg0gnFQgEghogRqYCgUBQA4SZCgQCQQ0QZioQCAQ14J5KJxU6hA6h4+7Scj/pECNTgUAgqAE1MVOHw7HmKYzVSu4CgUCwFtTEfYxG45omMFajQu4WM13rjkUgEOjPZ3afalEPRVm7N1MtFgs7d+7E7/evmQa4burV4rttbW33vanKsozVasXn8+FyudbselQTW6vRMoJPc79fE6PRyMDAAO3t7Z8pLv4zOaDNZqO7u5vNmzczPT3Nxx9/rGu9SrheheeBBx7gS1/6Eh0dHbzyyiu6hnFVMZlMDAwM4PV6sdls5PN5MpnMqlIOV0vVzCuVCpVKhWKxuGbxylarlZaWFrq7u/F6vcRiMU6dOqXr9VAUBY/Hw8DAALt27UKSJE6ePMnp06fX7L34ahX3u+WVUovFQmtrK9euXVtrKWuCzWbjxRdf5Hd+53fI5/McOnSIX/3qV6vykFWbaWdnJy+88AIPPPAA/f39GAwGXn75ZV5++WUtKqLehV6tViv9/f1s2bIFWZbp7+/nySef1OJ89Xxg2traeO6557RYY4PBQFNTE7FYTLcHx2g0apEpsixrSQCxWIxsNqubDqfTSX9/v5ba6vF4cLvdlEoljh49qkuHW01i2LZtG9/85jd5+umnURSFt956i7/6q7/i8uXLur3WKcsyDQ0N7N+/n4cffphQKMTx48e5cuUKS0tLK67oXgscDge9vb1s2bKFqampNTHTzs5Otm3bRqlU4uLFiywtLekaBa6qKo8//jgbNmzg2rVrxONx2tra+NrXvsabb77J1atXV1S0e0VmWp3St7S08MUvfpGenh4ymQzLy8t0dnbyyCOPMDU1xfj4uC4Vs6uRGEajkcXFRSKRCKqq0tnZqVUx1wOz2czjjz/Orl27OHr0qJa15HQ6sdvtujSQamCbzWbD4/FoyY/5fJ433niDq1ev3rI4c61QFIWNGzeyefNmKpUKpVKJZDJJPp/H4XDQ3NzM6Oho3XVUtRgMBsLhMMPDwzgcDrxeL3v27GFiYkKX66GqKvv27eORRx5hz5499PT0YDAY2LFjB6+88grHjh1jZmZG1xlEU1MTf/Inf8KePXtIJBL88pe/xGAw6FqBLRAI8NRTT9HS0oLRaKS/v5+5uTmOHj3K4uIi+Xy+7h3MwMAABw8e1KKHqoF6bW1tvPjii/zqV7/izJkzd+xltzRTp9Op/SBZlpEkCYfDwfPPP09XVxfhcBij0Ui5XGZiYgKr1YrRaNRtRFjVls1mSaVSmM1mvF4vi4uLNDU13bbKfK3w+/0cPHiQlpYWzGYziqJQLBYxmUy6JrZWp/eVSgWn08lDDz3Epk2bmJ+fZ3x8XBcNfr+fAwcO8MgjjzA5Ocn09LSWmrq0tITH49G1jSSTSU6ePMnU1BQejwer1UqhUMBisehipoFAgIcffphAIKDNWPx+P1arVQs81HvK/+yzz/JHf/RHDA0NMTw8jNPppKGhgWAwqJsGh8NBU1MTfr8fh8PB1q1bSSaTtLS0MDU1xfnz5+s+Wrbb7VgsFmRZZnl5GbfbjcFgoKGhAZ/Px+zsLJOTk3d8XW5ppp9sbFXjqoaC9fb2Isuylu9TjZF1u92YTCZdTKxYLBKPx4nH4/h8PjZv3ozD4aBYLBIMBnWbOhmNRjweD62trTQ3N5NIJEilUrrlw8M/RipX/4RCIV5//XWOHj3K5cuXdckYgusjZJvNpkUqG41GFhYWtA4lEonoshFVTV/IZrMEg0Gi0SgNDQ04nU5CoZBuI8FyuaxFYmSzWdLpNCaTiVAoRCQSIZfL6W6mlUqFyclJLl26xPj4ONlsVvdNqKph/tmf/RldXV3Mz8+Tz+eJxWIEg0Fd7s/c3ByFQoHm5mZtryMQCBAIBBgfH0dVVaxW6x1/3i3N9EbD/lwux4kTJ+jq6qKzs5NwOEwmk0FRFC5cuMDJkyd1M5BcLkc0GiUWi1EsFlEUhZ6eHqxWq3Zz9CCTyXDkyBFtnc7v95PJZLQUSr1YXl4mk8lgMpkIBoOagU1OTup2LRYXF3nppZfYtm0bW7Zswe12E4/HsVqtmEwmlpaWdBuV5vN54vE4ZrOZQqGA0WikWCySSqV0M7BYLMbZs2cxm82USiWy2SxGo5FwOMzs7OwdZR/VmlOnTvHcc89po+JYLPap/Ck9KBaL/OpXv2Lfvn3Mzc3xve99j+npaa2D0YNQKMTly5e1QZDb7cbtdmO1WhkaGmJ+fn5FG6arji3p6OjgmWeewel0ksvlMBgMnDlzhmPHjt3QhOv1KpjVaqW9vZ2Ojg7a2trw+/189NFHnDlz5oYGUg8dJpOJxsZGtm3bhtvtZnl5mUgkwtDQEHNzczccIdfrelQ3oaxWq1bVfWFh4aZmWi8dO3fu5Ctf+QpNTU3Mz89z9epVTpw4wejo6A2jnuulw+l00tHRgdPp1EYZi4uLjI2NkUwmddPR3NxMb28vmzZtorW1lcuXL/P+++8TCoVq3j5up8VkMvHFL36Rvr4+Tp06xccff8z8/PxNP6uer3E+8MADmEwmjhw5cttBWD1y5BobG9m0aRO9vb0EAgFt8/bq1au89957jI+P/4af1TwDymg00t3drYlSFIXx8fGbjsTqeUNkWcZoNGpnxNLp9E0X0+udcFilXC6vWaCeqqooiqIlLN7qHtdLhyRJWCwWWlpacLvdhEIhZmdnb3pN6nk9PB4PnZ2dGI1GUqkU0WiUhYWFG2qpdztVFAVVVbHb7UQikZtOZ+v9bn5Vx50sRdU7sbVSqdzRTKFeOmRZxmAwaD5is9kACIfDN7w/NTVTSZIwmUyUy2Vtyna7vO27vUiB0HFv6zCbzVr44q3Oeep1PQwGA+Vy+aYdnSh08tunY1XnTKtrUJ907bvlELJAcCNyuZxua3F3ggiCvPdYlZnmcjmRYyMQCASfYFXnIYSRCgQCwae5vyscCAQCQY0Q6aQCgUBQA8TIVCAQCGqAMFOBQCCoAcJMBQKBoAbUJJ20+ibFWrxps1KEDqHjXtdxN2m5n3TUZGQqDuzfmvs9vkQguB+oiZlKknTf58jcCrPZvNYSBILfwGKxrGl2271GTRzQYDBgsVhq8VH3HEajEbPZLDobwV1HY2Mj69atW2sZ9ww1ecKrVfgFn0aWZWw2m671M+9G7oaORFXVu6KNVisU3S3oHYB5L/OZxviyLGOxWHC73bqmTt5Ih9lsJpvNrumrrrIsax2LJEl4PB4ymYxuxbI/iclk0uJT1ioNU1EU/H4/drudxcVF4vG47hpkWaaxsZHGxkYKhQKTk5O6F0KuYjQa2bJlCwAXLlxYk3bxSS1+v59wOLxmGu5WJElalY+s2EyrwVsul4t169bR29tLuVzm6NGja1IAxWAw0NLSgsfjIRQKEQ6Hda/Io6oqra2tdHV10dTUhN1uJ5fLMTs7y8mTJ3XVoigK69ev54tf/CIdHR1cuHCBl19+mampKV11+P1+du7cSV9fH6VSidHRUY4dO6a7ke3du5cDBw4QiURIJBKsW7eO06dPMzs7q6sOr9fLk08+yXe+8x0A/vIv/5IjR47oqqGKwWBgy5YtHDx4UIt00TMV9G7DbrfT39+PLMuYTCYkSeLcuXMrbqsrNlOz2cyWLVt47rnn2L17Nw0NDQwNDWlFd/Usc2az2diyZQvr1q3D5XIBMDY2xoULF3SLN5Ykie7ubh5//HGampro6Ohg/fr1qKrKO++8w8jIiG7RJbIss3v3bv71v/7X7Nu3j1gsBsD777+vm5lKksTAwABf+tKX8Pv9DA0NsbCwgMFgwOFw6Gqm7e3t/NEf/REAv/jFL4hGo2zYsIE///M/59/8m3+j2xTXaDTy7LPP8tRTTzE9PY3P5+M73/kOiUSCwcFB3QcgLpeL/v5+vF4vdrudxx9/nLNnzxIKhe6qMoV6sXv3bv7gD/6Acrmsxcq88cYbvPLKKyvKolqVmb744os888wzFAoFAoEA2WyWjo4OrFZr3W9GNXrCYrHwzDPP0NfXx9jYGNFoFK/Xy9NPP8327dv56U9/ytjYWF21wPUHZf369ZTLZa5evUoymcRqtfLII4/w6KOPcujQIRYXF3XJPWpsbOTLX/4y7e3tfPTRR0xOThKJRLSYDD1CytxuN1/4whd48cUXOXnyJAsLCywuLlIoFPD5fCwsLOgyczAYDGzduhWr1cqPf/xjPvjgA5qbm9m1axf9/f34/X7dOpiGhga6u7s5ffo04XAYt9tNT08PX/jCF0in04yNjelqqAaDgWQySTgcpqWlhYGBAdrb2xkeHubixYt1H7Xb7XaamppwuVzYbDbcbjdNTU0sLS1x4cIFxsfHdcsJ8/l8vPDCC6xfv56TJ08yOztLa2srjz76KEePHl1RYuuKzdTn82EymRgfHyeVSnHp0iVyuRwWiwWn08nS0lJdG0a5XMZisWiZ1wDxeJxkMonRaKRUKqGqqi6bPpIk4ff7aWxsZG5ujlQqpaWStrS00N/fz8MPP8zo6Cizs7N17WgURWHz5s0Ui0Veeukl4vE45XIZq9WK2+3WbfPFbrfT09NDe3s7Z86coVwuUywWyefzGAwGLU6l3lQqFUKhEC+99BJHjhwhmUzywAMP8NWvflULbtOLSqVCOp0mGAySTCYpFAosLS3R3t7Oxo0bCYfDuq4nV3PbvF4vn//853nzzTeJRqM0NTVx9uzZun9/uVzG7XbT3NyMz+fj0Ucf5YEHHuCdd965YeZSvZAkiaamJhoaGrTU5ap/7dy5k/Xr16+o81+RmSqKgtPpJBqNoigK6XSaQqGAqqpaBni9qVZMv3TpEoODg3zzm99k27ZtnD59mmg0SrlcJhqN6jKFq1QqpFIpwuGwZhILCwtUKhVGRka0aIpqxEs9UVWVbDbLuXPnCAaDmM1mDAYDHR0dxGIx3TagqtHfExMTvxEhbDAYsNvtuiSllstlZmZmsNvt9PX1USwWaW9vJxKJcOjQIV03fxKJhDbaq1QqqKpKLpfj2rVrDA4O3jDYr560tbXR2NjIlStXeOONN5AkiUKhwPj4uLY0VE+y2SwLCwsEAgFMJhOVSoWZmRmGh4dZXFzUdbNUkiSmpqaYm5tjenqaxcVFwuEwkiSRTqfx+Xwkk8kbhkD+OnfsfgaDAZvNRi6XY2hoiGg0qo12FEVheXkZRVGQZVmXniUej/Pyyy+zdetWnnjiCdra2sjn8+TzeRYXF+/ox9eCZDJJKBRi3759VCoVotEopVKJq1evcvHiRQ4fPszo6Gjdpy3ZbJahoSH8fj8Wi4VkMonFYkFVVYLBoG69/dLSEu+88w52ux1VVfH7/eRyObLZLJlMBlVVb5sXViuqJ0waGhqIxWJcuHCBa9euMTk5qauZplIpLl++TGdnJwaDQQt+HBoaYmpqSlfzUFWVtrY2WltbmZ6e5uTJkzQ2NpLL5bhy5You16VSqTA7O4ssyySTSYLBICaTiatXrzI/P6/bkkelUiGTyRAIBEin08iyTKFQYHZ2llOnTjE7O7siLXdspqVSSVvfiUQidHZ20t3djaqqRCIRrl69Sjgc1vVc5eTkJP/9v/93VFVl69atABw6dIgTJ07osj4I10dA165dY8eOHaxfv55gMMj8/DyvvfaadrpAr8YRiUQIBoPs3buXVCqF1WrF4/HoMtqoUigUOHnyJOFwmA0bNuB0OjGbzSwtLTE6Oko0GtXteiwvLzM/P09fXx+Li4vMzs7i9/uJxWK6rlFWKhXGxsbw+/34/X6KxSLT09MMDw/r8pxIkoTNZsNkMmGz2RgeHv7U7CAYDHL06FEWFhbqrqVKqVRiamqKUCikJZRWl4P0ZHZ2lh//+MesX7+eRCJBLBZjamqKpaWlFX/WqtNJrVYrjY2N5PN5YrEYmUyGSqWiRcguLy9/Kn2xnjGtPT09PPbYYxgMBt5++21GR0frkvp4Mx2KorBu3Tq6urqYmZlhdHT0tsfE6nU9zGYzmzdvpr29HbfbTSaT4e23377pOeB6Rj1X43MBbe30ZtRLx/bt23G5XMzMzNDR0UFTUxMLCwtMTk4yMTHxG5rqWUyjvb2dQCBANBplfn7+lmvotSp04nA4cLlc5HI5bb22+v+3trbidDoJh8O6PzOroV46qmv55XKZUql02w6uplHPVYxG4w1z2auH1iuVSt3NtIrH48FoNN52zaVeOlRVRVXVO16r1aOBGo1GTCYThULhpg/uvf6gVDPQ0+m0tq5fLBYpl8uk0+nfaCv1zoiHO0smrZWZ3uo7qyayvLwsKr7VQMdnMtNaCLgTfht0VB/StdaxUoSOe0/H3aTlftKx9i9NCwQCwT2AMNMaodeGl0AguDsRZioQCAQ1QEQ9CwQCQQ0QI1OBQCCoAcJMBQKBoAbUJJ30TrjbjzUIHULHvaLjbtJyP+kQI1OBQCCoAcJMBQKBoAYIMxUIBIIaIMxUINCRalTI3ZCUKqgtn8lMq2mca4EkSbhcLlRVXZPvv1upPqSSJIkI7puwVtdEVVVaWlrYs2cPXq9X3Bvuvujrz8KqS+N7vV56enpwu90Eg0GmpqZ0rxju8/l44IEHiMfjZDIZpqam1ixyWpZlHA4HxWJRK0eoB9XKP5VKBY/Hg8vl0qJbDAYD6XSaWCxGPp/XNT3W5XLh8Xi0guHFYpHl5WUtYkYv7HY7LpcLs9mMxWLB6/USi8W4ePGirkWZDQYDra2tuN1uAoEAzc3NlEolLeZGT2RZxuVyaRX2M5mM7om+cD0vbPPmzbS0tGhlCa9du/ZbG+q3KjO12Wxs27aNhx56iIceegi3282rr77KD3/4QyYnJ3V5YCuVCl6vl29961vYbDYaGxsJBoP8j//xP3jnnXd0zyT3eDz8wR/8AT09Pbz33nu89957umSSl8tlZFmmubmZzZs343K5iMfj5PN5lpeXkWUZRVEIBoNcuXJFl6Ayi8XC888/T3t7O6FQiOXlZcxmM5Ikcfz4cS5evFh3DXA9YPBP//RP2bx5MzMzMwAMDAxw+fJlvvvd7+oab+zxeGhoaMDj8dDT00M8HieVSmGxWEin07pq8fv99PX10dDQgNvtJhqNcuTIEV1zsRobG3nyySd55JFHtHSISqXC//yf/5NXX31V1w7GYDDQ29vLpk2bcDgcTE1NMTg4uOJC5iueoxsMBrq7u2lqaiIejxMKhXC73XzjG99g69atuk37ZVnWilOPjo4yODjIwMAA//7f/3sOHDig+/JDX18fTzzxBA888ADf+MY3+NrXvkZDQ0Pdv7dSqZDNZolEIlQqFS0jS5IkTCYTRqORQCBAIBDQ5ZpUo54DgQDFYhGbzYbdbkdRFCwWi67tY8OGDezbtw+bzYYkSeTzeUKhEM3NzezYsUOXzLIqdrud9vZ2bTbX0tKC1+vFZrPpOipUFAW3243VasVqtbJjxw7+7b/9t3z5y1/GZDLposHj8bB37166u7sJh8Ncu3aN6elpnE4njzzyCG63Wxcd8I/t5MUXX+Rf/at/xV/8xV/wve99j6985Ssrbh8rbtlOp5O+vj6ampqQZZlgMMilS5fI5/N4PB7d1oEkScJoNDI3N8fc3BxXrlzh0KFDpNNpfvd3fxefz6eLjioej4epqSneeustrl27xqZNm3j88cd1W9NNJpOMj49rvakkSZjNZtxuNw6HA0mSdJnWKopCe3s70WiUhYUFLQeq2l5SqVTdNXxSx+nTpzl//jzLy8vk83mCwSDlcpkNGzbovt7e0dHBM888Q3d3t5ZqazabdR2FKYqC0WgkHo8zPz/P9PQ0hUKBPXv20NjYqIsGo9GI2WwmGAwyPj5OOBwmm81SLBZZv349AwMDuq2jejwetm/fTm9vL8VikUgkooVkrvR5WXHXXP2R1UiKWCympRrqOVWB62XvGhoaUBSFfD6vZes4nU66u7tZXFzUTUsymcRgMJDP57XY6eofPZYcyuUy4XCYQCCgrRO2trbS0tJCOp3mwoULuo2AyuUyBoMBs9lMa2srHR0deDweTp06pVsbKZVKTE9P09vbi6IoWK1WZFnG7XZrJlbvjr+6XlwNsdu5cyfbtm2jVCphMplIpVKMj4/raqb5fJ5KpYLZbKZQKHD16lV+9rOf3TZGpZbkcjlSqRSFQgG73U53dzcDAwN0dXWxtLSEz+fTJZhTkiQURSGVShEMBkkkEuTzea5du8aHH3644u9fsZnG43GuXr2Ky+XCYrGQzWZJJBIYjUYmJyd1W9QvlUoMDg7y0EMPsX37dm0TyuFwaDlUenL58mUuXrxIf38/mUyGyclJhoaGdF1Ml2WZpqYmenp66OnpYcOGDTgcDl599VXdInSLxSKzs7Ps3r0bn89HX18f27ZtI5VK8eqrr+pqpidOnKC1tZW9e/dit9vp7+/Hbrdz9uxZjh49WvPwNoPBgMViwWazaZ17Pp+nUCgwOTnJD3/4Q86cOcPnPvc5ZFkmEokwPT2t6zS/UqlQKpXw+XzaaPDChQta4rAepNNp0um0NmtyuVw0NTVhNps5fPgw165d06WDqVQqhEIhTp48iaqqdHV1sbi4yDvvvEMoFFrx563YTAuFAleuXCGTyTAwMEBDQwOSJDExMcHo6KiuO6QTExP8r//1v/jjP/5jNm/ejNfrZWJigp/97GcMDg7qpgOuxwr/+Mc/5sCBAySTScbGxnR/UEqlEplMBlmW8fl8BAIBZmZmOHr0KHNzc7ptDF69epW5uTl6e3tJp9McP36cS5cu8f777+u6MZjNZvnwww/xer24XC5mZ2c5fPgwP/nJT+piHDabDYfDQSqVIh6Pf+rfTU9PMzMzw5EjR3jrrbfo7e3lwoULpNPpmuu4HdVBj9frZXl5mdnZWcLhsG5ttVQqMTMzQ09PD42NjaiqysjICN///vf5v//3/+oa9wwQCoV45513tIj01Z42WXUGVHXNsqGhAYvFQiwWu2VUbD1TMK1WK01NTSiKQigUIpFI3NTU610sobpoXSqVdA8pqx7P6u7upru7m0AgwPj4OKdOnSIej99QT72uR39/Pw6Hg2w2y/LyMgsLC7eMAK/nfbFarQQCATKZDLFY7JaG/ll0WCyWislk+g0j/XUkScLpdFIul2/64Na70En1HDLcPuCvHvfG6XTS1dWF0WikXC6TSqWYmZm5ZSDl3V7oRATq3YM6qsscRqORXC53y+lsPdNa7yQ2t946Vspn0WE2myvlcvmORt/VLPt0Ol3zTu4fPv+uuCa36vglSVrRaPhubyPCTIUOoeMe03E3abmfdIh38wUCgaAGCDMVCASCGiDMVCAQCGqASCcVCASCGiBGpgKBQFADhJkKBAJBDRDppEKH0HGP6bibtNxPOsTIVCAQCGqAMFOBQCCoAcJMBQKBoAYIMxUIBIIasKrcBkVRKJfLWoGGtTqrWq0YpSgKiqIQi8XWJBjsRkiStGbXpZoaq3dQ292IwWDAaDRqRTWWl5fX7L5U9ciyrBWHXiuqz8xva3hdragWUs/n8yQSCQqFwqrbx4rNVJIk7HY7TqcTQKssv7y8rPvDa7Va8fv9OBwO7HY74XCYYDC4JjUiP4nFYqG5uZlkMqlrtX8Ak8nEnj17aGlp4cSJE8zMzKxJByPLMmazmXK5rFV31xur1cqDDz7IwYMHkWWZ6elpTp06xeDgINlsVlctqqrS3t7OunXrcDgcxGIxPvroozUxs/7+fg4cOEAgEODdd9/lwoULa2qqkiTh8XgAWFpa0q2tyLJMa2srXV1dmM1mlpeXmZyc5OrVq6uqu7viqlGqqhIIBGhvb9eiIKrV1aenp29a7q3WxxqqiZzVeGO/348kSczPz3PlyhVisVjNddhstkqlUqFSqWgpn5IkaXHLdrsdr9fLvn37eO655/i7v/s73njjjRs21Hoc8zAajbz44os8/vjjWgLC4cOHefvtt29qqPU6btLV1cXTTz+N0+lkenqa48ePMzExcdPPqrUOVVV59tlnefDBB3E6nSwuLlIsFvF6vZw5c4Zf/vKXN0zjrFftzoMHDzIwMIDD4cBmsxGNRnnttde4cOHCDcsU1utoVGNjI3/1V39FU1MTs7OzTE1NceXKFU6fPs3MzIyuNW/b29vZvXs3PT09tLa2Ui6XOX36NG+//bYu98Zms7F3714CgQDZbJZKpYLD4SAcDvPhhx+u2MtWPDKt5tqUSiWMRiPt7e3s2bOHcDjM//k//4dr167pVm3fYrHgdDpxOp1YrVbMZjMej4disciVK1dqPo369YtrMBhwuVxahfVq/MLAwIBWFNlkMunW62/fvp0/+ZM/YW5ujvHxcWw2G//kn/wTZmZmdItXrrJhwwaeeuopUqkUPT09bNq0iZ/85CdcunRJl5GH3W6npaWFK1eukM1mMZvNmM1mXC4XBw8eZHBwULdo45aWFp5//nnMZjPJZBKHw0FzczOjo6NcunRJt+fFZrPx7W9/m4aGBsbHx5mYmCAWi9HQ0MDTTz/NO++8o1tUe2dnJy+88ALbt2+nv7+fzs5OJEkiFovxzjvv1P37VVWlu7sbr9eLwWCgWCySy+XweDw88cQTZDIZTpw4saLZ9oo3oMrlMsVikWw2q0UKd3d388wzz7Bp0yZt+l9vKpUKxWIRl8tFS0sLjY2N+Hw+fD4fdrsdi8VS88C0Uqmk/SmXy5RKJYrFojYlqFYNHxoa4ic/+QnDw8O6TbFVVeXBBx+ku7tbCzdMJpP4/X6ee+45jEajLjo+qadQKGipoF1dXfze7/2ebjG+1eTcxcVF8vk8iqJgs9moVCr4fD76+/t1ScCUJImGhgba2tpobW3F5/PR29vLY489RktLi25pvgCbNm3ihRdeoFAoEIvFSKfT5HI5SqUSvb29DAwM6JKd1tDQwMGDB+no6MBoNBIKhTh27BivvPIKs7OzuiwXGo1Gmpqa8Hg8BAIB/H4/LpcLt9vNxo0beeKJJ1bcVlc8Ml1eXiabzZJOp7FYLITDYU6cOIGiKGQyGcxmM7Is1723rVQqJBIJXC4Xu3btorW1lXw+z9WrVykWixgMBq3HqRflcplsNoskSaRSKRRF0QLj0uk0yWRS1/XKTCZDsVjEZDJpozCHw8GOHTvw+/3Mzc3ppmVqaop0Oo3T6cThcGjTOL0ifCuVCna7XYs2bmtro6+vj/7+flRV1c3EZFnG4/GQSqWIRqNEo1HC4TDvvvsu7733nm77DA6HA4fDQaVSYf369YyMjGgbuK2traiqyvT0tLZ8VS+MRiN79+5l165d2O12FhcXsVgsDAwMEIvFyGQyt4wuqRXFYpFYLIbdbmfDhg2kUilCoRCBQEAL+lupjhWbaaVS+VRujcFgYHl5mXQ6zfz8vK7mEY/HCYVCtLS08OCDD3L+/Hk+/PBDksmkbg9LtfGpqqqdciiXy+RyOS06Vg8KhQJvvPEGzz77LOvWrSORSNDT00NXVxeZTAaLxaKLjipjY2OcPn2a3//936ejo4OFhQVee+01lpaWdPl+m81Gd3c3nZ2dmM1mWlpaaG1tpbm5mStXrhAMBnVpq5VKhdnZWc6fP4/NZiMQCBCJRBgcHNQ1zTeZTHLs2DH+4i/+gi9/+cs4HA7a2towmUzYbDbefPNNxsbG6q6jWCwSjUYpFou43W6mpqaYnJwkl8tpz44e5PN5BgcHyWQy2O129uzZw/79+3G73UQiEY4dO7bijuUzBeqpqorNZsNkMmkGksvlbiiiXovYjY2NPP/88/T29jI4OMjg4CDhcJhMJkMymfyN9Z966bDZbDidTkwmE4VCgXQ6TSqVuunIo146du/eze/8zu/g8XgwGo2kUik++OADjh49Sjgc1k0HgM/n44UXXmDnzp3aNO5mvX0tdRgMBtra2mhvb9eyhvx+Pw0NDRgMBpaWlvjggw+YmZmpq44qFouFjRs30tXVRX9/PyaTiTfeeIMTJ07c9LPq+W6+1+ulubmZtrY2uru7mZyc5IMPPtDl3sD10XpHRwcbNmzQ9hai0SiFQoFsNkswGNR1I8zv9/PQQw/xz//5P6enp4df/OIX/Mf/+B9vGhBatwwoWZYxGAy3DU+r14VQFAW3261N6QuFwk0NvZ46ZFnGZrNhs9mA61PudDqt+y56tZNTVVXLSC8Wi586F6yHjirV84yFQuGWI8F6PLCA1iaraZzVGcvN0mPr2T4MBgNOpxOPx3PbI3x6FDqpHl8rlUprErr4ySWfO5kl1LOtqqrKQw89xKOPPqp1dDfzRhGop4OOauM0m80Ui0VtDVNvHStB6Lj3dNxNWn6bdHg8Hvbt28fJkyeJRqMr1rGqN6AEN6ZcLmsGWh2tCwSC3w5SqRTz8/Or3oQTZloH6r0jKhAIak+hUCCVSq3q7ScQhU4EAoEAuH5sq3rudjUIMxUIBAKuzygTicSqz/6KdFKBQCCoAWJkKhAIBDVAmKlAIBDUAGGmAoFAUAOEmQoEAkENEGYqEAgENUCYqUAgENSA/x8Tz7hF7X6kLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1280\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "base_model_frozen = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.0.5.0.block.0.0.weight\n",
      "base_model.0.5.0.block.0.1.weight\n",
      "base_model.0.5.0.block.0.1.bias\n",
      "base_model.0.5.0.block.1.0.weight\n",
      "base_model.0.5.0.block.1.1.weight\n",
      "base_model.0.5.0.block.1.1.bias\n",
      "base_model.0.5.0.block.2.fc1.weight\n",
      "base_model.0.5.0.block.2.fc1.bias\n",
      "base_model.0.5.0.block.2.fc2.weight\n",
      "base_model.0.5.0.block.2.fc2.bias\n",
      "base_model.0.5.0.block.3.0.weight\n",
      "base_model.0.5.0.block.3.1.weight\n",
      "base_model.0.5.0.block.3.1.bias\n",
      "base_model.0.5.1.block.0.0.weight\n",
      "base_model.0.5.1.block.0.1.weight\n",
      "base_model.0.5.1.block.0.1.bias\n",
      "base_model.0.5.1.block.1.0.weight\n",
      "base_model.0.5.1.block.1.1.weight\n",
      "base_model.0.5.1.block.1.1.bias\n",
      "base_model.0.5.1.block.2.fc1.weight\n",
      "base_model.0.5.1.block.2.fc1.bias\n",
      "base_model.0.5.1.block.2.fc2.weight\n",
      "base_model.0.5.1.block.2.fc2.bias\n",
      "base_model.0.5.1.block.3.0.weight\n",
      "base_model.0.5.1.block.3.1.weight\n",
      "base_model.0.5.1.block.3.1.bias\n",
      "base_model.0.5.2.block.0.0.weight\n",
      "base_model.0.5.2.block.0.1.weight\n",
      "base_model.0.5.2.block.0.1.bias\n",
      "base_model.0.5.2.block.1.0.weight\n",
      "base_model.0.5.2.block.1.1.weight\n",
      "base_model.0.5.2.block.1.1.bias\n",
      "base_model.0.5.2.block.2.fc1.weight\n",
      "base_model.0.5.2.block.2.fc1.bias\n",
      "base_model.0.5.2.block.2.fc2.weight\n",
      "base_model.0.5.2.block.2.fc2.bias\n",
      "base_model.0.5.2.block.3.0.weight\n",
      "base_model.0.5.2.block.3.1.weight\n",
      "base_model.0.5.2.block.3.1.bias\n",
      "base_model.0.6.0.block.0.0.weight\n",
      "base_model.0.6.0.block.0.1.weight\n",
      "base_model.0.6.0.block.0.1.bias\n",
      "base_model.0.6.0.block.1.0.weight\n",
      "base_model.0.6.0.block.1.1.weight\n",
      "base_model.0.6.0.block.1.1.bias\n",
      "base_model.0.6.0.block.2.fc1.weight\n",
      "base_model.0.6.0.block.2.fc1.bias\n",
      "base_model.0.6.0.block.2.fc2.weight\n",
      "base_model.0.6.0.block.2.fc2.bias\n",
      "base_model.0.6.0.block.3.0.weight\n",
      "base_model.0.6.0.block.3.1.weight\n",
      "base_model.0.6.0.block.3.1.bias\n",
      "base_model.0.6.1.block.0.0.weight\n",
      "base_model.0.6.1.block.0.1.weight\n",
      "base_model.0.6.1.block.0.1.bias\n",
      "base_model.0.6.1.block.1.0.weight\n",
      "base_model.0.6.1.block.1.1.weight\n",
      "base_model.0.6.1.block.1.1.bias\n",
      "base_model.0.6.1.block.2.fc1.weight\n",
      "base_model.0.6.1.block.2.fc1.bias\n",
      "base_model.0.6.1.block.2.fc2.weight\n",
      "base_model.0.6.1.block.2.fc2.bias\n",
      "base_model.0.6.1.block.3.0.weight\n",
      "base_model.0.6.1.block.3.1.weight\n",
      "base_model.0.6.1.block.3.1.bias\n",
      "base_model.0.6.2.block.0.0.weight\n",
      "base_model.0.6.2.block.0.1.weight\n",
      "base_model.0.6.2.block.0.1.bias\n",
      "base_model.0.6.2.block.1.0.weight\n",
      "base_model.0.6.2.block.1.1.weight\n",
      "base_model.0.6.2.block.1.1.bias\n",
      "base_model.0.6.2.block.2.fc1.weight\n",
      "base_model.0.6.2.block.2.fc1.bias\n",
      "base_model.0.6.2.block.2.fc2.weight\n",
      "base_model.0.6.2.block.2.fc2.bias\n",
      "base_model.0.6.2.block.3.0.weight\n",
      "base_model.0.6.2.block.3.1.weight\n",
      "base_model.0.6.2.block.3.1.bias\n",
      "base_model.0.6.3.block.0.0.weight\n",
      "base_model.0.6.3.block.0.1.weight\n",
      "base_model.0.6.3.block.0.1.bias\n",
      "base_model.0.6.3.block.1.0.weight\n",
      "base_model.0.6.3.block.1.1.weight\n",
      "base_model.0.6.3.block.1.1.bias\n",
      "base_model.0.6.3.block.2.fc1.weight\n",
      "base_model.0.6.3.block.2.fc1.bias\n",
      "base_model.0.6.3.block.2.fc2.weight\n",
      "base_model.0.6.3.block.2.fc2.bias\n",
      "base_model.0.6.3.block.3.0.weight\n",
      "base_model.0.6.3.block.3.1.weight\n",
      "base_model.0.6.3.block.3.1.bias\n",
      "base_model.0.7.0.block.0.0.weight\n",
      "base_model.0.7.0.block.0.1.weight\n",
      "base_model.0.7.0.block.0.1.bias\n",
      "base_model.0.7.0.block.1.0.weight\n",
      "base_model.0.7.0.block.1.1.weight\n",
      "base_model.0.7.0.block.1.1.bias\n",
      "base_model.0.7.0.block.2.fc1.weight\n",
      "base_model.0.7.0.block.2.fc1.bias\n",
      "base_model.0.7.0.block.2.fc2.weight\n",
      "base_model.0.7.0.block.2.fc2.bias\n",
      "base_model.0.7.0.block.3.0.weight\n",
      "base_model.0.7.0.block.3.1.weight\n",
      "base_model.0.7.0.block.3.1.bias\n",
      "base_model.0.8.0.weight\n",
      "base_model.0.8.1.weight\n",
      "base_model.0.8.1.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    # print(name)\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[13]) < 5:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[13]) >= 5:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   #scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    scaler_bounds=(10**(4), 10**(6)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "    \n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=800,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_dl(data, transforms, expand_dim=False):\n",
    "    setup_ds = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(data, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(data.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    transform=transforms,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "\n",
    "                                    temp_uncertainty=16,\n",
    "                                    expand_dim=expand_dim\n",
    "                                    )\n",
    "    setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            drop_last=False,\n",
    "                                            # pin_memory=True,\n",
    "                                            # num_workers=16,\n",
    "                                            # persistent_workers=True,\n",
    "                                            )\n",
    "\n",
    "    return setup_ds, setup_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1702, 32, 32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_labeled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "no_transforms = torch.nn.Sequential(\n",
    ")\n",
    "scale_rsz_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR)\n",
    ")\n",
    "tile_transforms = torch.nn.Sequential(\n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "\n",
    "\n",
    "scripted_all_transforms = torch.jit.script(all_transforms)\n",
    "scripted_no_transforms = torch.jit.script(no_transforms)\n",
    "scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "scripted_tile_transform = torch.jit.script(tile_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_scale_rsz_transform\n",
    "run_transform = scripted_tile_transform\n",
    "\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_train, setup_transform, expand_dim=True)\n",
    "input_xtr = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=True)\n",
    "input_xtr_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_val, setup_transform, expand_dim=True)\n",
    "input_xval = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_val_SYT, setup_transform, expand_dim=True)\n",
    "input_xval_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "# print(\"Starting!\")\n",
    "# tik = time.time()\n",
    "# ds_run = util.dataset_simCLR(\n",
    "#                             torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "#                             torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                             n_transforms=1,\n",
    "#                             class_weights=np.array([1]),\n",
    "#                             transform=run_transform,\n",
    "#                             DEVICE='cpu',\n",
    "#                             dtype_X=torch.float32,\n",
    "#                             dtype_y=torch.int64,\n",
    "\n",
    "#                             temp_uncertainty=16,\n",
    "#                             expand_dim=False\n",
    "#                             )\n",
    "# dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "#                                         batch_size=1024,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         # pin_memory=True,\n",
    "#                                         # num_workers=16,\n",
    "#                                         # persistent_workers=True,\n",
    "#                                         )\n",
    "# features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "# tok = time.time()\n",
    "\n",
    "# print(f'Tile Transformation Alone Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1702, 1, 224, 224]),\n",
       " torch.Size([54704, 1, 224, 224]),\n",
       " torch.Size([426, 1, 224, 224]),\n",
       " torch.Size([13677, 1, 224, 224]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_xtr.shape, input_xtr_SYT.shape, input_xval.shape, input_xval_SYT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_rsz_transforms = torch.nn.Sequential(\n",
    "#     augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "#     torchvision.transforms.Resize(size=(224,224),\n",
    "#                                   interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "# tile_transforms = torch.nn.Sequential(augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "\n",
    "\n",
    "# scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "# scripted_tile_transform = torch.jit.script(tile_transforms)\n",
    "\n",
    "\n",
    "# setup_transform = scripted_scale_rsz_transform\n",
    "# run_transform = scripted_tile_transform\n",
    "\n",
    "\n",
    "\n",
    "# # setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=False)\n",
    "\n",
    "# setup_ds = util.dataset_simCLR(\n",
    "#                                 torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "#                                 torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "#                                 n_transforms=1,\n",
    "#                                 transform=setup_transform,\n",
    "#                                 class_weights=np.array([1]),\n",
    "#                                 DEVICE='cpu',\n",
    "#                                 dtype_X=torch.float32,\n",
    "#                                 dtype_y=torch.int64,\n",
    "\n",
    "#                                 temp_uncertainty=16,\n",
    "#                                 expand_dim=False\n",
    "#                                 )\n",
    "# setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "#                                         batch_size=1024,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         # pin_memory=True,\n",
    "#                                         # num_workers=16,\n",
    "#                                         # persistent_workers=True,\n",
    "#                                         )\n",
    "\n",
    "\n",
    "# input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "# # setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=False)\n",
    "# # input_x_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C1\n",
      "C2\n",
      "C1\n",
      "C2\n",
      "C1\n",
      "C2\n",
      "C1\n",
      "C2\n"
     ]
    }
   ],
   "source": [
    "acc_train_lst, acc_val_lst = [], []\n",
    "acc_train_SYT_lst, acc_val_SYT_lst = [], []\n",
    "\n",
    "print('A')\n",
    "ds_run, dl_run = get_ds_dl(input_xtr, run_transform, expand_dim=False)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "ds_run, dl_run = get_ds_dl(input_xval, run_transform, expand_dim=False)\n",
    "features_val = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "print('B')\n",
    "ds_run, dl_run = get_ds_dl(input_xtr_SYT, run_transform, expand_dim=False)\n",
    "features_train_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "ds_run, dl_run = get_ds_dl(input_xval_SYT, run_transform, expand_dim=False)\n",
    "features_val_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "\n",
    "acc_train, acc_val = {}, {}\n",
    "acc_train_SYT, acc_val_SYT = {}, {}\n",
    "C_toUse = np.array([1e1,1e0,1e-1,1e-2])\n",
    "for C in C_toUse:\n",
    "    print('C1')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    acc_train_tmp = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train[C] = acc_train_tmp\n",
    "    acc_val_tmp = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val[C] = acc_val_tmp\n",
    "    \n",
    "    print('C2')\n",
    "    logreg_SYT = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "    logreg_SYT.fit(features_train_SYT, y_labeled_train_SYT)\n",
    "    acc_train_tmp = logreg_SYT.score(features_train_SYT, y_labeled_train_SYT)\n",
    "    acc_train_SYT[C] = acc_train_tmp\n",
    "    acc_val_tmp = logreg_SYT.score(features_val_SYT, y_labeled_val_SYT)\n",
    "    acc_val_SYT[C] = acc_val_tmp\n",
    "\n",
    "acc_train_lst.append(acc_train)\n",
    "acc_val_lst.append(acc_val)\n",
    "acc_train_SYT_lst.append(acc_train_SYT)\n",
    "acc_val_SYT_lst.append(acc_val_SYT)\n",
    "\n",
    "\n",
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train_df, acc_val_df, acc_train_SYT_df, acc_val_SYT_df = pd.DataFrame(acc_train_lst), pd.DataFrame(acc_val_lst), pd.DataFrame(acc_train_SYT_lst), pd.DataFrame(acc_val_SYT_lst)\n",
    "# display(acc_train_df)\n",
    "# display(acc_val_df)\n",
    "# display(acc_train_SYT_df)\n",
    "# display(acc_val_SYT_df)\n",
    "\n",
    "# acc_train_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train.csv')\n",
    "# acc_val_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val.csv')\n",
    "# acc_train_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train_SYT.csv')\n",
    "# acc_val_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val_SYT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=efficient2'\n",
    "model.forward = model.forward_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/889, loss_train: 7.3489, loss_val: nan, pos_over_neg: 1.0218603610992432 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 7.1667, loss_val: nan, pos_over_neg: 1.2531288862228394 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 6.9281, loss_val: nan, pos_over_neg: 3.424379348754883 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 6.7751, loss_val: nan, pos_over_neg: 6.434996604919434 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 6.7004, loss_val: nan, pos_over_neg: 5.847537040710449 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 6.6438, loss_val: nan, pos_over_neg: 7.788357257843018 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 6.541, loss_val: nan, pos_over_neg: 11.10680103302002 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 6.5071, loss_val: nan, pos_over_neg: 16.07928466796875 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 6.4453, loss_val: nan, pos_over_neg: 19.676834106445312 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 6.3991, loss_val: nan, pos_over_neg: 22.576356887817383 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 6.365, loss_val: nan, pos_over_neg: 34.20755386352539 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 6.3463, loss_val: nan, pos_over_neg: 38.53498840332031 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 6.311, loss_val: nan, pos_over_neg: 40.20724868774414 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 6.3021, loss_val: nan, pos_over_neg: 39.71895980834961 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 6.2807, loss_val: nan, pos_over_neg: 47.25564956665039 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 6.2777, loss_val: nan, pos_over_neg: 66.3509292602539 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 6.2622, loss_val: nan, pos_over_neg: 109.88153839111328 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 6.2499, loss_val: nan, pos_over_neg: 155.49415588378906 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 6.2144, loss_val: nan, pos_over_neg: 190.2143096923828 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 6.2115, loss_val: nan, pos_over_neg: 259.9737243652344 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 6.2112, loss_val: nan, pos_over_neg: 273.42486572265625 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 6.1973, loss_val: nan, pos_over_neg: 176.2691650390625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 6.176, loss_val: nan, pos_over_neg: 153.86004638671875 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 6.156, loss_val: nan, pos_over_neg: 135.2069549560547 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 6.1359, loss_val: nan, pos_over_neg: 155.93704223632812 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 6.1443, loss_val: nan, pos_over_neg: 352.9247741699219 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 6.1289, loss_val: nan, pos_over_neg: 244.03587341308594 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 6.1181, loss_val: nan, pos_over_neg: 459.6436462402344 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 6.0781, loss_val: nan, pos_over_neg: 897.4336547851562 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 6.0872, loss_val: nan, pos_over_neg: 598.072265625 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 6.0783, loss_val: nan, pos_over_neg: 856.1463623046875 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 6.0654, loss_val: nan, pos_over_neg: 918.43896484375 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 6.047, loss_val: nan, pos_over_neg: 609.09228515625 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 6.0476, loss_val: nan, pos_over_neg: 777.4209594726562 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 6.0515, loss_val: nan, pos_over_neg: 457.71917724609375 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 6.0434, loss_val: nan, pos_over_neg: 414.9387512207031 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 6.033, loss_val: nan, pos_over_neg: 500.8128967285156 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 6.0371, loss_val: nan, pos_over_neg: 679.1396484375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 6.0104, loss_val: nan, pos_over_neg: 2016.349609375 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 6.0036, loss_val: nan, pos_over_neg: 1137.2921142578125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 6.0141, loss_val: nan, pos_over_neg: 751.8474731445312 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.9952, loss_val: nan, pos_over_neg: 704.4049072265625 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 6.0054, loss_val: nan, pos_over_neg: 374.786865234375 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.9908, loss_val: nan, pos_over_neg: 505.5580139160156 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.9962, loss_val: nan, pos_over_neg: 1314.767333984375 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.978, loss_val: nan, pos_over_neg: 2519.429443359375 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.9824, loss_val: nan, pos_over_neg: 1660.6746826171875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.9635, loss_val: nan, pos_over_neg: 2279.42919921875 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.9576, loss_val: nan, pos_over_neg: 829.4918823242188 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.9601, loss_val: nan, pos_over_neg: 778.9636840820312 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.9566, loss_val: nan, pos_over_neg: 251.94119262695312 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.9517, loss_val: nan, pos_over_neg: 2493.60986328125 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.9486, loss_val: nan, pos_over_neg: 12647.9384765625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.9397, loss_val: nan, pos_over_neg: -4970.1328125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.9741, loss_val: nan, pos_over_neg: 66516.9921875 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1047.183837890625 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.9315, loss_val: nan, pos_over_neg: 623.5249633789062 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.935, loss_val: nan, pos_over_neg: 670.3714599609375 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.9147, loss_val: nan, pos_over_neg: 5880.59228515625 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.9143, loss_val: nan, pos_over_neg: 1812.440673828125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.9325, loss_val: nan, pos_over_neg: -17262.384765625 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.9118, loss_val: nan, pos_over_neg: 665.1663208007812 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.9256, loss_val: nan, pos_over_neg: 436.7052001953125 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.9272, loss_val: nan, pos_over_neg: 711.2052001953125 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8951, loss_val: nan, pos_over_neg: -10353.447265625 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.9086, loss_val: nan, pos_over_neg: 454.3318786621094 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8945, loss_val: nan, pos_over_neg: 550.3419799804688 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8958, loss_val: nan, pos_over_neg: 600.2252807617188 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.9113, loss_val: nan, pos_over_neg: 233.8683319091797 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.9062, loss_val: nan, pos_over_neg: 1093.67041015625 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.9126, loss_val: nan, pos_over_neg: -2361.476806640625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8867, loss_val: nan, pos_over_neg: -34897.8203125 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.8892, loss_val: nan, pos_over_neg: 562.2448120117188 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8747, loss_val: nan, pos_over_neg: 523.6532592773438 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8855, loss_val: nan, pos_over_neg: 634.6865234375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8984, loss_val: nan, pos_over_neg: 565.1267700195312 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8775, loss_val: nan, pos_over_neg: 1372.900634765625 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.863, loss_val: nan, pos_over_neg: -22089.525390625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.86, loss_val: nan, pos_over_neg: -2478.9365234375 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8768, loss_val: nan, pos_over_neg: 1411.687255859375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8814, loss_val: nan, pos_over_neg: 704.1934204101562 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.8766, loss_val: nan, pos_over_neg: 406.9653015136719 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.8777, loss_val: nan, pos_over_neg: 1481.156982421875 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 358.30706787109375 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 1851.8226318359375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: -27159.75 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8596, loss_val: nan, pos_over_neg: 7856.36962890625 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.8776, loss_val: nan, pos_over_neg: 354.3143615722656 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 1550.126953125 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8514, loss_val: nan, pos_over_neg: 4069.149169921875 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 2110.541259765625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 789.1513671875 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1566.046630859375 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8697, loss_val: nan, pos_over_neg: -2678.0107421875 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 947.2224731445312 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1352.6988525390625 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 543.8713989257812 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 863.08203125 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 972.7201538085938 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8527, loss_val: nan, pos_over_neg: 349.6490478515625 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 23660.255859375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 936.5053100585938 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 785.1295166015625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 428.9679260253906 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 802.4525756835938 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8471, loss_val: nan, pos_over_neg: 4870.951171875 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.841, loss_val: nan, pos_over_neg: 1229.29833984375 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 1152.7652587890625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.83, loss_val: nan, pos_over_neg: 6389.421875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 421.2371520996094 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 661.7796630859375 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 3471.34765625 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8244, loss_val: nan, pos_over_neg: -4292.27001953125 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 6885.16357421875 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: 1749.771728515625 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 5330.88037109375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 537.7903442382812 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 825.2919921875 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 4042.569091796875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 2115.540771484375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: -3710.85546875 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 1200.03564453125 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 573.8773803710938 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 4124.2080078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 4351.94580078125 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1008.4231567382812 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1358.898681640625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 8465.66796875 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: -1799.256591796875 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 3989.9140625 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 612.7644653320312 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1554.16845703125 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1117.1136474609375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 4629.33447265625 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 657.7847290039062 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 567.651123046875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 437.41070556640625 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 719.4803466796875 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 829.2825317382812 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 668.8380737304688 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 625.8037719726562 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 2242.93115234375 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1234.493896484375 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 287.6842956542969 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 851.6539306640625 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 294.3477783203125 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 532.2883911132812 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 431.4588623046875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1122.669189453125 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 321.5580139160156 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 516.164794921875 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1474.064208984375 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 875.1029052734375 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1376.8670654296875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 872.866455078125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 2439.79443359375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 571.9491577148438 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 411.513916015625 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 1016.9119262695312 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 623.551025390625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 2240.5078125 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 3803.31005859375 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 553.9735717773438 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 594.4166259765625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1374.1939697265625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 956.6944580078125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1501.050537109375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 473.1106872558594 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 606.9022827148438 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 246.7510223388672 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 268.206298828125 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 612.2196655273438 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1599.5450439453125 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 806.0829467773438 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1679.5732421875 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 814.0196533203125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 246.86419677734375 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 212.8710479736328 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.7569, loss_val: nan, pos_over_neg: 944.4927368164062 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1097.1055908203125 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: 317.5439147949219 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 569.370361328125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 687.9852294921875 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 266.98028564453125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1188.77197265625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7553, loss_val: nan, pos_over_neg: -2159.19189453125 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 938.0068969726562 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 6499.3681640625 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 915.2655639648438 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 466.4402770996094 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1293.5863037109375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1202.5467529296875 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 1015.565673828125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1013.2996826171875 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 913.5841674804688 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 4445.43701171875 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 513.7888793945312 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1210.133544921875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 387.99200439453125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 697.1975708007812 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7441, loss_val: nan, pos_over_neg: -2974.465576171875 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7432, loss_val: nan, pos_over_neg: 3833.87451171875 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.754, loss_val: nan, pos_over_neg: 503.4884338378906 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7506, loss_val: nan, pos_over_neg: 3068.182861328125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 2061.68359375 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7489, loss_val: nan, pos_over_neg: 794.5281982421875 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.754, loss_val: nan, pos_over_neg: -4591.52978515625 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1063.281982421875 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 602.2520141601562 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7391, loss_val: nan, pos_over_neg: 768.0819702148438 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 566.5869750976562 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7513, loss_val: nan, pos_over_neg: 279.22735595703125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1150.795654296875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1308.8621826171875 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7428, loss_val: nan, pos_over_neg: 1186.48291015625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1039.8841552734375 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 633.4956665039062 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7533, loss_val: nan, pos_over_neg: -13477.783203125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.7386, loss_val: nan, pos_over_neg: 770.466064453125 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 544.4683227539062 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7344, loss_val: nan, pos_over_neg: 795.678466796875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.7543, loss_val: nan, pos_over_neg: 790.8665771484375 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.7527, loss_val: nan, pos_over_neg: 537.876220703125 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7447, loss_val: nan, pos_over_neg: 327.19622802734375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.7445, loss_val: nan, pos_over_neg: 508.4851989746094 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 230.1172637939453 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7397, loss_val: nan, pos_over_neg: 746.8460693359375 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7482, loss_val: nan, pos_over_neg: 457.1522521972656 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.7487, loss_val: nan, pos_over_neg: 373.4698181152344 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7404, loss_val: nan, pos_over_neg: 573.5433349609375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7419, loss_val: nan, pos_over_neg: 469.5032958984375 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7433, loss_val: nan, pos_over_neg: 1175.694091796875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.7434, loss_val: nan, pos_over_neg: 416.9049072265625 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.7422, loss_val: nan, pos_over_neg: 547.6319580078125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7482, loss_val: nan, pos_over_neg: 626.483154296875 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7377, loss_val: nan, pos_over_neg: 919.76513671875 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.7376, loss_val: nan, pos_over_neg: 343.3659362792969 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7342, loss_val: nan, pos_over_neg: 953.4727172851562 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7311, loss_val: nan, pos_over_neg: 1252.315185546875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7228, loss_val: nan, pos_over_neg: 488.5873718261719 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.734, loss_val: nan, pos_over_neg: 540.7297973632812 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.7429, loss_val: nan, pos_over_neg: 1104.275634765625 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.7478, loss_val: nan, pos_over_neg: 672.8380737304688 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7381, loss_val: nan, pos_over_neg: 682.0009155273438 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7323, loss_val: nan, pos_over_neg: 1188.198486328125 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.7195, loss_val: nan, pos_over_neg: 827.6441040039062 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7375, loss_val: nan, pos_over_neg: 617.9528198242188 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7396, loss_val: nan, pos_over_neg: 1333.5426025390625 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7274, loss_val: nan, pos_over_neg: 1118.230712890625 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1240.9549560546875 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.736, loss_val: nan, pos_over_neg: 545.4193115234375 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.726, loss_val: nan, pos_over_neg: 665.3795776367188 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7386, loss_val: nan, pos_over_neg: 837.991455078125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.7507, loss_val: nan, pos_over_neg: 814.4208374023438 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7396, loss_val: nan, pos_over_neg: 401.8033447265625 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7268, loss_val: nan, pos_over_neg: 1490.945556640625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7258, loss_val: nan, pos_over_neg: 1949.8035888671875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.7293, loss_val: nan, pos_over_neg: 485.41259765625 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7321, loss_val: nan, pos_over_neg: 992.9728393554688 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.7226, loss_val: nan, pos_over_neg: 1073.87744140625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.7399, loss_val: nan, pos_over_neg: 444.8865661621094 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7308, loss_val: nan, pos_over_neg: 643.1492309570312 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7475, loss_val: nan, pos_over_neg: 563.2283325195312 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7306, loss_val: nan, pos_over_neg: 655.9197998046875 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.7273, loss_val: nan, pos_over_neg: 1357.3109130859375 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7261, loss_val: nan, pos_over_neg: 3258.30078125 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.7363, loss_val: nan, pos_over_neg: 3344.908203125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7355, loss_val: nan, pos_over_neg: 452.0653991699219 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7221, loss_val: nan, pos_over_neg: 9908.5712890625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7268, loss_val: nan, pos_over_neg: 805.0778198242188 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7135, loss_val: nan, pos_over_neg: 1409.4991455078125 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7321, loss_val: nan, pos_over_neg: 598.9817504882812 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7244, loss_val: nan, pos_over_neg: 745.1209716796875 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7281, loss_val: nan, pos_over_neg: 300.9827575683594 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.732, loss_val: nan, pos_over_neg: 887.66015625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7279, loss_val: nan, pos_over_neg: 750.0911254882812 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7231, loss_val: nan, pos_over_neg: 509.9466552734375 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7295, loss_val: nan, pos_over_neg: 399.53436279296875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7393, loss_val: nan, pos_over_neg: 765.7774658203125 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.727, loss_val: nan, pos_over_neg: 811.3058471679688 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7307, loss_val: nan, pos_over_neg: 466.72198486328125 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.7182, loss_val: nan, pos_over_neg: 766.148193359375 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7236, loss_val: nan, pos_over_neg: 1043.6256103515625 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7329, loss_val: nan, pos_over_neg: 374.68890380859375 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.723, loss_val: nan, pos_over_neg: 316.5936279296875 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7292, loss_val: nan, pos_over_neg: 760.6557006835938 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.7176, loss_val: nan, pos_over_neg: 648.0968627929688 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.7261, loss_val: nan, pos_over_neg: 312.2889404296875 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.71, loss_val: nan, pos_over_neg: 577.163330078125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7156, loss_val: nan, pos_over_neg: 1676.6402587890625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.7199, loss_val: nan, pos_over_neg: 429.3632507324219 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.719, loss_val: nan, pos_over_neg: 476.7124938964844 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7146, loss_val: nan, pos_over_neg: 950.1343994140625 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.7253, loss_val: nan, pos_over_neg: 907.7017822265625 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.7172, loss_val: nan, pos_over_neg: 757.0682373046875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.7111, loss_val: nan, pos_over_neg: 1505.34716796875 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.724, loss_val: nan, pos_over_neg: 1278.6417236328125 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7338, loss_val: nan, pos_over_neg: 688.8074340820312 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7244, loss_val: nan, pos_over_neg: 328.9518127441406 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7068, loss_val: nan, pos_over_neg: 813.4170532226562 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.7168, loss_val: nan, pos_over_neg: 1328.9058837890625 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.723, loss_val: nan, pos_over_neg: 874.0831909179688 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7114, loss_val: nan, pos_over_neg: 1395.7305908203125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.7194, loss_val: nan, pos_over_neg: 1087.8604736328125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.71, loss_val: nan, pos_over_neg: 4606.27685546875 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.7225, loss_val: nan, pos_over_neg: 796.0230102539062 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.7261, loss_val: nan, pos_over_neg: 268.4637451171875 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.7159, loss_val: nan, pos_over_neg: 529.8861083984375 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7148, loss_val: nan, pos_over_neg: 1201.794677734375 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.7167, loss_val: nan, pos_over_neg: 1530.36767578125 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7273, loss_val: nan, pos_over_neg: 2199.86767578125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.7298, loss_val: nan, pos_over_neg: 429.0675354003906 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7248, loss_val: nan, pos_over_neg: 1136.3377685546875 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7167, loss_val: nan, pos_over_neg: 734.6085815429688 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7115, loss_val: nan, pos_over_neg: 703.9172973632812 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.7264, loss_val: nan, pos_over_neg: 554.6452026367188 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7138, loss_val: nan, pos_over_neg: 1464.9422607421875 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.705, loss_val: nan, pos_over_neg: 747.14306640625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.713, loss_val: nan, pos_over_neg: 1862.5777587890625 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7169, loss_val: nan, pos_over_neg: 1007.0101928710938 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7094, loss_val: nan, pos_over_neg: 1497.471923828125 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.7066, loss_val: nan, pos_over_neg: 698.9130249023438 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7031, loss_val: nan, pos_over_neg: 454.9764404296875 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7118, loss_val: nan, pos_over_neg: 800.2095336914062 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7235, loss_val: nan, pos_over_neg: 1359.373779296875 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7129, loss_val: nan, pos_over_neg: 719.7752685546875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.7161, loss_val: nan, pos_over_neg: 1735.2884521484375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7235, loss_val: nan, pos_over_neg: 546.156494140625 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7197, loss_val: nan, pos_over_neg: 837.7635498046875 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7128, loss_val: nan, pos_over_neg: 670.8521728515625 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.7007, loss_val: nan, pos_over_neg: 771.8200073242188 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1077.075439453125 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7057, loss_val: nan, pos_over_neg: 562.53173828125 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.7159, loss_val: nan, pos_over_neg: 448.2051086425781 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7252, loss_val: nan, pos_over_neg: 303.8927917480469 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.7193, loss_val: nan, pos_over_neg: 224.2359161376953 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7113, loss_val: nan, pos_over_neg: 836.89599609375 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7241, loss_val: nan, pos_over_neg: 389.2817687988281 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7116, loss_val: nan, pos_over_neg: 1528.8177490234375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.7215, loss_val: nan, pos_over_neg: 713.1276245117188 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7269, loss_val: nan, pos_over_neg: 362.7642517089844 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7123, loss_val: nan, pos_over_neg: 492.6635437011719 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.7091, loss_val: nan, pos_over_neg: 633.3531494140625 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7075, loss_val: nan, pos_over_neg: 560.193115234375 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7039, loss_val: nan, pos_over_neg: 509.63555908203125 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.7088, loss_val: nan, pos_over_neg: 489.088623046875 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.709, loss_val: nan, pos_over_neg: 263.78741455078125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7207, loss_val: nan, pos_over_neg: 426.4969482421875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.7126, loss_val: nan, pos_over_neg: 1766.586669921875 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7086, loss_val: nan, pos_over_neg: 462.1112060546875 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1296.5589599609375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7115, loss_val: nan, pos_over_neg: 402.64013671875 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.6959, loss_val: nan, pos_over_neg: 530.8043823242188 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.7124, loss_val: nan, pos_over_neg: 711.2910766601562 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1509.0634765625 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7073, loss_val: nan, pos_over_neg: 626.671630859375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7197, loss_val: nan, pos_over_neg: 372.64385986328125 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1023.5354614257812 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7042, loss_val: nan, pos_over_neg: 596.4537353515625 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.696, loss_val: nan, pos_over_neg: 396.9399108886719 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7082, loss_val: nan, pos_over_neg: 1004.0164794921875 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.7034, loss_val: nan, pos_over_neg: 474.38555908203125 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7253, loss_val: nan, pos_over_neg: 592.951171875 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7, loss_val: nan, pos_over_neg: 875.0552368164062 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.7126, loss_val: nan, pos_over_neg: 1142.809326171875 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1129.59814453125 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.7087, loss_val: nan, pos_over_neg: 463.93109130859375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7002, loss_val: nan, pos_over_neg: 311.90155029296875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7039, loss_val: nan, pos_over_neg: 534.6918334960938 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1141.0360107421875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1095.886962890625 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7122, loss_val: nan, pos_over_neg: 469.194091796875 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.6991, loss_val: nan, pos_over_neg: 552.9301147460938 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.6986, loss_val: nan, pos_over_neg: 4295.4716796875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.6905, loss_val: nan, pos_over_neg: 2571.01318359375 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.7051, loss_val: nan, pos_over_neg: 565.69140625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.6918, loss_val: nan, pos_over_neg: 631.7407836914062 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.7099, loss_val: nan, pos_over_neg: 387.435546875 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7084, loss_val: nan, pos_over_neg: 1343.7781982421875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.6983, loss_val: nan, pos_over_neg: 541.1742553710938 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.704, loss_val: nan, pos_over_neg: 591.7958984375 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1724.755615234375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7016, loss_val: nan, pos_over_neg: 507.74652099609375 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1672.54248046875 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.6997, loss_val: nan, pos_over_neg: 30385.013671875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.6963, loss_val: nan, pos_over_neg: 594.272705078125 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.7122, loss_val: nan, pos_over_neg: 394.90032958984375 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7007, loss_val: nan, pos_over_neg: 725.565673828125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.6945, loss_val: nan, pos_over_neg: 2167.410400390625 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.697, loss_val: nan, pos_over_neg: 1180.2369384765625 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.6995, loss_val: nan, pos_over_neg: 8877.7490234375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7082, loss_val: nan, pos_over_neg: 1762.5272216796875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1244.7471923828125 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.6995, loss_val: nan, pos_over_neg: 748.0725708007812 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7084, loss_val: nan, pos_over_neg: 275.3643493652344 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7065, loss_val: nan, pos_over_neg: 1118.750244140625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.7057, loss_val: nan, pos_over_neg: 607.9346923828125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.6957, loss_val: nan, pos_over_neg: 602.113037109375 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.6988, loss_val: nan, pos_over_neg: 4589.578125 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.6869, loss_val: nan, pos_over_neg: 2180.00146484375 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1583.707763671875 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7019, loss_val: nan, pos_over_neg: 607.4296875 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.6853, loss_val: nan, pos_over_neg: 1473.0018310546875 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.6846, loss_val: nan, pos_over_neg: 2648.092529296875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7043, loss_val: nan, pos_over_neg: 470.0244445800781 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.6818, loss_val: nan, pos_over_neg: 937.1571044921875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.6949, loss_val: nan, pos_over_neg: 2083.19384765625 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.6844, loss_val: nan, pos_over_neg: 1184.755615234375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7025, loss_val: nan, pos_over_neg: 596.7007446289062 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.6996, loss_val: nan, pos_over_neg: 383.7035217285156 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.6979, loss_val: nan, pos_over_neg: 468.56304931640625 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.6964, loss_val: nan, pos_over_neg: 473.0917663574219 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.6935, loss_val: nan, pos_over_neg: 593.34130859375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7004, loss_val: nan, pos_over_neg: 583.5435180664062 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.6922, loss_val: nan, pos_over_neg: 711.732177734375 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.7078, loss_val: nan, pos_over_neg: 506.791259765625 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.6909, loss_val: nan, pos_over_neg: 906.9627685546875 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.687, loss_val: nan, pos_over_neg: 2192.74609375 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.7043, loss_val: nan, pos_over_neg: 590.326904296875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.6992, loss_val: nan, pos_over_neg: 641.366455078125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.6866, loss_val: nan, pos_over_neg: 1957.15576171875 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.6889, loss_val: nan, pos_over_neg: 270.48809814453125 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.6945, loss_val: nan, pos_over_neg: 263.9834289550781 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.6784, loss_val: nan, pos_over_neg: 814.6396484375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7006, loss_val: nan, pos_over_neg: 453.6277770996094 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.699, loss_val: nan, pos_over_neg: 284.15509033203125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7003, loss_val: nan, pos_over_neg: 581.0400390625 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.6941, loss_val: nan, pos_over_neg: 700.3143920898438 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.6965, loss_val: nan, pos_over_neg: 436.7662658691406 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7054, loss_val: nan, pos_over_neg: 358.6233825683594 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.6905, loss_val: nan, pos_over_neg: 457.9362487792969 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7021, loss_val: nan, pos_over_neg: 235.77230834960938 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7004, loss_val: nan, pos_over_neg: 401.3380432128906 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.6967, loss_val: nan, pos_over_neg: 928.9376220703125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.6816, loss_val: nan, pos_over_neg: 1085.8109130859375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.6881, loss_val: nan, pos_over_neg: 503.2202453613281 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7003, loss_val: nan, pos_over_neg: 503.783203125 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.6932, loss_val: nan, pos_over_neg: 5639.99462890625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.6877, loss_val: nan, pos_over_neg: 650.107421875 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.6863, loss_val: nan, pos_over_neg: 496.599853515625 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.6934, loss_val: nan, pos_over_neg: 419.4753723144531 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.6975, loss_val: nan, pos_over_neg: 917.56689453125 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.6868, loss_val: nan, pos_over_neg: 958.8253784179688 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.6914, loss_val: nan, pos_over_neg: 923.6404418945312 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.6838, loss_val: nan, pos_over_neg: 505.1504821777344 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.6732, loss_val: nan, pos_over_neg: 2092.5546875 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.6949, loss_val: nan, pos_over_neg: 555.796875 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.6978, loss_val: nan, pos_over_neg: 527.9979248046875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.6831, loss_val: nan, pos_over_neg: 528.5537109375 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.6877, loss_val: nan, pos_over_neg: 390.8191223144531 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.6894, loss_val: nan, pos_over_neg: 556.1755981445312 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.6842, loss_val: nan, pos_over_neg: 418.24639892578125 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.6656, loss_val: nan, pos_over_neg: 924.127685546875 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.6991, loss_val: nan, pos_over_neg: 590.3086547851562 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.6883, loss_val: nan, pos_over_neg: 5182.64111328125 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.6869, loss_val: nan, pos_over_neg: 909.1759643554688 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.6915, loss_val: nan, pos_over_neg: 680.9579467773438 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.6852, loss_val: nan, pos_over_neg: 815.8421020507812 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.6932, loss_val: nan, pos_over_neg: 386.51220703125 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.674, loss_val: nan, pos_over_neg: 432.2128601074219 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.68, loss_val: nan, pos_over_neg: 573.241455078125 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.682, loss_val: nan, pos_over_neg: 1155.9425048828125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.6815, loss_val: nan, pos_over_neg: 1777.5750732421875 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.6764, loss_val: nan, pos_over_neg: 513.9968872070312 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.6922, loss_val: nan, pos_over_neg: 691.5914306640625 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.6958, loss_val: nan, pos_over_neg: 827.400390625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.697, loss_val: nan, pos_over_neg: 425.0632019042969 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.6979, loss_val: nan, pos_over_neg: 255.7156219482422 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.6784, loss_val: nan, pos_over_neg: 574.1370849609375 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.6789, loss_val: nan, pos_over_neg: 745.5687255859375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.6829, loss_val: nan, pos_over_neg: 307.4411926269531 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.6807, loss_val: nan, pos_over_neg: 439.9731750488281 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.6852, loss_val: nan, pos_over_neg: 293.5269470214844 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.6692, loss_val: nan, pos_over_neg: 1969.116943359375 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.68, loss_val: nan, pos_over_neg: 578.0689697265625 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.687, loss_val: nan, pos_over_neg: 568.1376953125 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.6979, loss_val: nan, pos_over_neg: 359.6657409667969 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.6773, loss_val: nan, pos_over_neg: 1228.5548095703125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.6818, loss_val: nan, pos_over_neg: 4999.34521484375 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.6738, loss_val: nan, pos_over_neg: 1625.2469482421875 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.6761, loss_val: nan, pos_over_neg: 373.6577453613281 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.6767, loss_val: nan, pos_over_neg: 613.9653930664062 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.6868, loss_val: nan, pos_over_neg: 675.0491943359375 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.6844, loss_val: nan, pos_over_neg: 2370.603759765625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.6835, loss_val: nan, pos_over_neg: 1215.0877685546875 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.6889, loss_val: nan, pos_over_neg: 867.4149169921875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.6871, loss_val: nan, pos_over_neg: 1132.6707763671875 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.6806, loss_val: nan, pos_over_neg: 498.4694519042969 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.6819, loss_val: nan, pos_over_neg: 1701.531005859375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.6831, loss_val: nan, pos_over_neg: 629.4862670898438 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.6806, loss_val: nan, pos_over_neg: 340.0316162109375 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.6807, loss_val: nan, pos_over_neg: 1031.3671875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.6879, loss_val: nan, pos_over_neg: 1390.6324462890625 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.691, loss_val: nan, pos_over_neg: 523.7779541015625 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.6866, loss_val: nan, pos_over_neg: 727.880615234375 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.6741, loss_val: nan, pos_over_neg: 5573.5927734375 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.6765, loss_val: nan, pos_over_neg: 1502.318359375 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.6752, loss_val: nan, pos_over_neg: 1046.7701416015625 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.6702, loss_val: nan, pos_over_neg: 131086.875 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.6883, loss_val: nan, pos_over_neg: 434.5584716796875 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.6794, loss_val: nan, pos_over_neg: 2017.5303955078125 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.6767, loss_val: nan, pos_over_neg: 640.125 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.6878, loss_val: nan, pos_over_neg: 2591.067138671875 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.6941, loss_val: nan, pos_over_neg: 2014.7081298828125 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1111.922119140625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.6754, loss_val: nan, pos_over_neg: 1115.8919677734375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.681, loss_val: nan, pos_over_neg: 686.2711791992188 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.6897, loss_val: nan, pos_over_neg: 563.23095703125 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.6704, loss_val: nan, pos_over_neg: 1787.2095947265625 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.6865, loss_val: nan, pos_over_neg: 788.054443359375 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.6804, loss_val: nan, pos_over_neg: 535.9326782226562 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.6772, loss_val: nan, pos_over_neg: 733.822265625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.6735, loss_val: nan, pos_over_neg: 3124.304443359375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1275.19287109375 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.6984, loss_val: nan, pos_over_neg: 597.134033203125 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.6761, loss_val: nan, pos_over_neg: 722.0657348632812 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.6731, loss_val: nan, pos_over_neg: 1725.0745849609375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.6797, loss_val: nan, pos_over_neg: 582.00341796875 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.6803, loss_val: nan, pos_over_neg: 1317.9791259765625 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.673, loss_val: nan, pos_over_neg: 683.6522827148438 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.6855, loss_val: nan, pos_over_neg: 3701.19677734375 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.6835, loss_val: nan, pos_over_neg: 627.7420043945312 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.6755, loss_val: nan, pos_over_neg: 1439.64697265625 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.6583, loss_val: nan, pos_over_neg: 2599.6474609375 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.6756, loss_val: nan, pos_over_neg: 1295.4652099609375 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.682, loss_val: nan, pos_over_neg: 329.54327392578125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.6663, loss_val: nan, pos_over_neg: 1736.5084228515625 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.6761, loss_val: nan, pos_over_neg: 2270.0537109375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.6861, loss_val: nan, pos_over_neg: 580.7030639648438 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.688, loss_val: nan, pos_over_neg: 895.6817016601562 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.6698, loss_val: nan, pos_over_neg: 1782.0377197265625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.6652, loss_val: nan, pos_over_neg: 2841.172119140625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.6878, loss_val: nan, pos_over_neg: 742.154296875 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.677, loss_val: nan, pos_over_neg: 1059.46435546875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.6751, loss_val: nan, pos_over_neg: 1807.20751953125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.6743, loss_val: nan, pos_over_neg: 603.7905883789062 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.6783, loss_val: nan, pos_over_neg: 664.5919189453125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.6742, loss_val: nan, pos_over_neg: 998.03076171875 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.6806, loss_val: nan, pos_over_neg: 827.041259765625 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.6654, loss_val: nan, pos_over_neg: 8598.021484375 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.6688, loss_val: nan, pos_over_neg: 1214.0911865234375 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.6839, loss_val: nan, pos_over_neg: 549.6812133789062 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.696, loss_val: nan, pos_over_neg: 978.6973266601562 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.6728, loss_val: nan, pos_over_neg: 1041.0919189453125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.6757, loss_val: nan, pos_over_neg: 424.5945129394531 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.6689, loss_val: nan, pos_over_neg: 779.5328979492188 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.6732, loss_val: nan, pos_over_neg: 682.4164428710938 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.6681, loss_val: nan, pos_over_neg: 908.9738159179688 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.6903, loss_val: nan, pos_over_neg: 827.2609252929688 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.6803, loss_val: nan, pos_over_neg: 851.587646484375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.6735, loss_val: nan, pos_over_neg: 947.1522827148438 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.6612, loss_val: nan, pos_over_neg: 1749.912109375 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.6799, loss_val: nan, pos_over_neg: 374.65087890625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.6696, loss_val: nan, pos_over_neg: 928.6107788085938 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.674, loss_val: nan, pos_over_neg: 3044.140380859375 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.6781, loss_val: nan, pos_over_neg: 1769.3992919921875 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.6671, loss_val: nan, pos_over_neg: 10536.484375 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.6817, loss_val: nan, pos_over_neg: 924.87548828125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.6642, loss_val: nan, pos_over_neg: 790.3319702148438 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.668, loss_val: nan, pos_over_neg: 1262.871337890625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.6687, loss_val: nan, pos_over_neg: 619.2615356445312 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.6806, loss_val: nan, pos_over_neg: 1695.6534423828125 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.671, loss_val: nan, pos_over_neg: 3240.732177734375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.6642, loss_val: nan, pos_over_neg: 1096.472900390625 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.6638, loss_val: nan, pos_over_neg: 2475.567138671875 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.6776, loss_val: nan, pos_over_neg: 568.9646606445312 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.6707, loss_val: nan, pos_over_neg: 444.45556640625 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.6665, loss_val: nan, pos_over_neg: 1349.736083984375 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.6554, loss_val: nan, pos_over_neg: 2098.593994140625 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.6792, loss_val: nan, pos_over_neg: 1404.4312744140625 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.6803, loss_val: nan, pos_over_neg: 1020.5990600585938 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.6632, loss_val: nan, pos_over_neg: 869.8843383789062 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.6665, loss_val: nan, pos_over_neg: 3875.705322265625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.6755, loss_val: nan, pos_over_neg: 907.5659790039062 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.6627, loss_val: nan, pos_over_neg: 503.5311584472656 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.6715, loss_val: nan, pos_over_neg: 713.1887817382812 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.679, loss_val: nan, pos_over_neg: 977.5358276367188 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.6691, loss_val: nan, pos_over_neg: 588.890380859375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.6722, loss_val: nan, pos_over_neg: 7667.50830078125 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.6804, loss_val: nan, pos_over_neg: 884.96533203125 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.6682, loss_val: nan, pos_over_neg: 489.09295654296875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.6725, loss_val: nan, pos_over_neg: 505.5605773925781 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.6756, loss_val: nan, pos_over_neg: 697.6220703125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.6896, loss_val: nan, pos_over_neg: 748.1408081054688 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.6585, loss_val: nan, pos_over_neg: 549.25048828125 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.6713, loss_val: nan, pos_over_neg: 1031.14111328125 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.6689, loss_val: nan, pos_over_neg: 872.3756103515625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.6754, loss_val: nan, pos_over_neg: 786.906494140625 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.6752, loss_val: nan, pos_over_neg: 701.0320434570312 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.6672, loss_val: nan, pos_over_neg: 518.9160766601562 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.6659, loss_val: nan, pos_over_neg: 651.0394287109375 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.6696, loss_val: nan, pos_over_neg: 6628.048828125 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.6571, loss_val: nan, pos_over_neg: 990.8352661132812 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.6696, loss_val: nan, pos_over_neg: 590.69287109375 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.6601, loss_val: nan, pos_over_neg: 566.4134521484375 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.6677, loss_val: nan, pos_over_neg: 414.37689208984375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.6656, loss_val: nan, pos_over_neg: 914.7318115234375 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.6616, loss_val: nan, pos_over_neg: 479.4557800292969 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.6649, loss_val: nan, pos_over_neg: -72894.9140625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.6681, loss_val: nan, pos_over_neg: 745.0709838867188 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.6693, loss_val: nan, pos_over_neg: 761.762451171875 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.6762, loss_val: nan, pos_over_neg: 682.4417114257812 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.6709, loss_val: nan, pos_over_neg: 2811.90283203125 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.6671, loss_val: nan, pos_over_neg: 2673.50244140625 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.6661, loss_val: nan, pos_over_neg: 669.73046875 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.6665, loss_val: nan, pos_over_neg: 802.4369506835938 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.671, loss_val: nan, pos_over_neg: 556.06982421875 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.6633, loss_val: nan, pos_over_neg: 527.1639404296875 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.6763, loss_val: nan, pos_over_neg: 1308.2850341796875 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.6781, loss_val: nan, pos_over_neg: 413.2215270996094 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.66, loss_val: nan, pos_over_neg: 597.2542724609375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.6676, loss_val: nan, pos_over_neg: 582.798828125 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.6625, loss_val: nan, pos_over_neg: 1264.43701171875 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.6672, loss_val: nan, pos_over_neg: 939.107666015625 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.6601, loss_val: nan, pos_over_neg: 1549.109375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.6692, loss_val: nan, pos_over_neg: 817.1765747070312 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.6659, loss_val: nan, pos_over_neg: 734.5960693359375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.661, loss_val: nan, pos_over_neg: 583.8779296875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.6607, loss_val: nan, pos_over_neg: 473.4911804199219 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.6734, loss_val: nan, pos_over_neg: 454.4364929199219 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.6727, loss_val: nan, pos_over_neg: 858.7201538085938 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.6776, loss_val: nan, pos_over_neg: 538.6572265625 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.6776, loss_val: nan, pos_over_neg: 439.7542724609375 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.6544, loss_val: nan, pos_over_neg: 995.7623291015625 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.6624, loss_val: nan, pos_over_neg: 1518.493408203125 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.6742, loss_val: nan, pos_over_neg: 343.2113037109375 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.6778, loss_val: nan, pos_over_neg: 434.9922790527344 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.6743, loss_val: nan, pos_over_neg: 1907.86279296875 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.6671, loss_val: nan, pos_over_neg: 1251.8526611328125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.6734, loss_val: nan, pos_over_neg: 342.0252380371094 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.6632, loss_val: nan, pos_over_neg: 511.3385925292969 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.6671, loss_val: nan, pos_over_neg: 1450.18359375 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.6589, loss_val: nan, pos_over_neg: 690.105224609375 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.6695, loss_val: nan, pos_over_neg: 533.5558471679688 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.6614, loss_val: nan, pos_over_neg: 1131.92822265625 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.6677, loss_val: nan, pos_over_neg: 1516.1016845703125 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.6619, loss_val: nan, pos_over_neg: 1891.490478515625 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.664, loss_val: nan, pos_over_neg: 2095.5419921875 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.6769, loss_val: nan, pos_over_neg: 932.12255859375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.6592, loss_val: nan, pos_over_neg: 1384.08349609375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.6632, loss_val: nan, pos_over_neg: 951.4544677734375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.6617, loss_val: nan, pos_over_neg: 422.394287109375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.6683, loss_val: nan, pos_over_neg: 494.16217041015625 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.6694, loss_val: nan, pos_over_neg: 786.814453125 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.6625, loss_val: nan, pos_over_neg: 1671.62744140625 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.6538, loss_val: nan, pos_over_neg: 511.5753479003906 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.6556, loss_val: nan, pos_over_neg: 933.8928833007812 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.6767, loss_val: nan, pos_over_neg: 1024.51123046875 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.6684, loss_val: nan, pos_over_neg: 596.325439453125 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.6621, loss_val: nan, pos_over_neg: 1031.898681640625 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.6665, loss_val: nan, pos_over_neg: 685.9998779296875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.6548, loss_val: nan, pos_over_neg: 736.16845703125 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.6626, loss_val: nan, pos_over_neg: 803.569091796875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.6732, loss_val: nan, pos_over_neg: 570.2564697265625 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.6686, loss_val: nan, pos_over_neg: 493.8278503417969 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.6742, loss_val: nan, pos_over_neg: 497.00189208984375 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.6675, loss_val: nan, pos_over_neg: 377.2472229003906 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.6691, loss_val: nan, pos_over_neg: 463.3353576660156 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.6545, loss_val: nan, pos_over_neg: 974.5845947265625 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.6663, loss_val: nan, pos_over_neg: 799.8858642578125 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.668, loss_val: nan, pos_over_neg: 1217.8336181640625 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.6616, loss_val: nan, pos_over_neg: 1193.3460693359375 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.6539, loss_val: nan, pos_over_neg: 585.0053100585938 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.6752, loss_val: nan, pos_over_neg: 1147.918212890625 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.6576, loss_val: nan, pos_over_neg: 574.2796630859375 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.6637, loss_val: nan, pos_over_neg: 1054.1402587890625 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.6626, loss_val: nan, pos_over_neg: 1047.52294921875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.6584, loss_val: nan, pos_over_neg: 1061.556640625 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.6608, loss_val: nan, pos_over_neg: 594.8040161132812 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.6778, loss_val: nan, pos_over_neg: 574.3298950195312 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.6488, loss_val: nan, pos_over_neg: 3124.95458984375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.6755, loss_val: nan, pos_over_neg: 333.1360778808594 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.6603, loss_val: nan, pos_over_neg: 511.6055603027344 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.6649, loss_val: nan, pos_over_neg: 541.3695068359375 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.6736, loss_val: nan, pos_over_neg: 590.6812744140625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.6651, loss_val: nan, pos_over_neg: 1848.6446533203125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.6717, loss_val: nan, pos_over_neg: 568.4029541015625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.6623, loss_val: nan, pos_over_neg: 1746.973388671875 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.656, loss_val: nan, pos_over_neg: 1841.7369384765625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.6593, loss_val: nan, pos_over_neg: 1228.2635498046875 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.665, loss_val: nan, pos_over_neg: 419.0223693847656 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.6568, loss_val: nan, pos_over_neg: 1254.1802978515625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.659, loss_val: nan, pos_over_neg: 1298.0751953125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.6675, loss_val: nan, pos_over_neg: 1391.157470703125 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.659, loss_val: nan, pos_over_neg: 820.1375732421875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.6545, loss_val: nan, pos_over_neg: 668.3045654296875 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.6528, loss_val: nan, pos_over_neg: 423.7813720703125 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.6612, loss_val: nan, pos_over_neg: 718.147705078125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.6659, loss_val: nan, pos_over_neg: 370.598876953125 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.6531, loss_val: nan, pos_over_neg: 656.0325927734375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.6641, loss_val: nan, pos_over_neg: 3972.373046875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.6699, loss_val: nan, pos_over_neg: 846.0616455078125 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.6601, loss_val: nan, pos_over_neg: 398.7135925292969 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.66, loss_val: nan, pos_over_neg: 790.669189453125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.6643, loss_val: nan, pos_over_neg: 7968.48876953125 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.6716, loss_val: nan, pos_over_neg: 452.2430725097656 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.6648, loss_val: nan, pos_over_neg: 345.27044677734375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.6578, loss_val: nan, pos_over_neg: 423.4248352050781 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.653, loss_val: nan, pos_over_neg: 411.0808410644531 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.6649, loss_val: nan, pos_over_neg: 1337.214111328125 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.6551, loss_val: nan, pos_over_neg: 757.7018432617188 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.6518, loss_val: nan, pos_over_neg: 439.1001892089844 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.6562, loss_val: nan, pos_over_neg: 968.3016967773438 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.6636, loss_val: nan, pos_over_neg: 2367.83984375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.6646, loss_val: nan, pos_over_neg: 529.3809814453125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.665, loss_val: nan, pos_over_neg: 1661.067626953125 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.658, loss_val: nan, pos_over_neg: 538.9882202148438 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.6473, loss_val: nan, pos_over_neg: 615.2860717773438 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.6501, loss_val: nan, pos_over_neg: -45968.296875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.6622, loss_val: nan, pos_over_neg: 843.5949096679688 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.6553, loss_val: nan, pos_over_neg: 461.6141357421875 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.6537, loss_val: nan, pos_over_neg: 1265.6854248046875 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.6505, loss_val: nan, pos_over_neg: 1294.7249755859375 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.6616, loss_val: nan, pos_over_neg: 445.3731689453125 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.667, loss_val: nan, pos_over_neg: 609.467529296875 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.6549, loss_val: nan, pos_over_neg: 386.7532653808594 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.6553, loss_val: nan, pos_over_neg: 471.5757751464844 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.6563, loss_val: nan, pos_over_neg: 411.4943542480469 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.6638, loss_val: nan, pos_over_neg: 909.2666015625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.6548, loss_val: nan, pos_over_neg: 616.2342529296875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.6615, loss_val: nan, pos_over_neg: 911.285400390625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.6566, loss_val: nan, pos_over_neg: 418.59649658203125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.6512, loss_val: nan, pos_over_neg: 1413.3131103515625 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.6602, loss_val: nan, pos_over_neg: 600.096923828125 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.6617, loss_val: nan, pos_over_neg: 282.63812255859375 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.6568, loss_val: nan, pos_over_neg: 503.5696716308594 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.6513, loss_val: nan, pos_over_neg: 757.1390380859375 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.655, loss_val: nan, pos_over_neg: 855.3756103515625 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.6509, loss_val: nan, pos_over_neg: 1127.3070068359375 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.6541, loss_val: nan, pos_over_neg: 719.8496704101562 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.6588, loss_val: nan, pos_over_neg: 714.1581420898438 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.6597, loss_val: nan, pos_over_neg: 858.7571411132812 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.666, loss_val: nan, pos_over_neg: 482.9621887207031 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.6725, loss_val: nan, pos_over_neg: 481.5760498046875 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.6493, loss_val: nan, pos_over_neg: 384.21539306640625 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.6667, loss_val: nan, pos_over_neg: 407.91259765625 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.6526, loss_val: nan, pos_over_neg: 876.934326171875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.6641, loss_val: nan, pos_over_neg: 468.2299499511719 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.6597, loss_val: nan, pos_over_neg: 639.9947509765625 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.6526, loss_val: nan, pos_over_neg: 1141.972412109375 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.6636, loss_val: nan, pos_over_neg: 491.18310546875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.6675, loss_val: nan, pos_over_neg: 533.2275390625 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.6632, loss_val: nan, pos_over_neg: 426.13641357421875 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.6427, loss_val: nan, pos_over_neg: 618.3015747070312 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.6583, loss_val: nan, pos_over_neg: 568.9711303710938 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.6639, loss_val: nan, pos_over_neg: 1195.9522705078125 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.6625, loss_val: nan, pos_over_neg: 821.5418090820312 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.6495, loss_val: nan, pos_over_neg: 705.1655883789062 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.6486, loss_val: nan, pos_over_neg: 997.3168334960938 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.656, loss_val: nan, pos_over_neg: 1723.5885009765625 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.6503, loss_val: nan, pos_over_neg: 878.5119018554688 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.6498, loss_val: nan, pos_over_neg: 1001.59130859375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.6719, loss_val: nan, pos_over_neg: 313.94866943359375 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.6571, loss_val: nan, pos_over_neg: 506.87322998046875 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.6455, loss_val: nan, pos_over_neg: 2737.529296875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.6646, loss_val: nan, pos_over_neg: 539.7184448242188 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.6561, loss_val: nan, pos_over_neg: 622.3983154296875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.6571, loss_val: nan, pos_over_neg: 421.1098937988281 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.669, loss_val: nan, pos_over_neg: 452.61474609375 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.6445, loss_val: nan, pos_over_neg: -3789.333251953125 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.6556, loss_val: nan, pos_over_neg: 567.906005859375 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.6514, loss_val: nan, pos_over_neg: 447.93463134765625 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.6511, loss_val: nan, pos_over_neg: 394.58123779296875 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.6551, loss_val: nan, pos_over_neg: 1609.203857421875 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.6497, loss_val: nan, pos_over_neg: 759.9659423828125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.661, loss_val: nan, pos_over_neg: 307.5673522949219 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.646, loss_val: nan, pos_over_neg: 497.7040100097656 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.6524, loss_val: nan, pos_over_neg: 1858.989013671875 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.6419, loss_val: nan, pos_over_neg: 5010.10205078125 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.6645, loss_val: nan, pos_over_neg: 592.4672241210938 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.6446, loss_val: nan, pos_over_neg: 979.5810546875 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.6583, loss_val: nan, pos_over_neg: 928.2109375 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.651, loss_val: nan, pos_over_neg: 833.826416015625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.6428, loss_val: nan, pos_over_neg: 828.6251220703125 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.6474, loss_val: nan, pos_over_neg: 877.82275390625 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.6505, loss_val: nan, pos_over_neg: 991.1022338867188 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.6557, loss_val: nan, pos_over_neg: 982.696044921875 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.647, loss_val: nan, pos_over_neg: 4258.65478515625 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.6427, loss_val: nan, pos_over_neg: 949.6759033203125 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.6625, loss_val: nan, pos_over_neg: 358.7720031738281 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.6574, loss_val: nan, pos_over_neg: 676.7351684570312 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.6605, loss_val: nan, pos_over_neg: 1297.613525390625 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.6481, loss_val: nan, pos_over_neg: 890.55029296875 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.647, loss_val: nan, pos_over_neg: 926.6613159179688 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.6679, loss_val: nan, pos_over_neg: 731.9151000976562 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.651, loss_val: nan, pos_over_neg: 871.26220703125 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.6513, loss_val: nan, pos_over_neg: 1282.3397216796875 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.6587, loss_val: nan, pos_over_neg: 639.2155151367188 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.6599, loss_val: nan, pos_over_neg: 490.29266357421875 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.6422, loss_val: nan, pos_over_neg: 1876.529296875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.6634, loss_val: nan, pos_over_neg: 711.792236328125 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.6505, loss_val: nan, pos_over_neg: 480.6962890625 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.6488, loss_val: nan, pos_over_neg: 565.53369140625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.6485, loss_val: nan, pos_over_neg: 1670.6014404296875 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.6536, loss_val: nan, pos_over_neg: 642.9589233398438 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.6475, loss_val: nan, pos_over_neg: 382.5798645019531 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.6392, loss_val: nan, pos_over_neg: 4867.2021484375 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.6486, loss_val: nan, pos_over_neg: 3522.348876953125 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.6464, loss_val: nan, pos_over_neg: 3420.431396484375 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.6529, loss_val: nan, pos_over_neg: 1963.8232421875 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.6451, loss_val: nan, pos_over_neg: 1270.50244140625 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.6586, loss_val: nan, pos_over_neg: 895.6670532226562 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.6489, loss_val: nan, pos_over_neg: 2571.27978515625 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.6553, loss_val: nan, pos_over_neg: 572.5531005859375 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.6563, loss_val: nan, pos_over_neg: 537.828125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.6592, loss_val: nan, pos_over_neg: 465.9076232910156 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.6571, loss_val: nan, pos_over_neg: 742.6650390625 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.6584, loss_val: nan, pos_over_neg: 794.886962890625 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.6701, loss_val: nan, pos_over_neg: 844.9498901367188 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.6564, loss_val: nan, pos_over_neg: 599.807373046875 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.6478, loss_val: nan, pos_over_neg: 6104.64794921875 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.6537, loss_val: nan, pos_over_neg: 962.091064453125 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.6469, loss_val: nan, pos_over_neg: 657.5126342773438 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.6404, loss_val: nan, pos_over_neg: 1409.4832763671875 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.6506, loss_val: nan, pos_over_neg: 2254.065185546875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.6562, loss_val: nan, pos_over_neg: 427.0272521972656 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.6469, loss_val: nan, pos_over_neg: 579.0537719726562 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.6436, loss_val: nan, pos_over_neg: 379.47406005859375 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.6486, loss_val: nan, pos_over_neg: 984.10302734375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.6362, loss_val: nan, pos_over_neg: 809.5416870117188 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.6455, loss_val: nan, pos_over_neg: 850.3189086914062 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.6473, loss_val: nan, pos_over_neg: 773.6583862304688 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.658, loss_val: nan, pos_over_neg: 899.096435546875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.6466, loss_val: nan, pos_over_neg: 2419.173583984375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.6578, loss_val: nan, pos_over_neg: 688.155029296875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.6342, loss_val: nan, pos_over_neg: -19915.15625 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.6495, loss_val: nan, pos_over_neg: 677.685546875 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.66, loss_val: nan, pos_over_neg: 369.8232421875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.6457, loss_val: nan, pos_over_neg: 1194.1981201171875 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.6493, loss_val: nan, pos_over_neg: 2186.880615234375 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.6367, loss_val: nan, pos_over_neg: 1159.5584716796875 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.6446, loss_val: nan, pos_over_neg: 801.7667846679688 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.6492, loss_val: nan, pos_over_neg: 568.0203857421875 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.6432, loss_val: nan, pos_over_neg: 575.6425170898438 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.6463, loss_val: nan, pos_over_neg: 8827.8564453125 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.6486, loss_val: nan, pos_over_neg: 674.3551635742188 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.6503, loss_val: nan, pos_over_neg: 369.4943542480469 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.6521, loss_val: nan, pos_over_neg: 449.04010009765625 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.6534, loss_val: nan, pos_over_neg: 761.577880859375 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.6475, loss_val: nan, pos_over_neg: 608.5081787109375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.6521, loss_val: nan, pos_over_neg: 429.3903503417969 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.6509, loss_val: nan, pos_over_neg: 852.0665283203125 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.6544, loss_val: nan, pos_over_neg: 603.8151245117188 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.6422, loss_val: nan, pos_over_neg: 1162.785888671875 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.6463, loss_val: nan, pos_over_neg: -62646.1640625 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.6556, loss_val: nan, pos_over_neg: 311.3584899902344 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.6354, loss_val: nan, pos_over_neg: 865.2379150390625 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.6501, loss_val: nan, pos_over_neg: 542.288818359375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.6517, loss_val: nan, pos_over_neg: 672.1221923828125 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.6442, loss_val: nan, pos_over_neg: 1458.2320556640625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.6438, loss_val: nan, pos_over_neg: 2232.685302734375 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.6337, loss_val: nan, pos_over_neg: 493.3099365234375 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.6488, loss_val: nan, pos_over_neg: 863.746826171875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.6499, loss_val: nan, pos_over_neg: 988.5103759765625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.6423, loss_val: nan, pos_over_neg: 383.6317138671875 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.646, loss_val: nan, pos_over_neg: 402.3556823730469 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.6566, loss_val: nan, pos_over_neg: 423.1148986816406 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.6483, loss_val: nan, pos_over_neg: 496.01641845703125 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.6475, loss_val: nan, pos_over_neg: 2000.759521484375 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.6449, loss_val: nan, pos_over_neg: 9260.8359375 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.6594, loss_val: nan, pos_over_neg: 1389.2760009765625 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.6435, loss_val: nan, pos_over_neg: 611.93505859375 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.6482, loss_val: nan, pos_over_neg: 1827.3973388671875 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.6331, loss_val: nan, pos_over_neg: 3568.31640625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.6461, loss_val: nan, pos_over_neg: 740.5406494140625 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.6393, loss_val: nan, pos_over_neg: 1655.857177734375 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.63, loss_val: nan, pos_over_neg: 724.4833984375 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.6439, loss_val: nan, pos_over_neg: 573.4011840820312 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.6457, loss_val: nan, pos_over_neg: 995.1722412109375 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.6509, loss_val: nan, pos_over_neg: 648.172119140625 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.6441, loss_val: nan, pos_over_neg: 899.0829467773438 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.6429, loss_val: nan, pos_over_neg: 799.9176635742188 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.6335, loss_val: nan, pos_over_neg: 916.5488891601562 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.6663, loss_val: nan, pos_over_neg: 310.29290771484375 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.647, loss_val: nan, pos_over_neg: 701.4261474609375 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.6464, loss_val: nan, pos_over_neg: 511.6698303222656 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.6494, loss_val: nan, pos_over_neg: 692.2650146484375 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.6496, loss_val: nan, pos_over_neg: 522.3507080078125 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.6442, loss_val: nan, pos_over_neg: 644.4362182617188 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.6466, loss_val: nan, pos_over_neg: 738.1001586914062 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.648, loss_val: nan, pos_over_neg: 668.0697631835938 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.6419, loss_val: nan, pos_over_neg: 5429.84033203125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.6332, loss_val: nan, pos_over_neg: 3023.89208984375 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.6469, loss_val: nan, pos_over_neg: 250.5546112060547 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.651, loss_val: nan, pos_over_neg: 479.08575439453125 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.6403, loss_val: nan, pos_over_neg: 1286.658203125 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.6508, loss_val: nan, pos_over_neg: 366.50946044921875 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.6466, loss_val: nan, pos_over_neg: 511.3894348144531 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.6539, loss_val: nan, pos_over_neg: 622.32421875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [37:05<185449:35:19, 2225.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/889, loss_train: 5.6426, loss_val: nan, pos_over_neg: 580.55322265625 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.6384, loss_val: nan, pos_over_neg: 1160.01220703125 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.6497, loss_val: nan, pos_over_neg: 670.5980834960938 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.6483, loss_val: nan, pos_over_neg: 381.47003173828125 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.6506, loss_val: nan, pos_over_neg: 576.7413330078125 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.6593, loss_val: nan, pos_over_neg: 684.2301635742188 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.6331, loss_val: nan, pos_over_neg: 1103.7979736328125 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.6461, loss_val: nan, pos_over_neg: 476.3114318847656 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.6518, loss_val: nan, pos_over_neg: 417.4671630859375 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.6537, loss_val: nan, pos_over_neg: 825.7745361328125 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.6432, loss_val: nan, pos_over_neg: 76415.6796875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.6469, loss_val: nan, pos_over_neg: 705.6328125 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.6383, loss_val: nan, pos_over_neg: 936.7738647460938 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.6501, loss_val: nan, pos_over_neg: 500.54229736328125 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.645, loss_val: nan, pos_over_neg: 488.0703125 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.6511, loss_val: nan, pos_over_neg: 1914.47509765625 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.6516, loss_val: nan, pos_over_neg: 708.37353515625 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.6354, loss_val: nan, pos_over_neg: 1405.9945068359375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.6394, loss_val: nan, pos_over_neg: 1372.2264404296875 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.645, loss_val: nan, pos_over_neg: 3134.945556640625 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.652, loss_val: nan, pos_over_neg: 871.56201171875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.6485, loss_val: nan, pos_over_neg: 630.5017700195312 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.6427, loss_val: nan, pos_over_neg: 697.2425537109375 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.6475, loss_val: nan, pos_over_neg: 1219.32861328125 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.6534, loss_val: nan, pos_over_neg: 921.8825073242188 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.6373, loss_val: nan, pos_over_neg: 713.2190551757812 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.6512, loss_val: nan, pos_over_neg: 1805.8658447265625 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.6417, loss_val: nan, pos_over_neg: 918.3600463867188 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.6522, loss_val: nan, pos_over_neg: 517.0955200195312 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.6539, loss_val: nan, pos_over_neg: 621.0191650390625 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.6441, loss_val: nan, pos_over_neg: 6014.0712890625 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.6447, loss_val: nan, pos_over_neg: 1117.09814453125 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.653, loss_val: nan, pos_over_neg: 1418.6923828125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.648, loss_val: nan, pos_over_neg: 2068.21044921875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.6394, loss_val: nan, pos_over_neg: 20536.951171875 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.6515, loss_val: nan, pos_over_neg: 4608.771484375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.6436, loss_val: nan, pos_over_neg: 2230.7470703125 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.643, loss_val: nan, pos_over_neg: 2224.8125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.6302, loss_val: nan, pos_over_neg: 1307.1392822265625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.6375, loss_val: nan, pos_over_neg: 1202.4967041015625 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.629, loss_val: nan, pos_over_neg: 2248.25341796875 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.6386, loss_val: nan, pos_over_neg: 2581.404541015625 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.6356, loss_val: nan, pos_over_neg: 1132.095458984375 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.6538, loss_val: nan, pos_over_neg: 583.5780029296875 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.6439, loss_val: nan, pos_over_neg: 1132.306396484375 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.6429, loss_val: nan, pos_over_neg: 1860.1036376953125 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.6401, loss_val: nan, pos_over_neg: 2301.333740234375 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.6412, loss_val: nan, pos_over_neg: -27913.576171875 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.6337, loss_val: nan, pos_over_neg: 1201.931884765625 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.6416, loss_val: nan, pos_over_neg: 1179.2384033203125 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.6346, loss_val: nan, pos_over_neg: 4801.06884765625 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.6393, loss_val: nan, pos_over_neg: 1771.9259033203125 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.6447, loss_val: nan, pos_over_neg: 405.0830383300781 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.6506, loss_val: nan, pos_over_neg: 375.1271667480469 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.6578, loss_val: nan, pos_over_neg: 807.0767822265625 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.6424, loss_val: nan, pos_over_neg: 2649.329345703125 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.6333, loss_val: nan, pos_over_neg: 6620.86083984375 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.6304, loss_val: nan, pos_over_neg: 3943.7587890625 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.6449, loss_val: nan, pos_over_neg: 1053.1644287109375 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.633, loss_val: nan, pos_over_neg: 59501.42578125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.6465, loss_val: nan, pos_over_neg: 541.8936157226562 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.6374, loss_val: nan, pos_over_neg: 2630.478515625 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.6465, loss_val: nan, pos_over_neg: 689.2297973632812 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.6441, loss_val: nan, pos_over_neg: 2910.034423828125 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.6408, loss_val: nan, pos_over_neg: 3932.116455078125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.6257, loss_val: nan, pos_over_neg: 766.3124389648438 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.6424, loss_val: nan, pos_over_neg: 843.61181640625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.6449, loss_val: nan, pos_over_neg: 1914.416015625 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.6333, loss_val: nan, pos_over_neg: -8836.4345703125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.6423, loss_val: nan, pos_over_neg: 1279.505126953125 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.6323, loss_val: nan, pos_over_neg: 595.8494262695312 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.638, loss_val: nan, pos_over_neg: 872.6303100585938 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.6404, loss_val: nan, pos_over_neg: 1219.75927734375 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.6387, loss_val: nan, pos_over_neg: 908.2654418945312 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.6416, loss_val: nan, pos_over_neg: 672.5584716796875 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.637, loss_val: nan, pos_over_neg: 1611.8612060546875 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.6528, loss_val: nan, pos_over_neg: 1159.0352783203125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.6414, loss_val: nan, pos_over_neg: 745.4762573242188 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.6437, loss_val: nan, pos_over_neg: 689.8947143554688 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.6274, loss_val: nan, pos_over_neg: 1267.3985595703125 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.6399, loss_val: nan, pos_over_neg: 2875.887939453125 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.6424, loss_val: nan, pos_over_neg: 638.5198974609375 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.6599, loss_val: nan, pos_over_neg: 562.9344482421875 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.6393, loss_val: nan, pos_over_neg: 668.429443359375 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.645, loss_val: nan, pos_over_neg: 812.1445922851562 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.6375, loss_val: nan, pos_over_neg: 1357.647216796875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.6345, loss_val: nan, pos_over_neg: 401.530029296875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.637, loss_val: nan, pos_over_neg: 1042.4873046875 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.645, loss_val: nan, pos_over_neg: 2837.14404296875 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.6477, loss_val: nan, pos_over_neg: 531.9739990234375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.6454, loss_val: nan, pos_over_neg: 395.3129577636719 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.6434, loss_val: nan, pos_over_neg: 522.3285522460938 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.6403, loss_val: nan, pos_over_neg: 627.6145629882812 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.64, loss_val: nan, pos_over_neg: 1102.0035400390625 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.6452, loss_val: nan, pos_over_neg: 536.9924926757812 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.6362, loss_val: nan, pos_over_neg: 1492.5606689453125 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.6343, loss_val: nan, pos_over_neg: 888.4248657226562 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.6545, loss_val: nan, pos_over_neg: 595.916015625 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.6419, loss_val: nan, pos_over_neg: 800.3335571289062 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.6305, loss_val: nan, pos_over_neg: 2303.103271484375 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.6374, loss_val: nan, pos_over_neg: 623.1336059570312 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.6384, loss_val: nan, pos_over_neg: 632.2237548828125 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.6335, loss_val: nan, pos_over_neg: 9940.2685546875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.635, loss_val: nan, pos_over_neg: 874.4368896484375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.648, loss_val: nan, pos_over_neg: 504.26715087890625 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.633, loss_val: nan, pos_over_neg: 511.7735900878906 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.6363, loss_val: nan, pos_over_neg: 346.89581298828125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.6338, loss_val: nan, pos_over_neg: 779.4275512695312 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.6384, loss_val: nan, pos_over_neg: 875.174560546875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.6447, loss_val: nan, pos_over_neg: 581.5887451171875 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.6415, loss_val: nan, pos_over_neg: 698.9011840820312 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.6404, loss_val: nan, pos_over_neg: 689.5789184570312 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.6295, loss_val: nan, pos_over_neg: 1873.332763671875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.6354, loss_val: nan, pos_over_neg: 1100.203857421875 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.6284, loss_val: nan, pos_over_neg: 816.9374389648438 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.6364, loss_val: nan, pos_over_neg: 1879.560546875 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.6351, loss_val: nan, pos_over_neg: 798.5541381835938 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.6403, loss_val: nan, pos_over_neg: 1295.0382080078125 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.6331, loss_val: nan, pos_over_neg: 908.66796875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.6322, loss_val: nan, pos_over_neg: 1075.943603515625 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.6253, loss_val: nan, pos_over_neg: 606.9998779296875 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.6418, loss_val: nan, pos_over_neg: 631.8201293945312 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.6311, loss_val: nan, pos_over_neg: 670.067626953125 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.6355, loss_val: nan, pos_over_neg: 1121.2431640625 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.6401, loss_val: nan, pos_over_neg: 462.1195373535156 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.6372, loss_val: nan, pos_over_neg: 556.2992553710938 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.6361, loss_val: nan, pos_over_neg: 2361.38671875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.6326, loss_val: nan, pos_over_neg: 14005.4892578125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.6404, loss_val: nan, pos_over_neg: 699.1903076171875 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.6346, loss_val: nan, pos_over_neg: 928.7198486328125 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.6492, loss_val: nan, pos_over_neg: 1179.0400390625 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.6571, loss_val: nan, pos_over_neg: 612.8578491210938 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.6294, loss_val: nan, pos_over_neg: 889.2239990234375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.6447, loss_val: nan, pos_over_neg: 418.84893798828125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.6384, loss_val: nan, pos_over_neg: 737.3184204101562 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.633, loss_val: nan, pos_over_neg: 1446.63427734375 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.6457, loss_val: nan, pos_over_neg: 669.2477416992188 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.6433, loss_val: nan, pos_over_neg: 1958.0240478515625 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.6598, loss_val: nan, pos_over_neg: 1155.6064453125 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.6376, loss_val: nan, pos_over_neg: 1154.9156494140625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.6447, loss_val: nan, pos_over_neg: 585.9804077148438 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.6408, loss_val: nan, pos_over_neg: 682.8494262695312 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.6369, loss_val: nan, pos_over_neg: 3932.6435546875 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.6426, loss_val: nan, pos_over_neg: 1979.294677734375 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.6335, loss_val: nan, pos_over_neg: 1286.8057861328125 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.6278, loss_val: nan, pos_over_neg: 888.6976928710938 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.629, loss_val: nan, pos_over_neg: 814.3641357421875 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.623, loss_val: nan, pos_over_neg: 1892.549072265625 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.6447, loss_val: nan, pos_over_neg: 542.5562133789062 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.6491, loss_val: nan, pos_over_neg: 424.8349914550781 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.6476, loss_val: nan, pos_over_neg: 676.0230712890625 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.6405, loss_val: nan, pos_over_neg: 4541.15673828125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.649, loss_val: nan, pos_over_neg: 614.7982177734375 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.6503, loss_val: nan, pos_over_neg: 345.53985595703125 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.6365, loss_val: nan, pos_over_neg: 615.8119506835938 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.6271, loss_val: nan, pos_over_neg: 1128.041748046875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.6401, loss_val: nan, pos_over_neg: 1315.093505859375 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.6464, loss_val: nan, pos_over_neg: 1149.06005859375 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.6549, loss_val: nan, pos_over_neg: 1617.146484375 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.645, loss_val: nan, pos_over_neg: 623.0359497070312 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.6366, loss_val: nan, pos_over_neg: 1788.3807373046875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.6466, loss_val: nan, pos_over_neg: 1118.9342041015625 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.6293, loss_val: nan, pos_over_neg: 831.9097290039062 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.6338, loss_val: nan, pos_over_neg: 1262.0989990234375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.6513, loss_val: nan, pos_over_neg: 573.0217895507812 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.6394, loss_val: nan, pos_over_neg: 10670.3974609375 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.6364, loss_val: nan, pos_over_neg: 1661.800048828125 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.642, loss_val: nan, pos_over_neg: 2584.787353515625 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.628, loss_val: nan, pos_over_neg: 556.0234375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.6377, loss_val: nan, pos_over_neg: 398.9468994140625 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.64, loss_val: nan, pos_over_neg: 813.37646484375 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.6363, loss_val: nan, pos_over_neg: 664.2578735351562 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.6412, loss_val: nan, pos_over_neg: 480.9523010253906 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.6364, loss_val: nan, pos_over_neg: 894.2745971679688 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.6493, loss_val: nan, pos_over_neg: 701.3518676757812 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.6435, loss_val: nan, pos_over_neg: 593.82763671875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.6484, loss_val: nan, pos_over_neg: 485.0022277832031 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.6485, loss_val: nan, pos_over_neg: 525.4127807617188 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.6394, loss_val: nan, pos_over_neg: 1153.9884033203125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.6338, loss_val: nan, pos_over_neg: 1098.77587890625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.6395, loss_val: nan, pos_over_neg: 710.5908203125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.6453, loss_val: nan, pos_over_neg: 634.631103515625 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.6219, loss_val: nan, pos_over_neg: 1932.4493408203125 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.6485, loss_val: nan, pos_over_neg: 351.3835144042969 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.6361, loss_val: nan, pos_over_neg: 2487.5478515625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.6299, loss_val: nan, pos_over_neg: 1512.201904296875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.6379, loss_val: nan, pos_over_neg: 1005.48583984375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.6362, loss_val: nan, pos_over_neg: 531.327392578125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.63, loss_val: nan, pos_over_neg: 401.8213806152344 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.6303, loss_val: nan, pos_over_neg: 897.1017456054688 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.644, loss_val: nan, pos_over_neg: 4229.09912109375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.6495, loss_val: nan, pos_over_neg: 1519.8004150390625 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.6477, loss_val: nan, pos_over_neg: 1069.0662841796875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.6297, loss_val: nan, pos_over_neg: 517.4378051757812 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.6288, loss_val: nan, pos_over_neg: 4633.716796875 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.6501, loss_val: nan, pos_over_neg: 629.0664672851562 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.6398, loss_val: nan, pos_over_neg: 1194.10986328125 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.6324, loss_val: nan, pos_over_neg: 1003.0560302734375 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.62, loss_val: nan, pos_over_neg: 5037.0380859375 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.6475, loss_val: nan, pos_over_neg: 626.5997924804688 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.6328, loss_val: nan, pos_over_neg: 833.1122436523438 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.6375, loss_val: nan, pos_over_neg: 5806.05908203125 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.6338, loss_val: nan, pos_over_neg: -3699.848388671875 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.6243, loss_val: nan, pos_over_neg: 23293.375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.6381, loss_val: nan, pos_over_neg: 799.6884155273438 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.6278, loss_val: nan, pos_over_neg: 820.0753173828125 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.6399, loss_val: nan, pos_over_neg: 909.8912963867188 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.6319, loss_val: nan, pos_over_neg: 532.19873046875 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.6363, loss_val: nan, pos_over_neg: 852.9657592773438 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.6399, loss_val: nan, pos_over_neg: 826.5833129882812 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.6416, loss_val: nan, pos_over_neg: 555.7155151367188 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.6332, loss_val: nan, pos_over_neg: 931.601318359375 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.6282, loss_val: nan, pos_over_neg: 827.199462890625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.6343, loss_val: nan, pos_over_neg: 476.6511535644531 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.6348, loss_val: nan, pos_over_neg: 1788.4139404296875 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.6323, loss_val: nan, pos_over_neg: 1296.3499755859375 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.6193, loss_val: nan, pos_over_neg: 1732.1807861328125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.6441, loss_val: nan, pos_over_neg: 1203.9444580078125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.6299, loss_val: nan, pos_over_neg: 15892.810546875 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.6428, loss_val: nan, pos_over_neg: 620.6728515625 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.6155, loss_val: nan, pos_over_neg: 2525.58984375 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.6217, loss_val: nan, pos_over_neg: 1277.738037109375 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.6297, loss_val: nan, pos_over_neg: 660.5115966796875 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.6276, loss_val: nan, pos_over_neg: 834.3255004882812 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.6336, loss_val: nan, pos_over_neg: 1216.1357421875 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.6353, loss_val: nan, pos_over_neg: 1203.0531005859375 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.6408, loss_val: nan, pos_over_neg: 469.2203063964844 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.6284, loss_val: nan, pos_over_neg: 3618.7216796875 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.6227, loss_val: nan, pos_over_neg: 1176.9197998046875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.6339, loss_val: nan, pos_over_neg: 1706.680419921875 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.636, loss_val: nan, pos_over_neg: 1032.52392578125 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.6336, loss_val: nan, pos_over_neg: 1679.781005859375 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.6331, loss_val: nan, pos_over_neg: 1198.054443359375 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.636, loss_val: nan, pos_over_neg: 491.63677978515625 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.6486, loss_val: nan, pos_over_neg: 1784.5924072265625 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.6314, loss_val: nan, pos_over_neg: 1813.5379638671875 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.6388, loss_val: nan, pos_over_neg: 6399.884765625 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.635, loss_val: nan, pos_over_neg: 3487.755859375 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.6236, loss_val: nan, pos_over_neg: 2192.250244140625 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.6264, loss_val: nan, pos_over_neg: 2601.806396484375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.632, loss_val: nan, pos_over_neg: 124048.8125 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.6439, loss_val: nan, pos_over_neg: 1830.8916015625 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.6354, loss_val: nan, pos_over_neg: 1768.06005859375 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.6318, loss_val: nan, pos_over_neg: 3786.904541015625 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.6465, loss_val: nan, pos_over_neg: 1011.0269165039062 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.6271, loss_val: nan, pos_over_neg: 2434.155517578125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.6448, loss_val: nan, pos_over_neg: 1147.39306640625 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.6251, loss_val: nan, pos_over_neg: 1886.2734375 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.6312, loss_val: nan, pos_over_neg: 2326.80810546875 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.6387, loss_val: nan, pos_over_neg: 468.9833068847656 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.6353, loss_val: nan, pos_over_neg: 1137.46923828125 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.6189, loss_val: nan, pos_over_neg: 3153.296142578125 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.6307, loss_val: nan, pos_over_neg: 965.1836547851562 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.6225, loss_val: nan, pos_over_neg: 3408.880126953125 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.6185, loss_val: nan, pos_over_neg: 3262.954345703125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.6261, loss_val: nan, pos_over_neg: -46256.4921875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.6268, loss_val: nan, pos_over_neg: -1989.25537109375 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.6385, loss_val: nan, pos_over_neg: 6467.765625 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.6346, loss_val: nan, pos_over_neg: 569.941162109375 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.6227, loss_val: nan, pos_over_neg: 2106.017822265625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.6262, loss_val: nan, pos_over_neg: 1142.47900390625 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.6315, loss_val: nan, pos_over_neg: 952.6825561523438 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.6311, loss_val: nan, pos_over_neg: 646.82275390625 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.6325, loss_val: nan, pos_over_neg: 2241.76318359375 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.6291, loss_val: nan, pos_over_neg: 1591.0826416015625 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.6298, loss_val: nan, pos_over_neg: 1005.2825317382812 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.6368, loss_val: nan, pos_over_neg: 691.4158325195312 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.6424, loss_val: nan, pos_over_neg: 379.647216796875 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.6425, loss_val: nan, pos_over_neg: 669.1973876953125 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.623, loss_val: nan, pos_over_neg: 1053.0325927734375 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.6388, loss_val: nan, pos_over_neg: 308.69525146484375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.6247, loss_val: nan, pos_over_neg: -3719.563232421875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.6313, loss_val: nan, pos_over_neg: 2153.0732421875 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.6331, loss_val: nan, pos_over_neg: 1894.750244140625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.6244, loss_val: nan, pos_over_neg: 1060.8016357421875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.6402, loss_val: nan, pos_over_neg: 2535.443603515625 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.6419, loss_val: nan, pos_over_neg: 1369.00048828125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.6295, loss_val: nan, pos_over_neg: 4993.27294921875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.6322, loss_val: nan, pos_over_neg: 499.569091796875 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.6401, loss_val: nan, pos_over_neg: 553.6351318359375 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.6204, loss_val: nan, pos_over_neg: 1070.7762451171875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.6233, loss_val: nan, pos_over_neg: 465.1319274902344 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.6335, loss_val: nan, pos_over_neg: 927.9174194335938 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.6441, loss_val: nan, pos_over_neg: 297.5333251953125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.6301, loss_val: nan, pos_over_neg: 898.7000122070312 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.6214, loss_val: nan, pos_over_neg: 2151.87451171875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.6397, loss_val: nan, pos_over_neg: 426.61883544921875 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.6173, loss_val: nan, pos_over_neg: 4307.4140625 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.6274, loss_val: nan, pos_over_neg: 1867.21435546875 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.6357, loss_val: nan, pos_over_neg: 668.8795166015625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.6302, loss_val: nan, pos_over_neg: 988.06591796875 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.6261, loss_val: nan, pos_over_neg: 1175.9127197265625 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.6326, loss_val: nan, pos_over_neg: 1001.6409301757812 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.6231, loss_val: nan, pos_over_neg: 7322.6416015625 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.6239, loss_val: nan, pos_over_neg: 1309.784423828125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.6352, loss_val: nan, pos_over_neg: 622.2007446289062 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.6204, loss_val: nan, pos_over_neg: 1479.31982421875 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.6278, loss_val: nan, pos_over_neg: 962.59619140625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.6334, loss_val: nan, pos_over_neg: 1497.7205810546875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.6442, loss_val: nan, pos_over_neg: 962.9642944335938 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.6247, loss_val: nan, pos_over_neg: 773.6367797851562 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.6232, loss_val: nan, pos_over_neg: 30430.630859375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.6242, loss_val: nan, pos_over_neg: -6803.88623046875 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.6348, loss_val: nan, pos_over_neg: 1065.9375 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.6309, loss_val: nan, pos_over_neg: 816.3375244140625 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.6274, loss_val: nan, pos_over_neg: 819.097412109375 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.6406, loss_val: nan, pos_over_neg: 783.6553955078125 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.6241, loss_val: nan, pos_over_neg: -7253.3798828125 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.6329, loss_val: nan, pos_over_neg: 2027.979248046875 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.6193, loss_val: nan, pos_over_neg: 1169.820556640625 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.6194, loss_val: nan, pos_over_neg: 4913.70458984375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.6303, loss_val: nan, pos_over_neg: 952.8524169921875 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.6383, loss_val: nan, pos_over_neg: 688.4442749023438 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.6537, loss_val: nan, pos_over_neg: 854.5335693359375 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.6334, loss_val: nan, pos_over_neg: 1116.447021484375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.627, loss_val: nan, pos_over_neg: 2646.31787109375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.6417, loss_val: nan, pos_over_neg: 2098.754150390625 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.6249, loss_val: nan, pos_over_neg: 5722.85400390625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.6411, loss_val: nan, pos_over_neg: 1300.7532958984375 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.6412, loss_val: nan, pos_over_neg: 785.6105346679688 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.6452, loss_val: nan, pos_over_neg: 2213.382568359375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.625, loss_val: nan, pos_over_neg: 681.1655883789062 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.6425, loss_val: nan, pos_over_neg: 1937.192626953125 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.628, loss_val: nan, pos_over_neg: 2732.33935546875 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.6299, loss_val: nan, pos_over_neg: 1172.453125 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.6343, loss_val: nan, pos_over_neg: 826.6885375976562 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.6333, loss_val: nan, pos_over_neg: 995.9494018554688 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.6222, loss_val: nan, pos_over_neg: 1843.4134521484375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.626, loss_val: nan, pos_over_neg: 692.0090942382812 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.6272, loss_val: nan, pos_over_neg: 513.4006958007812 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.6378, loss_val: nan, pos_over_neg: 653.1124877929688 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.6224, loss_val: nan, pos_over_neg: 1138.2904052734375 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.6231, loss_val: nan, pos_over_neg: 842.1996459960938 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.638, loss_val: nan, pos_over_neg: 686.6917114257812 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.6334, loss_val: nan, pos_over_neg: 472.9164123535156 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.6381, loss_val: nan, pos_over_neg: 1066.62158203125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.6248, loss_val: nan, pos_over_neg: 1968.8592529296875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.6279, loss_val: nan, pos_over_neg: 1071.538330078125 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.6291, loss_val: nan, pos_over_neg: 640.7640380859375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.6326, loss_val: nan, pos_over_neg: 821.9429931640625 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.6293, loss_val: nan, pos_over_neg: 1288.6795654296875 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.6343, loss_val: nan, pos_over_neg: 2481.609130859375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.6285, loss_val: nan, pos_over_neg: 1165.0128173828125 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.6238, loss_val: nan, pos_over_neg: 1689.7928466796875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.633, loss_val: nan, pos_over_neg: 988.7650756835938 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.6321, loss_val: nan, pos_over_neg: 1848.484619140625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.6268, loss_val: nan, pos_over_neg: 1509.3638916015625 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.6216, loss_val: nan, pos_over_neg: 793.1649169921875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.6242, loss_val: nan, pos_over_neg: 1609.34033203125 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.6258, loss_val: nan, pos_over_neg: 1344.1046142578125 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.6165, loss_val: nan, pos_over_neg: 10385.7578125 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.626, loss_val: nan, pos_over_neg: 1060.094970703125 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.633, loss_val: nan, pos_over_neg: 576.2070922851562 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.6207, loss_val: nan, pos_over_neg: 455.7673645019531 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.6257, loss_val: nan, pos_over_neg: 649.6314086914062 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.6334, loss_val: nan, pos_over_neg: 1212.8743896484375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.6307, loss_val: nan, pos_over_neg: 528.49755859375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.6262, loss_val: nan, pos_over_neg: 291.0899963378906 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.6297, loss_val: nan, pos_over_neg: 661.1149291992188 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.6294, loss_val: nan, pos_over_neg: 857.140625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.6301, loss_val: nan, pos_over_neg: 778.1566772460938 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.6275, loss_val: nan, pos_over_neg: 1345.9659423828125 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.6174, loss_val: nan, pos_over_neg: 817.1019287109375 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.6242, loss_val: nan, pos_over_neg: 1786.9464111328125 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.6309, loss_val: nan, pos_over_neg: 797.799072265625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.6344, loss_val: nan, pos_over_neg: 963.046142578125 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.6248, loss_val: nan, pos_over_neg: 2429.2392578125 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.6321, loss_val: nan, pos_over_neg: 681.4337768554688 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.619, loss_val: nan, pos_over_neg: 822.7728271484375 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.6258, loss_val: nan, pos_over_neg: 643.7108154296875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.6435, loss_val: nan, pos_over_neg: 596.1004028320312 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.6213, loss_val: nan, pos_over_neg: 949.441650390625 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.6225, loss_val: nan, pos_over_neg: 1184.329345703125 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.6373, loss_val: nan, pos_over_neg: 903.7174682617188 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.6154, loss_val: nan, pos_over_neg: 2261.511474609375 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.6285, loss_val: nan, pos_over_neg: 1113.5858154296875 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.6333, loss_val: nan, pos_over_neg: 786.9143676757812 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.6085, loss_val: nan, pos_over_neg: 1387.4398193359375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.6272, loss_val: nan, pos_over_neg: 1834.951416015625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.6341, loss_val: nan, pos_over_neg: 558.0545043945312 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.6318, loss_val: nan, pos_over_neg: 683.51708984375 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.627, loss_val: nan, pos_over_neg: -3074.70556640625 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.6258, loss_val: nan, pos_over_neg: 1546.5516357421875 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.6347, loss_val: nan, pos_over_neg: 3161.202392578125 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.6332, loss_val: nan, pos_over_neg: -9873.875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.6383, loss_val: nan, pos_over_neg: 1048.437744140625 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.6322, loss_val: nan, pos_over_neg: 619.9580688476562 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.6215, loss_val: nan, pos_over_neg: 2232.005126953125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.63, loss_val: nan, pos_over_neg: 1232.3465576171875 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.636, loss_val: nan, pos_over_neg: 492.2424621582031 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.6227, loss_val: nan, pos_over_neg: -24289.533203125 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.6283, loss_val: nan, pos_over_neg: 932.5987548828125 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.6296, loss_val: nan, pos_over_neg: 631.1640014648438 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.6353, loss_val: nan, pos_over_neg: 1581.44970703125 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.6263, loss_val: nan, pos_over_neg: 901.3025512695312 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.6309, loss_val: nan, pos_over_neg: 811.90185546875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.6318, loss_val: nan, pos_over_neg: 1146.2611083984375 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.635, loss_val: nan, pos_over_neg: 493.53814697265625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.6278, loss_val: nan, pos_over_neg: 699.4132080078125 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.6233, loss_val: nan, pos_over_neg: 970.4485473632812 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.6219, loss_val: nan, pos_over_neg: 1153.66162109375 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.625, loss_val: nan, pos_over_neg: 899.7607421875 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.6228, loss_val: nan, pos_over_neg: 1045.51611328125 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.6239, loss_val: nan, pos_over_neg: 3307.2666015625 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.6197, loss_val: nan, pos_over_neg: 3824.031494140625 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.6344, loss_val: nan, pos_over_neg: 1094.38525390625 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.6321, loss_val: nan, pos_over_neg: 938.82421875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.6254, loss_val: nan, pos_over_neg: 1212.024658203125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.6296, loss_val: nan, pos_over_neg: 899.0216064453125 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.6378, loss_val: nan, pos_over_neg: 1075.3763427734375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.6285, loss_val: nan, pos_over_neg: 4516.04931640625 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.6248, loss_val: nan, pos_over_neg: 1268.942626953125 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.6419, loss_val: nan, pos_over_neg: 1439.91357421875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.621, loss_val: nan, pos_over_neg: 947.27978515625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 1170.8153076171875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.6319, loss_val: nan, pos_over_neg: 1719.7965087890625 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.6257, loss_val: nan, pos_over_neg: 1176.7109375 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.6366, loss_val: nan, pos_over_neg: 325.9474792480469 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.6178, loss_val: nan, pos_over_neg: 686.0408935546875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.629, loss_val: nan, pos_over_neg: 1413.247802734375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.6249, loss_val: nan, pos_over_neg: 830.9508056640625 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.6244, loss_val: nan, pos_over_neg: 897.6188354492188 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.6178, loss_val: nan, pos_over_neg: 1792.3619384765625 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.6243, loss_val: nan, pos_over_neg: 82652.59375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.6201, loss_val: nan, pos_over_neg: 1827.0601806640625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.6264, loss_val: nan, pos_over_neg: 360.4649353027344 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.6266, loss_val: nan, pos_over_neg: 944.9295043945312 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.6237, loss_val: nan, pos_over_neg: 904.6206665039062 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.6191, loss_val: nan, pos_over_neg: 2970.60009765625 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.6214, loss_val: nan, pos_over_neg: 838.5331420898438 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.6206, loss_val: nan, pos_over_neg: 766.6603393554688 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.622, loss_val: nan, pos_over_neg: 1051.901123046875 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.6179, loss_val: nan, pos_over_neg: 1268.4002685546875 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.6313, loss_val: nan, pos_over_neg: 1636.3692626953125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.6206, loss_val: nan, pos_over_neg: 621.6226806640625 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.6435, loss_val: nan, pos_over_neg: 526.5263061523438 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.6123, loss_val: nan, pos_over_neg: 1270.4755859375 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.6107, loss_val: nan, pos_over_neg: 1765.3917236328125 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.6194, loss_val: nan, pos_over_neg: 1365.11474609375 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.6323, loss_val: nan, pos_over_neg: 1198.037109375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.6293, loss_val: nan, pos_over_neg: 827.1898803710938 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.622, loss_val: nan, pos_over_neg: 1395.4140625 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.6048, loss_val: nan, pos_over_neg: 1873.40625 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.6163, loss_val: nan, pos_over_neg: 7808.03466796875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.6073, loss_val: nan, pos_over_neg: -1669.8709716796875 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.626, loss_val: nan, pos_over_neg: 1459.45751953125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.6234, loss_val: nan, pos_over_neg: 833.2417602539062 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.6284, loss_val: nan, pos_over_neg: 553.4280395507812 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.6243, loss_val: nan, pos_over_neg: 2622.222412109375 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.636, loss_val: nan, pos_over_neg: 926.4418334960938 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.6331, loss_val: nan, pos_over_neg: 1459.3348388671875 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.6229, loss_val: nan, pos_over_neg: 642.6535034179688 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.6265, loss_val: nan, pos_over_neg: 2230.32275390625 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.6223, loss_val: nan, pos_over_neg: 1042.130859375 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.6208, loss_val: nan, pos_over_neg: 1703.852294921875 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.6345, loss_val: nan, pos_over_neg: 696.2573852539062 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.6384, loss_val: nan, pos_over_neg: 1414.9656982421875 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.6208, loss_val: nan, pos_over_neg: 6535.5576171875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.6169, loss_val: nan, pos_over_neg: 1774.3240966796875 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.6201, loss_val: nan, pos_over_neg: 4777.64501953125 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.6124, loss_val: nan, pos_over_neg: 2551.476806640625 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.6281, loss_val: nan, pos_over_neg: 546.8632202148438 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.6345, loss_val: nan, pos_over_neg: 612.4369506835938 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.6196, loss_val: nan, pos_over_neg: 1452.53173828125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.6185, loss_val: nan, pos_over_neg: 1606.7344970703125 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.629, loss_val: nan, pos_over_neg: 767.0074462890625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.6326, loss_val: nan, pos_over_neg: 687.1238403320312 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.62, loss_val: nan, pos_over_neg: 2781.40283203125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.6298, loss_val: nan, pos_over_neg: 936.0526733398438 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.6105, loss_val: nan, pos_over_neg: 2014.8760986328125 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.6229, loss_val: nan, pos_over_neg: 630.2125244140625 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.6332, loss_val: nan, pos_over_neg: 504.73736572265625 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.6192, loss_val: nan, pos_over_neg: 1043.5167236328125 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.6308, loss_val: nan, pos_over_neg: 1791.82568359375 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.6264, loss_val: nan, pos_over_neg: 699.333984375 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.6299, loss_val: nan, pos_over_neg: 608.2905883789062 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.6187, loss_val: nan, pos_over_neg: 3628.00634765625 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.629, loss_val: nan, pos_over_neg: 612.4004516601562 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.6264, loss_val: nan, pos_over_neg: 446.0633544921875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.629, loss_val: nan, pos_over_neg: 1269.9029541015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.6244, loss_val: nan, pos_over_neg: 1223.0958251953125 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.6238, loss_val: nan, pos_over_neg: 21211.203125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.6191, loss_val: nan, pos_over_neg: 1079.974365234375 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.6264, loss_val: nan, pos_over_neg: 828.5853881835938 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.617, loss_val: nan, pos_over_neg: 665.482666015625 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.6216, loss_val: nan, pos_over_neg: 2434.784912109375 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.6368, loss_val: nan, pos_over_neg: 706.1932983398438 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.6284, loss_val: nan, pos_over_neg: 786.2698364257812 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.6259, loss_val: nan, pos_over_neg: 701.1693725585938 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.6313, loss_val: nan, pos_over_neg: 999.5616455078125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.6164, loss_val: nan, pos_over_neg: 1937.4498291015625 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.6274, loss_val: nan, pos_over_neg: 941.7009887695312 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.6176, loss_val: nan, pos_over_neg: 1233.87109375 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.6295, loss_val: nan, pos_over_neg: 672.7548828125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.625, loss_val: nan, pos_over_neg: 2566.10498046875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.624, loss_val: nan, pos_over_neg: 814.5028686523438 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.6311, loss_val: nan, pos_over_neg: 1273.269287109375 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.6248, loss_val: nan, pos_over_neg: 1169.19921875 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.6345, loss_val: nan, pos_over_neg: 443.4737548828125 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.617, loss_val: nan, pos_over_neg: 1723.358642578125 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.6275, loss_val: nan, pos_over_neg: 1204.1170654296875 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.6163, loss_val: nan, pos_over_neg: 1278.54052734375 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 2065.272216796875 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.6116, loss_val: nan, pos_over_neg: 966.2022094726562 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.6232, loss_val: nan, pos_over_neg: 630.62744140625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.6293, loss_val: nan, pos_over_neg: 1176.200439453125 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.6354, loss_val: nan, pos_over_neg: 755.72509765625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.6376, loss_val: nan, pos_over_neg: 798.4337158203125 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.6383, loss_val: nan, pos_over_neg: 466.0820007324219 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.6389, loss_val: nan, pos_over_neg: 611.2391967773438 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.6147, loss_val: nan, pos_over_neg: 1006.7479858398438 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.6178, loss_val: nan, pos_over_neg: 938.1082763671875 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.6272, loss_val: nan, pos_over_neg: 752.6471557617188 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.6219, loss_val: nan, pos_over_neg: 623.5186767578125 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.6261, loss_val: nan, pos_over_neg: 566.3670043945312 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.622, loss_val: nan, pos_over_neg: 763.6207885742188 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.6199, loss_val: nan, pos_over_neg: 938.6477661132812 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 1033.45068359375 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.6277, loss_val: nan, pos_over_neg: 1807.9696044921875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.6061, loss_val: nan, pos_over_neg: 1151.14697265625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.6172, loss_val: nan, pos_over_neg: 742.3583984375 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.6197, loss_val: nan, pos_over_neg: 787.3229370117188 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.6139, loss_val: nan, pos_over_neg: 1230.0926513671875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.6242, loss_val: nan, pos_over_neg: 560.8787231445312 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.6276, loss_val: nan, pos_over_neg: 1026.6998291015625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.6187, loss_val: nan, pos_over_neg: 1256.8646240234375 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.6204, loss_val: nan, pos_over_neg: 2805.470458984375 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.6206, loss_val: nan, pos_over_neg: -5449.5625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.6191, loss_val: nan, pos_over_neg: 2331.316650390625 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.6242, loss_val: nan, pos_over_neg: 893.50244140625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.6209, loss_val: nan, pos_over_neg: 2871.26953125 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.6249, loss_val: nan, pos_over_neg: 904.1487426757812 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.619, loss_val: nan, pos_over_neg: 785.0143432617188 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.6176, loss_val: nan, pos_over_neg: 1682.912109375 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.63, loss_val: nan, pos_over_neg: 1483.9072265625 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.6257, loss_val: nan, pos_over_neg: 1606.0296630859375 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.6227, loss_val: nan, pos_over_neg: -8943.3505859375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.6164, loss_val: nan, pos_over_neg: 1301.1949462890625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.6267, loss_val: nan, pos_over_neg: 567.5726318359375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.6165, loss_val: nan, pos_over_neg: 2545.79931640625 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.6096, loss_val: nan, pos_over_neg: 3005.042236328125 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.6275, loss_val: nan, pos_over_neg: 1878.94775390625 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.6285, loss_val: nan, pos_over_neg: 898.3110961914062 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.6288, loss_val: nan, pos_over_neg: 923.514892578125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.6263, loss_val: nan, pos_over_neg: 5055.7119140625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.6203, loss_val: nan, pos_over_neg: 1404.9320068359375 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.636, loss_val: nan, pos_over_neg: 1274.7860107421875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.6249, loss_val: nan, pos_over_neg: 635.9998779296875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.6224, loss_val: nan, pos_over_neg: 692.8790283203125 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.6127, loss_val: nan, pos_over_neg: 1924.272705078125 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.6217, loss_val: nan, pos_over_neg: 2108.916015625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.6198, loss_val: nan, pos_over_neg: 2106.061767578125 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.6223, loss_val: nan, pos_over_neg: 3129.89453125 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.6209, loss_val: nan, pos_over_neg: 615.2089233398438 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.6282, loss_val: nan, pos_over_neg: 749.47216796875 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.6096, loss_val: nan, pos_over_neg: 3372.088623046875 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.6121, loss_val: nan, pos_over_neg: 1122.201904296875 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.6169, loss_val: nan, pos_over_neg: 795.7323608398438 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.628, loss_val: nan, pos_over_neg: 539.9348754882812 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.6293, loss_val: nan, pos_over_neg: 772.465087890625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.629, loss_val: nan, pos_over_neg: 4539.88134765625 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.6244, loss_val: nan, pos_over_neg: -50303.59375 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.6216, loss_val: nan, pos_over_neg: 5943.37548828125 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.6144, loss_val: nan, pos_over_neg: 1114.703857421875 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.6162, loss_val: nan, pos_over_neg: 961.0897827148438 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.6044, loss_val: nan, pos_over_neg: 7609.98291015625 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.6158, loss_val: nan, pos_over_neg: 828.369873046875 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.6201, loss_val: nan, pos_over_neg: 590.9030151367188 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.6182, loss_val: nan, pos_over_neg: 1889.010498046875 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.6232, loss_val: nan, pos_over_neg: 799.2955322265625 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.6214, loss_val: nan, pos_over_neg: 1144.6309814453125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.6171, loss_val: nan, pos_over_neg: 913.4974975585938 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.6185, loss_val: nan, pos_over_neg: -11724.72265625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.6184, loss_val: nan, pos_over_neg: 3991.620361328125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.6283, loss_val: nan, pos_over_neg: 745.4310913085938 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.6086, loss_val: nan, pos_over_neg: 725.8186645507812 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.6275, loss_val: nan, pos_over_neg: 641.1919555664062 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.624, loss_val: nan, pos_over_neg: 619.2250366210938 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.6276, loss_val: nan, pos_over_neg: 523.8006591796875 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.6242, loss_val: nan, pos_over_neg: 814.187744140625 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.6217, loss_val: nan, pos_over_neg: 1382.1326904296875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.6219, loss_val: nan, pos_over_neg: 932.9091796875 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.6186, loss_val: nan, pos_over_neg: 5999.72216796875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.6247, loss_val: nan, pos_over_neg: 599.511962890625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.6118, loss_val: nan, pos_over_neg: -24630.67578125 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.6187, loss_val: nan, pos_over_neg: 1980.5828857421875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.6099, loss_val: nan, pos_over_neg: 3206.862060546875 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.6278, loss_val: nan, pos_over_neg: 620.1589965820312 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.6207, loss_val: nan, pos_over_neg: 548.6583251953125 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.6228, loss_val: nan, pos_over_neg: 643.263671875 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 852.382080078125 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.6299, loss_val: nan, pos_over_neg: 531.2620849609375 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.6274, loss_val: nan, pos_over_neg: 846.6082763671875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.6101, loss_val: nan, pos_over_neg: -6222.11669921875 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.6245, loss_val: nan, pos_over_neg: -4433.52978515625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.6325, loss_val: nan, pos_over_neg: 1026.343505859375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.6255, loss_val: nan, pos_over_neg: 4726.28466796875 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.6163, loss_val: nan, pos_over_neg: 1895.33837890625 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.6213, loss_val: nan, pos_over_neg: 1512.6544189453125 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.6244, loss_val: nan, pos_over_neg: 1345.8931884765625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.63, loss_val: nan, pos_over_neg: 412.7358703613281 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.6158, loss_val: nan, pos_over_neg: 715.8400268554688 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.6171, loss_val: nan, pos_over_neg: 592.854248046875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.6169, loss_val: nan, pos_over_neg: 691.0735473632812 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.6225, loss_val: nan, pos_over_neg: 809.79638671875 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.6182, loss_val: nan, pos_over_neg: 868.1962280273438 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.6139, loss_val: nan, pos_over_neg: 1345.7523193359375 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.6197, loss_val: nan, pos_over_neg: 841.1318969726562 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.6146, loss_val: nan, pos_over_neg: 729.0668334960938 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.6211, loss_val: nan, pos_over_neg: 497.8283996582031 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.6201, loss_val: nan, pos_over_neg: 684.98388671875 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.625, loss_val: nan, pos_over_neg: 1422.9437255859375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.619, loss_val: nan, pos_over_neg: 2723.87353515625 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.6145, loss_val: nan, pos_over_neg: 848.2808227539062 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.6129, loss_val: nan, pos_over_neg: 1156.1976318359375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.6181, loss_val: nan, pos_over_neg: 1105.0179443359375 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.613, loss_val: nan, pos_over_neg: 767.231689453125 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.61, loss_val: nan, pos_over_neg: 1732.2471923828125 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.6214, loss_val: nan, pos_over_neg: 2145.143798828125 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.6348, loss_val: nan, pos_over_neg: 375.04541015625 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 1992.85302734375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.6149, loss_val: nan, pos_over_neg: 1639.8148193359375 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.6127, loss_val: nan, pos_over_neg: 3559.677734375 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.6352, loss_val: nan, pos_over_neg: 988.6138916015625 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.6163, loss_val: nan, pos_over_neg: 2924.604248046875 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.6232, loss_val: nan, pos_over_neg: 1007.7276611328125 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.6183, loss_val: nan, pos_over_neg: 753.0145263671875 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.6177, loss_val: nan, pos_over_neg: 1297.6533203125 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.6163, loss_val: nan, pos_over_neg: 2215.337646484375 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.6223, loss_val: nan, pos_over_neg: 1795.4439697265625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.6157, loss_val: nan, pos_over_neg: 18293.685546875 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.623, loss_val: nan, pos_over_neg: 2036.4686279296875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.6237, loss_val: nan, pos_over_neg: 1016.0140991210938 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.6147, loss_val: nan, pos_over_neg: 2077.921630859375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.6283, loss_val: nan, pos_over_neg: 1505.9044189453125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.617, loss_val: nan, pos_over_neg: 1597.1436767578125 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.6149, loss_val: nan, pos_over_neg: 2292.336181640625 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.6237, loss_val: nan, pos_over_neg: 1743.7716064453125 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.616, loss_val: nan, pos_over_neg: 1053.439453125 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.6098, loss_val: nan, pos_over_neg: 1561.4222412109375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.6164, loss_val: nan, pos_over_neg: 1039.5555419921875 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.6132, loss_val: nan, pos_over_neg: 1334.421630859375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.6166, loss_val: nan, pos_over_neg: 1007.8265380859375 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.6114, loss_val: nan, pos_over_neg: 1105.6551513671875 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.6217, loss_val: nan, pos_over_neg: 646.31396484375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.6196, loss_val: nan, pos_over_neg: 736.5093383789062 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.6122, loss_val: nan, pos_over_neg: -9674.966796875 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.6142, loss_val: nan, pos_over_neg: 767.9118041992188 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.6261, loss_val: nan, pos_over_neg: 856.172119140625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.6165, loss_val: nan, pos_over_neg: 2608.512939453125 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.6173, loss_val: nan, pos_over_neg: 2805.212646484375 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.618, loss_val: nan, pos_over_neg: 687.99853515625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 516.5478515625 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.6199, loss_val: nan, pos_over_neg: 3877.158935546875 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.6189, loss_val: nan, pos_over_neg: 2010.367919921875 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.617, loss_val: nan, pos_over_neg: 1327.43115234375 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.6121, loss_val: nan, pos_over_neg: 572.1549072265625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.6221, loss_val: nan, pos_over_neg: 1170.05126953125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.6221, loss_val: nan, pos_over_neg: 1748.964599609375 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.6045, loss_val: nan, pos_over_neg: 1862.08203125 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.624, loss_val: nan, pos_over_neg: 546.28857421875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.6224, loss_val: nan, pos_over_neg: 527.8837890625 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 612.0787963867188 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.616, loss_val: nan, pos_over_neg: 497.9497375488281 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.6172, loss_val: nan, pos_over_neg: 1092.3482666015625 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.6221, loss_val: nan, pos_over_neg: 401.940673828125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.612, loss_val: nan, pos_over_neg: 390.6739196777344 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.6227, loss_val: nan, pos_over_neg: 515.9144897460938 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 1105.91943359375 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.6168, loss_val: nan, pos_over_neg: 885.7823486328125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.611, loss_val: nan, pos_over_neg: 443.5403747558594 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.6104, loss_val: nan, pos_over_neg: 857.452880859375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.6194, loss_val: nan, pos_over_neg: 784.2899780273438 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.6201, loss_val: nan, pos_over_neg: 548.4619140625 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.6257, loss_val: nan, pos_over_neg: 740.35302734375 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.6183, loss_val: nan, pos_over_neg: 465.55322265625 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.6298, loss_val: nan, pos_over_neg: 312.22283935546875 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.618, loss_val: nan, pos_over_neg: 1231.7960205078125 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.6217, loss_val: nan, pos_over_neg: 1077.295166015625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 1974.27490234375 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.6174, loss_val: nan, pos_over_neg: 1031.834716796875 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.622, loss_val: nan, pos_over_neg: 1095.66943359375 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.6243, loss_val: nan, pos_over_neg: 872.9668579101562 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.6236, loss_val: nan, pos_over_neg: 580.5022583007812 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.619, loss_val: nan, pos_over_neg: 1818.76318359375 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.6192, loss_val: nan, pos_over_neg: 822.3370971679688 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.6234, loss_val: nan, pos_over_neg: 813.7526245117188 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.6109, loss_val: nan, pos_over_neg: 631.4480590820312 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.628, loss_val: nan, pos_over_neg: 1381.1844482421875 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.6254, loss_val: nan, pos_over_neg: 528.1812133789062 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.6209, loss_val: nan, pos_over_neg: 790.4109497070312 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.6212, loss_val: nan, pos_over_neg: 671.3472900390625 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.6086, loss_val: nan, pos_over_neg: 1814.578369140625 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.6157, loss_val: nan, pos_over_neg: 827.7245483398438 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.607, loss_val: nan, pos_over_neg: 3331.8076171875 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.6213, loss_val: nan, pos_over_neg: 1667.2637939453125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.6198, loss_val: nan, pos_over_neg: 509.5924072265625 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.6214, loss_val: nan, pos_over_neg: 831.666748046875 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.6154, loss_val: nan, pos_over_neg: 5474.810546875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 1662.5948486328125 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.6177, loss_val: nan, pos_over_neg: 782.5811767578125 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.6171, loss_val: nan, pos_over_neg: 772.8343505859375 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.6181, loss_val: nan, pos_over_neg: 844.3340454101562 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.6136, loss_val: nan, pos_over_neg: 904.8460083007812 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.6115, loss_val: nan, pos_over_neg: 826.9093627929688 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.6113, loss_val: nan, pos_over_neg: 1532.4381103515625 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.6152, loss_val: nan, pos_over_neg: 1169.6278076171875 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.6193, loss_val: nan, pos_over_neg: 1107.4171142578125 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.6168, loss_val: nan, pos_over_neg: 1418.387451171875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.612, loss_val: nan, pos_over_neg: 1615.548583984375 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.61, loss_val: nan, pos_over_neg: 1116.210205078125 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.6217, loss_val: nan, pos_over_neg: 671.422119140625 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.6081, loss_val: nan, pos_over_neg: 1597.32861328125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.6179, loss_val: nan, pos_over_neg: 549.0783081054688 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.6194, loss_val: nan, pos_over_neg: 1517.1473388671875 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.6143, loss_val: nan, pos_over_neg: 782.6874389648438 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.6191, loss_val: nan, pos_over_neg: 764.1242065429688 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.6052, loss_val: nan, pos_over_neg: 4046.5458984375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.6058, loss_val: nan, pos_over_neg: 3244.696533203125 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.6125, loss_val: nan, pos_over_neg: 1987.1129150390625 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.6283, loss_val: nan, pos_over_neg: 797.7402954101562 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.6133, loss_val: nan, pos_over_neg: 6236.28515625 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.6089, loss_val: nan, pos_over_neg: 1124.503662109375 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.6204, loss_val: nan, pos_over_neg: 1076.8612060546875 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.6183, loss_val: nan, pos_over_neg: 942.39990234375 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.6288, loss_val: nan, pos_over_neg: 841.0679321289062 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.6143, loss_val: nan, pos_over_neg: 8164.0732421875 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.6254, loss_val: nan, pos_over_neg: 722.6842651367188 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.6139, loss_val: nan, pos_over_neg: 3142.716796875 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.6262, loss_val: nan, pos_over_neg: 1427.15478515625 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.6192, loss_val: nan, pos_over_neg: 1744.1907958984375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.6147, loss_val: nan, pos_over_neg: 17176.322265625 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.6141, loss_val: nan, pos_over_neg: 1115.07080078125 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: -5651.0146484375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.6119, loss_val: nan, pos_over_neg: 717.7005004882812 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.6173, loss_val: nan, pos_over_neg: 791.8668212890625 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.6217, loss_val: nan, pos_over_neg: 589.5205688476562 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.6169, loss_val: nan, pos_over_neg: 1114.72021484375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.6049, loss_val: nan, pos_over_neg: -27824.958984375 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.6182, loss_val: nan, pos_over_neg: 1448.9326171875 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.6119, loss_val: nan, pos_over_neg: 2650.20654296875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.6226, loss_val: nan, pos_over_neg: 1322.7236328125 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.6115, loss_val: nan, pos_over_neg: 3132.888427734375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.615, loss_val: nan, pos_over_neg: 1360.447021484375 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.6148, loss_val: nan, pos_over_neg: 2963.418701171875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.6285, loss_val: nan, pos_over_neg: 698.8969116210938 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.6039, loss_val: nan, pos_over_neg: 1445.8780517578125 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 919.2222290039062 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.6041, loss_val: nan, pos_over_neg: 31129.9765625 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.6148, loss_val: nan, pos_over_neg: 1562.3499755859375 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.6223, loss_val: nan, pos_over_neg: 1750.7818603515625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.6267, loss_val: nan, pos_over_neg: 1159.6160888671875 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.6095, loss_val: nan, pos_over_neg: 947.4537353515625 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.6171, loss_val: nan, pos_over_neg: 918.7821655273438 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.6193, loss_val: nan, pos_over_neg: 1274.5277099609375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.6095, loss_val: nan, pos_over_neg: 11360.576171875 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.6092, loss_val: nan, pos_over_neg: 1622.3653564453125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.6096, loss_val: nan, pos_over_neg: 1591.3175048828125 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.6102, loss_val: nan, pos_over_neg: 3173.97998046875 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.6145, loss_val: nan, pos_over_neg: 7054.10888671875 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.6031, loss_val: nan, pos_over_neg: 12597.61328125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.6213, loss_val: nan, pos_over_neg: 832.4142456054688 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.6177, loss_val: nan, pos_over_neg: 3158.334228515625 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.6039, loss_val: nan, pos_over_neg: 6598.6123046875 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.6224, loss_val: nan, pos_over_neg: 2604.0 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.6185, loss_val: nan, pos_over_neg: 5020.59521484375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.6133, loss_val: nan, pos_over_neg: 1842.242919921875 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.6202, loss_val: nan, pos_over_neg: 1212.7752685546875 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.6172, loss_val: nan, pos_over_neg: 4356.787109375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.6189, loss_val: nan, pos_over_neg: 1306.1793212890625 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.6195, loss_val: nan, pos_over_neg: 780.46728515625 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.6115, loss_val: nan, pos_over_neg: 1572.516845703125 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.6259, loss_val: nan, pos_over_neg: 3570.623779296875 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.6173, loss_val: nan, pos_over_neg: 1357.931884765625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.6185, loss_val: nan, pos_over_neg: 3242.583251953125 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.6102, loss_val: nan, pos_over_neg: -12513.24609375 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.6287, loss_val: nan, pos_over_neg: 2319.590576171875 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.612, loss_val: nan, pos_over_neg: 5255.76611328125 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.6044, loss_val: nan, pos_over_neg: 1083.3768310546875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.6181, loss_val: nan, pos_over_neg: 2227.425048828125 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.6172, loss_val: nan, pos_over_neg: 1008.3323364257812 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.6191, loss_val: nan, pos_over_neg: 755.8839111328125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 610.7100830078125 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.6046, loss_val: nan, pos_over_neg: 1516.771728515625 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.614, loss_val: nan, pos_over_neg: 1115.6011962890625 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.6112, loss_val: nan, pos_over_neg: 5993.06005859375 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.6155, loss_val: nan, pos_over_neg: 3447.943359375 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.6111, loss_val: nan, pos_over_neg: 2976.465576171875 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.6223, loss_val: nan, pos_over_neg: 1183.3349609375 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.6177, loss_val: nan, pos_over_neg: 904.2860107421875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.6105, loss_val: nan, pos_over_neg: 1258.3077392578125 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.6015, loss_val: nan, pos_over_neg: 1433.6761474609375 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.6108, loss_val: nan, pos_over_neg: 842.128662109375 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.6175, loss_val: nan, pos_over_neg: 551.5128173828125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 1621.3228759765625 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.6041, loss_val: nan, pos_over_neg: 1853.91162109375 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.6039, loss_val: nan, pos_over_neg: 1061.35595703125 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.6178, loss_val: nan, pos_over_neg: 459.92108154296875 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.6133, loss_val: nan, pos_over_neg: 1724.594970703125 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.6239, loss_val: nan, pos_over_neg: 2499.1435546875 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.6158, loss_val: nan, pos_over_neg: 2153.3388671875 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.6042, loss_val: nan, pos_over_neg: 1007.2482299804688 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.6094, loss_val: nan, pos_over_neg: 1769.2794189453125 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.6175, loss_val: nan, pos_over_neg: 2359.450439453125 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.6107, loss_val: nan, pos_over_neg: 947.116455078125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.6208, loss_val: nan, pos_over_neg: 624.9954223632812 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.6262, loss_val: nan, pos_over_neg: 345.2449951171875 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.6186, loss_val: nan, pos_over_neg: 508.76422119140625 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.6247, loss_val: nan, pos_over_neg: 602.9156494140625 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.597, loss_val: nan, pos_over_neg: 1306.9530029296875 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.6194, loss_val: nan, pos_over_neg: 1136.406982421875 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.6232, loss_val: nan, pos_over_neg: 2137.46044921875 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.6188, loss_val: nan, pos_over_neg: 819.688232421875 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 2695.3720703125 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.6148, loss_val: nan, pos_over_neg: 1536.895751953125 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.622, loss_val: nan, pos_over_neg: -119184.265625 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.6256, loss_val: nan, pos_over_neg: 2172.69677734375 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.6191, loss_val: nan, pos_over_neg: 801.7157592773438 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.6221, loss_val: nan, pos_over_neg: 773.02001953125 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 4908.40380859375 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.6192, loss_val: nan, pos_over_neg: 3722.966064453125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.6229, loss_val: nan, pos_over_neg: 469.10308837890625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.6169, loss_val: nan, pos_over_neg: 2649.573486328125 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.617, loss_val: nan, pos_over_neg: 1172.3341064453125 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 4256.85107421875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.6115, loss_val: nan, pos_over_neg: 1773.53564453125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.6175, loss_val: nan, pos_over_neg: 570.6332397460938 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.609, loss_val: nan, pos_over_neg: 611.0731201171875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 753.933349609375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.6028, loss_val: nan, pos_over_neg: -7114.416015625 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.6141, loss_val: nan, pos_over_neg: 1430.857421875 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.6134, loss_val: nan, pos_over_neg: 775.5177001953125 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.613, loss_val: nan, pos_over_neg: 957.0958862304688 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.6043, loss_val: nan, pos_over_neg: 8021.05029296875 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 1825.14404296875 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.6164, loss_val: nan, pos_over_neg: 1154.8045654296875 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.6019, loss_val: nan, pos_over_neg: 714.716064453125 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.6156, loss_val: nan, pos_over_neg: 589.151123046875 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.6054, loss_val: nan, pos_over_neg: 2510.48046875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.621, loss_val: nan, pos_over_neg: 2697.675537109375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.6154, loss_val: nan, pos_over_neg: 1293.1317138671875 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.6105, loss_val: nan, pos_over_neg: 875.6434936523438 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.6104, loss_val: nan, pos_over_neg: 828.8927001953125 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.6136, loss_val: nan, pos_over_neg: 3711.458251953125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.612, loss_val: nan, pos_over_neg: 1349.1058349609375 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.6227, loss_val: nan, pos_over_neg: 1124.340087890625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.6095, loss_val: nan, pos_over_neg: 546.0723266601562 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.6151, loss_val: nan, pos_over_neg: 966.41748046875 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.6135, loss_val: nan, pos_over_neg: 795.5975341796875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.6092, loss_val: nan, pos_over_neg: 1297.914306640625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.62, loss_val: nan, pos_over_neg: 565.1655883789062 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.6243, loss_val: nan, pos_over_neg: 659.9542846679688 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.6254, loss_val: nan, pos_over_neg: 522.8447875976562 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.6111, loss_val: nan, pos_over_neg: 755.240234375 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 1339.98095703125 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.6077, loss_val: nan, pos_over_neg: 616.6403198242188 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.6226, loss_val: nan, pos_over_neg: 761.4273071289062 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.6265, loss_val: nan, pos_over_neg: 2431.955078125 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.6111, loss_val: nan, pos_over_neg: 2025.2685546875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.6039, loss_val: nan, pos_over_neg: 1535.0887451171875 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.6126, loss_val: nan, pos_over_neg: 790.6959228515625 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.6151, loss_val: nan, pos_over_neg: 664.672119140625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.6188, loss_val: nan, pos_over_neg: 1512.6337890625 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.6083, loss_val: nan, pos_over_neg: 2955.71533203125 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.6264, loss_val: nan, pos_over_neg: 1952.2420654296875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.6126, loss_val: nan, pos_over_neg: 5371.4423828125 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.6135, loss_val: nan, pos_over_neg: 493.8307189941406 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.6106, loss_val: nan, pos_over_neg: 11152.9443359375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.6062, loss_val: nan, pos_over_neg: 2776.433349609375 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.6188, loss_val: nan, pos_over_neg: 3534.598388671875 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.6073, loss_val: nan, pos_over_neg: 1310.61767578125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.6069, loss_val: nan, pos_over_neg: 1394.3370361328125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.6126, loss_val: nan, pos_over_neg: 996.4862670898438 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.612, loss_val: nan, pos_over_neg: 2284.497802734375 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.615, loss_val: nan, pos_over_neg: 1560.595458984375 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.6184, loss_val: nan, pos_over_neg: 661.4105224609375 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 968.4512939453125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 3020.744384765625 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.6202, loss_val: nan, pos_over_neg: 1426.48876953125 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.6074, loss_val: nan, pos_over_neg: 4148.6982421875 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.6121, loss_val: nan, pos_over_neg: 1849.54345703125 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.6154, loss_val: nan, pos_over_neg: 1378.5447998046875 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.6163, loss_val: nan, pos_over_neg: 1588.558349609375 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.606, loss_val: nan, pos_over_neg: -9007.0888671875 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.6, loss_val: nan, pos_over_neg: 1059.8170166015625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.6075, loss_val: nan, pos_over_neg: 3593.097412109375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.6076, loss_val: nan, pos_over_neg: 1520.695556640625 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.615, loss_val: nan, pos_over_neg: 655.7814331054688 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.6099, loss_val: nan, pos_over_neg: 455.0945129394531 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [1:14:37<186741:03:52, 2240.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/889, loss_train: 5.6018, loss_val: nan, pos_over_neg: 927.1401977539062 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.6143, loss_val: nan, pos_over_neg: 1287.66357421875 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.6112, loss_val: nan, pos_over_neg: 5417.9150390625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.6119, loss_val: nan, pos_over_neg: -7497.62890625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.6114, loss_val: nan, pos_over_neg: 9203.353515625 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.6028, loss_val: nan, pos_over_neg: -12166.1025390625 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.6145, loss_val: nan, pos_over_neg: 3813.99267578125 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 1934.8857421875 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.6089, loss_val: nan, pos_over_neg: 8156.11865234375 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.6074, loss_val: nan, pos_over_neg: 2424.263427734375 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 939.4931640625 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.6076, loss_val: nan, pos_over_neg: 551.75244140625 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.6139, loss_val: nan, pos_over_neg: 1231.0467529296875 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: 640.7982788085938 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.6123, loss_val: nan, pos_over_neg: 1031.900634765625 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 3599.906494140625 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.6073, loss_val: nan, pos_over_neg: 4402.01220703125 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.6083, loss_val: nan, pos_over_neg: 2910.744384765625 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.6036, loss_val: nan, pos_over_neg: 1048.3048095703125 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.6238, loss_val: nan, pos_over_neg: 1167.6834716796875 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.6195, loss_val: nan, pos_over_neg: 1004.089599609375 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.6085, loss_val: nan, pos_over_neg: 2405.76318359375 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 1496.8985595703125 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.6208, loss_val: nan, pos_over_neg: 924.37890625 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.6183, loss_val: nan, pos_over_neg: 556.290771484375 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.6064, loss_val: nan, pos_over_neg: 682.4129638671875 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.621, loss_val: nan, pos_over_neg: 705.7368774414062 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.6211, loss_val: nan, pos_over_neg: 730.9612426757812 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.613, loss_val: nan, pos_over_neg: 744.0194091796875 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.6136, loss_val: nan, pos_over_neg: 536.0245971679688 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.6179, loss_val: nan, pos_over_neg: 1811.8411865234375 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.6084, loss_val: nan, pos_over_neg: 3229.784912109375 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.6007, loss_val: nan, pos_over_neg: 3811.110595703125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.6033, loss_val: nan, pos_over_neg: 1369.177978515625 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.605, loss_val: nan, pos_over_neg: 3779.142333984375 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.6123, loss_val: nan, pos_over_neg: 1379.764892578125 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.6053, loss_val: nan, pos_over_neg: 1839.1619873046875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 1306.355224609375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 3926.375244140625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.6081, loss_val: nan, pos_over_neg: 3524.598876953125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.6113, loss_val: nan, pos_over_neg: 1210.37060546875 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.6125, loss_val: nan, pos_over_neg: 873.9572143554688 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.6052, loss_val: nan, pos_over_neg: 1135.4205322265625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.6018, loss_val: nan, pos_over_neg: 3054.7978515625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.6023, loss_val: nan, pos_over_neg: 2744.657470703125 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.6219, loss_val: nan, pos_over_neg: 905.6484985351562 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.6044, loss_val: nan, pos_over_neg: 2633.9326171875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.6047, loss_val: nan, pos_over_neg: 5281.744140625 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.616, loss_val: nan, pos_over_neg: 4243.73095703125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.5968, loss_val: nan, pos_over_neg: 813.3179931640625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: 3997.2763671875 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.6158, loss_val: nan, pos_over_neg: 555.50634765625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.6116, loss_val: nan, pos_over_neg: 482.6650085449219 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.6175, loss_val: nan, pos_over_neg: 974.306640625 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.6281, loss_val: nan, pos_over_neg: 878.5614013671875 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.6176, loss_val: nan, pos_over_neg: 1050.4857177734375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.6182, loss_val: nan, pos_over_neg: 997.4805297851562 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 706.580322265625 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.6092, loss_val: nan, pos_over_neg: 710.64990234375 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.6105, loss_val: nan, pos_over_neg: 752.4598388671875 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.6065, loss_val: nan, pos_over_neg: 623.0862426757812 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.6134, loss_val: nan, pos_over_neg: 551.8894653320312 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.6104, loss_val: nan, pos_over_neg: 332.979736328125 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.6159, loss_val: nan, pos_over_neg: 520.8406372070312 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.61, loss_val: nan, pos_over_neg: 1001.7855224609375 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.623, loss_val: nan, pos_over_neg: 541.4170532226562 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.6097, loss_val: nan, pos_over_neg: 880.3480224609375 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.6074, loss_val: nan, pos_over_neg: 1765.513427734375 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 1321.243408203125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.6109, loss_val: nan, pos_over_neg: 2009.079833984375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.6114, loss_val: nan, pos_over_neg: 1297.586669921875 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.6051, loss_val: nan, pos_over_neg: 541.595947265625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 775.7540283203125 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.6112, loss_val: nan, pos_over_neg: 760.469970703125 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.6151, loss_val: nan, pos_over_neg: 727.5150756835938 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.6178, loss_val: nan, pos_over_neg: 522.3367309570312 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.6225, loss_val: nan, pos_over_neg: 574.02001953125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.6139, loss_val: nan, pos_over_neg: 669.4827880859375 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.6081, loss_val: nan, pos_over_neg: 665.3153686523438 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: 1241.4912109375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.6086, loss_val: nan, pos_over_neg: 12707.6064453125 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.6236, loss_val: nan, pos_over_neg: 387.8719787597656 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.6008, loss_val: nan, pos_over_neg: 1173.840087890625 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 3259.236083984375 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.6197, loss_val: nan, pos_over_neg: 611.945556640625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.6084, loss_val: nan, pos_over_neg: 1066.734375 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 1249.097412109375 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: 934.5521240234375 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.6043, loss_val: nan, pos_over_neg: 1020.2330322265625 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.6084, loss_val: nan, pos_over_neg: 874.0084228515625 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 35391.34765625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.6021, loss_val: nan, pos_over_neg: 1296.96630859375 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.6235, loss_val: nan, pos_over_neg: 429.3179931640625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: 1183.6156005859375 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: 550.5496826171875 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 679.467529296875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.6125, loss_val: nan, pos_over_neg: 855.122314453125 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.6307, loss_val: nan, pos_over_neg: 854.0826416015625 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.5922, loss_val: nan, pos_over_neg: 1029.998779296875 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.6114, loss_val: nan, pos_over_neg: 951.9774169921875 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.6066, loss_val: nan, pos_over_neg: 3009.4326171875 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.5983, loss_val: nan, pos_over_neg: 5115.60888671875 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.6055, loss_val: nan, pos_over_neg: 1611.0733642578125 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.6081, loss_val: nan, pos_over_neg: 909.2872314453125 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.5994, loss_val: nan, pos_over_neg: 2481.24658203125 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.6062, loss_val: nan, pos_over_neg: 3255.658447265625 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.6006, loss_val: nan, pos_over_neg: 1172.76953125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 1033.69140625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.6122, loss_val: nan, pos_over_neg: 789.89453125 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 2694.464599609375 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.6189, loss_val: nan, pos_over_neg: 1239.2822265625 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.6153, loss_val: nan, pos_over_neg: 2315.91259765625 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.6101, loss_val: nan, pos_over_neg: 2262.0185546875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.6011, loss_val: nan, pos_over_neg: 2834.8349609375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.6099, loss_val: nan, pos_over_neg: 5999.796875 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 926.5997924804688 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.6141, loss_val: nan, pos_over_neg: 719.0800170898438 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 809.09912109375 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.612, loss_val: nan, pos_over_neg: 1873.3814697265625 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.6099, loss_val: nan, pos_over_neg: 1207.154541015625 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.6068, loss_val: nan, pos_over_neg: 685.2360229492188 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.6198, loss_val: nan, pos_over_neg: 363.2109680175781 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.6194, loss_val: nan, pos_over_neg: 938.0814819335938 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.6166, loss_val: nan, pos_over_neg: 488.65802001953125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.6087, loss_val: nan, pos_over_neg: 602.3175048828125 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.6137, loss_val: nan, pos_over_neg: 734.0950927734375 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.6012, loss_val: nan, pos_over_neg: 2312.832275390625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 1148.8125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.6069, loss_val: nan, pos_over_neg: 2003.88818359375 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.6058, loss_val: nan, pos_over_neg: 787.0884399414062 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 1065.43505859375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.6111, loss_val: nan, pos_over_neg: 1398.70556640625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.6149, loss_val: nan, pos_over_neg: 544.2188720703125 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.6158, loss_val: nan, pos_over_neg: 584.368896484375 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 1532.9566650390625 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.6084, loss_val: nan, pos_over_neg: -4820.24169921875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.5976, loss_val: nan, pos_over_neg: 3955.55224609375 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.6106, loss_val: nan, pos_over_neg: 2347.8369140625 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.6164, loss_val: nan, pos_over_neg: 507.15582275390625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 2056.066650390625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: -66593.34375 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.6139, loss_val: nan, pos_over_neg: 3462.418212890625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.6108, loss_val: nan, pos_over_neg: 560.7260131835938 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.6079, loss_val: nan, pos_over_neg: 640.6741943359375 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.6022, loss_val: nan, pos_over_neg: -106474.5390625 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 23162.1171875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 1215.71435546875 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.6083, loss_val: nan, pos_over_neg: 600.4741821289062 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.6027, loss_val: nan, pos_over_neg: 1687.876953125 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.6162, loss_val: nan, pos_over_neg: 2246.264892578125 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 2946.835693359375 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.5991, loss_val: nan, pos_over_neg: 3168.4228515625 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.6064, loss_val: nan, pos_over_neg: 719.8201904296875 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.6021, loss_val: nan, pos_over_neg: 618.2926635742188 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.6114, loss_val: nan, pos_over_neg: 1248.6446533203125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.608, loss_val: nan, pos_over_neg: 1348.92626953125 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.5962, loss_val: nan, pos_over_neg: 621.1065063476562 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.6137, loss_val: nan, pos_over_neg: 464.0338439941406 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.6136, loss_val: nan, pos_over_neg: 462.3694152832031 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.6158, loss_val: nan, pos_over_neg: 935.71435546875 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.6057, loss_val: nan, pos_over_neg: 2231.444580078125 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.6068, loss_val: nan, pos_over_neg: 5422.84912109375 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.6105, loss_val: nan, pos_over_neg: 2576.85205078125 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.5996, loss_val: nan, pos_over_neg: 1271.1363525390625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.6161, loss_val: nan, pos_over_neg: 1179.93212890625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.6118, loss_val: nan, pos_over_neg: 1626.67041015625 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.6005, loss_val: nan, pos_over_neg: 879.7781982421875 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.611, loss_val: nan, pos_over_neg: 409.2357177734375 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.6007, loss_val: nan, pos_over_neg: 706.27197265625 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.6115, loss_val: nan, pos_over_neg: 407.9765930175781 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.6087, loss_val: nan, pos_over_neg: 953.3462524414062 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.6104, loss_val: nan, pos_over_neg: 2867.513916015625 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.6061, loss_val: nan, pos_over_neg: 1885.0648193359375 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.6047, loss_val: nan, pos_over_neg: 731.4485473632812 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.6126, loss_val: nan, pos_over_neg: 696.9127807617188 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.6182, loss_val: nan, pos_over_neg: 1563.145751953125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.6002, loss_val: nan, pos_over_neg: 2148.79150390625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.6065, loss_val: nan, pos_over_neg: 1146.29833984375 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.6018, loss_val: nan, pos_over_neg: 1068.77880859375 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.6086, loss_val: nan, pos_over_neg: 2853.171630859375 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 74272.6640625 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.6054, loss_val: nan, pos_over_neg: 4755.38623046875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.6097, loss_val: nan, pos_over_neg: 2463.0341796875 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.6025, loss_val: nan, pos_over_neg: 939.6216430664062 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.6042, loss_val: nan, pos_over_neg: 1140.3209228515625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.5955, loss_val: nan, pos_over_neg: 1891.5716552734375 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.6083, loss_val: nan, pos_over_neg: 1174.4141845703125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.6031, loss_val: nan, pos_over_neg: 1482.2353515625 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.6112, loss_val: nan, pos_over_neg: 9622.998046875 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.6011, loss_val: nan, pos_over_neg: 1377.9150390625 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.6072, loss_val: nan, pos_over_neg: 1380.0404052734375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.6121, loss_val: nan, pos_over_neg: 688.4495239257812 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.6102, loss_val: nan, pos_over_neg: 732.1554565429688 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.6104, loss_val: nan, pos_over_neg: 2043.465087890625 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.6008, loss_val: nan, pos_over_neg: 1992.8916015625 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.6166, loss_val: nan, pos_over_neg: 481.20672607421875 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.6103, loss_val: nan, pos_over_neg: 752.2257080078125 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 2018.6036376953125 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.6087, loss_val: nan, pos_over_neg: 2079.0166015625 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.6242, loss_val: nan, pos_over_neg: 1404.8304443359375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.6093, loss_val: nan, pos_over_neg: 1031.0120849609375 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: -7343.2236328125 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.5955, loss_val: nan, pos_over_neg: 2088.416015625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 699.37353515625 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.6102, loss_val: nan, pos_over_neg: 954.6348266601562 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 1228.917724609375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.5991, loss_val: nan, pos_over_neg: 767.33935546875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: 1252.230712890625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.6096, loss_val: nan, pos_over_neg: 373.348876953125 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.6053, loss_val: nan, pos_over_neg: 1297.718994140625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.6017, loss_val: nan, pos_over_neg: 914.9199829101562 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 1967.59033203125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.6041, loss_val: nan, pos_over_neg: 1258.0244140625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.6033, loss_val: nan, pos_over_neg: 1342.9945068359375 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.6072, loss_val: nan, pos_over_neg: 903.1004638671875 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.6076, loss_val: nan, pos_over_neg: 2506.3154296875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.6152, loss_val: nan, pos_over_neg: 616.4945068359375 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 1017.3958740234375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.6031, loss_val: nan, pos_over_neg: 1034.07861328125 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.6154, loss_val: nan, pos_over_neg: 511.451171875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.6197, loss_val: nan, pos_over_neg: 798.2382202148438 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.6069, loss_val: nan, pos_over_neg: 988.6707153320312 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.6023, loss_val: nan, pos_over_neg: 957.4913940429688 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.626, loss_val: nan, pos_over_neg: 700.19189453125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.6091, loss_val: nan, pos_over_neg: 1371.412841796875 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.6102, loss_val: nan, pos_over_neg: 1508.29443359375 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.6116, loss_val: nan, pos_over_neg: 1325.2005615234375 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.6093, loss_val: nan, pos_over_neg: 1613.7958984375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.6099, loss_val: nan, pos_over_neg: 975.5382690429688 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.6118, loss_val: nan, pos_over_neg: 1563.721923828125 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.5999, loss_val: nan, pos_over_neg: 1007.1656494140625 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.6086, loss_val: nan, pos_over_neg: 1371.8697509765625 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.6045, loss_val: nan, pos_over_neg: 609.25830078125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.6093, loss_val: nan, pos_over_neg: 2945.129150390625 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.6064, loss_val: nan, pos_over_neg: 5967.70556640625 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.6085, loss_val: nan, pos_over_neg: 644.8970336914062 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.6077, loss_val: nan, pos_over_neg: 1108.1968994140625 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.5998, loss_val: nan, pos_over_neg: 12625.53125 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.6088, loss_val: nan, pos_over_neg: -7738.17822265625 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.6002, loss_val: nan, pos_over_neg: -4556.7373046875 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.6158, loss_val: nan, pos_over_neg: 1938.1402587890625 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.6125, loss_val: nan, pos_over_neg: 1461.891845703125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.6176, loss_val: nan, pos_over_neg: 747.115966796875 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.6039, loss_val: nan, pos_over_neg: 5657.79150390625 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.6036, loss_val: nan, pos_over_neg: 2536.0224609375 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.6099, loss_val: nan, pos_over_neg: 1349.4256591796875 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.6084, loss_val: nan, pos_over_neg: 4101.68212890625 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.5935, loss_val: nan, pos_over_neg: 2116.353515625 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.6055, loss_val: nan, pos_over_neg: 1065.808837890625 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.5926, loss_val: nan, pos_over_neg: 3591.448486328125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.6132, loss_val: nan, pos_over_neg: 913.9144897460938 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 856.8046875 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 1499.2196044921875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 1214.488525390625 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.598, loss_val: nan, pos_over_neg: 414.2903137207031 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.6003, loss_val: nan, pos_over_neg: 4927.92724609375 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.6025, loss_val: nan, pos_over_neg: 6637.9541015625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.6096, loss_val: nan, pos_over_neg: 557.5964965820312 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.6068, loss_val: nan, pos_over_neg: 941.296142578125 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.605, loss_val: nan, pos_over_neg: 2975.4619140625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 1115.1181640625 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 1510.42333984375 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.6082, loss_val: nan, pos_over_neg: 954.720947265625 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.6053, loss_val: nan, pos_over_neg: 1181.7100830078125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.6159, loss_val: nan, pos_over_neg: 459.99761962890625 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.6166, loss_val: nan, pos_over_neg: 427.8082580566406 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.6012, loss_val: nan, pos_over_neg: -3363.68115234375 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.6166, loss_val: nan, pos_over_neg: 641.6155395507812 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.6045, loss_val: nan, pos_over_neg: 2574.4775390625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.6162, loss_val: nan, pos_over_neg: 1544.8624267578125 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.6007, loss_val: nan, pos_over_neg: 7066.537109375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.6042, loss_val: nan, pos_over_neg: -903286.8125 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 1005.2637329101562 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.611, loss_val: nan, pos_over_neg: 3343.5791015625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.5999, loss_val: nan, pos_over_neg: 1432.45263671875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.6027, loss_val: nan, pos_over_neg: 1607.50537109375 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.6022, loss_val: nan, pos_over_neg: 1988.5606689453125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.6156, loss_val: nan, pos_over_neg: 959.0668334960938 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.6053, loss_val: nan, pos_over_neg: 2695.595947265625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.6041, loss_val: nan, pos_over_neg: 646.2434692382812 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 1106.5106201171875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 3314.2177734375 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 2180.8544921875 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.6137, loss_val: nan, pos_over_neg: 2548.211181640625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.6113, loss_val: nan, pos_over_neg: 793.109130859375 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.5968, loss_val: nan, pos_over_neg: 2005.4749755859375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.6123, loss_val: nan, pos_over_neg: 960.3321533203125 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.608, loss_val: nan, pos_over_neg: 2148.27587890625 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.5978, loss_val: nan, pos_over_neg: 852.4263305664062 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.6076, loss_val: nan, pos_over_neg: 1351.59033203125 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.6048, loss_val: nan, pos_over_neg: 1026.3487548828125 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.6049, loss_val: nan, pos_over_neg: 870.3054809570312 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.6146, loss_val: nan, pos_over_neg: 713.4421997070312 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.602, loss_val: nan, pos_over_neg: -23467.458984375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: 2610.234130859375 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.6006, loss_val: nan, pos_over_neg: 2659.786865234375 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.6035, loss_val: nan, pos_over_neg: 2578.622314453125 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.6082, loss_val: nan, pos_over_neg: 1413.5445556640625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.6099, loss_val: nan, pos_over_neg: 1238.376220703125 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.6016, loss_val: nan, pos_over_neg: 893.7459716796875 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.598, loss_val: nan, pos_over_neg: 2648.80517578125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.6069, loss_val: nan, pos_over_neg: 3255.1484375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.6089, loss_val: nan, pos_over_neg: 1199.7052001953125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 2856.844970703125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.6104, loss_val: nan, pos_over_neg: 1485.6986083984375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.6051, loss_val: nan, pos_over_neg: 1878.406005859375 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.5988, loss_val: nan, pos_over_neg: 1817.12353515625 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.6046, loss_val: nan, pos_over_neg: 1375.3369140625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 6131.07763671875 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.6024, loss_val: nan, pos_over_neg: 1129.962158203125 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 2304.32177734375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.6079, loss_val: nan, pos_over_neg: 713.6887817382812 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: 629.687255859375 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.6122, loss_val: nan, pos_over_neg: 1636.2576904296875 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: -191523.5625 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.6113, loss_val: nan, pos_over_neg: 572.9911499023438 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.6057, loss_val: nan, pos_over_neg: 1004.0337524414062 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.5974, loss_val: nan, pos_over_neg: 1215.769775390625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.6074, loss_val: nan, pos_over_neg: 9984.2275390625 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.606, loss_val: nan, pos_over_neg: -3747.155029296875 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.6058, loss_val: nan, pos_over_neg: 2849.0546875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.6084, loss_val: nan, pos_over_neg: 890.9888305664062 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.6092, loss_val: nan, pos_over_neg: 779.7817993164062 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 139301.453125 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.6045, loss_val: nan, pos_over_neg: 1180.2313232421875 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.6137, loss_val: nan, pos_over_neg: 431.78765869140625 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.6002, loss_val: nan, pos_over_neg: 920.1444702148438 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.6012, loss_val: nan, pos_over_neg: 673.3450927734375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.6087, loss_val: nan, pos_over_neg: 728.4133911132812 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.6069, loss_val: nan, pos_over_neg: 1384.3447265625 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.6145, loss_val: nan, pos_over_neg: 1592.9998779296875 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 844.9195556640625 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.5994, loss_val: nan, pos_over_neg: 926.2431640625 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.605, loss_val: nan, pos_over_neg: 4506.92041015625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.6061, loss_val: nan, pos_over_neg: 1877.576416015625 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.6118, loss_val: nan, pos_over_neg: 554.5328369140625 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.6101, loss_val: nan, pos_over_neg: 852.0621337890625 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 792.5475463867188 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 40972.625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 627.913330078125 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.6044, loss_val: nan, pos_over_neg: 473.6672668457031 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.6068, loss_val: nan, pos_over_neg: 856.058349609375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 779.3269653320312 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.611, loss_val: nan, pos_over_neg: 875.920654296875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.6117, loss_val: nan, pos_over_neg: 2039.67138671875 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 897.7857666015625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.6016, loss_val: nan, pos_over_neg: 1805.6939697265625 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.6127, loss_val: nan, pos_over_neg: 3776.261474609375 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.5982, loss_val: nan, pos_over_neg: 1266.4798583984375 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.6068, loss_val: nan, pos_over_neg: 1127.2186279296875 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.6005, loss_val: nan, pos_over_neg: 2265.820556640625 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.6014, loss_val: nan, pos_over_neg: 685.4948120117188 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.5996, loss_val: nan, pos_over_neg: 766.427978515625 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.6082, loss_val: nan, pos_over_neg: 498.5273132324219 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.6046, loss_val: nan, pos_over_neg: 452.75555419921875 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.6042, loss_val: nan, pos_over_neg: -17596.96484375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.6007, loss_val: nan, pos_over_neg: 10215.95703125 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.6139, loss_val: nan, pos_over_neg: 1882.812255859375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.6023, loss_val: nan, pos_over_neg: 975.2781372070312 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.6037, loss_val: nan, pos_over_neg: 1043.9356689453125 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.6054, loss_val: nan, pos_over_neg: 980.8530883789062 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: 1768.5438232421875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.6098, loss_val: nan, pos_over_neg: 4576.9189453125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.5996, loss_val: nan, pos_over_neg: 2018.8155517578125 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 1045.160888671875 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.6037, loss_val: nan, pos_over_neg: 2059.795166015625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.6097, loss_val: nan, pos_over_neg: 1543.8292236328125 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.6007, loss_val: nan, pos_over_neg: 2095.65673828125 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.61, loss_val: nan, pos_over_neg: 942.5824584960938 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.6115, loss_val: nan, pos_over_neg: 709.8319091796875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.6041, loss_val: nan, pos_over_neg: 4919.56494140625 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.5986, loss_val: nan, pos_over_neg: -2860.3310546875 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.6179, loss_val: nan, pos_over_neg: 1631.078857421875 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.6057, loss_val: nan, pos_over_neg: 2554.39111328125 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 1676.786376953125 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.5962, loss_val: nan, pos_over_neg: -1876.8892822265625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.6061, loss_val: nan, pos_over_neg: 3431.762451171875 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.6037, loss_val: nan, pos_over_neg: 2790.135009765625 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: -3560.76318359375 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.6035, loss_val: nan, pos_over_neg: 2816.0283203125 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.6048, loss_val: nan, pos_over_neg: -53297.27734375 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.6106, loss_val: nan, pos_over_neg: 755.2864379882812 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.6067, loss_val: nan, pos_over_neg: 2639.79931640625 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.6067, loss_val: nan, pos_over_neg: 1262.734130859375 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.6082, loss_val: nan, pos_over_neg: 5853.5810546875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.6012, loss_val: nan, pos_over_neg: 2949.601318359375 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.5958, loss_val: nan, pos_over_neg: -6191.8125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.5978, loss_val: nan, pos_over_neg: 1432.31689453125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.6037, loss_val: nan, pos_over_neg: 1022.2437133789062 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.6044, loss_val: nan, pos_over_neg: 2641.02978515625 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 5299.08349609375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 5832.40771484375 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: 465.95050048828125 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.6036, loss_val: nan, pos_over_neg: 815.3278198242188 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.6243, loss_val: nan, pos_over_neg: 918.216552734375 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.5958, loss_val: nan, pos_over_neg: 6668.091796875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.6006, loss_val: nan, pos_over_neg: 1843.038330078125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.612, loss_val: nan, pos_over_neg: 1006.3736572265625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.6026, loss_val: nan, pos_over_neg: 8036.5380859375 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 1086.582275390625 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 1211.747802734375 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 3819.972412109375 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.6064, loss_val: nan, pos_over_neg: 1385.7489013671875 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 3879.8642578125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 653.9271240234375 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.6108, loss_val: nan, pos_over_neg: 809.012939453125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.6004, loss_val: nan, pos_over_neg: 865.4205932617188 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.6148, loss_val: nan, pos_over_neg: 610.131591796875 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.6002, loss_val: nan, pos_over_neg: 674.2381591796875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.5992, loss_val: nan, pos_over_neg: 8650.1845703125 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.6032, loss_val: nan, pos_over_neg: 821.8487548828125 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: 790.5612182617188 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.6053, loss_val: nan, pos_over_neg: 1069.336181640625 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.6006, loss_val: nan, pos_over_neg: 628.870849609375 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.602, loss_val: nan, pos_over_neg: 1618.8953857421875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: 4375.43359375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 1031.659912109375 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.6055, loss_val: nan, pos_over_neg: 477.1951599121094 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.6055, loss_val: nan, pos_over_neg: 533.0753784179688 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.6088, loss_val: nan, pos_over_neg: 1278.6903076171875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.5882, loss_val: nan, pos_over_neg: -3583.248291015625 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.6046, loss_val: nan, pos_over_neg: 1700.7724609375 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.6057, loss_val: nan, pos_over_neg: 660.4049072265625 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.5975, loss_val: nan, pos_over_neg: 1301.6893310546875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.5965, loss_val: nan, pos_over_neg: 828.577392578125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.605, loss_val: nan, pos_over_neg: 1054.62060546875 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 758.7302856445312 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.6091, loss_val: nan, pos_over_neg: 529.9614868164062 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.5984, loss_val: nan, pos_over_neg: 617.385986328125 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.6053, loss_val: nan, pos_over_neg: 736.0482788085938 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.6157, loss_val: nan, pos_over_neg: 459.4626770019531 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.6082, loss_val: nan, pos_over_neg: 587.4990844726562 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.5943, loss_val: nan, pos_over_neg: 853.06103515625 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 724.4097900390625 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 750.1560668945312 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 757.0223388671875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.6026, loss_val: nan, pos_over_neg: 658.7421875 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.5996, loss_val: nan, pos_over_neg: 566.7728271484375 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.6067, loss_val: nan, pos_over_neg: 1118.8388671875 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.6029, loss_val: nan, pos_over_neg: 577.8563232421875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.6015, loss_val: nan, pos_over_neg: 711.389404296875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.6027, loss_val: nan, pos_over_neg: 775.7234497070312 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.6012, loss_val: nan, pos_over_neg: 1344.5933837890625 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.6031, loss_val: nan, pos_over_neg: 1137.749755859375 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.5994, loss_val: nan, pos_over_neg: 2964.33349609375 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.6128, loss_val: nan, pos_over_neg: 561.1735229492188 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.6007, loss_val: nan, pos_over_neg: 774.1156005859375 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: 910.3667602539062 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 1113.5552978515625 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.6124, loss_val: nan, pos_over_neg: 594.1905517578125 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.6077, loss_val: nan, pos_over_neg: 514.0121459960938 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 762.2615966796875 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.6068, loss_val: nan, pos_over_neg: 822.4486694335938 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.6013, loss_val: nan, pos_over_neg: 586.6573486328125 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 2864.05419921875 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.5982, loss_val: nan, pos_over_neg: 1389.5750732421875 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.5956, loss_val: nan, pos_over_neg: 1272.53125 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.6051, loss_val: nan, pos_over_neg: 838.7939453125 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.5955, loss_val: nan, pos_over_neg: 612.86669921875 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.607, loss_val: nan, pos_over_neg: 326.2846374511719 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.6028, loss_val: nan, pos_over_neg: 1157.0626220703125 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.6093, loss_val: nan, pos_over_neg: 807.9631958007812 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 1832.262451171875 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 1178.9681396484375 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.6043, loss_val: nan, pos_over_neg: 3138.923095703125 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.611, loss_val: nan, pos_over_neg: 1568.458984375 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.5944, loss_val: nan, pos_over_neg: 3305.30859375 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 17272.083984375 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.5951, loss_val: nan, pos_over_neg: 922.3714599609375 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.5966, loss_val: nan, pos_over_neg: 2608.6162109375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.5943, loss_val: nan, pos_over_neg: 831.45556640625 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.6016, loss_val: nan, pos_over_neg: 662.8892822265625 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.5921, loss_val: nan, pos_over_neg: 909.377685546875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.6064, loss_val: nan, pos_over_neg: 1050.0091552734375 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.5944, loss_val: nan, pos_over_neg: 942.3284912109375 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.6087, loss_val: nan, pos_over_neg: 6379.51806640625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.5984, loss_val: nan, pos_over_neg: 1438.2462158203125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.6016, loss_val: nan, pos_over_neg: 1437.5986328125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: 1645.4156494140625 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 1999.2537841796875 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 2298.233642578125 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.6, loss_val: nan, pos_over_neg: 1415.698486328125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 1165.896484375 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.6001, loss_val: nan, pos_over_neg: 1190.956298828125 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 2236.59814453125 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.595, loss_val: nan, pos_over_neg: 1052.35400390625 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.5941, loss_val: nan, pos_over_neg: 2292.297119140625 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.6001, loss_val: nan, pos_over_neg: 4988.35009765625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.5961, loss_val: nan, pos_over_neg: 11141.869140625 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.6039, loss_val: nan, pos_over_neg: 780.81884765625 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.6069, loss_val: nan, pos_over_neg: 918.2070922851562 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.6147, loss_val: nan, pos_over_neg: 1943.3172607421875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.6061, loss_val: nan, pos_over_neg: 1240.8121337890625 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.6054, loss_val: nan, pos_over_neg: 729.3240966796875 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.6033, loss_val: nan, pos_over_neg: 1436.662841796875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.6083, loss_val: nan, pos_over_neg: 1503.1878662109375 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.6032, loss_val: nan, pos_over_neg: 1278.8848876953125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.602, loss_val: nan, pos_over_neg: 1518.70166015625 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.5982, loss_val: nan, pos_over_neg: 952.7733764648438 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.6018, loss_val: nan, pos_over_neg: 1923.194091796875 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.6007, loss_val: nan, pos_over_neg: 2319.113037109375 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 1335.971435546875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.6054, loss_val: nan, pos_over_neg: 939.2643432617188 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 760.31298828125 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.603, loss_val: nan, pos_over_neg: 723.646728515625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.6035, loss_val: nan, pos_over_neg: 1577.8492431640625 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.5966, loss_val: nan, pos_over_neg: 1737.7509765625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.605, loss_val: nan, pos_over_neg: 603.9611206054688 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.6001, loss_val: nan, pos_over_neg: 1065.969482421875 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: 918.9237060546875 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: 1673.9765625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.6092, loss_val: nan, pos_over_neg: 1103.8248291015625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.6125, loss_val: nan, pos_over_neg: 755.4102783203125 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.5965, loss_val: nan, pos_over_neg: 751.4521484375 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 656.7474365234375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.5991, loss_val: nan, pos_over_neg: 1426.0391845703125 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.6042, loss_val: nan, pos_over_neg: 1518.5084228515625 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: 1525.5430908203125 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 1401.0145263671875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: 1081.8214111328125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 1175.762939453125 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.6076, loss_val: nan, pos_over_neg: 3000.47314453125 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.5989, loss_val: nan, pos_over_neg: 721.6963500976562 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.5991, loss_val: nan, pos_over_neg: 2074.36083984375 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 582.1707763671875 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.5945, loss_val: nan, pos_over_neg: 2180.07421875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.5988, loss_val: nan, pos_over_neg: 886.0953979492188 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.5989, loss_val: nan, pos_over_neg: 890.9320068359375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.6047, loss_val: nan, pos_over_neg: 989.2147216796875 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.5926, loss_val: nan, pos_over_neg: 785.3026123046875 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: 2353.365478515625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.6078, loss_val: nan, pos_over_neg: 1021.808349609375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.5994, loss_val: nan, pos_over_neg: 2605.253173828125 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.6018, loss_val: nan, pos_over_neg: 900.1475219726562 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 1604.6466064453125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 699.6590576171875 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.6026, loss_val: nan, pos_over_neg: 1333.90380859375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 2235.286865234375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.5991, loss_val: nan, pos_over_neg: 2169.5087890625 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.6072, loss_val: nan, pos_over_neg: 832.7332153320312 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.5971, loss_val: nan, pos_over_neg: 2493.35302734375 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 2120.92822265625 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.6072, loss_val: nan, pos_over_neg: 713.2449951171875 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.5986, loss_val: nan, pos_over_neg: 1403.4093017578125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.6015, loss_val: nan, pos_over_neg: -3873.181640625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.5908, loss_val: nan, pos_over_neg: 1375.202880859375 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 703.0243530273438 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 3875.407470703125 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.6012, loss_val: nan, pos_over_neg: 855.1781005859375 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.5971, loss_val: nan, pos_over_neg: 2803.215087890625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.6059, loss_val: nan, pos_over_neg: -4736.30224609375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.5971, loss_val: nan, pos_over_neg: 905.6548461914062 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.6032, loss_val: nan, pos_over_neg: 1423.8201904296875 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.6021, loss_val: nan, pos_over_neg: 973.7182006835938 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.6025, loss_val: nan, pos_over_neg: 1070.99560546875 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.5988, loss_val: nan, pos_over_neg: 1184.765869140625 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: 1690.693603515625 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.6024, loss_val: nan, pos_over_neg: 3943.1240234375 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.602, loss_val: nan, pos_over_neg: 1658.8035888671875 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 1545.9931640625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.5976, loss_val: nan, pos_over_neg: 3517.3466796875 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 9194.1689453125 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: -21506.32421875 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.6026, loss_val: nan, pos_over_neg: 1546.9593505859375 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.6004, loss_val: nan, pos_over_neg: 984.29150390625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 28233.048828125 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 1730.843017578125 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 2100.0234375 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.6025, loss_val: nan, pos_over_neg: 728.1530151367188 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.5948, loss_val: nan, pos_over_neg: 1814.677001953125 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.6114, loss_val: nan, pos_over_neg: 5468.484375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 4390.4677734375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.595, loss_val: nan, pos_over_neg: 914.6734619140625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.596, loss_val: nan, pos_over_neg: 1822.345947265625 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.6115, loss_val: nan, pos_over_neg: 1334.73828125 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.6081, loss_val: nan, pos_over_neg: 596.8662719726562 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 3021.011474609375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.6132, loss_val: nan, pos_over_neg: 855.0287475585938 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.599, loss_val: nan, pos_over_neg: -16021.78515625 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 685281.3125 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: -8210.0791015625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.5943, loss_val: nan, pos_over_neg: -10630.2294921875 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.5966, loss_val: nan, pos_over_neg: 3125.449462890625 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.602, loss_val: nan, pos_over_neg: 10442.5478515625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.5914, loss_val: nan, pos_over_neg: 5293.28857421875 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.5957, loss_val: nan, pos_over_neg: 1730.0169677734375 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.5965, loss_val: nan, pos_over_neg: 2932.26611328125 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.6061, loss_val: nan, pos_over_neg: 1200.80615234375 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.6032, loss_val: nan, pos_over_neg: 2807.613037109375 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.5928, loss_val: nan, pos_over_neg: 2066.884033203125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.5957, loss_val: nan, pos_over_neg: 3167.53857421875 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.5968, loss_val: nan, pos_over_neg: 2666.926025390625 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.6017, loss_val: nan, pos_over_neg: 1198.947021484375 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.6047, loss_val: nan, pos_over_neg: 1766.0093994140625 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.6088, loss_val: nan, pos_over_neg: 971.7784423828125 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.596, loss_val: nan, pos_over_neg: 769.4481201171875 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 2611.119873046875 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.6014, loss_val: nan, pos_over_neg: 1354.202880859375 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: -17395.890625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.6089, loss_val: nan, pos_over_neg: 1027.7510986328125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.6017, loss_val: nan, pos_over_neg: 823.4967651367188 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.6026, loss_val: nan, pos_over_neg: 1387.146484375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.5989, loss_val: nan, pos_over_neg: 1203.7080078125 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.5995, loss_val: nan, pos_over_neg: 2904.41748046875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.5996, loss_val: nan, pos_over_neg: 1321.1588134765625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.6013, loss_val: nan, pos_over_neg: 1477.7255859375 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 1139.003173828125 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: 3017.603515625 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 634.9085693359375 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.5995, loss_val: nan, pos_over_neg: 1105.582275390625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 1492.483154296875 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.5984, loss_val: nan, pos_over_neg: 582.8922119140625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.5921, loss_val: nan, pos_over_neg: 693.0502319335938 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 920.7596435546875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.6065, loss_val: nan, pos_over_neg: 536.1053466796875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.5982, loss_val: nan, pos_over_neg: 7165.48876953125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.6027, loss_val: nan, pos_over_neg: 1122.6339111328125 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.6032, loss_val: nan, pos_over_neg: 3485.017578125 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: 6536.2724609375 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.5941, loss_val: nan, pos_over_neg: 1691.1064453125 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.5974, loss_val: nan, pos_over_neg: 1143.4862060546875 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.6027, loss_val: nan, pos_over_neg: 791.01806640625 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 645.0220336914062 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.5962, loss_val: nan, pos_over_neg: 3268.3056640625 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: 2887.604248046875 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 3063.612548828125 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.6017, loss_val: nan, pos_over_neg: 2535.15087890625 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 4889.7666015625 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.5793, loss_val: nan, pos_over_neg: 3023.140625 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.5915, loss_val: nan, pos_over_neg: -52438.26171875 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: -5462.7109375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.6069, loss_val: nan, pos_over_neg: 8273.1650390625 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.6103, loss_val: nan, pos_over_neg: 1032.9573974609375 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: -4652.37109375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.6085, loss_val: nan, pos_over_neg: 922.76953125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.6054, loss_val: nan, pos_over_neg: 759.5188598632812 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 992.7206420898438 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 2052.2734375 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 4829.740234375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.6018, loss_val: nan, pos_over_neg: 10471.9873046875 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.6093, loss_val: nan, pos_over_neg: 912.4645385742188 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.6005, loss_val: nan, pos_over_neg: 1845.9537353515625 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 1324.0250244140625 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.599, loss_val: nan, pos_over_neg: 656.2780151367188 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.6019, loss_val: nan, pos_over_neg: 677.4925537109375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.5981, loss_val: nan, pos_over_neg: 1424.4620361328125 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.6034, loss_val: nan, pos_over_neg: 952.2313842773438 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.6004, loss_val: nan, pos_over_neg: 1353.16796875 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 1505.8194580078125 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.6131, loss_val: nan, pos_over_neg: 1352.0587158203125 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.6031, loss_val: nan, pos_over_neg: 668.0288696289062 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 1233.7269287109375 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.6001, loss_val: nan, pos_over_neg: 2321.177490234375 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.5932, loss_val: nan, pos_over_neg: 2312.43798828125 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 542.1559448242188 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 826.899658203125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.5825, loss_val: nan, pos_over_neg: 1786.0982666015625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.5922, loss_val: nan, pos_over_neg: 965.09765625 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.6008, loss_val: nan, pos_over_neg: 507.7876281738281 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.6003, loss_val: nan, pos_over_neg: 912.4337768554688 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 3001.13330078125 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.604, loss_val: nan, pos_over_neg: 2755.2138671875 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 1419.71875 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.5881, loss_val: nan, pos_over_neg: 6865.9111328125 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.6101, loss_val: nan, pos_over_neg: 410.4950866699219 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.6055, loss_val: nan, pos_over_neg: 2086.875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.5928, loss_val: nan, pos_over_neg: 4672.89990234375 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.5978, loss_val: nan, pos_over_neg: -11885.9541015625 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.598, loss_val: nan, pos_over_neg: 1478.20556640625 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.6027, loss_val: nan, pos_over_neg: 640.9500732421875 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.6042, loss_val: nan, pos_over_neg: 1143.2979736328125 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.6128, loss_val: nan, pos_over_neg: 913.697021484375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.6125, loss_val: nan, pos_over_neg: 620.0792846679688 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.5988, loss_val: nan, pos_over_neg: 1865.6513671875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.6023, loss_val: nan, pos_over_neg: 880.8587646484375 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.6076, loss_val: nan, pos_over_neg: 3017.39453125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: 1182.5699462890625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 778.621826171875 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.5953, loss_val: nan, pos_over_neg: 1232.54150390625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 4780.31884765625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.598, loss_val: nan, pos_over_neg: 3053.60107421875 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 1448.9976806640625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 2010.459716796875 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.5935, loss_val: nan, pos_over_neg: 1702.3599853515625 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.5942, loss_val: nan, pos_over_neg: 16048.8720703125 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.5956, loss_val: nan, pos_over_neg: 593.2698364257812 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.5936, loss_val: nan, pos_over_neg: 707.3875732421875 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.5945, loss_val: nan, pos_over_neg: 2763.48974609375 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.6045, loss_val: nan, pos_over_neg: 516.4976196289062 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 987.535888671875 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 1921.375732421875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.6096, loss_val: nan, pos_over_neg: 812.4447021484375 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.6072, loss_val: nan, pos_over_neg: 1571.6771240234375 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.5957, loss_val: nan, pos_over_neg: 1417.2911376953125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 673.7246704101562 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.607, loss_val: nan, pos_over_neg: 886.9004516601562 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.598, loss_val: nan, pos_over_neg: 1599.9945068359375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 671.0921020507812 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.6026, loss_val: nan, pos_over_neg: 950.9199829101562 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.6066, loss_val: nan, pos_over_neg: 4966.36767578125 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.6046, loss_val: nan, pos_over_neg: 921.4493408203125 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 1390.511962890625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.6041, loss_val: nan, pos_over_neg: 441.8941955566406 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.6026, loss_val: nan, pos_over_neg: 807.0825805664062 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 2241.263671875 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.5953, loss_val: nan, pos_over_neg: 674.1544189453125 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: 1065.466064453125 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 2548.435546875 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: 4559.95947265625 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: 4946.65234375 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 3126.10791015625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: 1574.8238525390625 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 3270.0400390625 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.6028, loss_val: nan, pos_over_neg: 1210.1871337890625 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 2579.24658203125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.6053, loss_val: nan, pos_over_neg: 700.1488647460938 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.5906, loss_val: nan, pos_over_neg: 1791.285888671875 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.5976, loss_val: nan, pos_over_neg: 740.0634765625 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.6083, loss_val: nan, pos_over_neg: 792.2327880859375 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 11313.455078125 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: 2490.0615234375 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.5986, loss_val: nan, pos_over_neg: 1522.9820556640625 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.6042, loss_val: nan, pos_over_neg: 7856.8662109375 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 2594.705322265625 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.5975, loss_val: nan, pos_over_neg: 1330.0343017578125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.6096, loss_val: nan, pos_over_neg: 962.4380493164062 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 841.6680297851562 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.5994, loss_val: nan, pos_over_neg: 2110.357421875 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 1772.3563232421875 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.6014, loss_val: nan, pos_over_neg: 1759.149169921875 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 17981.986328125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.5966, loss_val: nan, pos_over_neg: 812.3563842773438 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 1040.33203125 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: 9519.310546875 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 2129.845458984375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.5951, loss_val: nan, pos_over_neg: 2471.853759765625 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 1995.3714599609375 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.6023, loss_val: nan, pos_over_neg: 1172.7625732421875 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.6009, loss_val: nan, pos_over_neg: 2446.619873046875 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.597, loss_val: nan, pos_over_neg: 4571.44287109375 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.5962, loss_val: nan, pos_over_neg: 2284.63427734375 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.6071, loss_val: nan, pos_over_neg: 671.2562255859375 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.5943, loss_val: nan, pos_over_neg: 1671.2857666015625 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.596, loss_val: nan, pos_over_neg: 1318.424560546875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.589, loss_val: nan, pos_over_neg: 598.722412109375 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.6024, loss_val: nan, pos_over_neg: 2021.572265625 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.596, loss_val: nan, pos_over_neg: 2449.798828125 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.6054, loss_val: nan, pos_over_neg: 956.8116455078125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.5995, loss_val: nan, pos_over_neg: 1883.6783447265625 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 2071.95751953125 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 1303.6138916015625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: 2017.5836181640625 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 1974.0325927734375 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.5908, loss_val: nan, pos_over_neg: 1735.9766845703125 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 1302.38623046875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 923.6400756835938 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 750.3344116210938 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 991.4400024414062 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.5869, loss_val: nan, pos_over_neg: 717.7916259765625 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.5975, loss_val: nan, pos_over_neg: 789.9307861328125 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.603, loss_val: nan, pos_over_neg: 412.658203125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.5964, loss_val: nan, pos_over_neg: 738.497314453125 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 1011.9891357421875 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.6122, loss_val: nan, pos_over_neg: 3196.166748046875 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.5944, loss_val: nan, pos_over_neg: 1718.3465576171875 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.5992, loss_val: nan, pos_over_neg: 1091.984375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.5958, loss_val: nan, pos_over_neg: 1366.9246826171875 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.6032, loss_val: nan, pos_over_neg: 59151.02734375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.6114, loss_val: nan, pos_over_neg: 819.29052734375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 993.27001953125 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 452.0328063964844 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.6021, loss_val: nan, pos_over_neg: 1008.5558471679688 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 441.1325988769531 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.599, loss_val: nan, pos_over_neg: 2990.88134765625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.6082, loss_val: nan, pos_over_neg: 672.2313232421875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: 381.58343505859375 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.5921, loss_val: nan, pos_over_neg: 1363.8494873046875 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 2246.794677734375 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.6016, loss_val: nan, pos_over_neg: 2571.362060546875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.6005, loss_val: nan, pos_over_neg: 598.3124389648438 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.594, loss_val: nan, pos_over_neg: 880.161865234375 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.6102, loss_val: nan, pos_over_neg: 999.6712646484375 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.6002, loss_val: nan, pos_over_neg: 1263.18798828125 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.5941, loss_val: nan, pos_over_neg: 774.6221313476562 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.5984, loss_val: nan, pos_over_neg: 832.2328491210938 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 1627.3331298828125 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.6063, loss_val: nan, pos_over_neg: 773.6130981445312 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.6002, loss_val: nan, pos_over_neg: 478.58514404296875 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.5881, loss_val: nan, pos_over_neg: -11628.6572265625 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.5958, loss_val: nan, pos_over_neg: 7601.76513671875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.592, loss_val: nan, pos_over_neg: 1006.6566162109375 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.5907, loss_val: nan, pos_over_neg: 1493.8814697265625 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.5978, loss_val: nan, pos_over_neg: 824.667236328125 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 2546.135986328125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.6085, loss_val: nan, pos_over_neg: 830.7693481445312 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.6004, loss_val: nan, pos_over_neg: 648.2316284179688 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 702.3245239257812 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.6035, loss_val: nan, pos_over_neg: 997.726806640625 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.5869, loss_val: nan, pos_over_neg: 3504.462158203125 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.5869, loss_val: nan, pos_over_neg: 3220.9443359375 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 992.8956909179688 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.592, loss_val: nan, pos_over_neg: 1435.0958251953125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.6008, loss_val: nan, pos_over_neg: 1250.501953125 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 2072.7685546875 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 4489.3203125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 2259.561767578125 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.5992, loss_val: nan, pos_over_neg: 1635.298828125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.5887, loss_val: nan, pos_over_neg: 1022.145263671875 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.6028, loss_val: nan, pos_over_neg: 728.6982421875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.5984, loss_val: nan, pos_over_neg: 4556.93994140625 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.6072, loss_val: nan, pos_over_neg: 416.9803771972656 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.6086, loss_val: nan, pos_over_neg: 391.2614440917969 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.5934, loss_val: nan, pos_over_neg: 2415.796875 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.5909, loss_val: nan, pos_over_neg: 1334.3021240234375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: 1312.5921630859375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.5947, loss_val: nan, pos_over_neg: 4492.92724609375 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.5903, loss_val: nan, pos_over_neg: 2434.822998046875 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.5934, loss_val: nan, pos_over_neg: 1396.7767333984375 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 1594.65087890625 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.5978, loss_val: nan, pos_over_neg: 2219.447021484375 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.6006, loss_val: nan, pos_over_neg: 1557.6241455078125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.5921, loss_val: nan, pos_over_neg: 938.3637084960938 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.5863, loss_val: nan, pos_over_neg: 1497.0185546875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.5919, loss_val: nan, pos_over_neg: 532.9194946289062 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.5914, loss_val: nan, pos_over_neg: 816.806396484375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.5976, loss_val: nan, pos_over_neg: 601.9293823242188 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.5945, loss_val: nan, pos_over_neg: -10827.2578125 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 85222.3515625 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 2714.922119140625 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.6033, loss_val: nan, pos_over_neg: 1063.014892578125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.5887, loss_val: nan, pos_over_neg: 1113.5283203125 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.601, loss_val: nan, pos_over_neg: 1028.492919921875 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.609, loss_val: nan, pos_over_neg: 981.7362670898438 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.5879, loss_val: nan, pos_over_neg: -11735.2529296875 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.6009, loss_val: nan, pos_over_neg: 1418.84228515625 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 458.442138671875 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.6116, loss_val: nan, pos_over_neg: 1297.5472412109375 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.5975, loss_val: nan, pos_over_neg: 2106.509765625 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.5976, loss_val: nan, pos_over_neg: -3769.476806640625 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 2622.90283203125 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.5988, loss_val: nan, pos_over_neg: 2273.298828125 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 763.4569702148438 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.6001, loss_val: nan, pos_over_neg: 745.9481201171875 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.5966, loss_val: nan, pos_over_neg: 1185.93994140625 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.5934, loss_val: nan, pos_over_neg: 1299.5076904296875 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 614.9116821289062 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.6019, loss_val: nan, pos_over_neg: 2143.026611328125 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 910.9476318359375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.5907, loss_val: nan, pos_over_neg: 1320.240966796875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 856.0047607421875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.5833, loss_val: nan, pos_over_neg: 679.8202514648438 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 1402.2708740234375 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 517.4248046875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 2949.128173828125 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.5907, loss_val: nan, pos_over_neg: 600.5761108398438 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 486.08697509765625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 1641.74658203125 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.5998, loss_val: nan, pos_over_neg: 647.7815551757812 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.5935, loss_val: nan, pos_over_neg: 2592.5185546875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.5942, loss_val: nan, pos_over_neg: 5757.27734375 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 856.4273681640625 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 1258.289306640625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.5979, loss_val: nan, pos_over_neg: 1065.1995849609375 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 890.8111572265625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.5819, loss_val: nan, pos_over_neg: 2177.996826171875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 635.2830810546875 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.5935, loss_val: nan, pos_over_neg: 1034.65673828125 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.5941, loss_val: nan, pos_over_neg: 611.350341796875 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 742.2424926757812 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.5944, loss_val: nan, pos_over_neg: 1402.08154296875 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 689.5169067382812 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.597, loss_val: nan, pos_over_neg: 1633.835205078125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 1158.1884765625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: 1272.0555419921875 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 1075.32080078125 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.6067, loss_val: nan, pos_over_neg: 1026.9150390625 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.6023, loss_val: nan, pos_over_neg: 929.516357421875 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 777.4249877929688 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.5927, loss_val: nan, pos_over_neg: 1029.574951171875 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: 1391.444580078125 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.5919, loss_val: nan, pos_over_neg: 952.9931640625 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 711.9783935546875 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.6044, loss_val: nan, pos_over_neg: 2402.614501953125 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 898.4530639648438 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 2585.17041015625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 1268.353271484375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 2032.5775146484375 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 1509.953857421875 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.5879, loss_val: nan, pos_over_neg: 1159.162353515625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:51:17<185184:37:32, 2222.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 910.777099609375 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.592, loss_val: nan, pos_over_neg: 1901.3179931640625 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.5929, loss_val: nan, pos_over_neg: 699.388427734375 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.5908, loss_val: nan, pos_over_neg: 1543.68798828125 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 4394.8759765625 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.5928, loss_val: nan, pos_over_neg: 1712.730224609375 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 1062.359619140625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: -3636.27392578125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 4411.93603515625 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 2237.833984375 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 1672.2613525390625 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.5989, loss_val: nan, pos_over_neg: 1387.0947265625 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.5948, loss_val: nan, pos_over_neg: 2582.5283203125 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 4252.0966796875 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.596, loss_val: nan, pos_over_neg: 2033.5078125 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.5905, loss_val: nan, pos_over_neg: -5904.0751953125 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.6052, loss_val: nan, pos_over_neg: 960.424072265625 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 16627.05078125 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 2539.892333984375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.6004, loss_val: nan, pos_over_neg: 2489.568359375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 11360.63671875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 799.420654296875 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 969.1692504882812 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 1977.4273681640625 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 825.5076293945312 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 968.5534057617188 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.599, loss_val: nan, pos_over_neg: 632.33642578125 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: -19626.0234375 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 1822.4312744140625 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 1931.7059326171875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.5877, loss_val: nan, pos_over_neg: 1704.0206298828125 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.5942, loss_val: nan, pos_over_neg: 718.05810546875 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.6012, loss_val: nan, pos_over_neg: 2881.912353515625 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.6005, loss_val: nan, pos_over_neg: 907.2752075195312 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 2546.531005859375 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.5964, loss_val: nan, pos_over_neg: 1110.1964111328125 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.6006, loss_val: nan, pos_over_neg: 756.965087890625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.5821, loss_val: nan, pos_over_neg: 1153.715087890625 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.6001, loss_val: nan, pos_over_neg: 554.2767333984375 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.6016, loss_val: nan, pos_over_neg: 663.0842895507812 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 529.5698852539062 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.5978, loss_val: nan, pos_over_neg: 1046.8551025390625 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.5921, loss_val: nan, pos_over_neg: 598.840087890625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.5999, loss_val: nan, pos_over_neg: 685.5702514648438 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: 1318.2484130859375 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: 2541.1240234375 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.5854, loss_val: nan, pos_over_neg: -30897.623046875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.606, loss_val: nan, pos_over_neg: 4598.9091796875 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.6004, loss_val: nan, pos_over_neg: 1353.2274169921875 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.5903, loss_val: nan, pos_over_neg: 1612.11376953125 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 1145.759765625 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: -5099.9248046875 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 3232.946533203125 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.5877, loss_val: nan, pos_over_neg: 1346.0196533203125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 713.5772094726562 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.5974, loss_val: nan, pos_over_neg: 688.6986694335938 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.5866, loss_val: nan, pos_over_neg: 2298.94677734375 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.5924, loss_val: nan, pos_over_neg: 1970.7630615234375 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.5883, loss_val: nan, pos_over_neg: 1314.2537841796875 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.5921, loss_val: nan, pos_over_neg: 1221.4097900390625 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.5984, loss_val: nan, pos_over_neg: 582.0375366210938 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.5959, loss_val: nan, pos_over_neg: 933.4913940429688 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.5959, loss_val: nan, pos_over_neg: 570.1412963867188 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.5906, loss_val: nan, pos_over_neg: 2104.36181640625 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.6, loss_val: nan, pos_over_neg: 719.5901489257812 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 839.097412109375 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.5945, loss_val: nan, pos_over_neg: 743.5028076171875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.6002, loss_val: nan, pos_over_neg: 1184.32470703125 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.5922, loss_val: nan, pos_over_neg: 1300.5250244140625 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.5998, loss_val: nan, pos_over_neg: 996.4888916015625 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 2080.611572265625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 1368.2513427734375 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 2043.34765625 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: -5994.357421875 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 1403.54931640625 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: 1442.4249267578125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 927.0457153320312 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 1041.73828125 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: 989.2920532226562 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 1750.2711181640625 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 1728.170166015625 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 12338.4140625 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.5833, loss_val: nan, pos_over_neg: 2165.79443359375 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.5931, loss_val: nan, pos_over_neg: 993.012451171875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.5956, loss_val: nan, pos_over_neg: 2901.0888671875 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.6023, loss_val: nan, pos_over_neg: 743.926025390625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: -9164.9853515625 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.6073, loss_val: nan, pos_over_neg: 8090.703125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 1460.6265869140625 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 1133.17138671875 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 1716.6060791015625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.5924, loss_val: nan, pos_over_neg: 730.3994140625 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 1509.4532470703125 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 23634.232421875 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: -7451.18896484375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.5992, loss_val: nan, pos_over_neg: 939.0625 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.5964, loss_val: nan, pos_over_neg: 669.1360473632812 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 7762.42919921875 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 1319.7076416015625 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.6, loss_val: nan, pos_over_neg: 3423.458740234375 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.6032, loss_val: nan, pos_over_neg: 447.1555480957031 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 1998.1097412109375 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.5928, loss_val: nan, pos_over_neg: 1279.2978515625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.5905, loss_val: nan, pos_over_neg: 1374.900634765625 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.5964, loss_val: nan, pos_over_neg: 862.6228637695312 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.5919, loss_val: nan, pos_over_neg: 1412.917236328125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.5947, loss_val: nan, pos_over_neg: 677.17041015625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.5877, loss_val: nan, pos_over_neg: 1616.806396484375 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.5856, loss_val: nan, pos_over_neg: 2004.221435546875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.594, loss_val: nan, pos_over_neg: 814.7142944335938 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 1080.8074951171875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.5903, loss_val: nan, pos_over_neg: 1392.5399169921875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.5937, loss_val: nan, pos_over_neg: -6235.05712890625 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 951.8856201171875 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.5929, loss_val: nan, pos_over_neg: 714.3399658203125 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 6583.98974609375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 1697.8673095703125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 1657.162353515625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.5934, loss_val: nan, pos_over_neg: 963.13330078125 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.5941, loss_val: nan, pos_over_neg: 1700.73828125 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 1295.650634765625 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.5905, loss_val: nan, pos_over_neg: 8440.9912109375 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.5924, loss_val: nan, pos_over_neg: 737.7195434570312 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 1930.6319580078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 1366.3817138671875 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 852.5037841796875 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.5959, loss_val: nan, pos_over_neg: 891.1867065429688 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.5856, loss_val: nan, pos_over_neg: -38747.546875 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.5937, loss_val: nan, pos_over_neg: 1287.7177734375 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.5919, loss_val: nan, pos_over_neg: 2211.977294921875 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 771.8880004882812 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.5954, loss_val: nan, pos_over_neg: -10179.3671875 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.5863, loss_val: nan, pos_over_neg: 1418.85693359375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 1050.7052001953125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.5994, loss_val: nan, pos_over_neg: 693.0997924804688 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.6056, loss_val: nan, pos_over_neg: 429.7069091796875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 1428.427001953125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 2212.488037109375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.5944, loss_val: nan, pos_over_neg: 10272.306640625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 3470.008544921875 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 2041.07666015625 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.5996, loss_val: nan, pos_over_neg: 1622.985107421875 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.5954, loss_val: nan, pos_over_neg: 902.7786254882812 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.592, loss_val: nan, pos_over_neg: 1019.1148071289062 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 1627.9981689453125 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.5953, loss_val: nan, pos_over_neg: 1000.8431396484375 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.592, loss_val: nan, pos_over_neg: 416.6256408691406 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.599, loss_val: nan, pos_over_neg: 520.9270629882812 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 477.7742614746094 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 855.1287841796875 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.5951, loss_val: nan, pos_over_neg: 1102.9305419921875 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: -2831.261474609375 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 1619.0194091796875 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.5887, loss_val: nan, pos_over_neg: 493.71136474609375 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.5966, loss_val: nan, pos_over_neg: 2596.5224609375 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.5833, loss_val: nan, pos_over_neg: -6592.9853515625 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 557.1754150390625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.5924, loss_val: nan, pos_over_neg: 497.4302978515625 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.5908, loss_val: nan, pos_over_neg: 742.0506591796875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 1460.807373046875 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.5907, loss_val: nan, pos_over_neg: 671.2823486328125 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 1482.0250244140625 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.5981, loss_val: nan, pos_over_neg: 4367.24755859375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.5977, loss_val: nan, pos_over_neg: 29107.140625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.5975, loss_val: nan, pos_over_neg: 976.9343872070312 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.5948, loss_val: nan, pos_over_neg: 8544.23046875 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.5914, loss_val: nan, pos_over_neg: 5399273.0 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.5954, loss_val: nan, pos_over_neg: 1599.28076171875 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 1862.5789794921875 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.5991, loss_val: nan, pos_over_neg: 628.479248046875 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 3588.869873046875 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.5916, loss_val: nan, pos_over_neg: 1201.542724609375 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 676.2933349609375 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.587, loss_val: nan, pos_over_neg: 15053.3896484375 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.5954, loss_val: nan, pos_over_neg: 9870.9345703125 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: -3687.06884765625 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.6029, loss_val: nan, pos_over_neg: 1900.1400146484375 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.5844, loss_val: nan, pos_over_neg: 2565.170654296875 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 1671.9400634765625 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.598, loss_val: nan, pos_over_neg: 1354.6734619140625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.5962, loss_val: nan, pos_over_neg: 1418.8804931640625 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.5871, loss_val: nan, pos_over_neg: 1058.7550048828125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 528.0160522460938 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 4529.31005859375 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.5932, loss_val: nan, pos_over_neg: 2099.0927734375 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.5956, loss_val: nan, pos_over_neg: 777.152099609375 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.5999, loss_val: nan, pos_over_neg: 1551.689208984375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.5865, loss_val: nan, pos_over_neg: -100128.6875 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.5895, loss_val: nan, pos_over_neg: 37135.9921875 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.5943, loss_val: nan, pos_over_neg: 2159.13525390625 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.5879, loss_val: nan, pos_over_neg: 2031.28759765625 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 1621.8148193359375 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: 2124.56982421875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.5903, loss_val: nan, pos_over_neg: 911.1056518554688 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 736.8342895507812 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 908.2613525390625 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: 4707.16650390625 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.5914, loss_val: nan, pos_over_neg: 800.1945190429688 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 912.3187866210938 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.5865, loss_val: nan, pos_over_neg: 995.6969604492188 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 980.301513671875 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 1006.3920288085938 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.6003, loss_val: nan, pos_over_neg: 1041.98095703125 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 603.0572509765625 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 855.9118041992188 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.586, loss_val: nan, pos_over_neg: 1067.9332275390625 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.5937, loss_val: nan, pos_over_neg: 1071.6231689453125 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 911.951171875 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.5884, loss_val: nan, pos_over_neg: 951.75634765625 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 809.2634887695312 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 870.463134765625 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 995.3705444335938 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 1313.3572998046875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 956.0437622070312 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: 1314.47021484375 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.5929, loss_val: nan, pos_over_neg: 1264.740966796875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 483.37640380859375 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 1390.091552734375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 2287.131591796875 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.596, loss_val: nan, pos_over_neg: 638.1435546875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.5986, loss_val: nan, pos_over_neg: 1029.96044921875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 1353.63671875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 1288.8260498046875 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.5881, loss_val: nan, pos_over_neg: 1261.9791259765625 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 954.7503662109375 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.5877, loss_val: nan, pos_over_neg: 1127.4896240234375 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 533.83203125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 1402.761962890625 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.5801, loss_val: nan, pos_over_neg: 1345.7359619140625 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 942.7373657226562 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.5903, loss_val: nan, pos_over_neg: 738.1168823242188 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 1107.617919921875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.5819, loss_val: nan, pos_over_neg: -7346.18408203125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 1233.61669921875 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 1076.4110107421875 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.5929, loss_val: nan, pos_over_neg: 717.3549194335938 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.587, loss_val: nan, pos_over_neg: 4415.03125 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.5973, loss_val: nan, pos_over_neg: 1454.624267578125 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.595, loss_val: nan, pos_over_neg: 1283.0447998046875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 3678.3916015625 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 874.5332641601562 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 6834.53271484375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.6031, loss_val: nan, pos_over_neg: 1080.7369384765625 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 2021.765625 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: -11876.7490234375 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 584.7047729492188 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.5911, loss_val: nan, pos_over_neg: 739.5040283203125 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.5932, loss_val: nan, pos_over_neg: 1105.3922119140625 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 2457.7333984375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: -211388.15625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 1668.2373046875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.5821, loss_val: nan, pos_over_neg: 1522.0263671875 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1273.5322265625 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 1405.9991455078125 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.5909, loss_val: nan, pos_over_neg: -11462.4208984375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 2763.15869140625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 1461.60302734375 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.5963, loss_val: nan, pos_over_neg: 1890.042724609375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: 5248.3037109375 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.587, loss_val: nan, pos_over_neg: 1973.4356689453125 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.5857, loss_val: nan, pos_over_neg: 511.2944641113281 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.5796, loss_val: nan, pos_over_neg: 1208.9239501953125 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 1233.0947265625 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: 1050.8560791015625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 804.9303588867188 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.5932, loss_val: nan, pos_over_neg: 899.078125 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 1829.0694580078125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: 1170.4202880859375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 1922.8499755859375 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.5899, loss_val: nan, pos_over_neg: 1039.9466552734375 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 841.9269409179688 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.597, loss_val: nan, pos_over_neg: 3187.498046875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 1646.7752685546875 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.5928, loss_val: nan, pos_over_neg: 895.4126586914062 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 1006.5096435546875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.6005, loss_val: nan, pos_over_neg: 2632.089599609375 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 1178.1920166015625 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.5906, loss_val: nan, pos_over_neg: 865.3168334960938 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.5915, loss_val: nan, pos_over_neg: 907.0595092773438 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.5822, loss_val: nan, pos_over_neg: 806.0308227539062 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.5865, loss_val: nan, pos_over_neg: 1648.87353515625 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.586, loss_val: nan, pos_over_neg: 2564.279052734375 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 2008.9716796875 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 20872.16015625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 59909.68359375 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 1634.06298828125 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.5873, loss_val: nan, pos_over_neg: 11283.404296875 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 18932.255859375 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.5935, loss_val: nan, pos_over_neg: 2027.93310546875 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 671.4595947265625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: 1260.02734375 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.5883, loss_val: nan, pos_over_neg: 954.747802734375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.5971, loss_val: nan, pos_over_neg: 1599.072509765625 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 134959.5 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: 2039.5611572265625 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.5945, loss_val: nan, pos_over_neg: 1765.33740234375 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.5941, loss_val: nan, pos_over_neg: 784.2268676757812 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 917.9252319335938 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 39556.56640625 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.5906, loss_val: nan, pos_over_neg: 5003.57568359375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 2568.20361328125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 2113.30712890625 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.594, loss_val: nan, pos_over_neg: 1292.614013671875 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.5944, loss_val: nan, pos_over_neg: 520.58642578125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.5956, loss_val: nan, pos_over_neg: 581.4561767578125 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 1234.51025390625 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: -909617.6875 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 1991.97900390625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.5966, loss_val: nan, pos_over_neg: 747.9579467773438 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: 2112.83935546875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 1845.262939453125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: 2584.50244140625 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.5997, loss_val: nan, pos_over_neg: 857.1090698242188 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 1136.63623046875 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 1468.0665283203125 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.6014, loss_val: nan, pos_over_neg: 638.2627563476562 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: -316821.125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 2268.95068359375 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.5887, loss_val: nan, pos_over_neg: 939.3963012695312 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: 694.3243408203125 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 3118.299560546875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.6, loss_val: nan, pos_over_neg: 1462.2598876953125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.6025, loss_val: nan, pos_over_neg: 583.243408203125 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1094.41845703125 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 5805.54736328125 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.5947, loss_val: nan, pos_over_neg: 6681.6357421875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 1143.6729736328125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.5786, loss_val: nan, pos_over_neg: 2256.83056640625 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 1348.1065673828125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 697.2155151367188 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 961.2216186523438 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 584.71142578125 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.5999, loss_val: nan, pos_over_neg: 379.4842834472656 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.586, loss_val: nan, pos_over_neg: 738.4337768554688 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 769.7672729492188 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 709.7491455078125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 1036.0155029296875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: 820.3748168945312 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.5859, loss_val: nan, pos_over_neg: 2094.30126953125 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 404.98992919921875 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: 1108.3311767578125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 4402.4775390625 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.5928, loss_val: nan, pos_over_neg: 638.5059204101562 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 602.390380859375 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.598, loss_val: nan, pos_over_neg: 829.5965576171875 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.5847, loss_val: nan, pos_over_neg: 1224.3792724609375 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.5866, loss_val: nan, pos_over_neg: 1095.77392578125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.5932, loss_val: nan, pos_over_neg: 627.06591796875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 813.4404296875 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.5915, loss_val: nan, pos_over_neg: 1209.1585693359375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.5989, loss_val: nan, pos_over_neg: 360.9762268066406 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: 902.8285522460938 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.5871, loss_val: nan, pos_over_neg: 599.0375366210938 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 723.729736328125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.5988, loss_val: nan, pos_over_neg: 989.2223510742188 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 2109.931884765625 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 3264.02197265625 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.5937, loss_val: nan, pos_over_neg: 837.5216674804688 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 1038.5516357421875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: 681.1626586914062 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: 1397.1571044921875 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 2394.53759765625 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 963.7608642578125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.5936, loss_val: nan, pos_over_neg: 625.3115234375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.5835, loss_val: nan, pos_over_neg: 2716.062255859375 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 3616.107177734375 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.6041, loss_val: nan, pos_over_neg: 843.4025268554688 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 922.8037109375 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.5854, loss_val: nan, pos_over_neg: 12550.7470703125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.5868, loss_val: nan, pos_over_neg: 2099.409912109375 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.5919, loss_val: nan, pos_over_neg: 1031.116943359375 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 1548.5697021484375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.5993, loss_val: nan, pos_over_neg: 1474.7080078125 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.5847, loss_val: nan, pos_over_neg: 1700.054931640625 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.5871, loss_val: nan, pos_over_neg: -6966.6376953125 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 771.2608642578125 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 687.1497192382812 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 1459.7099609375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 1129.8582763671875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.5868, loss_val: nan, pos_over_neg: 5756.642578125 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 617.9369506835938 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.5895, loss_val: nan, pos_over_neg: 1181.1307373046875 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.5904, loss_val: nan, pos_over_neg: 790.0533447265625 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 904.0957641601562 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.5825, loss_val: nan, pos_over_neg: 1698.224365234375 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 1838.4544677734375 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 734.1605224609375 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.5911, loss_val: nan, pos_over_neg: 861.0842895507812 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.5936, loss_val: nan, pos_over_neg: 1432.7142333984375 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.5907, loss_val: nan, pos_over_neg: 5338.06787109375 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 1180.085693359375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.5995, loss_val: nan, pos_over_neg: -129446.1875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.5953, loss_val: nan, pos_over_neg: 2089.295654296875 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 1736.6837158203125 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 2525.844482421875 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 2012.7867431640625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.5899, loss_val: nan, pos_over_neg: 13512.4599609375 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: -3162.34375 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 9884.7490234375 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: 2275.05419921875 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 776.14990234375 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 1746.656005859375 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 4032.09912109375 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.5952, loss_val: nan, pos_over_neg: 480.2315673828125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1134.2645263671875 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 2618.230224609375 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.5929, loss_val: nan, pos_over_neg: 1000.3145751953125 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: 668.755615234375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 1792.673583984375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 2537.93408203125 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 1600.4696044921875 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 1386.934326171875 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 1802.61474609375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.586, loss_val: nan, pos_over_neg: 563.7940063476562 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 1906.169189453125 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 930.65087890625 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1154.3828125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.5931, loss_val: nan, pos_over_neg: 1005.63525390625 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.5985, loss_val: nan, pos_over_neg: 431.271240234375 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.5988, loss_val: nan, pos_over_neg: 802.446044921875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.5825, loss_val: nan, pos_over_neg: 2052.96142578125 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: -6649.056640625 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: 2097.86669921875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 1751.396240234375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.5968, loss_val: nan, pos_over_neg: 1126.5611572265625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.5974, loss_val: nan, pos_over_neg: 1759.173583984375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.575, loss_val: nan, pos_over_neg: -2196.322021484375 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: -8020.1123046875 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.5967, loss_val: nan, pos_over_neg: 1822.0703125 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.5866, loss_val: nan, pos_over_neg: 1377.691162109375 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.5879, loss_val: nan, pos_over_neg: 2907.300537109375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 9712.875 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 741.2598266601562 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.5757, loss_val: nan, pos_over_neg: 6004.7021484375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 1660.0982666015625 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 1157.689453125 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 939.1206665039062 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 2117.873046875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 2651.9306640625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 3743.45361328125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.5948, loss_val: nan, pos_over_neg: 2404.329345703125 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 2819.88232421875 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.5987, loss_val: nan, pos_over_neg: 1891.5989990234375 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.5818, loss_val: nan, pos_over_neg: 1194.93017578125 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: -18858.46484375 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.5915, loss_val: nan, pos_over_neg: 1513.13720703125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.5934, loss_val: nan, pos_over_neg: 1538.4185791015625 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.592, loss_val: nan, pos_over_neg: 2365.493896484375 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.5847, loss_val: nan, pos_over_neg: 1833.207763671875 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 2042.8974609375 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: 3802.327880859375 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 756.5523681640625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 1387.7054443359375 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 4161.73974609375 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 1150.0986328125 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.5942, loss_val: nan, pos_over_neg: 1654.6192626953125 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.581, loss_val: nan, pos_over_neg: 3494.93115234375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 1052.7371826171875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 1038.3985595703125 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 1661.1751708984375 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 2192.562744140625 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 1353.30029296875 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.5856, loss_val: nan, pos_over_neg: 1194.6497802734375 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.5876, loss_val: nan, pos_over_neg: 755.7388305664062 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 1839.4859619140625 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.5819, loss_val: nan, pos_over_neg: 1777.136474609375 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 1703.3846435546875 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.5796, loss_val: nan, pos_over_neg: 827.45751953125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 890.4053344726562 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.5868, loss_val: nan, pos_over_neg: 1350.3988037109375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 2432.691162109375 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.5899, loss_val: nan, pos_over_neg: 959.6524047851562 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: 1353.677734375 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.591, loss_val: nan, pos_over_neg: 736.9702758789062 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 3459.59716796875 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 1514.333251953125 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 2802.008544921875 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: 476.1549987792969 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 1060.779296875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.595, loss_val: nan, pos_over_neg: 2502.286376953125 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 938.6954345703125 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.5938, loss_val: nan, pos_over_neg: 1717.367431640625 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 1667.6297607421875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: 973.0276489257812 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 2737.373291015625 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.6029, loss_val: nan, pos_over_neg: 472.291015625 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.5801, loss_val: nan, pos_over_neg: 899.2619018554688 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 2926.379150390625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 1293.0450439453125 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 813.139892578125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.5964, loss_val: nan, pos_over_neg: 2235.138427734375 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 1016.6702270507812 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: 2637.675048828125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 2842.484619140625 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.5787, loss_val: nan, pos_over_neg: 3542.159912109375 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 1129.9951171875 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 1027.9320068359375 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 5775.7294921875 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 924.599365234375 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.5895, loss_val: nan, pos_over_neg: 1272.8453369140625 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.5943, loss_val: nan, pos_over_neg: 555.8434448242188 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.5872, loss_val: nan, pos_over_neg: 1701.5086669921875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: -18516.87890625 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 710.9041137695312 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 788.9719848632812 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 3167.855224609375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.5884, loss_val: nan, pos_over_neg: 1263.044189453125 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.5809, loss_val: nan, pos_over_neg: 600.8026123046875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 1179.146728515625 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.5866, loss_val: nan, pos_over_neg: 961.6611938476562 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 2105.61083984375 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 910.3382568359375 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 1571.93603515625 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 6025.31103515625 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1438.50732421875 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 1200.4100341796875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 531.2662353515625 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 1196.4951171875 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.5857, loss_val: nan, pos_over_neg: 679.9795532226562 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.5802, loss_val: nan, pos_over_neg: 1922.894287109375 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: 2108.65087890625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.587, loss_val: nan, pos_over_neg: 677.421142578125 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.5983, loss_val: nan, pos_over_neg: 1037.6282958984375 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 1899.5889892578125 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 1667.1409912109375 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 1203.421142578125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 1479.9847412109375 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.5959, loss_val: nan, pos_over_neg: 871.1388549804688 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 2415.149169921875 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: 4944.5 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 682.7352294921875 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 2202.58349609375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.5942, loss_val: nan, pos_over_neg: 448.0172119140625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 1663.1983642578125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.5915, loss_val: nan, pos_over_neg: 722.6005859375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.601, loss_val: nan, pos_over_neg: 646.0070190429688 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.5877, loss_val: nan, pos_over_neg: 3465.840576171875 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 7014.97900390625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.5954, loss_val: nan, pos_over_neg: 1024.710693359375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 3409.99658203125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 2812.915771484375 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 2146.105712890625 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 7683.8583984375 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.581, loss_val: nan, pos_over_neg: 1329.3734130859375 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.6003, loss_val: nan, pos_over_neg: 476.91766357421875 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.589, loss_val: nan, pos_over_neg: 691.8023681640625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 2018.2230224609375 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 688.7679443359375 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.5873, loss_val: nan, pos_over_neg: 2210.502197265625 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.5856, loss_val: nan, pos_over_neg: 2268.41259765625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 6923.76318359375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.585, loss_val: nan, pos_over_neg: 4369.654296875 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 1292.520751953125 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 1817.0267333984375 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 13827.99609375 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 2642.95751953125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: -6515.67138671875 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: 1480.6175537109375 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.5877, loss_val: nan, pos_over_neg: 3044.470458984375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 1931.3709716796875 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 538.5521240234375 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.5902, loss_val: nan, pos_over_neg: 5678.35205078125 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.5906, loss_val: nan, pos_over_neg: 1428.681640625 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 3382.10693359375 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.5859, loss_val: nan, pos_over_neg: 1670.812744140625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: 1232.7025146484375 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 14420.330078125 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.5876, loss_val: nan, pos_over_neg: 5356.24609375 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 7356.9716796875 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1087.5589599609375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.5924, loss_val: nan, pos_over_neg: 1297.232177734375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.5856, loss_val: nan, pos_over_neg: 1475.9786376953125 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 3105.026611328125 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: 3648.535888671875 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 1183.7777099609375 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 3117.849853515625 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 414134.65625 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 2642.6396484375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 6908.49169921875 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.5924, loss_val: nan, pos_over_neg: 1493.441650390625 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 1835.4722900390625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 12459.7421875 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 1401.915771484375 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 2679.57763671875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.585, loss_val: nan, pos_over_neg: 1352.455810546875 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.5877, loss_val: nan, pos_over_neg: 3002.53173828125 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.5968, loss_val: nan, pos_over_neg: 1678.87451171875 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: 8938.478515625 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.5926, loss_val: nan, pos_over_neg: 863.9224243164062 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.5826, loss_val: nan, pos_over_neg: 9489.416015625 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 1486.821044921875 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.5818, loss_val: nan, pos_over_neg: 7715.60009765625 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 2167.142822265625 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 680.197265625 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 698.599609375 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 1288.5084228515625 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.5881, loss_val: nan, pos_over_neg: 5379.9375 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.5833, loss_val: nan, pos_over_neg: 2371.435302734375 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 897.4835205078125 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 1052.2359619140625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 1169.129150390625 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: -2850.18212890625 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 3259.72216796875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.5924, loss_val: nan, pos_over_neg: 763.8189697265625 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 1345.1419677734375 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.5833, loss_val: nan, pos_over_neg: 2835.4912109375 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 2856.50927734375 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.5825, loss_val: nan, pos_over_neg: 793.2981567382812 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 976.2047119140625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 925.1044921875 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 1859.28173828125 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 643.7222290039062 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.5847, loss_val: nan, pos_over_neg: 2137.364501953125 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 2429.927978515625 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 1594.0025634765625 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.5872, loss_val: nan, pos_over_neg: 591.8616333007812 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 803.464111328125 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: 2071.16064453125 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 1790.70751953125 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 1921.120849609375 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 955.1965942382812 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.5915, loss_val: nan, pos_over_neg: 778.1148681640625 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.5839, loss_val: nan, pos_over_neg: 1148.9378662109375 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 7425.84326171875 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.5893, loss_val: nan, pos_over_neg: 750.0525512695312 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 2808.980712890625 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 1616.122314453125 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 1065.945068359375 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: -4707.392578125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 2023.447509765625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 3561.56103515625 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 3350.9755859375 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 2063.490234375 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 8512.15234375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 4029.5 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.5921, loss_val: nan, pos_over_neg: 2066.79541015625 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 2539.272705078125 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 2071.5048828125 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 990.5218505859375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 1630.37451171875 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 1008.0526123046875 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 726.9166259765625 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 1847.4730224609375 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.5911, loss_val: nan, pos_over_neg: 726.9268188476562 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.5835, loss_val: nan, pos_over_neg: 3492.727294921875 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1458.46044921875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 2196.762451171875 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 998.6854248046875 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 11312.619140625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 667.03955078125 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: -6848.8681640625 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 1169.7733154296875 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1399.2291259765625 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 719.96240234375 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 1406.885498046875 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.5865, loss_val: nan, pos_over_neg: 1399.90185546875 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 2835.4853515625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.5835, loss_val: nan, pos_over_neg: 619.796875 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1016.3016357421875 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 1090.747802734375 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 13496.43359375 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 1153.24951171875 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.5866, loss_val: nan, pos_over_neg: 718.6111450195312 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.5911, loss_val: nan, pos_over_neg: 1225.7525634765625 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 531.1660766601562 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 714.3728637695312 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 990.6312866210938 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 1901.85693359375 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 824.9012451171875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 1846.7801513671875 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: 1234.49755859375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: -16914.5 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.5882, loss_val: nan, pos_over_neg: 1481.7467041015625 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 788.8483276367188 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 1077.5751953125 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 11425.12890625 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 2985.818359375 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.5972, loss_val: nan, pos_over_neg: 1146.32861328125 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 2141.086181640625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 1106.027587890625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 956.0786743164062 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 1732.1158447265625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 1114.1065673828125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 746.6607666015625 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.5868, loss_val: nan, pos_over_neg: 2279.143310546875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 2871.376220703125 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.58, loss_val: nan, pos_over_neg: -8632.9794921875 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.5854, loss_val: nan, pos_over_neg: -26663.720703125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.5739, loss_val: nan, pos_over_neg: 8706.6865234375 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 6562.9921875 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.5839, loss_val: nan, pos_over_neg: 1344.20703125 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 1946.226806640625 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 2168.035888671875 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.5895, loss_val: nan, pos_over_neg: 666.3895263671875 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: 4317.84375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 1161.2452392578125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.5847, loss_val: nan, pos_over_neg: 2316.511474609375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.591, loss_val: nan, pos_over_neg: 3269.84619140625 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 2786.99853515625 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 1720.890380859375 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.5887, loss_val: nan, pos_over_neg: 2613.66015625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.5947, loss_val: nan, pos_over_neg: 1188.2010498046875 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.5969, loss_val: nan, pos_over_neg: 760.575439453125 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.5875, loss_val: nan, pos_over_neg: 680.7064819335938 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.5818, loss_val: nan, pos_over_neg: 2138.681640625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 8536.232421875 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.5884, loss_val: nan, pos_over_neg: 1038.0982666015625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 3765.8134765625 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 1790.23046875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.5905, loss_val: nan, pos_over_neg: 1021.84228515625 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 1422.4332275390625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 1588.18505859375 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 7569.0322265625 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: 951.7398681640625 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 721.1694946289062 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 3373.786865234375 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 5286.54833984375 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 1442.334716796875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 620.4259033203125 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 602.9846801757812 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.595, loss_val: nan, pos_over_neg: 1145.110595703125 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 1023.6222534179688 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 11739.1572265625 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.5949, loss_val: nan, pos_over_neg: 707.2970581054688 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.5787, loss_val: nan, pos_over_neg: 1430.4725341796875 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 4962.033203125 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 21427.357421875 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.6038, loss_val: nan, pos_over_neg: 612.9088134765625 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 5361.4189453125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.5876, loss_val: nan, pos_over_neg: 942.5094604492188 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.5915, loss_val: nan, pos_over_neg: 834.2490844726562 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 853.4459838867188 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 1913.61279296875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 2856.84521484375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 1930.111083984375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.5865, loss_val: nan, pos_over_neg: 2566.61767578125 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: -41082.72265625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.5859, loss_val: nan, pos_over_neg: 1251.1021728515625 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 1621.881591796875 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 908.5093994140625 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 2461.239013671875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: -144950.8125 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 1532.969970703125 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.5906, loss_val: nan, pos_over_neg: 960.9711303710938 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.5895, loss_val: nan, pos_over_neg: 713.623046875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.5859, loss_val: nan, pos_over_neg: 1106.06884765625 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 4436.71826171875 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 5751.6220703125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 3765.905029296875 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.5911, loss_val: nan, pos_over_neg: 1008.6417846679688 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 4544.9541015625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 2320.6865234375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.5873, loss_val: nan, pos_over_neg: 2465.0244140625 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.5793, loss_val: nan, pos_over_neg: 5327.72412109375 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 2196.078125 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 515.953857421875 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1052.5146484375 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 1906.3267822265625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 2290.674072265625 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1950.756103515625 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: 2097.825439453125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 1110.5367431640625 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 1265.1470947265625 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 2503.909423828125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 1591.6474609375 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.5826, loss_val: nan, pos_over_neg: -6646.849609375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.581, loss_val: nan, pos_over_neg: 684.2297973632812 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 1551.258544921875 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 672.8711547851562 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.593, loss_val: nan, pos_over_neg: 1394.3070068359375 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: 8095.384765625 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.5786, loss_val: nan, pos_over_neg: -28886.3671875 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 4420.05712890625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.591, loss_val: nan, pos_over_neg: 1147.1158447265625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.5835, loss_val: nan, pos_over_neg: 1194.6363525390625 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.5821, loss_val: nan, pos_over_neg: 2489.122802734375 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 5058.5908203125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.5839, loss_val: nan, pos_over_neg: 984.8576049804688 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 1707.00244140625 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 622.0496826171875 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 949.2059326171875 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 2699.71630859375 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 10973.7314453125 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 974.37255859375 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 3077.199951171875 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.5923, loss_val: nan, pos_over_neg: 1095.3070068359375 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: 822.3294067382812 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 1440.9613037109375 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 3660.970703125 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 2142.739013671875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.5822, loss_val: nan, pos_over_neg: 1087.7171630859375 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 1282.5137939453125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.5907, loss_val: nan, pos_over_neg: 768.4864501953125 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: -18619.291015625 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 739.8670654296875 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 4069.603759765625 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 2902.0810546875 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 25934.677734375 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 1167.403564453125 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.5821, loss_val: nan, pos_over_neg: 1275.572021484375 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 632.5842895507812 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 2342.659423828125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 4096.57861328125 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 1544.5849609375 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 401.84027099609375 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 967.0950927734375 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 1517.2392578125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 703.061279296875 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 1956.76416015625 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1658.9981689453125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.585, loss_val: nan, pos_over_neg: 946.0304565429688 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.5869, loss_val: nan, pos_over_neg: 971.6783447265625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.5844, loss_val: nan, pos_over_neg: 1103.6722412109375 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.5871, loss_val: nan, pos_over_neg: 729.1397094726562 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 862.892578125 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 824.2545776367188 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.5835, loss_val: nan, pos_over_neg: 1303.1348876953125 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 1086.366943359375 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 1177.0084228515625 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: 559.77880859375 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.5809, loss_val: nan, pos_over_neg: 903.8209838867188 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 879.8610229492188 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 9206.3291015625 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 1044.8187255859375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.5903, loss_val: nan, pos_over_neg: 770.9191284179688 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.5895, loss_val: nan, pos_over_neg: 1206.6710205078125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1123.9105224609375 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.5946, loss_val: nan, pos_over_neg: 1291.5963134765625 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 1474.6202392578125 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 2079.99267578125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.5839, loss_val: nan, pos_over_neg: 1232.4249267578125 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1898.6031494140625 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 985.4608764648438 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 992.3693237304688 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.5876, loss_val: nan, pos_over_neg: 1424.3382568359375 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 5041.5185546875 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.5731, loss_val: nan, pos_over_neg: 708.9064331054688 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 1064.9664306640625 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 1325.5767822265625 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 463.8006591796875 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 572.5103149414062 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.5879, loss_val: nan, pos_over_neg: 870.38818359375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 667.498046875 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 506.6679992675781 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.5802, loss_val: nan, pos_over_neg: 4414.611328125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1523.393310546875 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 598.9226684570312 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1695.6884765625 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 1109.9866943359375 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 633.4736328125 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 1734.9539794921875 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.5866, loss_val: nan, pos_over_neg: 630.030517578125 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 614.7672729492188 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1036.5836181640625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: 962.6691284179688 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.5879, loss_val: nan, pos_over_neg: 1440.4000244140625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 881.2101440429688 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 2114.002685546875 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.5844, loss_val: nan, pos_over_neg: 1307.275390625 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.5802, loss_val: nan, pos_over_neg: 5427.828125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.5871, loss_val: nan, pos_over_neg: 576.0498046875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 1624.170654296875 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 3149.083984375 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 3560.35986328125 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 2158.787109375 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 923.7914428710938 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 1577.4066162109375 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 2833.31201171875 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.5829, loss_val: nan, pos_over_neg: 1474.9349365234375 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 2004.2989501953125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.5939, loss_val: nan, pos_over_neg: 673.5693969726562 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 1002.1046142578125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.5887, loss_val: nan, pos_over_neg: 1011.8296508789062 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 8668.2900390625 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 1094.11279296875 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.586, loss_val: nan, pos_over_neg: 619.1212768554688 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: -9083.3466796875 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 3282.520263671875 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: 1598.97216796875 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 1962.205322265625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 1593.5928955078125 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 1560.5401611328125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 1963.052001953125 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 1863.6610107421875 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.5863, loss_val: nan, pos_over_neg: 1297.28173828125 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 787.70849609375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.5868, loss_val: nan, pos_over_neg: 1256.6650390625 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.575, loss_val: nan, pos_over_neg: -29772.751953125 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 1143.1160888671875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [2:28:08<184818:41:47, 2217.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 918.8753051757812 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 27554.955078125 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.5869, loss_val: nan, pos_over_neg: 3469.00146484375 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 25893.958984375 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: 2561.047607421875 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: 1250.5323486328125 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.5829, loss_val: nan, pos_over_neg: 1210.4766845703125 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.5882, loss_val: nan, pos_over_neg: 694.3914184570312 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.5964, loss_val: nan, pos_over_neg: 1202.4552001953125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: -39072.796875 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 1299.656982421875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.5918, loss_val: nan, pos_over_neg: 670.0618896484375 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 2136.553955078125 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 1187.337646484375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 1832.81640625 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 565.9117431640625 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.5787, loss_val: nan, pos_over_neg: 1272.746826171875 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.5809, loss_val: nan, pos_over_neg: 728.3980712890625 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 27145.73828125 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 1673.0140380859375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1081.2410888671875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 1269.0616455078125 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 788.2831420898438 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 1615.814208984375 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 1404.8170166015625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 9301.955078125 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 821.6632690429688 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1043.0587158203125 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 5335.53955078125 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 1265.4915771484375 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.5822, loss_val: nan, pos_over_neg: 892.5204467773438 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.5757, loss_val: nan, pos_over_neg: 822.72900390625 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 704.3637084960938 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.5756, loss_val: nan, pos_over_neg: 1885.8516845703125 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 2388.788818359375 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.585, loss_val: nan, pos_over_neg: 3211.654052734375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.5863, loss_val: nan, pos_over_neg: 2594.41259765625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 1101.862060546875 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 1391.4774169921875 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.5898, loss_val: nan, pos_over_neg: 838.8483276367188 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 1463.2886962890625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 967.3051147460938 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 3388.894287109375 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 1154.4468994140625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 2151.71826171875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.5833, loss_val: nan, pos_over_neg: 632.2131958007812 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 766.6520385742188 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 1173.2032470703125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 772.4564208984375 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1616.4951171875 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 1355.7286376953125 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 589.8511352539062 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: 1526.5206298828125 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 1129.6456298828125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 1794.91162109375 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.5826, loss_val: nan, pos_over_neg: 1848.9781494140625 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.5873, loss_val: nan, pos_over_neg: 1548.8138427734375 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 13618.5888671875 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: 1903.7852783203125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.5866, loss_val: nan, pos_over_neg: 445.9117431640625 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 1845.6484375 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 901.5055541992188 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1083.897216796875 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: 1543.185546875 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 1128.8404541015625 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.5796, loss_val: nan, pos_over_neg: 651.8002319335938 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: 1307.5928955078125 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 973.0856323242188 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1107.00341796875 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 1256.5570068359375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.5818, loss_val: nan, pos_over_neg: 1520.8726806640625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 463.73406982421875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 877.6461791992188 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 775.1738891601562 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1026.3460693359375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 1311.528076171875 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 562.2449951171875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 1308.2738037109375 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 919.9248046875 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.5847, loss_val: nan, pos_over_neg: 747.2503662109375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.5825, loss_val: nan, pos_over_neg: 624.4464721679688 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: 952.3223876953125 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.5751, loss_val: nan, pos_over_neg: 2118.87744140625 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 2660.228515625 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 870.7705078125 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 1311.689453125 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 10224.787109375 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 1742.52099609375 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.5756, loss_val: nan, pos_over_neg: 1162.298095703125 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 1240.190673828125 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1023.1373291015625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.5856, loss_val: nan, pos_over_neg: 781.8529052734375 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 942.3065185546875 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 2326.64990234375 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 2596.722900390625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 2908.560302734375 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 2402.09814453125 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 1250.9779052734375 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.5789, loss_val: nan, pos_over_neg: 1247.443115234375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 2182.49609375 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 676.9344482421875 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 1923.8355712890625 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 530.2430419921875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 816.1306762695312 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 3068.9033203125 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.5912, loss_val: nan, pos_over_neg: 841.1593627929688 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 942.8118896484375 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1504.478759765625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 9915.8056640625 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: 1610.585205078125 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 2768.26904296875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 2133.2353515625 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 1334.4354248046875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.5873, loss_val: nan, pos_over_neg: 1376.8013916015625 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 1018.1795654296875 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 1275.3297119140625 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.5888, loss_val: nan, pos_over_neg: 788.2282104492188 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1458.01513671875 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 4017.802734375 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 3342.514892578125 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.5809, loss_val: nan, pos_over_neg: 1747.8619384765625 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 1707.0196533203125 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 7012.6806640625 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1416.77880859375 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 2303.66552734375 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 1004.3612060546875 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 973.4148559570312 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.5933, loss_val: nan, pos_over_neg: 936.7571411132812 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 1537.62109375 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.5897, loss_val: nan, pos_over_neg: 674.5607299804688 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.5786, loss_val: nan, pos_over_neg: 4679.62255859375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 1388.10205078125 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 1885.5670166015625 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: 1178.8128662109375 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 4698.37841796875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: -26581.12890625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 2693.213623046875 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 98156.9765625 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 13774.1748046875 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 716.2302856445312 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 726.6157836914062 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 880.005615234375 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 10704.86328125 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 2245.258056640625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 624.8052368164062 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 1234.0635986328125 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 1271.9632568359375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.5821, loss_val: nan, pos_over_neg: 3610.447021484375 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 6758.0185546875 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.5825, loss_val: nan, pos_over_neg: 1817.2431640625 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 6370.2705078125 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 1969.728271484375 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 4197.994140625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 4025.780029296875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 14862.0556640625 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 3415.163330078125 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.5932, loss_val: nan, pos_over_neg: 678.1818237304688 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 2014.9913330078125 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.5899, loss_val: nan, pos_over_neg: -57664.2421875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 2233.28369140625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 884.981201171875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.5819, loss_val: nan, pos_over_neg: 3624.96142578125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 1972.2872314453125 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.5829, loss_val: nan, pos_over_neg: 1751.85205078125 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.5769, loss_val: nan, pos_over_neg: 12178.2587890625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: -3373.295654296875 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 2421.750244140625 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.5971, loss_val: nan, pos_over_neg: 1148.542236328125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.5789, loss_val: nan, pos_over_neg: 8234.2216796875 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 3928.943115234375 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: -20328.283203125 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: -9348.169921875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.5821, loss_val: nan, pos_over_neg: 871.552734375 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 1828.2955322265625 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 2290.010009765625 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 1684.4970703125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.578, loss_val: nan, pos_over_neg: 3392.82666015625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: -30694.287109375 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.5855, loss_val: nan, pos_over_neg: -120748.7578125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 2224.218017578125 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: -76492.5703125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 1922.7196044921875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 3825.01025390625 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 913.9229736328125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 1645.453125 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 1846.1737060546875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 2955.83935546875 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.5786, loss_val: nan, pos_over_neg: 2375.823974609375 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.581, loss_val: nan, pos_over_neg: 687.3351440429688 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 9100.08984375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 2240.438720703125 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 1767.0738525390625 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 2738.9853515625 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 5030.365234375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 2498.71484375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 1118.0616455078125 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.588, loss_val: nan, pos_over_neg: 785.02197265625 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.5891, loss_val: nan, pos_over_neg: 1247.0234375 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 2578.04638671875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 630.1533813476562 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 794.0381469726562 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 815.7607421875 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 1148.5291748046875 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 1750.5733642578125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 1856.099609375 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: 1510.2919921875 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 869.2967529296875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 1322.1868896484375 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 3340.84228515625 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1224.9005126953125 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.5751, loss_val: nan, pos_over_neg: 1393.773193359375 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: 2091.237060546875 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.5796, loss_val: nan, pos_over_neg: 13649.296875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 1447.6270751953125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 12783.62890625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 1738.5394287109375 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 749.1573486328125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 2084.536865234375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1018.4165649414062 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 985.7769165039062 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: -17223.830078125 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 54854.296875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 2449.08642578125 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 1243.2252197265625 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.5907, loss_val: nan, pos_over_neg: 660.2218627929688 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 4456.10302734375 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 1181.9219970703125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 450.59259033203125 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 974.4077758789062 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 740.9971923828125 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 2047.8330078125 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.5857, loss_val: nan, pos_over_neg: 2170.74169921875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 1518.78515625 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 2064.024169921875 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 974.0674438476562 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 1707.4722900390625 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: -190594.65625 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 1498.9168701171875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.587, loss_val: nan, pos_over_neg: 798.0665893554688 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 3391.26025390625 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1686.0936279296875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: 706.1204833984375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 1593.6546630859375 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 794.7644653320312 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.5756, loss_val: nan, pos_over_neg: 1011.189697265625 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 4846.5595703125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 2370.001953125 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.5822, loss_val: nan, pos_over_neg: 999.6621704101562 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 1758.683349609375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 3570.14990234375 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.5848, loss_val: nan, pos_over_neg: 2552.836181640625 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 2016.1807861328125 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 2112.630126953125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.5833, loss_val: nan, pos_over_neg: 1268.65283203125 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 2467.390625 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 2841.392578125 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 5573.32861328125 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 1804.195556640625 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 2774.080810546875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 1229.8304443359375 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 2428.6181640625 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.5793, loss_val: nan, pos_over_neg: 4280.2333984375 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 2272.101318359375 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.576, loss_val: nan, pos_over_neg: -4432.9560546875 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 14735.744140625 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 1848.62744140625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 3463.28369140625 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 1436.471435546875 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 540.9407348632812 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 747.857666015625 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 2565.92333984375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1675.9654541015625 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 2738.747802734375 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 1108.517822265625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 885.3533935546875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 1173.8507080078125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 1586.91845703125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 761.6306762695312 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 3927.6103515625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.585, loss_val: nan, pos_over_neg: 734.8555297851562 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 1064.5419921875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 17186.7890625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 1357.519775390625 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 3258.073486328125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1315.70068359375 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 874.27099609375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 2050.388916015625 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 11534.896484375 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.5801, loss_val: nan, pos_over_neg: 1616.5948486328125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 18868.203125 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 597.6685180664062 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.5787, loss_val: nan, pos_over_neg: 3457.864013671875 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 776.7877807617188 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.5751, loss_val: nan, pos_over_neg: 4997.0703125 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 2755.149658203125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 910.9762573242188 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 513.677978515625 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.5882, loss_val: nan, pos_over_neg: 479.97802734375 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.5865, loss_val: nan, pos_over_neg: 2222.86962890625 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 25302.99609375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 1271.7166748046875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 1074.1485595703125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1602.0579833984375 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 3280.3916015625 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 886.6801147460938 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 3511.81103515625 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.5911, loss_val: nan, pos_over_neg: 569.677978515625 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1570.9437255859375 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.5818, loss_val: nan, pos_over_neg: 580.9834594726562 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.5731, loss_val: nan, pos_over_neg: 2158.725341796875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 1428.2955322265625 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 2143.9501953125 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 742.8740844726562 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 906.7687377929688 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 2509.427978515625 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.5847, loss_val: nan, pos_over_neg: 1380.8050537109375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 1888.390625 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 1115.3125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 2392.139404296875 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 16181.6728515625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: 1363.06103515625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.5895, loss_val: nan, pos_over_neg: 501.26446533203125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 722.9818725585938 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 1379.919189453125 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 850.9359130859375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 1654.4593505859375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 1193.663330078125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 784.310302734375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 835.837158203125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 1849.4801025390625 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.5917, loss_val: nan, pos_over_neg: 2809.08203125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.5899, loss_val: nan, pos_over_neg: 2113.045166015625 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 1661.3148193359375 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 975.475830078125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 6462.91015625 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 983.1375122070312 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 1303.69873046875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 1846.3756103515625 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 1268.629150390625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 1935.7716064453125 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 1160.391845703125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.5854, loss_val: nan, pos_over_neg: 1367.0494384765625 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.5785, loss_val: nan, pos_over_neg: 1635.7769775390625 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.5889, loss_val: nan, pos_over_neg: 894.2821655273438 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.596, loss_val: nan, pos_over_neg: 934.5353393554688 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1096.9801025390625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1172.805908203125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 1154.232177734375 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 780.3488159179688 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 900.2965698242188 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 2479.591796875 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 906.5667114257812 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: 970.0511474609375 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.5793, loss_val: nan, pos_over_neg: 1116.7305908203125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 892.13525390625 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.5925, loss_val: nan, pos_over_neg: 1361.1273193359375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 1197.46630859375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 689.693603515625 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1284.203369140625 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1686.929443359375 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.5789, loss_val: nan, pos_over_neg: 75016.078125 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 3968.378173828125 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.5876, loss_val: nan, pos_over_neg: 1675.47119140625 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 33919.46875 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: -15922.2451171875 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1341.597900390625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 2678.339599609375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.576, loss_val: nan, pos_over_neg: 727.2998657226562 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 15792.609375 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1110.7279052734375 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 1033.3931884765625 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.5769, loss_val: nan, pos_over_neg: 19455.66796875 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 1919.759033203125 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.5793, loss_val: nan, pos_over_neg: 873.9845581054688 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: 803.1250610351562 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 1460.361083984375 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 4992.515625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.5675, loss_val: nan, pos_over_neg: 13095.4716796875 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 694.7547607421875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.5819, loss_val: nan, pos_over_neg: 1493.095703125 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.5936, loss_val: nan, pos_over_neg: 1099.219970703125 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 1359.128662109375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 1094.8826904296875 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 3642.468017578125 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 725.3009033203125 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 935.2272338867188 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 2526.613037109375 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 1288.6024169921875 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 851.6658325195312 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 661.0487670898438 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 753.69140625 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 1087.54443359375 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 958.9414672851562 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: -41901.72265625 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.5789, loss_val: nan, pos_over_neg: 580.4564208984375 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 2801.011474609375 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 1621.393798828125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 3529.09619140625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 9251.5615234375 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.585, loss_val: nan, pos_over_neg: 7811.64306640625 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1155.658203125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 2693.58056640625 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 2154.15234375 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 1035.34814453125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 483.7508544921875 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 467.34814453125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 2218.1845703125 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 2079.382080078125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 765.7442016601562 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1216.31298828125 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.578, loss_val: nan, pos_over_neg: 1334.5416259765625 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.5801, loss_val: nan, pos_over_neg: 1572.868896484375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 2346.559326171875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 908.9635009765625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 1290.9891357421875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 747.7423706054688 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 1287.5284423828125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 1107.5921630859375 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.5809, loss_val: nan, pos_over_neg: 2590.602783203125 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 3648.917724609375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 1070.019775390625 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 1599.2027587890625 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.5894, loss_val: nan, pos_over_neg: -6631.0966796875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 3267.913818359375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 3551.605712890625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.572, loss_val: nan, pos_over_neg: -6348.78173828125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 13424.5810546875 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 2030.226806640625 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 2179.59228515625 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 730.4318237304688 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1596.724365234375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 950.9937744140625 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 1698.6983642578125 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 4410.32080078125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 50016.578125 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 5453.00244140625 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: -6022.8857421875 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 1535.231201171875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 1476.6424560546875 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.578, loss_val: nan, pos_over_neg: -17293.85546875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.5818, loss_val: nan, pos_over_neg: 2632.806884765625 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 7486.25146484375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.59, loss_val: nan, pos_over_neg: 677.1797485351562 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 1382.121826171875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 1445.059814453125 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 1546.718505859375 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 1185.1700439453125 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 1029.9617919921875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 4852.87451171875 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 1487.8201904296875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 5469.90576171875 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 754.0767822265625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 944.2035522460938 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.5786, loss_val: nan, pos_over_neg: 5010.08203125 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 2421.748291015625 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 2672.0712890625 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 904.66162109375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 1073.3382568359375 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.5831, loss_val: nan, pos_over_neg: 785.2063598632812 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 2521.641845703125 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 4862.36669921875 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 1420.96630859375 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 2987.306640625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 1388.2193603515625 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 1681.1370849609375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 1181.4818115234375 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.5896, loss_val: nan, pos_over_neg: 588.8994750976562 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 757.9642944335938 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 999.9968872070312 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.5871, loss_val: nan, pos_over_neg: 1150.6478271484375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 8976.1435546875 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 1439.52978515625 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 1318.658447265625 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 2212.3974609375 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.5785, loss_val: nan, pos_over_neg: 2370.296142578125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 1972.2396240234375 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 3220.06201171875 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 1196.515625 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 671.4891357421875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: 849.6774291992188 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 699.1766967773438 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1468.8631591796875 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 825.9949951171875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 483.4981384277344 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 4033.825927734375 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 692.7869873046875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 1751.58642578125 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 2868.357177734375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.5839, loss_val: nan, pos_over_neg: 965.823974609375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 892.34521484375 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 2036.98583984375 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 1548.5728759765625 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: -18264.955078125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 1609.525634765625 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 1004.3687133789062 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 6678.27294921875 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 3078.174072265625 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 881.2620239257812 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 865.9110717773438 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 962.9053344726562 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 3934.15576171875 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 3855.55126953125 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 1267.180908203125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1002.6323852539062 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 1033.2725830078125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 166118.875 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 1050.7398681640625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 3339.05908203125 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: 670.2471923828125 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 2310.4560546875 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 1051.4141845703125 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 5751.54443359375 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 1847.3681640625 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 1111.972412109375 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.5844, loss_val: nan, pos_over_neg: 959.0955200195312 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1426.625 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 1526.851806640625 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 768.9171142578125 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 751.7562866210938 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 2811.336669921875 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1599.2236328125 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.5785, loss_val: nan, pos_over_neg: 1602.449462890625 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 5035.91357421875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 3596.916748046875 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.5822, loss_val: nan, pos_over_neg: 2796.96533203125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 992.8978881835938 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 6850.4453125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 5205.84228515625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 973.0732421875 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 766.0715942382812 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 1045.330322265625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 27716.953125 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 1810.238037109375 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 1662.0396728515625 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 3175.602294921875 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 3033.833251953125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: -9006.662109375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 8031.5205078125 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: -7967.43017578125 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.5913, loss_val: nan, pos_over_neg: 1135.130859375 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1635.136962890625 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 830.9888305664062 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 1255.026611328125 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 5306.98291015625 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.5863, loss_val: nan, pos_over_neg: 1246.5986328125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 2969.083251953125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 14828.9951171875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1527.075927734375 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 983.23388671875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 915.0355834960938 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 652.1685180664062 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 1637.835205078125 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 2038.9990234375 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1258.197998046875 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 1370.0579833984375 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 1187.067626953125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.576, loss_val: nan, pos_over_neg: 1068.648681640625 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 795.8273315429688 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 2365.49267578125 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1590.65673828125 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1204.3609619140625 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 1372.010009765625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: -6789.5927734375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 1476.2318115234375 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 1037.1077880859375 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 671.4721069335938 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 1876.734375 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.5839, loss_val: nan, pos_over_neg: 1146.0626220703125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 1425.905517578125 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 2652.2978515625 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 1756.75439453125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 692.7804565429688 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 2503.386474609375 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 5863.9189453125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.5839, loss_val: nan, pos_over_neg: 5194.07470703125 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 1450.97314453125 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 2080.687255859375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 1128.639404296875 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 907.7647705078125 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: -10302.00390625 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 2256.804931640625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 4605.46337890625 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 1698.1258544921875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 640.392822265625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 981.3485717773438 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.5878, loss_val: nan, pos_over_neg: 1831.48046875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 2256.002197265625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 11400.9775390625 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 3103.449951171875 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 1386.7835693359375 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 2084.1806640625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 3673.209716796875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: -38615.1796875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 1458.570556640625 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 874.7365112304688 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 2250.88720703125 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 2000.8736572265625 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 2396.740966796875 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 1161.488525390625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 2032.6470947265625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 938.336181640625 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1301.0247802734375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1138.1444091796875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 2752.225830078125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1229.547119140625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 2967.701171875 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 1271.0699462890625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 1450.6722412109375 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 1447.7120361328125 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.5857, loss_val: nan, pos_over_neg: 1013.1463012695312 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 763.022216796875 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 874.1005859375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 5023.54541015625 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 572.279541015625 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.5884, loss_val: nan, pos_over_neg: 1598.2333984375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 933.1475219726562 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.5899, loss_val: nan, pos_over_neg: 795.1396484375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 1416.406982421875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 2425.200927734375 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 3149.375 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 3424.63037109375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 2438.939697265625 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 2150.637939453125 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.5757, loss_val: nan, pos_over_neg: 9448.0810546875 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.5829, loss_val: nan, pos_over_neg: 1302.117919921875 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 1489.2833251953125 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 1028.5206298828125 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 4286.91357421875 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 2989.844970703125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.59, loss_val: nan, pos_over_neg: 1803.8485107421875 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 1516.83544921875 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 1367.9942626953125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 1070.88623046875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 2176.84619140625 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: -7067.18798828125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 1506.36083984375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 2017.823486328125 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1266.000732421875 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 1483.509521484375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 6975.61669921875 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 2279.2490234375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1420.4146728515625 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.5857, loss_val: nan, pos_over_neg: 1296.398193359375 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 1262.1741943359375 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 931.8551635742188 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 2481.919189453125 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.5845, loss_val: nan, pos_over_neg: 1828.7103271484375 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 1379.310791015625 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 5902.1962890625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 1708.7440185546875 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.5867, loss_val: nan, pos_over_neg: 1061.9156494140625 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 1464.304443359375 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 2516.33642578125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 2399.40234375 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 4182.65380859375 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.5811, loss_val: nan, pos_over_neg: 3799.167236328125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 1862.88623046875 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1141.7645263671875 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 788.9959716796875 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 962.7677001953125 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 1423.234375 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 979.6187744140625 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 1834.96875 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 3058.696533203125 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 602.185302734375 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 1217.4622802734375 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.578, loss_val: nan, pos_over_neg: 2197.789794921875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 1766.778564453125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 1231.436279296875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1159.984619140625 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 1256.176513671875 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 1104.414794921875 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 1084.658935546875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 802.810546875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 907.1353759765625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 1053.5654296875 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.5802, loss_val: nan, pos_over_neg: 1610.94775390625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 1493.9461669921875 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 700.0232543945312 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.5842, loss_val: nan, pos_over_neg: 447.791259765625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1099.4068603515625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.5731, loss_val: nan, pos_over_neg: -5241.08251953125 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 722.4288330078125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 4234.27197265625 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1545.8642578125 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 1480.216796875 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 1149.4508056640625 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 1435.0914306640625 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 990.08154296875 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.5864, loss_val: nan, pos_over_neg: 912.4810791015625 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 44436.6796875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1036.749267578125 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 1534.08251953125 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 1054.859130859375 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 1296.136962890625 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 2712.571044921875 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 10227.0439453125 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 1475.269287109375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 3027.881591796875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 2484.0205078125 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.5727, loss_val: nan, pos_over_neg: 1693.055419921875 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: -29411.36328125 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 1092.27734375 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 685.444091796875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 1321.2857666015625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1448.7880859375 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.5801, loss_val: nan, pos_over_neg: 1325.906982421875 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 5460.32763671875 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 2411.942626953125 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 2157.245361328125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 4014.75830078125 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 2223.899169921875 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1112.2587890625 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 4677.07275390625 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 1058.9974365234375 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 667.892822265625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 2126.788330078125 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.5829, loss_val: nan, pos_over_neg: 691.3027954101562 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 1653.1522216796875 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 1448.9510498046875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 49991.5625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 12493.7353515625 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: -3361.196044921875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 7505.796875 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 2802.17041015625 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: -21865.75 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 2328.0380859375 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 2442.496826171875 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 1191.6859130859375 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 1544.9136962890625 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.5675, loss_val: nan, pos_over_neg: 2015.95751953125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 932.9365844726562 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 2327.17578125 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.5892, loss_val: nan, pos_over_neg: 1255.2548828125 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 929.4910888671875 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 2558.874755859375 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.5852, loss_val: nan, pos_over_neg: 2474.08447265625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 1549.74267578125 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 2406.855224609375 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 1513.21923828125 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 1406.719482421875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 2276.1630859375 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 2052.411865234375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1093.5992431640625 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.5874, loss_val: nan, pos_over_neg: 1052.066650390625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 1222.0791015625 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.579, loss_val: nan, pos_over_neg: 706.05517578125 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 2174.998046875 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 1200.353515625 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.5836, loss_val: nan, pos_over_neg: 1065.9373779296875 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.5825, loss_val: nan, pos_over_neg: 1628.3701171875 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 10859.9267578125 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1836.9071044921875 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 1983.266845703125 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 1215.951904296875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1683.20947265625 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1509.790283203125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 742.7537231445312 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.578, loss_val: nan, pos_over_neg: 1461.767578125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 6624.29345703125 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: -7053.18798828125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 1121.751220703125 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 1512.978515625 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1660.57373046875 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 4785.791015625 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 1937.2442626953125 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 1026.5804443359375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 1123.1187744140625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 2976.412109375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.5809, loss_val: nan, pos_over_neg: 889.6715087890625 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 3317.870361328125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 2701.41162109375 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 2696.28759765625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 855.8779907226562 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: -10413.5068359375 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 487.3151550292969 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: -2835.51171875 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 1647.5369873046875 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 2323.037353515625 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 771.3362426757812 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: -61705.43359375 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 2601.0146484375 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: -5239.1650390625 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.576, loss_val: nan, pos_over_neg: 1317.6475830078125 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 1163.2847900390625 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 1884.6131591796875 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 3952.391357421875 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 9160.2314453125 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: -8766.720703125 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 2208.150634765625 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.5838, loss_val: nan, pos_over_neg: 1798.3585205078125 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1447.5673828125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 3329.52490234375 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 1221.769287109375 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 635.7614135742188 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 4488.4521484375 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 3685.065185546875 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 1335.60595703125 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.5823, loss_val: nan, pos_over_neg: 748.9292602539062 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 1582.48876953125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1173.1298828125 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: -24685.78125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 4829.388671875 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 2865.23095703125 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 1688.369140625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.5828, loss_val: nan, pos_over_neg: 1369.4188232421875 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 1238.4971923828125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 2858.66650390625 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 1884.1551513671875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 997.8008422851562 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 1067.9791259765625 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 1886.6903076171875 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.58, loss_val: nan, pos_over_neg: 868.6942749023438 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 1617.022216796875 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 1343.51318359375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 1373.3909912109375 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 966.7333374023438 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.5858, loss_val: nan, pos_over_neg: 580.88232421875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 2898.537841796875 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 619.6966552734375 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 2173.836181640625 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 1140.202392578125 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 965.5933837890625 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 656.4476928710938 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.5827, loss_val: nan, pos_over_neg: 4057.530517578125 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 3353.58154296875 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 1157.6737060546875 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 3338.754150390625 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 4378.97265625 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 8028.22998046875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 1155.66015625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 695.761474609375 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1485.5711669921875 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.5804, loss_val: nan, pos_over_neg: 958.1241455078125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1039.208251953125 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 16447.142578125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 3318.0146484375 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 1743.196044921875 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 2043.3580322265625 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 1009.8057250976562 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 2393.91455078125 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 1846.42822265625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 1184.39306640625 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 733.9873657226562 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 1754.4312744140625 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.59, loss_val: nan, pos_over_neg: 671.1968383789062 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.5812, loss_val: nan, pos_over_neg: 971.88134765625 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 1044.17626953125 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 5910.05615234375 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 2007.6419677734375 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.5739, loss_val: nan, pos_over_neg: 2393.198974609375 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 57793.1171875 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 7188.0986328125 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: -3062.4697265625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 604.24609375 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.5756, loss_val: nan, pos_over_neg: 1147.4921875 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.5731, loss_val: nan, pos_over_neg: 4815.619140625 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: -7760.22705078125 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.5801, loss_val: nan, pos_over_neg: 2492.707763671875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 1527.0416259765625 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: -4747.82373046875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 1913.9332275390625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 3518.069580078125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 987.0059814453125 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 2484.7841796875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.5857, loss_val: nan, pos_over_neg: 1937.3544921875 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1094.83056640625 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 2371.135986328125 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 2199.647216796875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 3068.302978515625 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1535.069580078125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.5816, loss_val: nan, pos_over_neg: 2370.264404296875 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.5822, loss_val: nan, pos_over_neg: 1619.967041015625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.5755, loss_val: nan, pos_over_neg: 2576.746337890625 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.5756, loss_val: nan, pos_over_neg: 2180.87744140625 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 813.8731079101562 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 1853.5736083984375 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 3938.22607421875 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 779.6481323242188 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 1781.5596923828125 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.5829, loss_val: nan, pos_over_neg: 773.5681762695312 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.5873, loss_val: nan, pos_over_neg: 1251.980712890625 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 1007.5269165039062 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 752.1822509765625 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 1714.718505859375 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 1164.3590087890625 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 1980.193359375 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 1787.3516845703125 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.5832, loss_val: nan, pos_over_neg: 1066.3665771484375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [3:04:43<184145:34:12, 2209.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 2701.80908203125 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: -5520.7392578125 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 845.5919189453125 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1122.6024169921875 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 1042.2950439453125 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 2926.828125 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 8020.0869140625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 2065.993408203125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 2093.46875 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.5885, loss_val: nan, pos_over_neg: 1270.630859375 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 1556.89013671875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 28760.58203125 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1420.2161865234375 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: -9896.4248046875 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 2270.322021484375 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 5894.34326171875 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 1455.5657958984375 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 1069.292236328125 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 784.90576171875 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 3195.931396484375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1844.1119384765625 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.5841, loss_val: nan, pos_over_neg: 1058.4991455078125 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 1164.848388671875 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 2774.8525390625 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 3205.2373046875 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 3275.9609375 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1399.3037109375 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 3282.06982421875 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 4675.0625 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 4884.142578125 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 4026.7783203125 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: -2627906.75 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: -5102.2939453125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 2262.634033203125 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 5963.08056640625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 1070.576904296875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 1583.8251953125 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 33693.77734375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 714.5953369140625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 4506.37548828125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 1417.6583251953125 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.5809, loss_val: nan, pos_over_neg: -127803.375 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1052.920166015625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: -8208.1484375 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.586, loss_val: nan, pos_over_neg: 1289.7254638671875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 4133.0703125 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 2338.810791015625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1333.53662109375 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 2524.877685546875 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 1848.031494140625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.562, loss_val: nan, pos_over_neg: -16049.1162109375 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 7715.81640625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 1891.2032470703125 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 839.7054443359375 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 1819.6441650390625 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 1105.83935546875 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 2662.15283203125 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 1301.2044677734375 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.584, loss_val: nan, pos_over_neg: 1048.810791015625 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 2093.085693359375 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 4925.50927734375 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 4531.4912109375 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.5757, loss_val: nan, pos_over_neg: 2905.249267578125 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1151.43701171875 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 1643.6929931640625 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.583, loss_val: nan, pos_over_neg: 2323.71728515625 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 2484.412353515625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 877.1255493164062 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 3138.300048828125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 909.5970458984375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 919.4717407226562 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 832.4746704101562 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 4015.343017578125 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 962.32861328125 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1218.4053955078125 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 1168.9923095703125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 2792.937255859375 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 1263.8814697265625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 1368.05810546875 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.5818, loss_val: nan, pos_over_neg: 1805.3531494140625 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 714.3748779296875 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 3252.373291015625 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.5835, loss_val: nan, pos_over_neg: 688.8245239257812 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.578, loss_val: nan, pos_over_neg: 1667.41748046875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 890.0928955078125 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 1317.8377685546875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1031.3519287109375 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 1782.2584228515625 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 848.9881591796875 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 2021.09814453125 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 1564.0108642578125 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1070.013671875 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 595.0208129882812 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 872.5670166015625 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.5785, loss_val: nan, pos_over_neg: -12734.40234375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 2522.631103515625 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 977.1473388671875 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 1053.8656005859375 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.5901, loss_val: nan, pos_over_neg: 427.6545104980469 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 907.37353515625 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 1234.3665771484375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 1298.7607421875 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 1067.6505126953125 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 3700.003662109375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 3892.249755859375 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1509.8572998046875 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.5837, loss_val: nan, pos_over_neg: 1513.6649169921875 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 3363.17236328125 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.5786, loss_val: nan, pos_over_neg: 1070.91064453125 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 3136.7021484375 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 155504.515625 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 916.1644897460938 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.5731, loss_val: nan, pos_over_neg: 689.2310180664062 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 1815.7396240234375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 26724.71484375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 983.60693359375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 438.4024658203125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 1278.9644775390625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 692.890869140625 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 773.2662353515625 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 1263.66650390625 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.5849, loss_val: nan, pos_over_neg: 412.638671875 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 681.7437133789062 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 2352.10205078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.5769, loss_val: nan, pos_over_neg: 1956.4498291015625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.5793, loss_val: nan, pos_over_neg: 1703.0870361328125 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 2315.9541015625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 971.1859130859375 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 1788.2344970703125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.5785, loss_val: nan, pos_over_neg: 869.088623046875 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1172.6942138671875 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 1992.17822265625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 722.7689208984375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 702.0316772460938 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 786.3316040039062 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 3155.346435546875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 3381.397216796875 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 4537.60986328125 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 1112.47265625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 4720.986328125 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 4050.30908203125 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 2989.642822265625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.585, loss_val: nan, pos_over_neg: 2057.43603515625 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 1282.20263671875 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 900.710205078125 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.578, loss_val: nan, pos_over_neg: 886.343505859375 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 889.4067993164062 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 1218.6427001953125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 2618.4931640625 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 2740.7939453125 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 1012.153076171875 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 3097.04248046875 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 1271.57763671875 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 2802.188232421875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.558, loss_val: nan, pos_over_neg: -22766.8203125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: -12688.775390625 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: -7248.4150390625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 4506.9189453125 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 3322.106201171875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 3166.033447265625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 3015.151123046875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 1570.0245361328125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 2112.152099609375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 1459.1702880859375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.5781, loss_val: nan, pos_over_neg: 1580.096435546875 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 1346.314453125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 9094.6474609375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 816.8788452148438 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: -9857.9404296875 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 1012.9284057617188 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 955.9349365234375 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 2559.85498046875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 1157.5567626953125 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1926.3084716796875 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 1460.5863037109375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 1682.663818359375 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 942.9136962890625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 1584.8514404296875 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 1022.2971801757812 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 1557.4324951171875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 1336.66845703125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 1910.4229736328125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 2689.384765625 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 1348.7509765625 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 602.9677734375 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: -7201.9541015625 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 35001.96484375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 2892.31787109375 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 2093.804931640625 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 4382.91357421875 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 7113.267578125 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 1192.6497802734375 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 2626.170166015625 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 1079.4337158203125 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.5793, loss_val: nan, pos_over_neg: 841.5172729492188 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 1169.3570556640625 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: -19701.08984375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.5862, loss_val: nan, pos_over_neg: 844.4815063476562 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 7503.865234375 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: -6133.912109375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 1531.9293212890625 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.5776, loss_val: nan, pos_over_neg: 1065.833984375 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 1167.856201171875 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 1235.4036865234375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 2489.9716796875 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1302.52880859375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 868.1802368164062 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 1213.96923828125 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.5786, loss_val: nan, pos_over_neg: 1098.66943359375 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.565, loss_val: nan, pos_over_neg: -8058.54443359375 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 3378.10888671875 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 1354.498046875 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1835.83935546875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 1658.9200439453125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: -7833.953125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 3730.53125 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1413.663818359375 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 1808.03125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 1335.94677734375 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 3768.86669921875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 5278.3349609375 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1099.8131103515625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 1298.37109375 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 1292.748291015625 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.5815, loss_val: nan, pos_over_neg: 875.0675659179688 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 1662.0010986328125 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 6623.16015625 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 1129.9224853515625 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 17857.158203125 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 3187.83349609375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 112407.8984375 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: -15229.318359375 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1773.8133544921875 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 1210.2237548828125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 3168.049072265625 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.564, loss_val: nan, pos_over_neg: -4106.89306640625 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 5839.40966796875 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 5466.22119140625 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 6129.49658203125 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 2272.20556640625 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 732.2982788085938 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 4078.5078125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 2133.783447265625 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 4109.49951171875 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 1873.8653564453125 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: -20861.396484375 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 1629.5296630859375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.5853, loss_val: nan, pos_over_neg: 2843.202392578125 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: -96279.5078125 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 1622.022705078125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 3666.602783203125 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 967.1998901367188 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 783.52685546875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1050.4857177734375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 890.341064453125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: -24547.265625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 1615.793701171875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 1717.8948974609375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 27491.755859375 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 8806.81640625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: -130922.453125 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.5767, loss_val: nan, pos_over_neg: 1680.821044921875 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 1075.7689208984375 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 3747.364013671875 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.5861, loss_val: nan, pos_over_neg: 482.1060485839844 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 1358.744140625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 5955.37353515625 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 1960.43017578125 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 69868.921875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 1134.416748046875 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 1556.5260009765625 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 4559.333984375 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 9615.33203125 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1512.0660400390625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 834.863525390625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1980.728271484375 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1401.864501953125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 13344.9951171875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1510.85791015625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 5871.927734375 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 4547.9970703125 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 2664.7958984375 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 893.4385375976562 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 812.8118896484375 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 1784.2896728515625 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 2135.150146484375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: -6859.4150390625 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 9444.5546875 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 1183.7940673828125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 1601.096923828125 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: -7787.71044921875 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 2048.503662109375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 701.7920532226562 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 5296.05126953125 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 1958.4608154296875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 856.9874877929688 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 2529.212646484375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 26947.326171875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 1206.9373779296875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 1281.4302978515625 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.5802, loss_val: nan, pos_over_neg: 2792.397216796875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 7747.52001953125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1340.340087890625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 3544.2041015625 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 4426.0498046875 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.5769, loss_val: nan, pos_over_neg: 944.1593017578125 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.5886, loss_val: nan, pos_over_neg: 960.6263427734375 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 2954.794921875 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: -8099.970703125 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 1028.5726318359375 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 1233.2333984375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 2387.37890625 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 2467.309814453125 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 10784.2861328125 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.5792, loss_val: nan, pos_over_neg: 2047.5335693359375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 4485.37060546875 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 3758.38427734375 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 3998.525146484375 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 736.8701782226562 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: 583.5096435546875 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.5807, loss_val: nan, pos_over_neg: 821.833984375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 941.488525390625 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 1998.917724609375 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1091.453369140625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 3441.64697265625 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 2856.709716796875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 22714.544921875 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 2083.76708984375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 4852.4873046875 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 976.1925048828125 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 1169.00537109375 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 812.7669067382812 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 1354.6190185546875 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 1007.3697509765625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 523.473388671875 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 1942.615966796875 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.5772, loss_val: nan, pos_over_neg: 1147.4085693359375 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 598.4194946289062 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 1525.030517578125 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1788.75634765625 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 9510.515625 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1549.865234375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 678.9761352539062 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.5774, loss_val: nan, pos_over_neg: 1867.3721923828125 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 2050.741455078125 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 736.4194946289062 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 2481.093994140625 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 1119.9630126953125 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.5805, loss_val: nan, pos_over_neg: 1071.8323974609375 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.5851, loss_val: nan, pos_over_neg: 2058.295166015625 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 1297.93115234375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 721.7528686523438 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 4596.5849609375 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 1123.1055908203125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1431.8736572265625 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 1372.8909912109375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 10852.43359375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1257.8822021484375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 862.8207397460938 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1738.0777587890625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 2427.1064453125 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.5751, loss_val: nan, pos_over_neg: 1911.572021484375 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 928.76611328125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 735.9498291015625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 521.3526611328125 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1224.744140625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 2521.79052734375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.5802, loss_val: nan, pos_over_neg: 2508.344482421875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.5799, loss_val: nan, pos_over_neg: 2439.5888671875 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 909.7896118164062 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1039.66357421875 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 973.39013671875 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 9081.66796875 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 1928.239990234375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 825.4694213867188 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 701.949462890625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 1296.7432861328125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 1399.8978271484375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 846.521240234375 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 2658.064453125 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 2245.320556640625 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 1969.1927490234375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 675.3226318359375 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 1975.4168701171875 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1721.212158203125 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 1154.776123046875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 680.5824584960938 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 567.672119140625 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 576.414306640625 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 1153.4466552734375 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 6709.31005859375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 630.0101928710938 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1316.4022216796875 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 1762.356201171875 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 787.1551513671875 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 1298.2235107421875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 1586.9423828125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 4696.28173828125 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 808.84033203125 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 583.8487548828125 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 1915.456298828125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: -3531.748779296875 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1646.916259765625 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 818.7958984375 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 1024.77392578125 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1566.1033935546875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 1124.6417236328125 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 18197.130859375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 11090.84765625 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 785.3378295898438 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 3424.451171875 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1482.8050537109375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1280.747802734375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 3107.1103515625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 938.4923706054688 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 1815.5069580078125 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 3737.13525390625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1698.8167724609375 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.5777, loss_val: nan, pos_over_neg: 947.9606323242188 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1403.3707275390625 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.5794, loss_val: nan, pos_over_neg: 1041.2041015625 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 1418.1279296875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: -44465.453125 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 805.0313720703125 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 2137.958984375 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 1452.7113037109375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 746.0474853515625 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 2123.86083984375 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 2603.61376953125 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 1694.549072265625 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 920.8615112304688 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 2000.6888427734375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 2514.7177734375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.5813, loss_val: nan, pos_over_neg: 1069.3057861328125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 866.91064453125 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 1884.7205810546875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 4207.2529296875 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 3714.486572265625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 5089.16455078125 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 3390.9638671875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 1476.807861328125 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 3211.2685546875 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 2975.707275390625 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 2137.354248046875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 61827.66796875 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 2423.57373046875 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 2819.436767578125 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 4928.93896484375 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 1921.134033203125 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1231.90234375 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1289.3179931640625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 1582.8255615234375 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 1292.474365234375 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 911.0233764648438 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 2753.52197265625 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 8284.76171875 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 2586.055908203125 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 824.1260986328125 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 2759.52392578125 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 1260.0950927734375 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1250.5440673828125 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.5843, loss_val: nan, pos_over_neg: 899.0938720703125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.574, loss_val: nan, pos_over_neg: 1299.7115478515625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 510.5015563964844 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 3147.5458984375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1982.0318603515625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 5386.0654296875 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 1665.325439453125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 507.9305114746094 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 941.309326171875 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1202.6790771484375 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 2473.9697265625 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1456.90283203125 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 5772.38525390625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 2790.77587890625 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 815.4392700195312 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 961.3363647460938 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 2137.208251953125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1217.7166748046875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 1085.198486328125 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 880.0406494140625 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 1152.5828857421875 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 1025.6556396484375 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1821.7430419921875 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 979.6992797851562 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 1028.97607421875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 1059.776123046875 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 831.6055297851562 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1103.4620361328125 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 3308.260498046875 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 1711.89306640625 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: -2645.289794921875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1264.9774169921875 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: -13683.271484375 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 2532.79931640625 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 1091.9559326171875 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 1470.5770263671875 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 4169.4423828125 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1905.0164794921875 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 1496.8004150390625 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 4688.365234375 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 1712.01904296875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: -7169.314453125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.5769, loss_val: nan, pos_over_neg: 1728.1265869140625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: -2514.690185546875 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.5785, loss_val: nan, pos_over_neg: 3097.954345703125 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 7290.8623046875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: -5079.35546875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 1546.97265625 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: -3959.78515625 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: -27136.05859375 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 2600.359619140625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 4209.02783203125 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 3180.468994140625 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 806.1710815429688 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: -7464.1640625 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1882.1929931640625 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.5834, loss_val: nan, pos_over_neg: 443.3907165527344 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1495.8662109375 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 2387.78271484375 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 748.0013427734375 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 1767.5545654296875 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 4606.66796875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: -11372.6396484375 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 1775.5238037109375 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 3448.20751953125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1202.218505859375 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 2346.188232421875 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.5802, loss_val: nan, pos_over_neg: 926.3170166015625 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1633.210693359375 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 826.3424682617188 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 1088.979248046875 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 1241.1392822265625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 1069.41357421875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.5739, loss_val: nan, pos_over_neg: 1834.3360595703125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 17154.81640625 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 2215.17236328125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1009.5457153320312 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 3253.072509765625 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 5521.75732421875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 1038.2071533203125 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 3861.984375 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1731.9853515625 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 4704.52001953125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 2919.927978515625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.5814, loss_val: nan, pos_over_neg: 735.24462890625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 2224.196533203125 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 1302.0419921875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1715.7841796875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1208.0850830078125 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 4535.8017578125 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1115.73388671875 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 1203.639892578125 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.5731, loss_val: nan, pos_over_neg: 1147.6917724609375 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.5732, loss_val: nan, pos_over_neg: 496.9166564941406 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 1382.474365234375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 1071.3052978515625 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 951.7250366210938 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 1199.64892578125 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.5727, loss_val: nan, pos_over_neg: 700.0061645507812 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1297.822021484375 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 2108.8095703125 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 2289.5546875 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: -30322.34375 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 1390.9853515625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 649.4287719726562 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1332.938720703125 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 988.8295288085938 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 1325.9210205078125 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1612.9844970703125 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1238.4658203125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 1651.702880859375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 561.416259765625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 893.3971557617188 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.5782, loss_val: nan, pos_over_neg: 623.9149169921875 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 1775.463623046875 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.5763, loss_val: nan, pos_over_neg: 1328.3797607421875 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1381.8094482421875 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 935.7179565429688 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: -90297.890625 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 3010.36572265625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1235.6710205078125 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 1619.217529296875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.5854, loss_val: nan, pos_over_neg: 1456.5006103515625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 3935.274169921875 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 2595.304443359375 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 948.890625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.5846, loss_val: nan, pos_over_neg: 998.7769165039062 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.5744, loss_val: nan, pos_over_neg: 1233.27685546875 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 1396.884033203125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 4431.59326171875 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 968.533447265625 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 3403.973876953125 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: -5072.7392578125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1817.6959228515625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.576, loss_val: nan, pos_over_neg: 1137.66259765625 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 993.3539428710938 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: -32765.615234375 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 4387.21337890625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 1053.721435546875 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 1651.3729248046875 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 1896.8663330078125 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 4721.61083984375 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 3972.699951171875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 2750.349365234375 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 2297.86669921875 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 9200.6533203125 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: -68223.375 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 4713.17822265625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: -55624.203125 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 854.7733764648438 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 2486.904541015625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 1616.6422119140625 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 3458.841064453125 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 1573730.25 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 1127.734375 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.5771, loss_val: nan, pos_over_neg: 3699.783447265625 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1130.25 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 2468.541015625 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 14826.0546875 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.5766, loss_val: nan, pos_over_neg: 2992.7275390625 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 940.4588012695312 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 1110.748779296875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 5596.173828125 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 1684.616943359375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 1289.0111083984375 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 866.6360473632812 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 2021.5203857421875 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 1718.23291015625 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.5739, loss_val: nan, pos_over_neg: 855.80615234375 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 2371.86962890625 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.5762, loss_val: nan, pos_over_neg: 3415.296875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1481.5740966796875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 1317.26953125 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 714.0228271484375 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 2750.31689453125 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.5773, loss_val: nan, pos_over_neg: 1260.8101806640625 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1216.4759521484375 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 975.2293090820312 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 2013.3095703125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 1225.966796875 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 1222.760009765625 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 832.6614379882812 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 1298.5269775390625 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 2334.578125 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 2792.852783203125 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 967.9141235351562 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 648.964111328125 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 37446.515625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1773.615234375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 4096.0810546875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 1246.6236572265625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 3920.181884765625 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1804.3636474609375 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 2499.524169921875 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 2150.416015625 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 5962.7685546875 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 1085.9735107421875 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 3055.00146484375 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 1142.7723388671875 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 939.222412109375 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 8054.09228515625 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.568, loss_val: nan, pos_over_neg: -254288.96875 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 5341.43115234375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 1248.7447509765625 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 2034.488525390625 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 999.740234375 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 6467.87744140625 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 4336.36376953125 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 2292.42578125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 1882.111572265625 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 4347.89990234375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 1986.356201171875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 12749.5556640625 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 6847.66650390625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 3503.439453125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 4962.20703125 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 2570.9052734375 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 2017.163330078125 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 1618.4820556640625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 4478.56298828125 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.5883, loss_val: nan, pos_over_neg: 1170.7967529296875 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1109.7579345703125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.5808, loss_val: nan, pos_over_neg: 1489.6610107421875 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: -18956.068359375 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1537.1649169921875 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 2355.4287109375 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 1666.0831298828125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.5791, loss_val: nan, pos_over_neg: 1274.038330078125 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 6064.095703125 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 1515.397216796875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.5723, loss_val: nan, pos_over_neg: 2226.785888671875 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 2303.29248046875 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 2420.488525390625 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 11052.2744140625 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 16850.5078125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.5727, loss_val: nan, pos_over_neg: 1360.6953125 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1205.1259765625 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 3339.4345703125 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 2860.3125 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 8316.33203125 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 876.8949584960938 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1755.694580078125 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 1351.739501953125 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: -56346.1328125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: -63243.26171875 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1864.0648193359375 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 37377.84765625 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1088.1519775390625 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: -3745.290771484375 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 1803.4227294921875 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 836.6829833984375 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 2112.7275390625 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 1765.6201171875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 1763.8236083984375 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 2661.28369140625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 1448.605224609375 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.5765, loss_val: nan, pos_over_neg: 1836.3436279296875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1331.1767578125 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 1435.54296875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 988.8023071289062 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 1551.367431640625 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 2551.8076171875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.582, loss_val: nan, pos_over_neg: 1154.5477294921875 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 4733.93212890625 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 2190.98583984375 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 712.0225830078125 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 857.4668579101562 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 1726.80224609375 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 3628.877197265625 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.5778, loss_val: nan, pos_over_neg: 487.2298889160156 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 541.131103515625 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1288.0750732421875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 3750.696533203125 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 2336.7333984375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.5747, loss_val: nan, pos_over_neg: 1269.65283203125 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1191.9615478515625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 3602.080810546875 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 1145.9285888671875 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 1565.920654296875 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1605.776123046875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.5758, loss_val: nan, pos_over_neg: 3038.765869140625 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.5739, loss_val: nan, pos_over_neg: 1105.6063232421875 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 3109.186279296875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1662.6578369140625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 870.8848876953125 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 1683.4892578125 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 2005.5550537109375 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 1943.0950927734375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.5759, loss_val: nan, pos_over_neg: 1829.0234375 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 2304.49072265625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 1849.9490966796875 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 10528.0693359375 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 3089.04248046875 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 1459.72607421875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 927.8428955078125 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 3551.962158203125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 1372.05029296875 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 1290.4002685546875 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 1123.7359619140625 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1073.9794921875 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 1254.49365234375 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 4193.99755859375 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 1803.334228515625 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.5806, loss_val: nan, pos_over_neg: 950.1527099609375 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1499.4212646484375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 967.5653686523438 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 1195.3271484375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 2271.8232421875 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 2265.797119140625 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 963.1228637695312 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1383.283203125 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 930.0557250976562 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 2943.287353515625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 1316.6363525390625 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 2540.0302734375 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 2100.086181640625 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1495.884033203125 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 2597.00390625 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 6898.36669921875 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 6564.97216796875 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 882.5781860351562 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.5675, loss_val: nan, pos_over_neg: 629.0792846679688 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 1820.4163818359375 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 790.0986328125 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 3754.586181640625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: -8741.7998046875 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 746.2594604492188 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1978.0927734375 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 3968.947998046875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 3400.141357421875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 22915.064453125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: -11977.353515625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.5757, loss_val: nan, pos_over_neg: 5067.09912109375 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 1201.49853515625 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 822.3340454101562 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 3497.261962890625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 3964.587646484375 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.5797, loss_val: nan, pos_over_neg: 702.5651245117188 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 3794.93701171875 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 869.5777587890625 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 7889.7177734375 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 822.4444580078125 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1213.0054931640625 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1020.0811157226562 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 722.977783203125 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 2105.504150390625 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 1273.5220947265625 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 2312.4814453125 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 1344.911865234375 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 823.1344604492188 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1155.416015625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 7014.076171875 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 774.1311645507812 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1369.683837890625 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 1089.632568359375 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 2801.640380859375 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 7916.28759765625 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 8336.115234375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1991.0982666015625 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.577, loss_val: nan, pos_over_neg: 806.627197265625 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 1007.9537353515625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.5737, loss_val: nan, pos_over_neg: 1310.92529296875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 4066.004638671875 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 1130.7552490234375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 2883.781494140625 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 2738.34375 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 834.5729370117188 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 3173.321044921875 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.5788, loss_val: nan, pos_over_neg: 4332.9326171875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1292.0283203125 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 3055.46923828125 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 15049.5263671875 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 3959.06298828125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 1318.1676025390625 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.5789, loss_val: nan, pos_over_neg: 892.3307495117188 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 1162.115234375 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2070.22998046875 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 2423.95458984375 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: -3282.424560546875 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 2569.620361328125 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 2930.775634765625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 9006.2412109375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 4373.83984375 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 1076.5782470703125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.5756, loss_val: nan, pos_over_neg: 9722.953125 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 3047.974609375 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 6889.3935546875 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 1651.006103515625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1707.98291015625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 2503.94677734375 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 4889.39599609375 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 3598.36669921875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 1227.0689697265625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 849.51123046875 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1470.6173095703125 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 1460.284423828125 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.564, loss_val: nan, pos_over_neg: -2657.729248046875 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.5741, loss_val: nan, pos_over_neg: 1052.63671875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1049.8680419921875 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 1871.598876953125 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.5739, loss_val: nan, pos_over_neg: 1294.2816162109375 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1034.8017578125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 4912.005859375 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 647.4524536132812 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 1254.15771484375 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1192.8865966796875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 1123.775390625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 960.3814086914062 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 2591.740966796875 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 1388.5274658203125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 2851.87939453125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 3467.47607421875 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.5751, loss_val: nan, pos_over_neg: 1076.9052734375 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1237.711181640625 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 1869.7437744140625 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.556, loss_val: nan, pos_over_neg: -9952.5751953125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 1705.320068359375 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 2575.781982421875 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 839.9946899414062 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: -40920.296875 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 2957.347412109375 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 3020.413818359375 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1592.30712890625 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 1683.444580078125 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 5082.0712890625 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 2333.554931640625 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.575, loss_val: nan, pos_over_neg: 1150.54541015625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 2040.375732421875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [3:41:41<184368:14:48, 2212.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 5321.65087890625 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 1091.3629150390625 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.5756, loss_val: nan, pos_over_neg: 1184.7325439453125 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 2225.368896484375 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 802.6255493164062 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 1366.9344482421875 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 5835.5302734375 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 2244.75830078125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 754.78466796875 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 831.50048828125 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: -14857.8857421875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 1169.56591796875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 1610.8984375 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 2459.886474609375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 64439.1015625 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 1559.0047607421875 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1425.1805419921875 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 3790.291748046875 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 8062.2119140625 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1518.2593994140625 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 2022.5518798828125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 4262.20263671875 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 3959.850341796875 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 1401.359375 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 2519.8623046875 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 2268.870361328125 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 1982.16015625 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 1157.0799560546875 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.5784, loss_val: nan, pos_over_neg: 2409.649658203125 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 2778.977294921875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.5761, loss_val: nan, pos_over_neg: 1323.8514404296875 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.5482, loss_val: nan, pos_over_neg: 1340.337158203125 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 43962.53125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 14056.5849609375 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1231.62158203125 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.5731, loss_val: nan, pos_over_neg: 884.3856201171875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.558, loss_val: nan, pos_over_neg: -5096.69873046875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 6476.33154296875 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 2845.724365234375 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 3937.831787109375 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 1309.85107421875 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 4530.4345703125 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 7839.01611328125 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1970.95654296875 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1061.570068359375 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 2182.72998046875 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 2308.05322265625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: -5394.75341796875 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 1382.34423828125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 3679.22607421875 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 7884.15478515625 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1393.7828369140625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 2245.980224609375 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 746.8141479492188 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 1920.7833251953125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 6525.44580078125 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1833.3719482421875 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1116.5137939453125 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1402.359130859375 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1224.89453125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 735.3382568359375 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 1042.83740234375 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 1634.6199951171875 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 4332.18408203125 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 1426.027587890625 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 1091.05712890625 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.5707, loss_val: nan, pos_over_neg: 1648.285888671875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.5685, loss_val: nan, pos_over_neg: 1044.739013671875 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1257.6146240234375 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.572, loss_val: nan, pos_over_neg: 2389.503173828125 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 2444.78076171875 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1547.5386962890625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 2203.277587890625 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 1600.361328125 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 1226.3109130859375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 883.3878784179688 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 669.5721435546875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 2729.6484375 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 1058.8917236328125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 5505.55859375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 2149.393798828125 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1671.46923828125 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 3194.83740234375 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 3921.579833984375 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 2696.759765625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 1619.9608154296875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 1132.372802734375 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1467.6075439453125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1051.1231689453125 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 1183.387451171875 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 1527.7608642578125 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 1061.469970703125 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 957.1248168945312 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 2016.0802001953125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: -2402.4384765625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 6784.796875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.5717, loss_val: nan, pos_over_neg: 2447.95068359375 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 1245.300537109375 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 2074.568115234375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 1003.0743408203125 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 2191.36865234375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 2260.180419921875 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 1369.547119140625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 4779.6357421875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 905.2701416015625 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 1090.4962158203125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 2145.6845703125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 1636.2662353515625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 3453.33642578125 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 919.2160034179688 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 18146.662109375 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 2105.242431640625 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 13830.49609375 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.5702, loss_val: nan, pos_over_neg: 652.4224243164062 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 4462.79052734375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: -6726.0517578125 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1317.9915771484375 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.5675, loss_val: nan, pos_over_neg: 1685.34619140625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 3043.5185546875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 1761.5133056640625 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 924.587646484375 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 1113.3616943359375 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 788.4447631835938 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 1033.946533203125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1453.8692626953125 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 1485.040283203125 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 1277.3912353515625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 2189.048828125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.5795, loss_val: nan, pos_over_neg: 709.0777587890625 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 5134.26318359375 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 1793.470947265625 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 1369.845458984375 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 7290.63525390625 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 2638.749267578125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 8879.212890625 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 633.9956665039062 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.5798, loss_val: nan, pos_over_neg: 1127.0758056640625 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 2173.8115234375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 1502.313720703125 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 2505.052978515625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 2609.800537109375 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 6163.0625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 2116.5341796875 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 1100.6590576171875 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 1939.197509765625 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.562, loss_val: nan, pos_over_neg: -6147.36279296875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.5775, loss_val: nan, pos_over_neg: 1839.173583984375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 1367.879638671875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 681.888671875 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1040.3482666015625 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 1972.1146240234375 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 1417.255615234375 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: -12196.4482421875 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: -12729.9794921875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1162.057861328125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 1488.1953125 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 2724.390869140625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 5572.671875 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 3320.1728515625 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 2028.157470703125 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 1914.998046875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 6672.24365234375 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 1874.7103271484375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 3862.256103515625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 20823.896484375 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 2410.641357421875 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 1117.7823486328125 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 766.867431640625 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1618.9588623046875 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 1126.348876953125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 2308.658447265625 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1638.0970458984375 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 1019.5463256835938 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 21434.6875 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 2305.166015625 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 1024.4267578125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1730.8372802734375 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 2297.906494140625 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 2870.195556640625 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 4351.14208984375 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 637.4346923828125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 2991.336669921875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 824.8563842773438 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 1277.5020751953125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 1514.8468017578125 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 962.02978515625 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 1420.85400390625 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 2609.350830078125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1719.332763671875 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 3622.30859375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 1813.8682861328125 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 848.2799682617188 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 831.0743408203125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 3090.46826171875 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 4608.8740234375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 2029.2088623046875 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 2122.398193359375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 1569.20703125 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 1513.337890625 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 2496.65380859375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 8186.58154296875 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 642.0198974609375 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 5390.45556640625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 1438.10498046875 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: -6078.15185546875 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1772.4259033203125 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: -4464.3994140625 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 736.4529418945312 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 911.8550415039062 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 3632.9228515625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 3044.9501953125 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1219.5130615234375 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 998.4967041015625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 1894.878662109375 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: -9483.068359375 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1224.9547119140625 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 1209.0128173828125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.556, loss_val: nan, pos_over_neg: -15921.9580078125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 8456.919921875 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 4411.927734375 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 3698.47265625 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1271.1156005859375 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 2277.384765625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 5148.01025390625 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 3189.36572265625 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 19235.13671875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 693.039794921875 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 2217.210693359375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 19276.732421875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 3049.572021484375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 1117.486328125 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 2208.75048828125 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.5753, loss_val: nan, pos_over_neg: 889.666259765625 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.5824, loss_val: nan, pos_over_neg: 1136.303955078125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 2363.89892578125 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 3744.36279296875 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 838.5835571289062 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 3124.2734375 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 1181.5311279296875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 1337.767333984375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 1080.947021484375 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 1087.3626708984375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 651.3986206054688 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: -12228.03125 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.5785, loss_val: nan, pos_over_neg: 1092.46875 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 2024.0855712890625 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 1679.996826171875 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 721.7658081054688 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 706.6005249023438 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.5721, loss_val: nan, pos_over_neg: 549.6303100585938 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1931.945556640625 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 2640.021484375 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1533.9493408203125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 4406.431640625 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 2703.464599609375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 7008.1328125 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1317.3187255859375 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 821.3737182617188 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 3546.990966796875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 1677.8319091796875 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 1414.022705078125 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 891.6177978515625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 3156.422119140625 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: -5175.912109375 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 2787.314453125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 1591.93505859375 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 4000.999267578125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 1702.1199951171875 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 1123.8785400390625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 1505.396728515625 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 15249.5712890625 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 5864.48095703125 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 2372.776611328125 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 1565.598876953125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 3407.089111328125 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 1533.6683349609375 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 8025.66064453125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 2381.740966796875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 1628.7799072265625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 17923.05859375 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 10059.73046875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: -14512.2060546875 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 1098.4901123046875 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 2760.9638671875 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1281.3372802734375 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 3226.762451171875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1309.9517822265625 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 2287.262939453125 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1234.52880859375 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.5779, loss_val: nan, pos_over_neg: 876.115234375 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 2473.96826171875 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 795.8381958007812 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1506.7127685546875 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 1974.5987548828125 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1042.6473388671875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1395.8184814453125 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 1803.636962890625 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 1146.5631103515625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.5754, loss_val: nan, pos_over_neg: 1127.900146484375 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 922.910888671875 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1238.85546875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 743.7632446289062 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 1091.953125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 1138.779052734375 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1226.674072265625 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 2999.702392578125 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: -10631.6025390625 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 1134.041259765625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 3632.26171875 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 38282.90625 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1530.78125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 2079.754150390625 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 1768.76123046875 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: -12476.8955078125 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1498.39990234375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 1856.5628662109375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 5731.56494140625 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 6849.412109375 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1198.1517333984375 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 2480.287841796875 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.5752, loss_val: nan, pos_over_neg: 633.4453125 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 8134.0498046875 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: -45247.265625 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: 1147.2874755859375 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 1041.8870849609375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 1200.271484375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 1849.0684814453125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 1884.8087158203125 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 1245.3984375 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 1166.02734375 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 860.1414794921875 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 25157.666015625 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 1666.0377197265625 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 1412.8448486328125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1005.1358032226562 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1202.3861083984375 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 58463.55859375 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 3524.326904296875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 1300.9163818359375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 8802.333984375 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 6153.654296875 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1229.8170166015625 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1369.5242919921875 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.5748, loss_val: nan, pos_over_neg: 4313.12353515625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 2690.598876953125 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 4231.2919921875 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 915.602783203125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 916.799560546875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 1516.7501220703125 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1590.02978515625 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 1470.724609375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 869.9912109375 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 540.2266845703125 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 2892.19873046875 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1762.4666748046875 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 821.9016723632812 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 1281.234619140625 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1669.7984619140625 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.5726, loss_val: nan, pos_over_neg: 1614.717041015625 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 3335.083984375 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 873.4041137695312 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 1893.69873046875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 1085.63720703125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 2876.6748046875 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 1098.21435546875 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 6929.4267578125 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.5768, loss_val: nan, pos_over_neg: 858.091552734375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 1521.4676513671875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 879.56494140625 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 884.8487548828125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 1576.6632080078125 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1214.2408447265625 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 1674.0380859375 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1083.0247802734375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 880.7337036132812 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 1076.87939453125 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 1329.347412109375 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.576, loss_val: nan, pos_over_neg: 648.7382202148438 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 843.7789306640625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1269.885498046875 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1152.430908203125 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 860.51220703125 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1020.1849365234375 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.5695, loss_val: nan, pos_over_neg: 791.7664184570312 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 789.2215576171875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 916.3914184570312 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1059.9591064453125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 799.0747680664062 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 1608.10009765625 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 733.8301391601562 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 616.3184814453125 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 1994.0545654296875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 1142.732177734375 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 945.0525512695312 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.5683, loss_val: nan, pos_over_neg: 1550.3792724609375 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1075.013427734375 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 804.5975341796875 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.5803, loss_val: nan, pos_over_neg: 494.1444091796875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1764.0859375 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 3671.806640625 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1745.449462890625 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 722.4623413085938 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.5552, loss_val: nan, pos_over_neg: 2395.15869140625 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 2081.696044921875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: -6942.0166015625 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: -29955.318359375 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.5817, loss_val: nan, pos_over_neg: 748.5474243164062 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 19682.546875 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 958.2244873046875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1053.9453125 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1318.729248046875 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 18672.326171875 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 38358.1796875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 12994.48046875 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.5518, loss_val: nan, pos_over_neg: 1396.688232421875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 2322.1396484375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 3058.184814453125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.5518, loss_val: nan, pos_over_neg: -3725.89404296875 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.5585, loss_val: nan, pos_over_neg: -23436.498046875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 3230.54443359375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 420.17041015625 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 597.5086059570312 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 2895.18115234375 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 10666.501953125 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 11049.306640625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 2881.417236328125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 13159.7216796875 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 1358.5936279296875 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 1590.8929443359375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.5725, loss_val: nan, pos_over_neg: 1096.0845947265625 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.5698, loss_val: nan, pos_over_neg: 1326.057861328125 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 3518.568115234375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: -13417.767578125 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 901.0765380859375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 1316.1246337890625 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 1188.6820068359375 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 1317.8492431640625 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1580.280517578125 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 1092.451904296875 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 808.710693359375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 1514.51904296875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 707.9557495117188 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 1318.631591796875 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 1497.07763671875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 1996.3511962890625 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 1691.3958740234375 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 4521.11083984375 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 1853.822265625 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 1654.8004150390625 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 2377.200439453125 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1472.433837890625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 3096.004638671875 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 5586.798828125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 2144.062255859375 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1988.08251953125 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 6761.8740234375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 7615.787109375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 4682.67529296875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: -11144.5546875 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 3886.396240234375 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 2094.0517578125 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1367.8486328125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1598.1611328125 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 1005.7445068359375 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 2610.7880859375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 2084.686279296875 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 2062.6943359375 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 1900.0050048828125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 15968.708984375 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: -5806.00732421875 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.5531, loss_val: nan, pos_over_neg: 4603.28857421875 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 80152.5625 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 5338.26708984375 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.5675, loss_val: nan, pos_over_neg: 10425.8779296875 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 1802.6416015625 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 2589.9140625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 811.2817993164062 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 2737.97021484375 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 8438.818359375 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 873.6277465820312 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 1020.2661743164062 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 918.4363403320312 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 1735.7330322265625 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 865.0073852539062 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 1979.9571533203125 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1420.412109375 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.5585, loss_val: nan, pos_over_neg: 948.485107421875 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 9920.9248046875 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 13958.6591796875 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.5573, loss_val: nan, pos_over_neg: 2167.428955078125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 1553.596435546875 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 2544.4140625 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 1398.295166015625 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 3609.35986328125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 4913.47509765625 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 4656.75439453125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 1188.66943359375 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 2209.418212890625 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.5764, loss_val: nan, pos_over_neg: 3025.372802734375 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 926.4011840820312 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1578.1383056640625 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1391.228271484375 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 1032.38134765625 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 1158.37158203125 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 1155.7275390625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1125.8414306640625 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: -8162.06201171875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1700.051025390625 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.5552, loss_val: nan, pos_over_neg: 2673.704345703125 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 2175.32861328125 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.5751, loss_val: nan, pos_over_neg: 1552.191162109375 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 1093.42236328125 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.5715, loss_val: nan, pos_over_neg: 692.4658813476562 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 2068.14990234375 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1754.8304443359375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 1216.583984375 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 742.9845581054688 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1646.4412841796875 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 9702.71484375 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 1801.3797607421875 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 1036.6529541015625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 2170.66748046875 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 2702.729248046875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 2770.0625 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1081.264892578125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 2104.47412109375 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 966.0814208984375 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 5511.74462890625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 3892.932861328125 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: -22245.77734375 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 1176.6365966796875 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 1435.871337890625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 1707.9835205078125 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 2221.542236328125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: -4358.45556640625 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 2508.188232421875 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 3536.107666015625 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 4524.9775390625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 7868.1904296875 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 4677.0595703125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 3454.71484375 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.5706, loss_val: nan, pos_over_neg: 1324.0582275390625 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 1889.75048828125 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 940.9776000976562 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1068.3319091796875 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 896.2924194335938 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.5736, loss_val: nan, pos_over_neg: 1010.0654296875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 905.759521484375 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1607.31884765625 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 1986.28369140625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 2922.582763671875 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 1140.3536376953125 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 821.2154541015625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 596.9387817382812 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 1135.0301513671875 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.573, loss_val: nan, pos_over_neg: 655.590576171875 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.5667, loss_val: nan, pos_over_neg: 3723.423828125 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 737.640380859375 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 1407.123291015625 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 1323.9654541015625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.5663, loss_val: nan, pos_over_neg: 3043.250244140625 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 1916.3980712890625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.5735, loss_val: nan, pos_over_neg: 573.4425048828125 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 2159.26953125 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.5585, loss_val: nan, pos_over_neg: 19841.755859375 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 2567.896484375 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 1159.6732177734375 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 1432.376220703125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 3638.13525390625 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 4398.80615234375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 1203.19677734375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 2962.120361328125 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 1279.009033203125 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.5502, loss_val: nan, pos_over_neg: -10622.912109375 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1566.448486328125 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1802.741943359375 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1565.698486328125 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 1031.654296875 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 1424.17919921875 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 2445.163330078125 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 1840.852294921875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 1426.796630859375 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 1727.16748046875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 1397.5694580078125 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 2089.305419921875 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 5227.33837890625 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 1337.4749755859375 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 1820.8924560546875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 2580.17431640625 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: -3715.978271484375 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 3296.102783203125 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 5034.2890625 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 6019.46337890625 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 1501.00048828125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 3302.16259765625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 1852.077880859375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 2517.53369140625 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 2841.159423828125 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 1476.2991943359375 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 2785.046142578125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 5302.650390625 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: -14650.8935546875 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 2951.7822265625 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 5289.5693359375 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 739.2191772460938 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 2112.8173828125 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 2322.10498046875 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 4359.9296875 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 1172.769775390625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 9256.3212890625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 2416.372314453125 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 3678.545654296875 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.5507, loss_val: nan, pos_over_neg: 1791.2176513671875 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 2036.75341796875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 5353.30419921875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 5101.4169921875 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: -95550.046875 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 5490.28369140625 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 1772.3206787109375 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: -9255.8818359375 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 729.6944580078125 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 1205.1824951171875 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 1056.0469970703125 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1124.2989501953125 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 27850.5546875 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1606.678955078125 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 851.155029296875 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 833.142822265625 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 8791.498046875 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 3077.99169921875 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 3775.905029296875 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 6154.052734375 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1046.628173828125 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 4041.57275390625 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 1276.12744140625 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: -8034.75244140625 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1002.9462280273438 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1371.3997802734375 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: -11396.55859375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: -4460.9521484375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 9573.85546875 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 2631.188232421875 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 2237.90234375 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.5675, loss_val: nan, pos_over_neg: 1076.832763671875 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 1110.6171875 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: -3272.662841796875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 174463.96875 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1625.0352783203125 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1972.365478515625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 3954.703857421875 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 13031.7919921875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1651.9532470703125 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 4223.19189453125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 1566.7786865234375 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.5729, loss_val: nan, pos_over_neg: 9618.646484375 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 82431.78125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 13920.5048828125 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 20844.25390625 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: -27392.517578125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 2272.95556640625 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: -15906.0341796875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 2681.56689453125 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: -16002.8046875 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 4467.0205078125 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 1421.2669677734375 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1870.3360595703125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 1138.9517822265625 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 814.9501953125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 5943.1669921875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 785.971923828125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 2260.789794921875 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 2657.0849609375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 1610.713623046875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 1513.4674072265625 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: -33996.00390625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 2524.530517578125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 2188.669677734375 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1039.4337158203125 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 1939.5595703125 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1209.04052734375 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1459.20068359375 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 5729.83203125 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 1207.36767578125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 872.0433349609375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 778.1199951171875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 832.1824340820312 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: -5089.90771484375 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 1716.6656494140625 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 1536.0863037109375 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.5536, loss_val: nan, pos_over_neg: 2015.603515625 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 1763.538330078125 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 987.05029296875 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 2558.89501953125 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1779.5181884765625 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 3480.427490234375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 1519.7491455078125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1773.046630859375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1279.6798095703125 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1418.3905029296875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 2265.418212890625 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 862.7740478515625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: -3650.662353515625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 1767.48486328125 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 965.1322021484375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 2753.7373046875 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 1170.3250732421875 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 778.6930541992188 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 4080.199462890625 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 1581.760009765625 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 3136.601806640625 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 1817.424072265625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1184.2110595703125 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 1868.997314453125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 1949.9046630859375 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.559, loss_val: nan, pos_over_neg: -9024.9248046875 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1547.689453125 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1022.1698608398438 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1676.472412109375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 3050.9462890625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 1289.1395263671875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 580.4065551757812 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 6510.1123046875 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1720.6190185546875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 1477.579345703125 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 4790.47265625 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 2401.301513671875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 508.86322021484375 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 852.3319702148438 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 5059.39306640625 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 2166.083984375 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 822.1782836914062 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 6136.80712890625 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: -82418.96875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 2284.37548828125 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 1827.5526123046875 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 3710.92822265625 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2593.154296875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 3885.43798828125 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 8351.6025390625 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1157.724365234375 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 1352.4189453125 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 5258.33349609375 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 4971.7646484375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 4855.35498046875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 1024034.25 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 1630.8013916015625 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 4259.50244140625 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 1247.385986328125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.5722, loss_val: nan, pos_over_neg: 1697.111083984375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 845.2151489257812 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 2559.037353515625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 1979.09814453125 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1055.7041015625 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 1123.3792724609375 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 1370.9827880859375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 4673.75927734375 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.5711, loss_val: nan, pos_over_neg: 4434.16259765625 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: -52304.87890625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 3348.75732421875 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 2794.394287109375 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 6766.31640625 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.5727, loss_val: nan, pos_over_neg: 902.5587768554688 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1964.4791259765625 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1002.724365234375 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1224.4052734375 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 6355.4765625 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 2324.1455078125 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 5561.146484375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 1657.578369140625 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 803.6394653320312 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: -10545.1455078125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 2187.645751953125 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 1927.4903564453125 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 919.328369140625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 1552.075439453125 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 1759.5966796875 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 1118.0509033203125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 2676.911865234375 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 1069.4344482421875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 1320.9039306640625 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 18883.263671875 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1168.2711181640625 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.5531, loss_val: nan, pos_over_neg: 1588.60107421875 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 1112.1470947265625 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 2456.5546875 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 1616.6014404296875 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 917.3272094726562 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 1468.90966796875 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.5727, loss_val: nan, pos_over_neg: 931.0963134765625 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 598.0420532226562 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 798.8649291992188 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 1701.145751953125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 3100.124267578125 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 11096.9375 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 1090.3974609375 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 841.8300170898438 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 1071.310791015625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1345.6263427734375 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 1676.474609375 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 2935.59814453125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 565.5447387695312 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.5603, loss_val: nan, pos_over_neg: 838.2469482421875 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 702.8045654296875 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 996.424560546875 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1154.6983642578125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 984.99560546875 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1881.548095703125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 576.8663330078125 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1127.7138671875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 1170.48779296875 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 3363.525634765625 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 1220.42578125 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 404.49542236328125 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1332.937255859375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1626.0501708984375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 1243.593994140625 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 1096.9715576171875 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 1536.1201171875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 838.724609375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1992.271240234375 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1445.195556640625 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.5522, loss_val: nan, pos_over_neg: 1060.306884765625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 3445.93359375 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 2537.125 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 568.5839233398438 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 3146.952392578125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1010.4375 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 4108.3017578125 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 919.6305541992188 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 2621.658447265625 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: -9855.7197265625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 1823.0467529296875 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 2305.378662109375 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 1441.699462890625 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 971.1326904296875 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 1544.2545166015625 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 795.6746826171875 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 3864.7578125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 2751.960693359375 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.5749, loss_val: nan, pos_over_neg: 1017.05224609375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1232.3541259765625 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 3191.724609375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 1027.853271484375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 2486.059814453125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 1551.935546875 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 2123.41650390625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.5529, loss_val: nan, pos_over_neg: 2615.5029296875 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 2427.815673828125 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 674.5833129882812 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 1256.0230712890625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1449.134521484375 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1910.6982421875 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 813.7225341796875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1552.1513671875 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: -43858.86328125 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.5497, loss_val: nan, pos_over_neg: 6967.1044921875 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: -5059.53466796875 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 2312.23193359375 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: -8771.587890625 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1342.8499755859375 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.5699, loss_val: nan, pos_over_neg: 1620.216796875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1340.3297119140625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 2397.735595703125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 1271.364501953125 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1226.545654296875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1682.06982421875 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 1108.0008544921875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: -68473.40625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: -4594.51416015625 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 2333.581787109375 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.5502, loss_val: nan, pos_over_neg: 2861.7978515625 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: -5363.6826171875 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: -2232.212890625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 2305.388916015625 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 1750.0023193359375 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 1524.1033935546875 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.5783, loss_val: nan, pos_over_neg: 694.8326416015625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.5496, loss_val: nan, pos_over_neg: 2466.03271484375 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 1807.2861328125 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 1874.8853759765625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.5716, loss_val: nan, pos_over_neg: 3011.404052734375 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.5738, loss_val: nan, pos_over_neg: 1306.4241943359375 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1608.945556640625 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: -3073.624755859375 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 1577.0526123046875 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 6853.0537109375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1754.4100341796875 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1843.5107421875 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: -12977.888671875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [4:21:26<189061:37:38, 2268.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 7249.767578125 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 2008.3450927734375 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 1303.4761962890625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 1978.223388671875 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 716.5050048828125 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 1260.2691650390625 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 957.1109619140625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 2308.13134765625 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1037.111328125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 764.8391723632812 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 828.73974609375 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 2221.383544921875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 7670.55078125 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 3850.087646484375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.5573, loss_val: nan, pos_over_neg: 1262.0845947265625 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 1374.892822265625 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1569.556640625 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 3883.692626953125 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.5557, loss_val: nan, pos_over_neg: 6020.623046875 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 2557.10400390625 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.5713, loss_val: nan, pos_over_neg: 2056.4580078125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 1156.5240478515625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 2549.019287109375 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1205.1759033203125 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 732.1426391601562 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 1016.9102172851562 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: -69570.7265625 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 6750.60791015625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.5498, loss_val: nan, pos_over_neg: 1416.184814453125 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.5689, loss_val: nan, pos_over_neg: 1164.627197265625 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 8208.2802734375 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 1598.355712890625 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 29636.064453125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 23466.23046875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.5573, loss_val: nan, pos_over_neg: 4748.2001953125 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 820.53076171875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 2286.039306640625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1556.1187744140625 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 5105.5478515625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 3363.759033203125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1589.6103515625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 1444.864501953125 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 919.1223754882812 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: -33268.80078125 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1850.52294921875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 2078.777587890625 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 1095.072998046875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 1679.9488525390625 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: -7012.900390625 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 7735.90771484375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 7290.505859375 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 2681.349609375 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 2121.014404296875 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.5515, loss_val: nan, pos_over_neg: -9601.337890625 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.5525, loss_val: nan, pos_over_neg: 1940.729736328125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 645.5247192382812 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.5719, loss_val: nan, pos_over_neg: 871.1887817382812 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 1963.4749755859375 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 4894.3271484375 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 1854.7735595703125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.5603, loss_val: nan, pos_over_neg: 1846.845947265625 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 1633.2137451171875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 8029.6640625 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: -249743.546875 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 4881.00048828125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 2037.430419921875 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 2115.286376953125 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 2121.691162109375 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 1238.50341796875 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 1533.9287109375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 2135.397705078125 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 2061.062744140625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 8238.2890625 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 763.0714111328125 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 982.5323486328125 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1967.828125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 3654.464111328125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1782.133056640625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 1334.0196533203125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.5525, loss_val: nan, pos_over_neg: 2136.852294921875 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.5682, loss_val: nan, pos_over_neg: 68127.75 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 1000.3560791015625 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1256.8187255859375 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: -3057.165771484375 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1758.70751953125 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 1236.6009521484375 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1867.1622314453125 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.5533, loss_val: nan, pos_over_neg: 3437.52783203125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 60498.56640625 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.5691, loss_val: nan, pos_over_neg: 1227.76904296875 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.5492, loss_val: nan, pos_over_neg: 1963.8402099609375 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 2678.1982421875 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.5641, loss_val: nan, pos_over_neg: 5780.8466796875 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 2509.7392578125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 15383.287109375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 1347.1053466796875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: 2003.81005859375 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 2338.779296875 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 2001.515380859375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 993.181640625 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: -13469.9072265625 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 28352.53515625 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 7047.3837890625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.565, loss_val: nan, pos_over_neg: -931237.875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 1259.9617919921875 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 1091.411865234375 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1904.8848876953125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 2235.57763671875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 7996.7392578125 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1847.19287109375 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.557, loss_val: nan, pos_over_neg: -12017.099609375 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 2825.858642578125 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 1118.1458740234375 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: -137733.0 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 4351.81787109375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1051.0267333984375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.5528, loss_val: nan, pos_over_neg: 1703.2674560546875 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 1090.8951416015625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 8886.8544921875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.5522, loss_val: nan, pos_over_neg: 3792.610107421875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 1530.421142578125 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 2075.92919921875 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 5793.30615234375 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1649.5992431640625 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 5941.2275390625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.5552, loss_val: nan, pos_over_neg: 1776.3646240234375 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 16801.53125 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 1755.549072265625 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1836.40283203125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1483.3006591796875 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 2386.723876953125 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 2103.28955078125 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 2009.9111328125 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 788.9765014648438 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.5696, loss_val: nan, pos_over_neg: 760.9676513671875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 2930.264404296875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 3428.7646484375 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1166.644775390625 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.5497, loss_val: nan, pos_over_neg: 1800.263427734375 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 1772.5931396484375 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: -47136.171875 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 845.5133056640625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 1910.139892578125 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 1125.9754638671875 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 2408.53759765625 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 6870.19775390625 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 1180.1221923828125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.5678, loss_val: nan, pos_over_neg: 1246.5089111328125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.5557, loss_val: nan, pos_over_neg: 3186.056396484375 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 2462.04833984375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 3043.151611328125 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 1602.712646484375 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 1862.9019775390625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 5425.9970703125 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.5507, loss_val: nan, pos_over_neg: 7601.46826171875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 757.2677612304688 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 2796.9306640625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.5718, loss_val: nan, pos_over_neg: 866.1693725585938 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.5603, loss_val: nan, pos_over_neg: 11567.8662109375 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1418.9888916015625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 2933.348876953125 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 4049.55126953125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 718.19677734375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 5120.3388671875 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 3233.630126953125 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.5733, loss_val: nan, pos_over_neg: 887.8587036132812 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: -24185.630859375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 1973.6553955078125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.5522, loss_val: nan, pos_over_neg: -16993.951171875 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1710.846923828125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.5668, loss_val: nan, pos_over_neg: 914.1229858398438 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: -18206.25 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 8819.6103515625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 1077.732177734375 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.5573, loss_val: nan, pos_over_neg: 4772.74169921875 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1692.3975830078125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1239.2340087890625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 2038.8494873046875 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1005.6871948242188 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 2590.35009765625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 12078.376953125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 2745.5751953125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.5745, loss_val: nan, pos_over_neg: 3936.050048828125 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 1410.3388671875 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 2196.0703125 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 1713.4630126953125 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.5501, loss_val: nan, pos_over_neg: -16309.8994140625 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 137022.171875 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1102.056396484375 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: -29786.9921875 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 2629.13134765625 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 1404.3714599609375 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 1349.0467529296875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 3928.787109375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1553.86181640625 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 3639.1728515625 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 6986.8779296875 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 3845.37353515625 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 3438.64306640625 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1643.3681640625 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1861.197021484375 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 1899.3238525390625 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 4177.55419921875 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 1030.228271484375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1684.6475830078125 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 18273.236328125 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 12376.513671875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 9984.3583984375 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 3669.418212890625 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 934.8287963867188 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1389.738525390625 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 1593.1689453125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 2165.31396484375 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 1691.199462890625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 494.6168212890625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 2153.040283203125 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1260.55810546875 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 887.8037719726562 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: -9006.7958984375 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 3045.761474609375 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 4783.00439453125 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 41138.47265625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1399.0513916015625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2502.181884765625 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 6895.08154296875 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 9368.5986328125 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 1680.565185546875 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 1490.8642578125 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 661.9070434570312 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1376.486328125 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.5714, loss_val: nan, pos_over_neg: 2271.4833984375 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 1018.74755859375 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 2065.499267578125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 2680.758544921875 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 1349.41015625 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1255.5513916015625 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: -2117.6748046875 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 3114.092529296875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 1814.6158447265625 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2147.081298828125 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1640.877197265625 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 2457.543212890625 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: -4553.892578125 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 1168.2344970703125 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 3354.57275390625 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 937.8145751953125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 1011.2266235351562 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 803.8892822265625 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 654.1946411132812 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.5526, loss_val: nan, pos_over_neg: 1740.57470703125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1072.2532958984375 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 2037.1907958984375 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 12745.6142578125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 1598.49951171875 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: -15047.0859375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 1786.5892333984375 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 4122.33935546875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1095.9122314453125 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 3943.36474609375 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 2033.7630615234375 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.5703, loss_val: nan, pos_over_neg: 1840.180908203125 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.5526, loss_val: nan, pos_over_neg: 4061.17236328125 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1699.902587890625 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 1261.255859375 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 973.6751708984375 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 1277.181640625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1580.8720703125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1346.75830078125 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 598.0542602539062 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 3449.13330078125 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 3267.6357421875 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.5536, loss_val: nan, pos_over_neg: 5038.875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.571, loss_val: nan, pos_over_neg: 5448.36962890625 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 3839.13134765625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 5026.04833984375 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1160.483642578125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 2574.240234375 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 2229.31591796875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 1101.5845947265625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 8847.4775390625 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 1925.6329345703125 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 1630.134765625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 784.4789428710938 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 2749.270751953125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 1501.9158935546875 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 1226.139404296875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.5746, loss_val: nan, pos_over_neg: 997.1652221679688 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: -3639.021240234375 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1352.839111328125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: -28618.48046875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1630.7762451171875 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.5694, loss_val: nan, pos_over_neg: 1716.843017578125 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.5585, loss_val: nan, pos_over_neg: 2143.478515625 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.5662, loss_val: nan, pos_over_neg: 1916.3616943359375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 2034.831298828125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 10507.3046875 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 3320.06787109375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 583.4293212890625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 1009.6930541992188 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1851.513427734375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 3473.6298828125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 1767.6966552734375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 11119.1298828125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 1101.156982421875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 102959.53125 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 1907.8719482421875 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.5533, loss_val: nan, pos_over_neg: 12219.37109375 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 3099.4248046875 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 1331.8416748046875 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: -3142.649658203125 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 1228.716796875 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 4326.9638671875 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: 2140.670166015625 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 1963.1207275390625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 8029.27880859375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.5697, loss_val: nan, pos_over_neg: 2275.1123046875 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 1339.14892578125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 17841.00390625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1561.057861328125 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 19829.197265625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 2086.546630859375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.553, loss_val: nan, pos_over_neg: -3410.869384765625 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1827.7733154296875 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 941.4684448242188 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 1231.91162109375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 1058.2247314453125 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 5157.3115234375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 1480.6602783203125 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1441.0283203125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2555.21728515625 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 4162.9228515625 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 3612.435791015625 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 2818.027099609375 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 2684.408203125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 6368.10791015625 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 3813.40478515625 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 992.465087890625 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 1669.2041015625 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 3778.593505859375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 3109.047607421875 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.544, loss_val: nan, pos_over_neg: 5054.01513671875 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 1385.83984375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 952.2933349609375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 2197.357421875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.5705, loss_val: nan, pos_over_neg: 2185.399169921875 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 1276.942138671875 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 1768.5509033203125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: 1416.8681640625 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 11988.8828125 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 3323.960205078125 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.5686, loss_val: nan, pos_over_neg: 2312.949951171875 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: 2495.0517578125 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1084.2998046875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 1667.4671630859375 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 9495.103515625 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 1563.6417236328125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 1004.6683349609375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 4330.5712890625 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 1410.7376708984375 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 4952.97802734375 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 1297.7811279296875 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: -3886.911865234375 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: 2333.13916015625 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 2477.5048828125 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.5562, loss_val: nan, pos_over_neg: 2165.32275390625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: -31999.533203125 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1598.4283447265625 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1806.10791015625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 1617.8065185546875 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.5603, loss_val: nan, pos_over_neg: 1389.7677001953125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.5512, loss_val: nan, pos_over_neg: 2483.154541015625 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 6377.775390625 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.5704, loss_val: nan, pos_over_neg: 1268.3955078125 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1958.38330078125 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 650.189453125 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 648.8451538085938 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1438.2418212890625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 1121.6722412109375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: -9485.84765625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 2430.93310546875 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 825.9284057617188 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.5494, loss_val: nan, pos_over_neg: 1954.947021484375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 997.520751953125 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 579.4714965820312 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1025.3941650390625 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 885.06005859375 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 726.161376953125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 561.3328857421875 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 909.30126953125 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 2764.547607421875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1574.5374755859375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 1792.857421875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1117.039306640625 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 787.5892944335938 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 1709.72802734375 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 1445.9268798828125 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 3897.14208984375 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 15992.1943359375 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 917.9246826171875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1291.7960205078125 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 1259.5335693359375 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 787.3591918945312 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.5709, loss_val: nan, pos_over_neg: 766.1480102539062 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 2003.401611328125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 820.6495361328125 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1487.7694091796875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 944.17333984375 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 4228.24072265625 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: 1806.6507568359375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 6894.1611328125 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 1603.4217529296875 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 3944.529541015625 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 2724.8779296875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 2446.777587890625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1339.132568359375 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 1753.860107421875 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: -36776.02734375 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 931.4833984375 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 938.419921875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 980.146728515625 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 1479.258544921875 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 5251.23046875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.5531, loss_val: nan, pos_over_neg: 29655.705078125 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 1331.0576171875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 2762.57666015625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 1315.7882080078125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1969.7044677734375 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 1683.6661376953125 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 4010.893310546875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1595.1539306640625 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.5675, loss_val: nan, pos_over_neg: 3032.64990234375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.5603, loss_val: nan, pos_over_neg: 866.7379150390625 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 2035.026123046875 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 733.733642578125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 822.8612060546875 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1523.421142578125 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 986.9073486328125 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: -6093.458984375 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.5528, loss_val: nan, pos_over_neg: -15454.681640625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 2798.231201171875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 1245.5281982421875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 1856.0802001953125 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2575.248046875 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 2210.2392578125 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 1090.6396484375 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 2662.5888671875 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 4899.83935546875 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 572.0977172851562 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1435.5909423828125 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2071.7412109375 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 1975.151611328125 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 850.4083251953125 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 842.7395629882812 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 879.8277587890625 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1761.7855224609375 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.5481, loss_val: nan, pos_over_neg: 3622.6025390625 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 2324.953857421875 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: -19734.681640625 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 1199.986572265625 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 3255.00048828125 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.5693, loss_val: nan, pos_over_neg: 1225.4520263671875 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 1151.203125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 994.9363403320312 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 1280.3160400390625 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: -12852.5966796875 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 5891.998046875 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 587.2105102539062 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 1268.451904296875 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 901.6650390625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1187.7493896484375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 1234.3184814453125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 812.8935546875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 3436.7109375 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 3918.36669921875 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 1630.7213134765625 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 937.4270629882812 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 2950.436279296875 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 1077.132080078125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.5518, loss_val: nan, pos_over_neg: 1920.2801513671875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.5585, loss_val: nan, pos_over_neg: -22392.015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 2221.953857421875 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.5642, loss_val: nan, pos_over_neg: 2347.7470703125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 1090.36279296875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1924.5552978515625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 882.8441772460938 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 1046.411865234375 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 1820.7523193359375 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 2076.656982421875 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 1794.559814453125 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 3356.095947265625 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1164.23193359375 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 2468.002685546875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 663.75439453125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 2609.884033203125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: -21101.119140625 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.5665, loss_val: nan, pos_over_neg: 669.7304077148438 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 4303.48828125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.5494, loss_val: nan, pos_over_neg: 2676.163330078125 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.549, loss_val: nan, pos_over_neg: -9110.388671875 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 2628.43017578125 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 1862.160888671875 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1987.0438232421875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 1547.2291259765625 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1405.6590576171875 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1690.481689453125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.5529, loss_val: nan, pos_over_neg: 2625.6943359375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 968.155517578125 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 1138.9267578125 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 1006.9198608398438 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 1199.026123046875 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 892.3043823242188 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.5666, loss_val: nan, pos_over_neg: 1834.46240234375 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 682.7611083984375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1591.3887939453125 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 3856.17333984375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1554.788330078125 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1126.401611328125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 935.062744140625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 1304.1854248046875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 922.6677856445312 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 1032.0416259765625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.5742, loss_val: nan, pos_over_neg: 643.7490844726562 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 2160.312255859375 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 998.289306640625 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 2152.353759765625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1374.4259033203125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 6628.9814453125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.5491, loss_val: nan, pos_over_neg: 6218.05517578125 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 1748.6827392578125 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 3938.24853515625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 1339.6737060546875 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.5499, loss_val: nan, pos_over_neg: 3729.231201171875 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: 7331.7861328125 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 846.3704833984375 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1645.4212646484375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 1008.2279052734375 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 873.5855102539062 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 499.4391174316406 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1250.8143310546875 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 1791.1689453125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1877.04931640625 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 692.769287109375 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: 1546.6666259765625 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1896.12646484375 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 2766.861083984375 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 45334.3671875 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 9920.193359375 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 1977.593017578125 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 2179.769775390625 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 3557.34619140625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.5497, loss_val: nan, pos_over_neg: 5892.6591796875 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 2666.594970703125 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1475.1060791015625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 1145.0653076171875 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: 1704.7774658203125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 970.661865234375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.5519, loss_val: nan, pos_over_neg: 9468.751953125 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.5488, loss_val: nan, pos_over_neg: 3293.30908203125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.5494, loss_val: nan, pos_over_neg: -13211.5107421875 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.5536, loss_val: nan, pos_over_neg: 2615.186767578125 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1604.842529296875 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1401.9703369140625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: 1453.3743896484375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 14280.9033203125 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 2316.3759765625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 3539.251220703125 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 1753.8212890625 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.5603, loss_val: nan, pos_over_neg: 1210.606689453125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 3336.67431640625 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 7052.58203125 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 610.9452514648438 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 885.818603515625 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 5211.720703125 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 2150.60400390625 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 1316.54345703125 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 2565.489013671875 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.5526, loss_val: nan, pos_over_neg: 1383.0101318359375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: -14104.447265625 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.5492, loss_val: nan, pos_over_neg: 14647.3779296875 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 3132.79150390625 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 964.2869873046875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 3256.738037109375 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1154.35009765625 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 983.1425170898438 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 922.5247192382812 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: -18319.61328125 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.5531, loss_val: nan, pos_over_neg: 1216.390869140625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 927.614013671875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 4627.47900390625 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1297.518310546875 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 1492.546630859375 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1015.6336669921875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1176.8885498046875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 3046.40185546875 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 1104.50439453125 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 2223.776123046875 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 6906.37353515625 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.5482, loss_val: nan, pos_over_neg: 27705.884765625 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 3841.247802734375 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 1605.5289306640625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 2329.631103515625 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: -5042.11328125 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1445.452392578125 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 715.8180541992188 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 890.6734008789062 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 2785.588134765625 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 5739.8642578125 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.5505, loss_val: nan, pos_over_neg: 1993.40966796875 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1402.2164306640625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1313.0721435546875 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 846.9479370117188 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1700.05419921875 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 1094.2889404296875 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 3690.021240234375 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 2097.9169921875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 4233.064453125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 1755.657958984375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 4711.654296875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: -103086.03125 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.5468, loss_val: nan, pos_over_neg: 1354.704833984375 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 1266.9173583984375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.5507, loss_val: nan, pos_over_neg: 2095.613525390625 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.5573, loss_val: nan, pos_over_neg: 974.2802734375 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 1425.8251953125 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.551, loss_val: nan, pos_over_neg: 1048.179443359375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1789.84228515625 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 3018.667724609375 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.5531, loss_val: nan, pos_over_neg: 2821.971923828125 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: 2210.00390625 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 1673.6636962890625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 987.1442260742188 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.5486, loss_val: nan, pos_over_neg: 1030.779296875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1548.8018798828125 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.5631, loss_val: nan, pos_over_neg: 1931.7215576171875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 2068.703369140625 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1479.1640625 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 4290.68896484375 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.5522, loss_val: nan, pos_over_neg: 663.9523315429688 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 1543.002197265625 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.56, loss_val: nan, pos_over_neg: -5786.8798828125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 784.864990234375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 1088.1314697265625 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1220.6737060546875 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 876.0244750976562 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.5687, loss_val: nan, pos_over_neg: 855.4841918945312 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.5536, loss_val: nan, pos_over_neg: 10499.3232421875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1659.7095947265625 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 2225.507080078125 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 1305.80078125 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 1604.8074951171875 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.56, loss_val: nan, pos_over_neg: -12922.4501953125 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: -2386.7880859375 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 1729.22607421875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 1093.557861328125 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 1477.6846923828125 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 1723.9578857421875 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 2328.482177734375 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.5479, loss_val: nan, pos_over_neg: 1536.7120361328125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.5489, loss_val: nan, pos_over_neg: 13775.3720703125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.5617, loss_val: nan, pos_over_neg: 1228.831787109375 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 4482.42626953125 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 939.75830078125 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 74645.1328125 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1898.810546875 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 1759.108642578125 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 1793.6153564453125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 4801.53125 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.5529, loss_val: nan, pos_over_neg: -3913.9521484375 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 20295.994140625 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 3524.4599609375 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1940.4903564453125 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 1979.198974609375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 16007.744140625 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.5485, loss_val: nan, pos_over_neg: -4456.76025390625 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 3863.232666015625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 2016.7672119140625 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 32808.5546875 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.5643, loss_val: nan, pos_over_neg: 1567.5413818359375 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 1450.3353271484375 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 2126.2666015625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: -7523.32958984375 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 2143.747802734375 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1410.9764404296875 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.5529, loss_val: nan, pos_over_neg: 48651.828125 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.5536, loss_val: nan, pos_over_neg: 1611.4891357421875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1402.9759521484375 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1522.977783203125 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 5633.900390625 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.5661, loss_val: nan, pos_over_neg: 1168.8206787109375 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 1082.920166015625 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.5679, loss_val: nan, pos_over_neg: 675.8744506835938 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1380.3212890625 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1208.0771484375 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 1472.116455078125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.5659, loss_val: nan, pos_over_neg: 1104.7464599609375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 2577.295654296875 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 2218.820556640625 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: -13277.8359375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.5655, loss_val: nan, pos_over_neg: 1332.293701171875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 850.7993774414062 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 2570.686279296875 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 1026.090576171875 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.5514, loss_val: nan, pos_over_neg: 1912.2626953125 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.552, loss_val: nan, pos_over_neg: -14946.8642578125 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 24699.30078125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.5525, loss_val: nan, pos_over_neg: 1916.7105712890625 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 4742.9384765625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 879.3502807617188 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.5476, loss_val: nan, pos_over_neg: 2931.127197265625 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 2507.954345703125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.5557, loss_val: nan, pos_over_neg: 4226.56884765625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.5522, loss_val: nan, pos_over_neg: 1081.156494140625 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.5653, loss_val: nan, pos_over_neg: 1080.165283203125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 1664.8807373046875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: -15345.478515625 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 1164.528564453125 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 8039.3505859375 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.5511, loss_val: nan, pos_over_neg: 1948.6630859375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 4629.04931640625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.5673, loss_val: nan, pos_over_neg: 998.5670776367188 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 3065.295166015625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.5498, loss_val: nan, pos_over_neg: -11687.1376953125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.5513, loss_val: nan, pos_over_neg: 6710.015625 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 3121.505859375 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 1937.9554443359375 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 1943.020263671875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: -4003.31396484375 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 1906.2845458984375 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 2880.420166015625 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 1137.8675537109375 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.57, loss_val: nan, pos_over_neg: 723.6077270507812 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 5427.09912109375 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.5429, loss_val: nan, pos_over_neg: 407519.84375 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.5517, loss_val: nan, pos_over_neg: 58920.5390625 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 2480.526123046875 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.5743, loss_val: nan, pos_over_neg: 2116.6689453125 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 4381.50244140625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 1084.84130859375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 28152.65625 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 980.17578125 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: 785.4280395507812 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 2366.895263671875 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.5465, loss_val: nan, pos_over_neg: 1671.5880126953125 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.5528, loss_val: nan, pos_over_neg: 1488.0782470703125 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 4925.01123046875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1505.05078125 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1813.6192626953125 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 2722.896240234375 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 1002.3193359375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 927.9146728515625 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.5507, loss_val: nan, pos_over_neg: 11252.169921875 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 1663.6802978515625 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 3944.3662109375 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 821.3253784179688 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 913.4706420898438 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: 1757.093505859375 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.5511, loss_val: nan, pos_over_neg: 1265.856689453125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.5501, loss_val: nan, pos_over_neg: 4295.322265625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.5619, loss_val: nan, pos_over_neg: 937.886474609375 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 2433.321044921875 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 2155.04931640625 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 886.3916625976562 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1172.2750244140625 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.5671, loss_val: nan, pos_over_neg: 2009.809326171875 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.5507, loss_val: nan, pos_over_neg: 1653.2398681640625 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 3095.46435546875 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1440.7327880859375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 5435.8359375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 1990.3370361328125 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 1740.0335693359375 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.5502, loss_val: nan, pos_over_neg: 2011.1822509765625 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1275.781005859375 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.5688, loss_val: nan, pos_over_neg: 747.8197631835938 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1536.7899169921875 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.552, loss_val: nan, pos_over_neg: 5465.69384765625 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 1997.1005859375 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 1171.759521484375 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 830.0911254882812 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 1028.9698486328125 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 1916.6199951171875 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 1245.7432861328125 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.5513, loss_val: nan, pos_over_neg: 9703.0791015625 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 726.6641235351562 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 1041.5780029296875 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1011.6958618164062 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 6315.7568359375 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1597.65283203125 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1315.0938720703125 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 2013.69921875 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 3386.271728515625 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 922.3853759765625 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.567, loss_val: nan, pos_over_neg: 1887.83349609375 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 887.0318603515625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 991.1219482421875 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 4854.3828125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 1048.6785888671875 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.5515, loss_val: nan, pos_over_neg: 875.071044921875 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 819.064453125 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1807.4610595703125 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 7592.14306640625 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.5552, loss_val: nan, pos_over_neg: 1277.5142822265625 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 1060.077880859375 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 801.5120849609375 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 658.5225219726562 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.5528, loss_val: nan, pos_over_neg: 3485.906494140625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 961.0827026367188 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.5477, loss_val: nan, pos_over_neg: 1848.7708740234375 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 1716.5 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 822.3596801757812 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 807.13720703125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 1580.0836181640625 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 1152.9967041015625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.5499, loss_val: nan, pos_over_neg: 830.95751953125 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 1589.449951171875 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.563, loss_val: nan, pos_over_neg: 911.7252197265625 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.5458, loss_val: nan, pos_over_neg: 1435.86328125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.5562, loss_val: nan, pos_over_neg: 2229.706787109375 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: 10358.1337890625 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 900.2134399414062 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1486.14404296875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 819.615234375 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 7736.39697265625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.5676, loss_val: nan, pos_over_neg: 1700.9925537109375 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 1203.0780029296875 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 1101.4251708984375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 2127.95556640625 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 3671.96533203125 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 2353.878173828125 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 836.3992309570312 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 1238.5269775390625 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 2510.986083984375 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.5485, loss_val: nan, pos_over_neg: 7174.3896484375 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: -4534.95458984375 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: 1730.213623046875 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 1090.35546875 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.5669, loss_val: nan, pos_over_neg: 1000.502197265625 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 3944.459228515625 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1575.5198974609375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.5496, loss_val: nan, pos_over_neg: 16514.607421875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 83789.34375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.5612, loss_val: nan, pos_over_neg: 1143.2440185546875 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 1565.2767333984375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.5511, loss_val: nan, pos_over_neg: 7023.95751953125 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 6240.89453125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 723.63037109375 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 2859.265869140625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.5496, loss_val: nan, pos_over_neg: 9681.4375 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 1214.119140625 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.5491, loss_val: nan, pos_over_neg: 1815.938232421875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 1336.93017578125 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 1479.365478515625 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 1285.9158935546875 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 1133.777587890625 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.5531, loss_val: nan, pos_over_neg: 1742.8055419921875 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 1083.4716796875 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 1462.3695068359375 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 934.425048828125 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.5603, loss_val: nan, pos_over_neg: 1487.6456298828125 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 1331.9744873046875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 1451.870361328125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1023.01708984375 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.551, loss_val: nan, pos_over_neg: 2941.572509765625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.5457, loss_val: nan, pos_over_neg: 1197.6199951171875 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 1965.356689453125 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.5474, loss_val: nan, pos_over_neg: 1359.5111083984375 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.5528, loss_val: nan, pos_over_neg: 2433.67822265625 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 1371.9990234375 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 1656.5631103515625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 770.766845703125 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 745.1085815429688 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 1001.2578125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.5518, loss_val: nan, pos_over_neg: 2290.062255859375 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.5517, loss_val: nan, pos_over_neg: 11986.7685546875 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 1246.534912109375 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.5654, loss_val: nan, pos_over_neg: 2262.30126953125 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1334.3203125 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1394.6290283203125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 3470.413818359375 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 1715.041748046875 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.5652, loss_val: nan, pos_over_neg: 1650.3961181640625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 1188.22265625 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.5493, loss_val: nan, pos_over_neg: 1515.00439453125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 3396.07080078125 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 2178.025146484375 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 1021.1950073242188 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 992.8653564453125 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.5634, loss_val: nan, pos_over_neg: 1874.363525390625 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 1758.6331787109375 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 1263.7928466796875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [5:02:34<194354:54:04, 2332.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/889, loss_train: 5.5559, loss_val: nan, pos_over_neg: 24539.310546875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.5497, loss_val: nan, pos_over_neg: 1458.3858642578125 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 4819.9951171875 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.5533, loss_val: nan, pos_over_neg: 3628.9228515625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.5495, loss_val: nan, pos_over_neg: 4721.189453125 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.5515, loss_val: nan, pos_over_neg: 26212.796875 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 2513.34375 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 490.0509948730469 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.5672, loss_val: nan, pos_over_neg: 597.0880126953125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 3578.675048828125 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 2555.38427734375 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 1441.416748046875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.5635, loss_val: nan, pos_over_neg: 1168.068115234375 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 2561.617431640625 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 862.6835327148438 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 3566.685302734375 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 1606.591552734375 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1894.02197265625 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 2191.360107421875 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: 3005.572998046875 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: 1598.4405517578125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 1262.905517578125 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 987.8555908203125 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 963.5390014648438 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 1613.9603271484375 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.568, loss_val: nan, pos_over_neg: 2773.188720703125 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.5499, loss_val: nan, pos_over_neg: 3103.119140625 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 1056.1314697265625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1173.536865234375 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1031.771240234375 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 3437.658203125 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 7277.74853515625 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 1186.8438720703125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.5463, loss_val: nan, pos_over_neg: 3769.3671875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.5501, loss_val: nan, pos_over_neg: 6347.00341796875 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 5100.82568359375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 2478.300537109375 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1604.826416015625 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 2710.514892578125 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 1502.9385986328125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 1642.95166015625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 820.0457763671875 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1575.005615234375 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.5494, loss_val: nan, pos_over_neg: 55557.96484375 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 2659.41552734375 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.5501, loss_val: nan, pos_over_neg: -2992.28173828125 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.5621, loss_val: nan, pos_over_neg: 3381.68505859375 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.562, loss_val: nan, pos_over_neg: -11213.6044921875 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.5684, loss_val: nan, pos_over_neg: -4338.3330078125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 960.1485595703125 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 2495.487060546875 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 4861.52099609375 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 2998.153076171875 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.5489, loss_val: nan, pos_over_neg: 2554.830078125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.5508, loss_val: nan, pos_over_neg: 5628.63916015625 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 1141.80029296875 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.5469, loss_val: nan, pos_over_neg: 7663.01220703125 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 3957.1669921875 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.5485, loss_val: nan, pos_over_neg: -7866.83251953125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 9475.63671875 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 885.9391479492188 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 2501.43408203125 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.5528, loss_val: nan, pos_over_neg: 3386.692626953125 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.5476, loss_val: nan, pos_over_neg: 2678.168701171875 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 1507.7620849609375 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.552, loss_val: nan, pos_over_neg: 1567.743408203125 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 1921.5418701171875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 2320.0634765625 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: -5531.216796875 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 1582.0472412109375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 969.7416381835938 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.5529, loss_val: nan, pos_over_neg: 27650.9921875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.551, loss_val: nan, pos_over_neg: -9160.81640625 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.5506, loss_val: nan, pos_over_neg: 2447.496337890625 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 4052.569580078125 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 2467.67626953125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 9024.2861328125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.5518, loss_val: nan, pos_over_neg: 6610.77099609375 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: -7371.79541015625 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.5536, loss_val: nan, pos_over_neg: 2734.19287109375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 1967.150146484375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.5511, loss_val: nan, pos_over_neg: 11383.1015625 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 1436.9056396484375 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.5487, loss_val: nan, pos_over_neg: -2921.03466796875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 3614.982666015625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 1665.8397216796875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 3152.611328125 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.5644, loss_val: nan, pos_over_neg: 1018.2384033203125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.5616, loss_val: nan, pos_over_neg: 1645.9224853515625 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 1574.6409912109375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.5459, loss_val: nan, pos_over_neg: 4846.84423828125 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 2211.0732421875 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 2456.572998046875 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 7351.93212890625 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.5467, loss_val: nan, pos_over_neg: 2089.37646484375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1094.5489501953125 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.5519, loss_val: nan, pos_over_neg: 4993.716796875 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 146821.78125 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1432.6341552734375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 2677.475341796875 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.5674, loss_val: nan, pos_over_neg: 1672.90478515625 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 2821.84130859375 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.564, loss_val: nan, pos_over_neg: 719.3403930664062 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 2782.5986328125 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 6031.19580078125 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 2752.267578125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 2852.23291015625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 2576.465576171875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: 1251.826171875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.5626, loss_val: nan, pos_over_neg: 1166.979736328125 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.5585, loss_val: nan, pos_over_neg: 935.5067749023438 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.5574, loss_val: nan, pos_over_neg: 1169.9007568359375 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 1062.6142578125 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 751.6134643554688 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 1003.865966796875 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 4727.76318359375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 68823.078125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 3440.736328125 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 2386.195068359375 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1749.685302734375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 2165.4228515625 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1929.8179931640625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 68820.1015625 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 770.8178100585938 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 2053.481201171875 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 1426.56494140625 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1152.1175537109375 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 18307.080078125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: -31780.7421875 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.5657, loss_val: nan, pos_over_neg: 1549.0377197265625 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 6025.623046875 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 2127.41552734375 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.5485, loss_val: nan, pos_over_neg: -13846.10546875 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 168979.421875 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.5692, loss_val: nan, pos_over_neg: 2042.7691650390625 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.5563, loss_val: nan, pos_over_neg: 1427.9415283203125 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 8946.11328125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.5502, loss_val: nan, pos_over_neg: 3310.77392578125 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 48509.24609375 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.5638, loss_val: nan, pos_over_neg: 1075.4879150390625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: 648.4142456054688 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 1396.8160400390625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.5471, loss_val: nan, pos_over_neg: 3335.67626953125 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.5632, loss_val: nan, pos_over_neg: 1024.9261474609375 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 3325.5869140625 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 771.9700317382812 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.5491, loss_val: nan, pos_over_neg: 2121.915283203125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 4429.04541015625 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.5531, loss_val: nan, pos_over_neg: 2118.521728515625 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 1069.900146484375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 2946.272216796875 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 867.876953125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 1141.853515625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.5557, loss_val: nan, pos_over_neg: 22104.951171875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.5505, loss_val: nan, pos_over_neg: 4406.94970703125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 1919.8740234375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 1983.2296142578125 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 1631.63037109375 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.5488, loss_val: nan, pos_over_neg: 6440.53125 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 3005.276611328125 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.5471, loss_val: nan, pos_over_neg: 10669.388671875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.5433, loss_val: nan, pos_over_neg: 2770.566650390625 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.5648, loss_val: nan, pos_over_neg: 884.80078125 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 1198.2586669921875 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.5515, loss_val: nan, pos_over_neg: 1467.724853515625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.5515, loss_val: nan, pos_over_neg: 3121.3115234375 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.5512, loss_val: nan, pos_over_neg: 3742.303955078125 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: -33091.36328125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 841.070556640625 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1701.1544189453125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.5533, loss_val: nan, pos_over_neg: 1225.048583984375 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.5596, loss_val: nan, pos_over_neg: -14066.6826171875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.5484, loss_val: nan, pos_over_neg: 1600.1884765625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1259.3984375 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 506.8495788574219 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.5708, loss_val: nan, pos_over_neg: 882.139892578125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.563, loss_val: nan, pos_over_neg: -3328.6298828125 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.562, loss_val: nan, pos_over_neg: 1455.066650390625 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 2984.535888671875 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 1962.7239990234375 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 4527.64404296875 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: -6184.30908203125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 2256.124755859375 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.5454, loss_val: nan, pos_over_neg: -133259.453125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.5484, loss_val: nan, pos_over_neg: 4733.19482421875 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.5474, loss_val: nan, pos_over_neg: 3721.6748046875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1041.7396240234375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.5552, loss_val: nan, pos_over_neg: 1425.376220703125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 4313.47705078125 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1961.123779296875 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.559, loss_val: nan, pos_over_neg: 3821.68603515625 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.5504, loss_val: nan, pos_over_neg: -26620.70703125 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 3142.452880859375 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.5512, loss_val: nan, pos_over_neg: 3374.847900390625 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 3256.52099609375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1545.47998046875 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.5548, loss_val: nan, pos_over_neg: 1104.692626953125 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.5535, loss_val: nan, pos_over_neg: 2298.5517578125 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 2374.497802734375 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.554, loss_val: nan, pos_over_neg: 12569.3427734375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 4426.8466796875 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 3111.78369140625 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.5624, loss_val: nan, pos_over_neg: 3543.302490234375 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 1981.548095703125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.5628, loss_val: nan, pos_over_neg: 1064.402587890625 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 2012.765869140625 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: 2284.5771484375 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.5459, loss_val: nan, pos_over_neg: 2312.025390625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 2164.224365234375 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.5506, loss_val: nan, pos_over_neg: 3427.501708984375 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.5465, loss_val: nan, pos_over_neg: -11706.3017578125 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 1268.8575439453125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 1039.1729736328125 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 1743.0675048828125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 4968.76806640625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 1741.1832275390625 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1977.9124755859375 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1466.142578125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.5636, loss_val: nan, pos_over_neg: 1012.466064453125 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: -8542.5302734375 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 1721.561767578125 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.5589, loss_val: nan, pos_over_neg: 3539.858642578125 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.5557, loss_val: nan, pos_over_neg: 8841.3203125 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 4672.36572265625 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.569, loss_val: nan, pos_over_neg: 768.5584106445312 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.5525, loss_val: nan, pos_over_neg: -4213.22021484375 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: -9218.5048828125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 1944.141845703125 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: 2102.41796875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 1223.7257080078125 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.5625, loss_val: nan, pos_over_neg: 924.6373901367188 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 1568.1541748046875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 4062.06396484375 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.5489, loss_val: nan, pos_over_neg: 72421.6484375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.5647, loss_val: nan, pos_over_neg: 7277.16650390625 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.5728, loss_val: nan, pos_over_neg: 1648.489013671875 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.5637, loss_val: nan, pos_over_neg: 1142.7408447265625 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.5552, loss_val: nan, pos_over_neg: -19139.89453125 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.5601, loss_val: nan, pos_over_neg: 2619.649169921875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 4710.94677734375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.5512, loss_val: nan, pos_over_neg: 2427.48486328125 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 1496.11083984375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.547, loss_val: nan, pos_over_neg: 1162.8875732421875 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.548, loss_val: nan, pos_over_neg: 2402.486572265625 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 803.1943969726562 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: -10601.6162109375 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: -11998.380859375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 1479.298828125 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 74927.234375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.5677, loss_val: nan, pos_over_neg: 1236.2840576171875 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: 38044.953125 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1592.420166015625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 2162.048828125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.5519, loss_val: nan, pos_over_neg: -27422.990234375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.5573, loss_val: nan, pos_over_neg: 1401.5250244140625 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 1954.707275390625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.5428, loss_val: nan, pos_over_neg: 10938.4404296875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.5439, loss_val: nan, pos_over_neg: 2354.928955078125 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 1889.2515869140625 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.5545, loss_val: nan, pos_over_neg: 1232.9954833984375 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 1393.27392578125 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 1388.0367431640625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.5458, loss_val: nan, pos_over_neg: 3618.63916015625 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.548, loss_val: nan, pos_over_neg: 1211.0958251953125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.5734, loss_val: nan, pos_over_neg: 1072.7540283203125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.5597, loss_val: nan, pos_over_neg: 7510.36083984375 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 2201.21728515625 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.5488, loss_val: nan, pos_over_neg: 1659.226806640625 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.5712, loss_val: nan, pos_over_neg: 925.0767822265625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.5455, loss_val: nan, pos_over_neg: 2872.053466796875 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1291.337158203125 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 3489.052001953125 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.5649, loss_val: nan, pos_over_neg: 5759.4658203125 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.5724, loss_val: nan, pos_over_neg: 1511.31298828125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.5543, loss_val: nan, pos_over_neg: 1275.732177734375 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 3835.769775390625 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 2338.974609375 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.5592, loss_val: nan, pos_over_neg: 7487.19970703125 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.5581, loss_val: nan, pos_over_neg: 2925.9794921875 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.5502, loss_val: nan, pos_over_neg: 1615.4989013671875 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.5585, loss_val: nan, pos_over_neg: 697.4116821289062 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.5568, loss_val: nan, pos_over_neg: 1700.515625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 2929.0166015625 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.5482, loss_val: nan, pos_over_neg: 31413.541015625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.5499, loss_val: nan, pos_over_neg: 15915.7158203125 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.5507, loss_val: nan, pos_over_neg: 29661.28515625 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.5577, loss_val: nan, pos_over_neg: 3060.73974609375 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.5586, loss_val: nan, pos_over_neg: 790.0093383789062 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1483.1292724609375 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 3875.91552734375 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.5425, loss_val: nan, pos_over_neg: -5826.1689453125 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 1887.3463134765625 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 2465.223876953125 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.5516, loss_val: nan, pos_over_neg: 1309.8934326171875 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.5515, loss_val: nan, pos_over_neg: 1612.4454345703125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.5615, loss_val: nan, pos_over_neg: 1474.8380126953125 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 1186.966552734375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.5656, loss_val: nan, pos_over_neg: 1102.2808837890625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.5602, loss_val: nan, pos_over_neg: 2044.390625 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.551, loss_val: nan, pos_over_neg: 2990.466064453125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.561, loss_val: nan, pos_over_neg: 1378.5361328125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.5551, loss_val: nan, pos_over_neg: 1001.2974243164062 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 3331.736328125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.5533, loss_val: nan, pos_over_neg: 5213.3369140625 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.5431, loss_val: nan, pos_over_neg: -260831.84375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.5496, loss_val: nan, pos_over_neg: 1167.34423828125 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.5598, loss_val: nan, pos_over_neg: 1581.7879638671875 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 3618.16796875 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 2732.43896484375 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.5534, loss_val: nan, pos_over_neg: 2084.00341796875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 2942.839111328125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.5471, loss_val: nan, pos_over_neg: 4152.40771484375 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: 1074.8551025390625 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 1122.359130859375 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 5782.89208984375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 4051.2900390625 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 14675.2021484375 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 1396.149169921875 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: -5478.9306640625 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: -4403.044921875 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.5609, loss_val: nan, pos_over_neg: 1165.00341796875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 5911.4931640625 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 897.0912475585938 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 1465.227783203125 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.5456, loss_val: nan, pos_over_neg: -4091.3935546875 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.5465, loss_val: nan, pos_over_neg: 14003.6064453125 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 1469.0491943359375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.5541, loss_val: nan, pos_over_neg: 888.84521484375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.5507, loss_val: nan, pos_over_neg: 10044.0078125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.5587, loss_val: nan, pos_over_neg: 2195.947021484375 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.5514, loss_val: nan, pos_over_neg: 1429.28076171875 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.546, loss_val: nan, pos_over_neg: 2822.5126953125 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 7650.30419921875 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 2658.38134765625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.5546, loss_val: nan, pos_over_neg: 1572.17578125 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 4349.76513671875 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: -8919.8837890625 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 4447.26318359375 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.5493, loss_val: nan, pos_over_neg: 4441.212890625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 1427.51416015625 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.5512, loss_val: nan, pos_over_neg: 2158.114990234375 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.5578, loss_val: nan, pos_over_neg: 1035.12939453125 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.5623, loss_val: nan, pos_over_neg: 1550.9444580078125 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 1269.895751953125 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 1309.6807861328125 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1365.3184814453125 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.5489, loss_val: nan, pos_over_neg: 9235.896484375 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: -4885.43798828125 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 2476.6044921875 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.5491, loss_val: nan, pos_over_neg: 5339.822265625 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.5525, loss_val: nan, pos_over_neg: 9629.599609375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: 4311.43115234375 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.5651, loss_val: nan, pos_over_neg: 1475.1800537109375 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.5664, loss_val: nan, pos_over_neg: 1230.049560546875 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.5555, loss_val: nan, pos_over_neg: 2233.75732421875 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.5496, loss_val: nan, pos_over_neg: 1585.7823486328125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.5515, loss_val: nan, pos_over_neg: 2008.0263671875 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.5529, loss_val: nan, pos_over_neg: 1089.296630859375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.5526, loss_val: nan, pos_over_neg: 2158.012939453125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 1627.29296875 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 2378.523681640625 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: -13818.2734375 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1066.1781005859375 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.56, loss_val: nan, pos_over_neg: 2437.093994140625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.5509, loss_val: nan, pos_over_neg: 2455.002685546875 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 2453.668701171875 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 2648.9443359375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.5499, loss_val: nan, pos_over_neg: 1378.5452880859375 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.549, loss_val: nan, pos_over_neg: 3507.357421875 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.5486, loss_val: nan, pos_over_neg: 1340.004638671875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.5512, loss_val: nan, pos_over_neg: 1402.679931640625 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 1281.7374267578125 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 5079.07763671875 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 1105.444580078125 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 859.1837768554688 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.5553, loss_val: nan, pos_over_neg: 4151.404296875 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.5557, loss_val: nan, pos_over_neg: 3458.342529296875 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 120742.9609375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 3404.80224609375 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.5517, loss_val: nan, pos_over_neg: 1142.81591796875 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.5437, loss_val: nan, pos_over_neg: 1179.3406982421875 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 1262.3206787109375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.5478, loss_val: nan, pos_over_neg: 1130.26708984375 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.5556, loss_val: nan, pos_over_neg: 1161.0499267578125 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1851.8037109375 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.5605, loss_val: nan, pos_over_neg: 843.85986328125 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.566, loss_val: nan, pos_over_neg: 772.752685546875 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.5562, loss_val: nan, pos_over_neg: 1540.73828125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.5571, loss_val: nan, pos_over_neg: 1485.1583251953125 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.5461, loss_val: nan, pos_over_neg: -14109.5908203125 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.5478, loss_val: nan, pos_over_neg: -25090.26171875 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 1308.2099609375 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 975.0604858398438 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.5442, loss_val: nan, pos_over_neg: 2262.958984375 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 1816.339111328125 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 4126.20458984375 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 1805.5123291015625 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 1104.637451171875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 2689.535888671875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.5467, loss_val: nan, pos_over_neg: 2837.23388671875 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: -52242.81640625 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.5575, loss_val: nan, pos_over_neg: 1789.3773193359375 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.5572, loss_val: nan, pos_over_neg: 2047.2286376953125 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 3287.522705078125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 2323.436279296875 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.5554, loss_val: nan, pos_over_neg: 3577.21923828125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.5561, loss_val: nan, pos_over_neg: 11475.7568359375 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.5566, loss_val: nan, pos_over_neg: 3814.56884765625 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.5506, loss_val: nan, pos_over_neg: 4817.2900390625 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.5458, loss_val: nan, pos_over_neg: -4670.88037109375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.5645, loss_val: nan, pos_over_neg: 3247.41162109375 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.556, loss_val: nan, pos_over_neg: 1210.248291015625 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.5579, loss_val: nan, pos_over_neg: -29011.41796875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.5569, loss_val: nan, pos_over_neg: 1232.41943359375 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 1234.6387939453125 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 1025.2239990234375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.5594, loss_val: nan, pos_over_neg: 1738.1497802734375 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.5476, loss_val: nan, pos_over_neg: 3784.645751953125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.5573, loss_val: nan, pos_over_neg: 3023.13525390625 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 931.8582763671875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.5538, loss_val: nan, pos_over_neg: 70317.3828125 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.5613, loss_val: nan, pos_over_neg: 2411.11572265625 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.5611, loss_val: nan, pos_over_neg: 1539.0172119140625 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.5633, loss_val: nan, pos_over_neg: 1977.0966796875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.5485, loss_val: nan, pos_over_neg: 1468.0994873046875 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.5544, loss_val: nan, pos_over_neg: 1349.8565673828125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 973.9303588867188 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.5495, loss_val: nan, pos_over_neg: 2363.975341796875 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: 1107.5057373046875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 1315.0263671875 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.5542, loss_val: nan, pos_over_neg: 2142.208984375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.5513, loss_val: nan, pos_over_neg: 4757.880859375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 1914.1981201171875 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.5524, loss_val: nan, pos_over_neg: 1880.836669921875 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.5488, loss_val: nan, pos_over_neg: 1104.5091552734375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.5461, loss_val: nan, pos_over_neg: 173379.796875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.5511, loss_val: nan, pos_over_neg: 1135.8382568359375 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 1345.0865478515625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.5701, loss_val: nan, pos_over_neg: 1168.64111328125 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.551, loss_val: nan, pos_over_neg: 5763.05859375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.5483, loss_val: nan, pos_over_neg: 2900.923095703125 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.5491, loss_val: nan, pos_over_neg: 1558.4718017578125 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 975.741943359375 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.5517, loss_val: nan, pos_over_neg: 1426.646484375 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.5622, loss_val: nan, pos_over_neg: 2624.142578125 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.5508, loss_val: nan, pos_over_neg: -49284.875 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.5487, loss_val: nan, pos_over_neg: 814.348876953125 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 727.7178344726562 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.5459, loss_val: nan, pos_over_neg: 2166.7509765625 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.5457, loss_val: nan, pos_over_neg: 2118.34228515625 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.5532, loss_val: nan, pos_over_neg: 2499.326416015625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.5513, loss_val: nan, pos_over_neg: 3591.37060546875 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.5519, loss_val: nan, pos_over_neg: 4780.56689453125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.565, loss_val: nan, pos_over_neg: 821.0191040039062 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.5627, loss_val: nan, pos_over_neg: 3860.490234375 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.5475, loss_val: nan, pos_over_neg: 1717.4263916015625 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.5483, loss_val: nan, pos_over_neg: 9646.7275390625 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.5398, loss_val: nan, pos_over_neg: -2646.6962890625 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.5595, loss_val: nan, pos_over_neg: 16122.3779296875 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.5465, loss_val: nan, pos_over_neg: 1414.6376953125 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.5514, loss_val: nan, pos_over_neg: 1040.636474609375 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.5567, loss_val: nan, pos_over_neg: 790.3809814453125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.5565, loss_val: nan, pos_over_neg: 1720.4122314453125 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.5503, loss_val: nan, pos_over_neg: 7301.72412109375 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.5537, loss_val: nan, pos_over_neg: 2092.41552734375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: 1080.771484375 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 1158.786376953125 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.5502, loss_val: nan, pos_over_neg: 2167.7998046875 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.5485, loss_val: nan, pos_over_neg: 7006.40771484375 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.5606, loss_val: nan, pos_over_neg: 18084.912109375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.5608, loss_val: nan, pos_over_neg: 2804.43017578125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.5501, loss_val: nan, pos_over_neg: 1288.0548095703125 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.5558, loss_val: nan, pos_over_neg: 3863.3291015625 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.5508, loss_val: nan, pos_over_neg: -26936.71484375 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.5658, loss_val: nan, pos_over_neg: 1630.9874267578125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.5618, loss_val: nan, pos_over_neg: 894.0818481445312 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.5506, loss_val: nan, pos_over_neg: 933.6007690429688 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.5599, loss_val: nan, pos_over_neg: 5052.35888671875 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.5549, loss_val: nan, pos_over_neg: 2046.2042236328125 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1242.9686279296875 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.5511, loss_val: nan, pos_over_neg: 2541.7314453125 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.558, loss_val: nan, pos_over_neg: 2078.263671875 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.5576, loss_val: nan, pos_over_neg: 2372.8662109375 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.5519, loss_val: nan, pos_over_neg: 35802.03515625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.5614, loss_val: nan, pos_over_neg: 1062.8702392578125 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.5494, loss_val: nan, pos_over_neg: 1962.8525390625 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.5593, loss_val: nan, pos_over_neg: 798.3662109375 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.5517, loss_val: nan, pos_over_neg: 8079.583984375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1644.9149169921875 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.5521, loss_val: nan, pos_over_neg: 1296.305419921875 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.555, loss_val: nan, pos_over_neg: 1721.0457763671875 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.552, loss_val: nan, pos_over_neg: 5022.44970703125 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.5547, loss_val: nan, pos_over_neg: 1794.5108642578125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.5498, loss_val: nan, pos_over_neg: 1129.2393798828125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.5584, loss_val: nan, pos_over_neg: 1035.949462890625 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.5591, loss_val: nan, pos_over_neg: 966.8648681640625 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.5564, loss_val: nan, pos_over_neg: 1452.1705322265625 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.5582, loss_val: nan, pos_over_neg: 2277.062744140625 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 839.894287109375 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.5463, loss_val: nan, pos_over_neg: 7671.0517578125 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.5639, loss_val: nan, pos_over_neg: 634.5653076171875 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.5607, loss_val: nan, pos_over_neg: 2034.662353515625 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.5646, loss_val: nan, pos_over_neg: 1225.426025390625 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.5588, loss_val: nan, pos_over_neg: 2062.421630859375 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.557, loss_val: nan, pos_over_neg: 1599.8677978515625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.5629, loss_val: nan, pos_over_neg: 1112.2650146484375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.5462, loss_val: nan, pos_over_neg: 1932.7166748046875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.5512, loss_val: nan, pos_over_neg: 3723.136474609375 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.5527, loss_val: nan, pos_over_neg: 4193.736328125 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.5539, loss_val: nan, pos_over_neg: 1100.2615966796875 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.55, loss_val: nan, pos_over_neg: 1208.7996826171875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.5472, loss_val: nan, pos_over_neg: 4940.24560546875 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.545, loss_val: nan, pos_over_neg: 18526.548828125 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.5523, loss_val: nan, pos_over_neg: -43564.10546875 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.5604, loss_val: nan, pos_over_neg: 1354.25537109375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.5465, loss_val: nan, pos_over_neg: -11317.46875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.5492, loss_val: nan, pos_over_neg: 2004.1593017578125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.5484, loss_val: nan, pos_over_neg: -20624.83984375 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.5681, loss_val: nan, pos_over_neg: 1324.639404296875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.5583, loss_val: nan, pos_over_neg: 1852.0166015625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.553, loss_val: nan, pos_over_neg: 993.21240234375 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.5439, loss_val: nan, pos_over_neg: 4038.919677734375 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "l2_alpha = 0.000\n",
    "n_epochs=300000\n",
    "\n",
    "acc_train_lst, acc_val_lst = [], []\n",
    "acc_train_SYT_lst, acc_val_SYT_lst = [], []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "   \n",
    "    \n",
    "    ds_run, dl_run = get_ds_dl(input_xtr, run_transform, expand_dim=False)\n",
    "    features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    ds_run, dl_run = get_ds_dl(input_xval, run_transform, expand_dim=False)\n",
    "    features_val = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    \n",
    "    ds_run, dl_run = get_ds_dl(input_xtr_SYT, run_transform, expand_dim=False)\n",
    "    features_train_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    ds_run, dl_run = get_ds_dl(input_xval_SYT, run_transform, expand_dim=False)\n",
    "    features_val_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "\n",
    "    acc_train, acc_val = {}, {}\n",
    "    acc_train_SYT, acc_val_SYT = {}, {}\n",
    "    C_toUse = np.array([1e1,1e0,1e-1,1e-2])\n",
    "    for C in C_toUse:\n",
    "        logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "        logreg.fit(features_train, y_labeled_train)\n",
    "        acc_train_tmp = logreg.score(features_train, y_labeled_train)\n",
    "        acc_train[C] = acc_train_tmp\n",
    "        acc_val_tmp = logreg.score(features_val, y_labeled_val)\n",
    "        acc_val[C] = acc_val_tmp\n",
    "        \n",
    "        logreg_SYT = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "        logreg_SYT.fit(features_train_SYT, y_labeled_train_SYT)\n",
    "        acc_train_tmp = logreg_SYT.score(features_train_SYT, y_labeled_train_SYT)\n",
    "        acc_train_SYT[C] = acc_train_tmp\n",
    "        acc_val_tmp = logreg_SYT.score(features_val_SYT, y_labeled_val_SYT)\n",
    "        acc_val_SYT[C] = acc_val_tmp\n",
    "\n",
    "    acc_train_lst.append(acc_train)\n",
    "    acc_val_lst.append(acc_val)\n",
    "    acc_train_SYT_lst.append(acc_train_SYT)\n",
    "    acc_val_SYT_lst.append(acc_val_SYT)\n",
    "\n",
    "    acc_train_df, acc_val_df, acc_train_SYT_df, acc_val_SYT_df = pd.DataFrame(acc_train_lst), pd.DataFrame(acc_val_lst), pd.DataFrame(acc_train_SYT_lst), pd.DataFrame(acc_val_SYT_lst)\n",
    "    \n",
    "    acc_train_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train.csv')\n",
    "    acc_train_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train_SYT.csv')\n",
    "    acc_val_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val.csv')\n",
    "    acc_val_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val_SYT.csv')\n",
    "    \n",
    "    \n",
    "    \n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "# model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=efficient'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc.estimate_size_of_float_array(input_shape=(80000,3,224,224), bitsize=32)/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape, features_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train_SYT)\n",
    "    # logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "    # acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "    # acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(1)).fit(features_train, y_labeled_train_SYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "#                                     torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "#                                     torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                                     n_transforms=1,\n",
    "#                                     class_weights=np.array([1]),\n",
    "#                                     # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "#                                     transform=scripted_transforms_validation,\n",
    "#                                     # DEVICE='cpu',\n",
    "#                                     DEVICE='cpu',\n",
    "#                                     dtype_X=torch.float32,\n",
    "#                                     dtype_y=torch.int64,\n",
    "                                    \n",
    "#                                     temp_uncertainty=16\n",
    "#                                     )\n",
    "# dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "# #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "#                                                 batch_size=1024,\n",
    "#                                                 shuffle=False,\n",
    "#                                                 drop_last=False,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 num_workers=32,\n",
    "#                                                 persistent_workers=True,\n",
    "#                                                 # prefetch_factor=0\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_labe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(images_dup[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(images_dup[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "#                                     torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "#                                     torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                                     n_transforms=1,\n",
    "#                                     class_weights=np.array([1]),\n",
    "#                                     # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "#                                     transform=scripted_transforms_validation,\n",
    "#                                     # DEVICE='cpu',\n",
    "#                                     DEVICE='cpu',\n",
    "#                                     dtype_X=torch.float32,\n",
    "#                                     dtype_y=torch.int64,\n",
    "                                    \n",
    "#                                     temp_uncertainty=16\n",
    "#                                     )\n",
    "# dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "# #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "#                                                 batch_size=1024,\n",
    "#                                                 shuffle=False,\n",
    "#                                                 drop_last=False,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 num_workers=32,\n",
    "#                                                 persistent_workers=True,\n",
    "#                                                 # prefetch_factor=0\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedded.shape, labels_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_dup, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(evr[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "\n",
    "# # mpl.rcParams['image.cmap'] = 'Set1'\n",
    "# %matplotlib notebook\n",
    "# plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# # plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# # plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Precomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "no_transforms = torch.nn.Sequential()\n",
    "scale_rsz_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "tile_transforms = torch.nn.Sequential(augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "\n",
    "\n",
    "scripted_all_transforms = torch.jit.script(all_transforms)\n",
    "scripted_no_transforms = torch.jit.script(no_transforms)\n",
    "scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "scripted_tile_transform = torch.jit.script(tile_transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_no_transforms\n",
    "run_transform = scripted_all_transforms\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16,\n",
    "                                expand_dim=False\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'All Transformations Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_all_transforms\n",
    "run_transform = scripted_no_transforms\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16,\n",
    "                                expand_dim=False\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'No Transformations Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup_transform = scripted_scale_rsz_transform\n",
    "run_transform = scripted_tile_transform\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16,\n",
    "                            expand_dim=False\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'Tile Transformation Alone Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
