{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [],
   "source": [
    "# DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)\n",
    "DEVICE=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "#             if classifier_fc_sizes is not None:\n",
    "#                 self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "            \n",
    "            # self.base_model = torch.nn.DataParallel(self.base_model)\n",
    "            \n",
    "            \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            \n",
    "            self.base_model.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.base_model.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            \n",
    "            self.base_model.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.base_model.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            \n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.forward_latent(X)\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        # interim = self.get_head(interim)\n",
    "        # interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight 6 True\n",
      "base_model.6.0.bn1.weight 6 True\n",
      "base_model.6.0.bn1.bias 6 True\n",
      "base_model.6.0.conv2.weight 6 True\n",
      "base_model.6.0.bn2.weight 6 True\n",
      "base_model.6.0.bn2.bias 6 True\n",
      "base_model.6.0.downsample.0.weight 6 True\n",
      "base_model.6.0.downsample.1.weight 6 True\n",
      "base_model.6.0.downsample.1.bias 6 True\n",
      "base_model.6.1.conv1.weight 6 True\n",
      "base_model.6.1.bn1.weight 6 True\n",
      "base_model.6.1.bn1.bias 6 True\n",
      "base_model.6.1.conv2.weight 6 True\n",
      "base_model.6.1.bn2.weight 6 True\n",
      "base_model.6.1.bn2.bias 6 True\n",
      "base_model.7.0.conv1.weight 7 True\n",
      "base_model.7.0.bn1.weight 7 True\n",
      "base_model.7.0.bn1.bias 7 True\n",
      "base_model.7.0.conv2.weight 7 True\n",
      "base_model.7.0.bn2.weight 7 True\n",
      "base_model.7.0.bn2.bias 7 True\n",
      "base_model.7.0.downsample.0.weight 7 True\n",
      "base_model.7.0.downsample.1.weight 7 True\n",
      "base_model.7.0.downsample.1.bias 7 True\n",
      "base_model.7.1.conv1.weight 7 True\n",
      "base_model.7.1.bn1.weight 7 True\n",
      "base_model.7.1.bn1.bias 7 True\n",
      "base_model.7.1.conv2.weight 7 True\n",
      "base_model.7.1.bn2.weight 7 True\n",
      "base_model.7.1.bn2.bias 7 True\n",
      "base_model.PreHead_0.weight P False\n",
      "base_model.PreHead_0.bias P False\n",
      "base_model.PreHead_1.weight P False\n",
      "base_model.PreHead_1.bias P False\n",
      "base_model.PostHead_0.weight P False\n",
      "base_model.PostHead_0.bias P False\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if name[11].isnumeric():\n",
    "            if int(name[11]) < 6:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name, name[11], name[11].isnumeric())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2),\n",
    "                                        scale=(0.4, 1.3), \n",
    "                                        shear=(-25, 25, -25, 25), \n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002,\n",
    "                                    prob=1),\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.1,\n",
    "                                    prob=1),\n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    DEVICE='cpu',\n",
    "                                    # DEVICE=0,\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=20,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=2\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 2)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0].cpu())\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 device(s) found.\n",
      "0 Quadro RTX 8000\n",
      "1 Quadro RTX 8000\n",
      "2 Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "torch_helpers.show_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parallel = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "# model_parallel = torch.nn.DataParallel(model, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var: masks_cat_raw,   device:cpu,   shape: torch.Size([711808, 36, 36]),   size: 3.690012672 GB,   requires_grad: False\n",
      "var: ROIs_without_NaNs,   device:cpu,   shape: torch.Size([711807]),   size: 0.005694456 GB,   requires_grad: False\n",
      "var: masks_cat,   device:cpu,   shape: torch.Size([711807, 36, 36]),   size: 3.690007488 GB,   requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "torch_helpers.show_all_tensors(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "# criterion = [_.to(DEVICE) for _ in criterion]\n",
    "# criterion = [_ for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parallel.to(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chopped_parallel = torch.nn.DataParallel(model_chopped, device_ids=[0,1,2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.to(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2404,  0.0128,  0.2456,  ...,  0.0054,  0.1058, -0.0937],\n",
       "        [ 0.2258,  0.0810,  0.1474,  ...,  0.0128, -0.0015,  0.0386],\n",
       "        [ 0.1850, -0.1030,  0.2427,  ..., -0.0209,  0.1576,  0.0269],\n",
       "        ...,\n",
       "        [ 0.1244, -0.0533,  0.2121,  ...,  0.0425,  0.1055, -0.0326],\n",
       "        [ 0.0988, -0.0051,  0.1536,  ...,  0.1117,  0.0398,  0.0576],\n",
       "        [ 0.1317,  0.0796,  0.1021,  ...,  0.0725,  0.0170,  0.0077]],\n",
       "       device='cuda:0', grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torch.rand(100,3,224,224, dtype=torch.float32, device=0)\n",
    "# model.forward(test_data)\n",
    "model_parallel.forward(test_data)\n",
    "# chopped_parallel.forward(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model_parallel.parameters():\n",
    "#     print(param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.6106, loss_val: nan, pos_over_neg: 1.0100092887878418 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.4306, loss_val: nan, pos_over_neg: 1.0811201333999634 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 7.3048, loss_val: nan, pos_over_neg: 1.6084449291229248 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 7.2868, loss_val: nan, pos_over_neg: 1.3654723167419434 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 7.058, loss_val: nan, pos_over_neg: 1.2637993097305298 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.9146, loss_val: nan, pos_over_neg: 1.451255202293396 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7889, loss_val: nan, pos_over_neg: 1.9823416471481323 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.6413, loss_val: nan, pos_over_neg: 2.8610496520996094 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.6275, loss_val: nan, pos_over_neg: 3.529315233230591 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.5229, loss_val: nan, pos_over_neg: 3.92543625831604 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.4852, loss_val: nan, pos_over_neg: 3.6489999294281006 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.4582, loss_val: nan, pos_over_neg: 3.394658327102661 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.3583, loss_val: nan, pos_over_neg: 3.660940170288086 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.2643, loss_val: nan, pos_over_neg: 4.729483604431152 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.1989, loss_val: nan, pos_over_neg: 8.11876392364502 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.1589, loss_val: nan, pos_over_neg: 15.538749694824219 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.0919, loss_val: nan, pos_over_neg: 20.589574813842773 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.0745, loss_val: nan, pos_over_neg: 19.653650283813477 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9896, loss_val: nan, pos_over_neg: 15.846778869628906 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.979, loss_val: nan, pos_over_neg: 12.495311737060547 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 13.392111778259277 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 18.149465560913086 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8661, loss_val: nan, pos_over_neg: 26.862796783447266 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 31.06601333618164 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 35.900753021240234 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 29.839229583740234 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7318, loss_val: nan, pos_over_neg: 34.23074722290039 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.6813, loss_val: nan, pos_over_neg: 38.39274597167969 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.6801, loss_val: nan, pos_over_neg: 36.64750671386719 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.6039, loss_val: nan, pos_over_neg: 50.955101013183594 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.5932, loss_val: nan, pos_over_neg: 51.26704406738281 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.5452, loss_val: nan, pos_over_neg: 40.1784782409668 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.4926, loss_val: nan, pos_over_neg: 40.78590393066406 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.5467, loss_val: nan, pos_over_neg: 49.33595657348633 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.541, loss_val: nan, pos_over_neg: 88.45403289794922 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.5117, loss_val: nan, pos_over_neg: 59.086727142333984 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.4281, loss_val: nan, pos_over_neg: 60.44749450683594 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.4247, loss_val: nan, pos_over_neg: 61.400856018066406 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.352, loss_val: nan, pos_over_neg: 47.290252685546875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.4012, loss_val: nan, pos_over_neg: 45.980751037597656 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.4402, loss_val: nan, pos_over_neg: 59.21748733520508 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.3228, loss_val: nan, pos_over_neg: 71.24930572509766 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.3524, loss_val: nan, pos_over_neg: 76.2256088256836 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.3625, loss_val: nan, pos_over_neg: 50.95716094970703 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.3611, loss_val: nan, pos_over_neg: 33.54733657836914 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.2859, loss_val: nan, pos_over_neg: 46.454429626464844 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.2655, loss_val: nan, pos_over_neg: 91.94129943847656 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.3197, loss_val: nan, pos_over_neg: 96.26688385009766 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.2902, loss_val: nan, pos_over_neg: 108.54254150390625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.3089, loss_val: nan, pos_over_neg: 69.74514770507812 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.223, loss_val: nan, pos_over_neg: 37.94501876831055 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.2135, loss_val: nan, pos_over_neg: 54.03105163574219 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.2562, loss_val: nan, pos_over_neg: 66.8348388671875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.2233, loss_val: nan, pos_over_neg: 62.93287658691406 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.1873, loss_val: nan, pos_over_neg: 70.97303009033203 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.0998, loss_val: nan, pos_over_neg: 78.3505859375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.1743, loss_val: nan, pos_over_neg: 49.891929626464844 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.1326, loss_val: nan, pos_over_neg: 59.34720993041992 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.1423, loss_val: nan, pos_over_neg: 71.21038818359375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.1906, loss_val: nan, pos_over_neg: 66.04652404785156 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.1247, loss_val: nan, pos_over_neg: 98.01042175292969 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.105, loss_val: nan, pos_over_neg: 77.56015014648438 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.081, loss_val: nan, pos_over_neg: 66.390625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.1219, loss_val: nan, pos_over_neg: 57.083404541015625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.1303, loss_val: nan, pos_over_neg: 81.03072357177734 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.0812, loss_val: nan, pos_over_neg: 98.02435302734375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.0554, loss_val: nan, pos_over_neg: 60.24885940551758 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.0639, loss_val: nan, pos_over_neg: 51.71074676513672 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.1158, loss_val: nan, pos_over_neg: 57.01509475708008 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.1026, loss_val: nan, pos_over_neg: 86.32465362548828 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.0768, loss_val: nan, pos_over_neg: 61.51651382446289 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.0268, loss_val: nan, pos_over_neg: 64.36505889892578 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.0141, loss_val: nan, pos_over_neg: 66.90707397460938 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.0287, loss_val: nan, pos_over_neg: 82.996826171875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.0265, loss_val: nan, pos_over_neg: 75.59764099121094 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 4.9894, loss_val: nan, pos_over_neg: 56.42458724975586 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.0334, loss_val: nan, pos_over_neg: 55.319480895996094 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 4.9584, loss_val: nan, pos_over_neg: 69.47603607177734 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 4.9459, loss_val: nan, pos_over_neg: 120.37332153320312 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 4.9871, loss_val: nan, pos_over_neg: 77.11437225341797 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 4.9874, loss_val: nan, pos_over_neg: 70.6766357421875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 4.9365, loss_val: nan, pos_over_neg: 61.194454193115234 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 4.9588, loss_val: nan, pos_over_neg: 53.66823196411133 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 4.9373, loss_val: nan, pos_over_neg: 88.9123764038086 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 4.9384, loss_val: nan, pos_over_neg: 99.35674285888672 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 4.9143, loss_val: nan, pos_over_neg: 76.77317810058594 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 4.9007, loss_val: nan, pos_over_neg: 86.6654052734375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 4.8709, loss_val: nan, pos_over_neg: 66.23966217041016 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 4.9365, loss_val: nan, pos_over_neg: 59.99204635620117 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 4.8693, loss_val: nan, pos_over_neg: 69.95247650146484 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 4.8649, loss_val: nan, pos_over_neg: 101.50975036621094 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 4.9135, loss_val: nan, pos_over_neg: 101.12616729736328 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 4.9104, loss_val: nan, pos_over_neg: 72.32825469970703 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 4.9005, loss_val: nan, pos_over_neg: 58.00322723388672 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 4.9231, loss_val: nan, pos_over_neg: 98.81159210205078 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 4.8631, loss_val: nan, pos_over_neg: 96.44501495361328 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 4.8075, loss_val: nan, pos_over_neg: 111.41935729980469 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 4.8764, loss_val: nan, pos_over_neg: 96.98907470703125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 4.7547, loss_val: nan, pos_over_neg: 101.27534484863281 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 4.822, loss_val: nan, pos_over_neg: 106.64264678955078 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 4.8993, loss_val: nan, pos_over_neg: 115.54216003417969 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 4.8418, loss_val: nan, pos_over_neg: 68.29241180419922 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 4.7873, loss_val: nan, pos_over_neg: 79.05561828613281 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 4.8388, loss_val: nan, pos_over_neg: 113.69963836669922 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 4.8373, loss_val: nan, pos_over_neg: 54.739749908447266 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 4.8082, loss_val: nan, pos_over_neg: 70.79679870605469 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 4.831, loss_val: nan, pos_over_neg: 126.80365753173828 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 4.7671, loss_val: nan, pos_over_neg: 102.84394073486328 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 4.7692, loss_val: nan, pos_over_neg: 123.92866516113281 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 4.7773, loss_val: nan, pos_over_neg: 72.71710968017578 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 4.806, loss_val: nan, pos_over_neg: 90.87579345703125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 4.8334, loss_val: nan, pos_over_neg: 106.88422393798828 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 4.7585, loss_val: nan, pos_over_neg: 79.67230987548828 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 4.7846, loss_val: nan, pos_over_neg: 83.11205291748047 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 4.7631, loss_val: nan, pos_over_neg: 100.80370330810547 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 4.7056, loss_val: nan, pos_over_neg: 99.13790130615234 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 4.7552, loss_val: nan, pos_over_neg: 78.02235412597656 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 4.7615, loss_val: nan, pos_over_neg: 79.73979949951172 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 4.7119, loss_val: nan, pos_over_neg: 125.16661834716797 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 4.743, loss_val: nan, pos_over_neg: 149.73672485351562 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 4.7533, loss_val: nan, pos_over_neg: 104.51974487304688 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 4.7707, loss_val: nan, pos_over_neg: 100.42884063720703 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 4.7407, loss_val: nan, pos_over_neg: 72.51827239990234 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 4.7194, loss_val: nan, pos_over_neg: 62.034908294677734 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 4.7553, loss_val: nan, pos_over_neg: 83.35787963867188 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 4.7087, loss_val: nan, pos_over_neg: 110.3826904296875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 4.7268, loss_val: nan, pos_over_neg: 78.24844360351562 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 4.6965, loss_val: nan, pos_over_neg: 116.50920867919922 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 4.7586, loss_val: nan, pos_over_neg: 63.390987396240234 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 4.7188, loss_val: nan, pos_over_neg: 98.57605743408203 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 4.7043, loss_val: nan, pos_over_neg: 71.65868377685547 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 4.6856, loss_val: nan, pos_over_neg: 95.74405670166016 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 4.7386, loss_val: nan, pos_over_neg: 108.42613983154297 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 4.746, loss_val: nan, pos_over_neg: 68.43022918701172 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 4.739, loss_val: nan, pos_over_neg: 79.54875946044922 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 4.7039, loss_val: nan, pos_over_neg: 97.94529724121094 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 4.6372, loss_val: nan, pos_over_neg: 116.17443084716797 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 4.7536, loss_val: nan, pos_over_neg: 59.00634002685547 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 4.6632, loss_val: nan, pos_over_neg: 104.79859161376953 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 4.7036, loss_val: nan, pos_over_neg: 80.53284454345703 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 4.7143, loss_val: nan, pos_over_neg: 85.99874877929688 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 4.7111, loss_val: nan, pos_over_neg: 146.62757873535156 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 4.6807, loss_val: nan, pos_over_neg: 104.06427001953125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 4.6732, loss_val: nan, pos_over_neg: 66.4682846069336 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 4.7151, loss_val: nan, pos_over_neg: 72.15486907958984 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 4.6573, loss_val: nan, pos_over_neg: 96.71090698242188 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 4.6796, loss_val: nan, pos_over_neg: 109.91934204101562 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 4.644, loss_val: nan, pos_over_neg: 80.74893951416016 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 4.6727, loss_val: nan, pos_over_neg: 87.56886291503906 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 4.689, loss_val: nan, pos_over_neg: 52.90232467651367 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 4.6202, loss_val: nan, pos_over_neg: 76.25741577148438 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 4.6406, loss_val: nan, pos_over_neg: 109.2862319946289 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 4.6436, loss_val: nan, pos_over_neg: 94.29341888427734 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 4.6279, loss_val: nan, pos_over_neg: 77.18363952636719 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 4.6122, loss_val: nan, pos_over_neg: 105.54435729980469 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 4.5775, loss_val: nan, pos_over_neg: 103.34194946289062 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 4.6115, loss_val: nan, pos_over_neg: 101.88607025146484 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 4.6043, loss_val: nan, pos_over_neg: 124.85124206542969 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 4.6347, loss_val: nan, pos_over_neg: 101.5020980834961 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 4.6012, loss_val: nan, pos_over_neg: 84.82207489013672 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 4.6659, loss_val: nan, pos_over_neg: 65.53297424316406 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 4.5546, loss_val: nan, pos_over_neg: 153.89369201660156 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 4.6086, loss_val: nan, pos_over_neg: 95.99696350097656 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 4.5562, loss_val: nan, pos_over_neg: 117.72724914550781 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 4.6021, loss_val: nan, pos_over_neg: 65.75831604003906 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 4.6243, loss_val: nan, pos_over_neg: 46.124732971191406 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 4.6165, loss_val: nan, pos_over_neg: 80.44316101074219 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 4.6226, loss_val: nan, pos_over_neg: 113.08394622802734 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 4.5922, loss_val: nan, pos_over_neg: 86.15156555175781 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 4.5868, loss_val: nan, pos_over_neg: 102.59397888183594 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 4.6337, loss_val: nan, pos_over_neg: 75.85256958007812 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 4.5976, loss_val: nan, pos_over_neg: 71.51020812988281 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 4.6449, loss_val: nan, pos_over_neg: 66.84339141845703 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 4.5738, loss_val: nan, pos_over_neg: 75.22587585449219 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 4.5557, loss_val: nan, pos_over_neg: 88.2454605102539 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 4.5558, loss_val: nan, pos_over_neg: 146.35464477539062 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 4.5891, loss_val: nan, pos_over_neg: 104.1111068725586 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 4.5516, loss_val: nan, pos_over_neg: 99.4378662109375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 4.5961, loss_val: nan, pos_over_neg: 83.56049346923828 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 4.5777, loss_val: nan, pos_over_neg: 84.25259399414062 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 4.6022, loss_val: nan, pos_over_neg: 79.78675842285156 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 4.5474, loss_val: nan, pos_over_neg: 65.38778686523438 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 4.5397, loss_val: nan, pos_over_neg: 75.60724639892578 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 4.5522, loss_val: nan, pos_over_neg: 93.79583740234375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 4.5549, loss_val: nan, pos_over_neg: 92.6357192993164 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 4.536, loss_val: nan, pos_over_neg: 107.47942352294922 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 4.5317, loss_val: nan, pos_over_neg: 101.42607879638672 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 4.5564, loss_val: nan, pos_over_neg: 113.0007553100586 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 4.5463, loss_val: nan, pos_over_neg: 80.4188232421875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 4.5553, loss_val: nan, pos_over_neg: 100.6851806640625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 4.5243, loss_val: nan, pos_over_neg: 78.6200942993164 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 4.5304, loss_val: nan, pos_over_neg: 159.4232177734375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 4.4919, loss_val: nan, pos_over_neg: 139.7393341064453 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 4.4869, loss_val: nan, pos_over_neg: 135.1275634765625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 4.5604, loss_val: nan, pos_over_neg: 115.0756607055664 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 4.5335, loss_val: nan, pos_over_neg: 171.720703125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 4.5664, loss_val: nan, pos_over_neg: 128.36891174316406 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 4.5066, loss_val: nan, pos_over_neg: 117.43611145019531 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 4.5309, loss_val: nan, pos_over_neg: 144.10446166992188 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 4.4979, loss_val: nan, pos_over_neg: 107.37210845947266 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 4.4651, loss_val: nan, pos_over_neg: 135.2125701904297 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 4.5097, loss_val: nan, pos_over_neg: 95.83651733398438 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 4.5128, loss_val: nan, pos_over_neg: 126.62980651855469 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 4.5229, loss_val: nan, pos_over_neg: 119.01602172851562 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 4.5428, loss_val: nan, pos_over_neg: 110.77928161621094 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 4.4654, loss_val: nan, pos_over_neg: 96.04358673095703 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 4.5086, loss_val: nan, pos_over_neg: 114.96697235107422 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 4.513, loss_val: nan, pos_over_neg: 119.14838409423828 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 4.4604, loss_val: nan, pos_over_neg: 131.78860473632812 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 4.4755, loss_val: nan, pos_over_neg: 148.25428771972656 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 4.5261, loss_val: nan, pos_over_neg: 93.204833984375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 4.518, loss_val: nan, pos_over_neg: 106.09412384033203 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 4.487, loss_val: nan, pos_over_neg: 109.11898803710938 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 4.4952, loss_val: nan, pos_over_neg: 106.20977020263672 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 4.5003, loss_val: nan, pos_over_neg: 100.60972595214844 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 4.5221, loss_val: nan, pos_over_neg: 86.56534576416016 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 4.5105, loss_val: nan, pos_over_neg: 79.55363464355469 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 4.4654, loss_val: nan, pos_over_neg: 130.0071258544922 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 4.531, loss_val: nan, pos_over_neg: 139.7374725341797 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 4.4957, loss_val: nan, pos_over_neg: 102.49424743652344 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 4.4334, loss_val: nan, pos_over_neg: 108.44002532958984 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 4.4731, loss_val: nan, pos_over_neg: 68.56725311279297 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 4.4356, loss_val: nan, pos_over_neg: 77.21109008789062 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 4.5075, loss_val: nan, pos_over_neg: 131.7688751220703 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 4.4341, loss_val: nan, pos_over_neg: 125.29129791259766 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 4.4999, loss_val: nan, pos_over_neg: 88.41297149658203 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 4.4634, loss_val: nan, pos_over_neg: 146.76524353027344 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 4.4598, loss_val: nan, pos_over_neg: 112.73296356201172 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 4.4479, loss_val: nan, pos_over_neg: 109.74137878417969 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 4.5199, loss_val: nan, pos_over_neg: 106.64706420898438 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 4.4436, loss_val: nan, pos_over_neg: 102.62055969238281 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 4.441, loss_val: nan, pos_over_neg: 99.63496398925781 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 4.4995, loss_val: nan, pos_over_neg: 110.83482360839844 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 4.431, loss_val: nan, pos_over_neg: 89.31523132324219 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 4.4547, loss_val: nan, pos_over_neg: 95.97828674316406 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 4.4352, loss_val: nan, pos_over_neg: 92.81977081298828 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 4.4359, loss_val: nan, pos_over_neg: 100.64485168457031 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 4.4733, loss_val: nan, pos_over_neg: 140.05709838867188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 4.4055, loss_val: nan, pos_over_neg: 157.94757080078125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 4.4955, loss_val: nan, pos_over_neg: 125.00022888183594 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 4.3918, loss_val: nan, pos_over_neg: 123.17376708984375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 4.4688, loss_val: nan, pos_over_neg: 88.76290130615234 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 4.4396, loss_val: nan, pos_over_neg: 97.7470474243164 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 4.4485, loss_val: nan, pos_over_neg: 110.94607543945312 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 4.4258, loss_val: nan, pos_over_neg: 127.87673950195312 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 4.3896, loss_val: nan, pos_over_neg: 154.02960205078125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 4.4199, loss_val: nan, pos_over_neg: 151.72344970703125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 4.399, loss_val: nan, pos_over_neg: 97.9468765258789 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 4.4004, loss_val: nan, pos_over_neg: 181.53436279296875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 4.386, loss_val: nan, pos_over_neg: 130.65573120117188 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 4.3724, loss_val: nan, pos_over_neg: 143.303466796875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 4.4323, loss_val: nan, pos_over_neg: 119.80046081542969 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 4.3796, loss_val: nan, pos_over_neg: 160.66978454589844 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 4.4407, loss_val: nan, pos_over_neg: 108.53348541259766 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 4.4407, loss_val: nan, pos_over_neg: 174.90411376953125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 4.4063, loss_val: nan, pos_over_neg: 186.36497497558594 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 4.4151, loss_val: nan, pos_over_neg: 142.3834686279297 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 4.4285, loss_val: nan, pos_over_neg: 123.8962631225586 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 4.4443, loss_val: nan, pos_over_neg: 114.80653381347656 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 4.4175, loss_val: nan, pos_over_neg: 98.82453918457031 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 4.3825, loss_val: nan, pos_over_neg: 146.3802032470703 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 4.3653, loss_val: nan, pos_over_neg: 134.79347229003906 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 4.4017, loss_val: nan, pos_over_neg: 170.29006958007812 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 4.397, loss_val: nan, pos_over_neg: 138.3511962890625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 4.3719, loss_val: nan, pos_over_neg: 153.54298400878906 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 4.3814, loss_val: nan, pos_over_neg: 113.14270782470703 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 4.4119, loss_val: nan, pos_over_neg: 100.92095947265625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 4.4053, loss_val: nan, pos_over_neg: 106.21723937988281 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 4.3893, loss_val: nan, pos_over_neg: 182.4680633544922 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 4.3902, loss_val: nan, pos_over_neg: 134.03811645507812 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 4.4548, loss_val: nan, pos_over_neg: 106.87659454345703 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 4.352, loss_val: nan, pos_over_neg: 117.77259826660156 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 4.3992, loss_val: nan, pos_over_neg: 102.00008392333984 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 4.3883, loss_val: nan, pos_over_neg: 84.33232116699219 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 4.3885, loss_val: nan, pos_over_neg: 96.61185455322266 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 4.3632, loss_val: nan, pos_over_neg: 113.86663055419922 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 4.3907, loss_val: nan, pos_over_neg: 133.35862731933594 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 4.3462, loss_val: nan, pos_over_neg: 146.83816528320312 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 4.3719, loss_val: nan, pos_over_neg: 195.0900421142578 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 4.371, loss_val: nan, pos_over_neg: 98.77704620361328 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 4.3085, loss_val: nan, pos_over_neg: 94.57488250732422 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 4.3701, loss_val: nan, pos_over_neg: 102.46903228759766 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 4.3571, loss_val: nan, pos_over_neg: 91.38505554199219 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 4.3756, loss_val: nan, pos_over_neg: 106.67595672607422 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 4.3359, loss_val: nan, pos_over_neg: 110.16141510009766 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 4.3414, loss_val: nan, pos_over_neg: 105.40555572509766 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 4.3049, loss_val: nan, pos_over_neg: 133.8924102783203 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 4.3717, loss_val: nan, pos_over_neg: 89.32312774658203 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 4.3445, loss_val: nan, pos_over_neg: 74.09676361083984 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 4.3738, loss_val: nan, pos_over_neg: 146.92074584960938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 4.3593, loss_val: nan, pos_over_neg: 104.28189849853516 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 4.3658, loss_val: nan, pos_over_neg: 77.22217559814453 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 4.3763, loss_val: nan, pos_over_neg: 129.21583557128906 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 4.3176, loss_val: nan, pos_over_neg: 98.7999496459961 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 4.318, loss_val: nan, pos_over_neg: 117.661865234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 4.3645, loss_val: nan, pos_over_neg: 169.31622314453125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 4.3045, loss_val: nan, pos_over_neg: 138.06747436523438 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 4.3714, loss_val: nan, pos_over_neg: 114.0635757446289 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 4.3515, loss_val: nan, pos_over_neg: 109.37447357177734 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 4.322, loss_val: nan, pos_over_neg: 89.11369323730469 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 4.3037, loss_val: nan, pos_over_neg: 153.57936096191406 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 4.3617, loss_val: nan, pos_over_neg: 108.97831726074219 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 4.3917, loss_val: nan, pos_over_neg: 109.90054321289062 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 4.3476, loss_val: nan, pos_over_neg: 82.24291229248047 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 4.3562, loss_val: nan, pos_over_neg: 91.54486083984375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 4.3874, loss_val: nan, pos_over_neg: 91.59260559082031 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 4.3096, loss_val: nan, pos_over_neg: 123.07073974609375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 4.3254, loss_val: nan, pos_over_neg: 213.07240295410156 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 4.3147, loss_val: nan, pos_over_neg: 200.65939331054688 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 4.3264, loss_val: nan, pos_over_neg: 130.49916076660156 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 4.3283, loss_val: nan, pos_over_neg: 72.18878173828125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 4.3722, loss_val: nan, pos_over_neg: 86.90546417236328 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 4.2895, loss_val: nan, pos_over_neg: 178.88035583496094 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 4.3013, loss_val: nan, pos_over_neg: 190.21710205078125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 4.3377, loss_val: nan, pos_over_neg: 171.7261505126953 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 4.2892, loss_val: nan, pos_over_neg: 144.91542053222656 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 4.3356, loss_val: nan, pos_over_neg: 118.41497039794922 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 4.3426, loss_val: nan, pos_over_neg: 104.05464935302734 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 4.2814, loss_val: nan, pos_over_neg: 104.99186706542969 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 4.2987, loss_val: nan, pos_over_neg: 156.0469970703125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 4.3626, loss_val: nan, pos_over_neg: 157.66171264648438 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 4.3527, loss_val: nan, pos_over_neg: 100.3263168334961 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 4.2835, loss_val: nan, pos_over_neg: 121.573486328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 4.2767, loss_val: nan, pos_over_neg: 168.35105895996094 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 4.3279, loss_val: nan, pos_over_neg: 123.1777114868164 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 4.3023, loss_val: nan, pos_over_neg: 121.15904998779297 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 4.2638, loss_val: nan, pos_over_neg: 197.1437530517578 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 4.3148, loss_val: nan, pos_over_neg: 125.38664245605469 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 4.2895, loss_val: nan, pos_over_neg: 220.6475067138672 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 4.2658, loss_val: nan, pos_over_neg: 166.6380615234375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 4.3388, loss_val: nan, pos_over_neg: 105.88420104980469 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 4.343, loss_val: nan, pos_over_neg: 135.14691162109375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 4.2744, loss_val: nan, pos_over_neg: 127.13609313964844 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 4.2806, loss_val: nan, pos_over_neg: 149.60382080078125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 4.2894, loss_val: nan, pos_over_neg: 218.7859344482422 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 4.2702, loss_val: nan, pos_over_neg: 187.1542205810547 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 4.323, loss_val: nan, pos_over_neg: 170.30027770996094 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 4.3059, loss_val: nan, pos_over_neg: 139.91726684570312 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 4.2662, loss_val: nan, pos_over_neg: 165.60055541992188 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 4.323, loss_val: nan, pos_over_neg: 127.9231185913086 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 4.284, loss_val: nan, pos_over_neg: 205.65464782714844 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 4.2782, loss_val: nan, pos_over_neg: 225.8633575439453 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 4.319, loss_val: nan, pos_over_neg: 116.29499816894531 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 4.2797, loss_val: nan, pos_over_neg: 143.9059295654297 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 4.2258, loss_val: nan, pos_over_neg: 219.43838500976562 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 4.2769, loss_val: nan, pos_over_neg: 165.91970825195312 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 4.3031, loss_val: nan, pos_over_neg: 160.58705139160156 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 4.3651, loss_val: nan, pos_over_neg: 129.57858276367188 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 4.2503, loss_val: nan, pos_over_neg: 139.1705322265625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 4.3073, loss_val: nan, pos_over_neg: 167.77789306640625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 4.2496, loss_val: nan, pos_over_neg: 159.8079071044922 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 4.2992, loss_val: nan, pos_over_neg: 191.08351135253906 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 4.3289, loss_val: nan, pos_over_neg: 133.26266479492188 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 4.3183, loss_val: nan, pos_over_neg: 174.82293701171875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 4.2931, loss_val: nan, pos_over_neg: 138.8896026611328 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 4.2834, loss_val: nan, pos_over_neg: 259.576904296875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 4.2726, loss_val: nan, pos_over_neg: 138.7542724609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 4.3108, loss_val: nan, pos_over_neg: 92.50743103027344 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 4.2998, loss_val: nan, pos_over_neg: 100.90611267089844 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 4.2814, loss_val: nan, pos_over_neg: 110.70594024658203 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 4.3065, loss_val: nan, pos_over_neg: 149.66348266601562 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 4.2839, loss_val: nan, pos_over_neg: 126.04762268066406 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 4.2337, loss_val: nan, pos_over_neg: 146.56036376953125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 4.3409, loss_val: nan, pos_over_neg: 112.65382385253906 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 4.3363, loss_val: nan, pos_over_neg: 94.49192810058594 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 4.256, loss_val: nan, pos_over_neg: 122.51921844482422 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 4.2647, loss_val: nan, pos_over_neg: 158.1798553466797 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 4.2401, loss_val: nan, pos_over_neg: 197.59970092773438 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 4.2338, loss_val: nan, pos_over_neg: 121.2422866821289 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 4.2725, loss_val: nan, pos_over_neg: 156.56893920898438 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 4.287, loss_val: nan, pos_over_neg: 151.33236694335938 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 4.226, loss_val: nan, pos_over_neg: 179.22207641601562 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 4.3017, loss_val: nan, pos_over_neg: 101.53435516357422 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 4.2602, loss_val: nan, pos_over_neg: 145.2141876220703 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 4.2855, loss_val: nan, pos_over_neg: 160.28233337402344 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 4.2646, loss_val: nan, pos_over_neg: 173.861328125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 4.3236, loss_val: nan, pos_over_neg: 154.08224487304688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 4.2151, loss_val: nan, pos_over_neg: 152.844970703125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 4.2806, loss_val: nan, pos_over_neg: 167.8987579345703 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 4.2505, loss_val: nan, pos_over_neg: 152.12879943847656 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 4.2066, loss_val: nan, pos_over_neg: 142.5716094970703 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 4.2555, loss_val: nan, pos_over_neg: 124.65496826171875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 4.241, loss_val: nan, pos_over_neg: 182.4636993408203 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 4.2305, loss_val: nan, pos_over_neg: 188.6475830078125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 4.2617, loss_val: nan, pos_over_neg: 175.0924835205078 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 4.3284, loss_val: nan, pos_over_neg: 106.0150375366211 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 4.1977, loss_val: nan, pos_over_neg: 130.7196044921875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 4.2448, loss_val: nan, pos_over_neg: 140.46824645996094 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 4.2989, loss_val: nan, pos_over_neg: 100.23178100585938 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 4.305, loss_val: nan, pos_over_neg: 135.06936645507812 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 4.2881, loss_val: nan, pos_over_neg: 158.85707092285156 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 4.2789, loss_val: nan, pos_over_neg: 142.09857177734375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 4.2328, loss_val: nan, pos_over_neg: 177.7854461669922 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 4.2284, loss_val: nan, pos_over_neg: 179.1440887451172 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 4.2949, loss_val: nan, pos_over_neg: 103.12368774414062 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 4.2889, loss_val: nan, pos_over_neg: 138.90516662597656 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 4.261, loss_val: nan, pos_over_neg: 198.45025634765625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 4.2514, loss_val: nan, pos_over_neg: 137.5988311767578 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 4.2548, loss_val: nan, pos_over_neg: 176.0218505859375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 4.2282, loss_val: nan, pos_over_neg: 142.06393432617188 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 4.1946, loss_val: nan, pos_over_neg: 163.63082885742188 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 4.2475, loss_val: nan, pos_over_neg: 156.4615478515625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 4.2053, loss_val: nan, pos_over_neg: 171.65000915527344 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 4.2359, loss_val: nan, pos_over_neg: 280.29345703125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 4.2015, loss_val: nan, pos_over_neg: 167.84934997558594 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 4.2591, loss_val: nan, pos_over_neg: 106.2336654663086 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 4.2237, loss_val: nan, pos_over_neg: 169.19778442382812 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 4.2266, loss_val: nan, pos_over_neg: 172.67889404296875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 4.2011, loss_val: nan, pos_over_neg: 118.99088287353516 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 4.2342, loss_val: nan, pos_over_neg: 98.09500885009766 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 4.2162, loss_val: nan, pos_over_neg: 180.8906707763672 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 4.204, loss_val: nan, pos_over_neg: 144.69432067871094 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 4.2246, loss_val: nan, pos_over_neg: 137.43775939941406 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 4.2219, loss_val: nan, pos_over_neg: 145.3663787841797 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 4.2386, loss_val: nan, pos_over_neg: 169.81787109375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 4.2238, loss_val: nan, pos_over_neg: 131.3186798095703 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 4.2243, loss_val: nan, pos_over_neg: 111.61493682861328 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 4.2521, loss_val: nan, pos_over_neg: 88.13189697265625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 4.2262, loss_val: nan, pos_over_neg: 146.21791076660156 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 4.2352, loss_val: nan, pos_over_neg: 131.8392333984375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 4.1959, loss_val: nan, pos_over_neg: 198.01043701171875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 4.2163, loss_val: nan, pos_over_neg: 254.20675659179688 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 4.1785, loss_val: nan, pos_over_neg: 166.1537322998047 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 4.2213, loss_val: nan, pos_over_neg: 141.3778533935547 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 4.2514, loss_val: nan, pos_over_neg: 137.60357666015625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 4.198, loss_val: nan, pos_over_neg: 99.13823699951172 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 4.2211, loss_val: nan, pos_over_neg: 130.0079803466797 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 4.1634, loss_val: nan, pos_over_neg: 182.76748657226562 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 4.2236, loss_val: nan, pos_over_neg: 126.2627182006836 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 4.2195, loss_val: nan, pos_over_neg: 132.7268829345703 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 4.2314, loss_val: nan, pos_over_neg: 151.19004821777344 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 4.2015, loss_val: nan, pos_over_neg: 189.33970642089844 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 4.1874, loss_val: nan, pos_over_neg: 134.81991577148438 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 4.1885, loss_val: nan, pos_over_neg: 131.02542114257812 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 4.2135, loss_val: nan, pos_over_neg: 121.95873260498047 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 4.1981, loss_val: nan, pos_over_neg: 138.7036590576172 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 4.2512, loss_val: nan, pos_over_neg: 124.08488464355469 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 4.2105, loss_val: nan, pos_over_neg: 118.28878021240234 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 4.2328, loss_val: nan, pos_over_neg: 142.07212829589844 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 4.179, loss_val: nan, pos_over_neg: 146.73826599121094 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 4.2283, loss_val: nan, pos_over_neg: 132.57797241210938 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 4.2247, loss_val: nan, pos_over_neg: 150.04415893554688 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 4.2191, loss_val: nan, pos_over_neg: 134.76202392578125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 4.2373, loss_val: nan, pos_over_neg: 127.42649841308594 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 4.2306, loss_val: nan, pos_over_neg: 124.49250793457031 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 4.1883, loss_val: nan, pos_over_neg: 206.635986328125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 4.1913, loss_val: nan, pos_over_neg: 182.1182861328125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 4.21, loss_val: nan, pos_over_neg: 186.56570434570312 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 4.2236, loss_val: nan, pos_over_neg: 167.86233520507812 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 4.1663, loss_val: nan, pos_over_neg: 174.4544677734375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 4.1965, loss_val: nan, pos_over_neg: 199.59654235839844 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 4.1874, loss_val: nan, pos_over_neg: 171.79678344726562 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 4.2222, loss_val: nan, pos_over_neg: 99.67432403564453 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 4.2259, loss_val: nan, pos_over_neg: 145.89837646484375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 4.2412, loss_val: nan, pos_over_neg: 111.56486511230469 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 4.2411, loss_val: nan, pos_over_neg: 139.81396484375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 4.1802, loss_val: nan, pos_over_neg: 211.9264678955078 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 4.196, loss_val: nan, pos_over_neg: 205.61080932617188 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 4.2006, loss_val: nan, pos_over_neg: 226.89984130859375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 4.2251, loss_val: nan, pos_over_neg: 192.9445037841797 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 4.2693, loss_val: nan, pos_over_neg: 123.685791015625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 4.1676, loss_val: nan, pos_over_neg: 160.332763671875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 4.2569, loss_val: nan, pos_over_neg: 132.09288024902344 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 4.1899, loss_val: nan, pos_over_neg: 149.4341583251953 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 4.1884, loss_val: nan, pos_over_neg: 141.88540649414062 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 4.1711, loss_val: nan, pos_over_neg: 153.1090087890625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 4.17, loss_val: nan, pos_over_neg: 208.69674682617188 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 4.1372, loss_val: nan, pos_over_neg: 256.9014892578125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 4.2027, loss_val: nan, pos_over_neg: 209.11709594726562 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 4.149, loss_val: nan, pos_over_neg: 146.1354522705078 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 4.2036, loss_val: nan, pos_over_neg: 178.2553253173828 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 4.1912, loss_val: nan, pos_over_neg: 153.7821807861328 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 4.1669, loss_val: nan, pos_over_neg: 104.22305297851562 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 4.2166, loss_val: nan, pos_over_neg: 165.79055786132812 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 4.1687, loss_val: nan, pos_over_neg: 245.15689086914062 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 4.1537, loss_val: nan, pos_over_neg: 132.80455017089844 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 4.2094, loss_val: nan, pos_over_neg: 152.22804260253906 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 4.14, loss_val: nan, pos_over_neg: 269.33837890625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 4.1212, loss_val: nan, pos_over_neg: 232.48866271972656 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 4.1364, loss_val: nan, pos_over_neg: 215.82736206054688 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 4.159, loss_val: nan, pos_over_neg: 162.6874237060547 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 4.1898, loss_val: nan, pos_over_neg: 124.58849334716797 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 4.2001, loss_val: nan, pos_over_neg: 157.0887908935547 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 4.1557, loss_val: nan, pos_over_neg: 198.20518493652344 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 4.1202, loss_val: nan, pos_over_neg: 257.6325988769531 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 4.1485, loss_val: nan, pos_over_neg: 239.93177795410156 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 4.1637, loss_val: nan, pos_over_neg: 203.8503875732422 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 4.1201, loss_val: nan, pos_over_neg: 128.4939727783203 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 4.1512, loss_val: nan, pos_over_neg: 155.16505432128906 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 4.1346, loss_val: nan, pos_over_neg: 182.4730682373047 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 4.2032, loss_val: nan, pos_over_neg: 137.57809448242188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 4.147, loss_val: nan, pos_over_neg: 170.42681884765625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 4.1733, loss_val: nan, pos_over_neg: 162.7513885498047 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 4.1686, loss_val: nan, pos_over_neg: 153.4692840576172 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 4.1718, loss_val: nan, pos_over_neg: 199.71778869628906 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 4.1282, loss_val: nan, pos_over_neg: 210.93482971191406 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 4.1499, loss_val: nan, pos_over_neg: 225.33680725097656 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 4.1984, loss_val: nan, pos_over_neg: 147.93495178222656 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 4.1489, loss_val: nan, pos_over_neg: 124.123779296875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 4.1502, loss_val: nan, pos_over_neg: 157.49777221679688 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 4.164, loss_val: nan, pos_over_neg: 144.5494384765625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 4.1633, loss_val: nan, pos_over_neg: 154.62623596191406 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 4.1235, loss_val: nan, pos_over_neg: 232.67079162597656 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 4.1444, loss_val: nan, pos_over_neg: 181.67893981933594 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 4.1497, loss_val: nan, pos_over_neg: 171.28915405273438 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 4.1266, loss_val: nan, pos_over_neg: 229.59864807128906 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 4.1689, loss_val: nan, pos_over_neg: 163.85971069335938 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 4.1436, loss_val: nan, pos_over_neg: 227.02626037597656 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 4.1215, loss_val: nan, pos_over_neg: 158.93313598632812 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 4.1661, loss_val: nan, pos_over_neg: 203.72763061523438 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 4.1553, loss_val: nan, pos_over_neg: 151.8385772705078 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 4.1988, loss_val: nan, pos_over_neg: 160.5801239013672 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 4.2133, loss_val: nan, pos_over_neg: 189.52130126953125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 4.1541, loss_val: nan, pos_over_neg: 200.8719482421875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 4.152, loss_val: nan, pos_over_neg: 216.53717041015625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 4.1605, loss_val: nan, pos_over_neg: 146.94808959960938 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 4.1787, loss_val: nan, pos_over_neg: 140.2659149169922 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 4.1759, loss_val: nan, pos_over_neg: 107.84630584716797 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 4.1242, loss_val: nan, pos_over_neg: 129.60585021972656 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 4.0905, loss_val: nan, pos_over_neg: 306.3993225097656 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 4.1284, loss_val: nan, pos_over_neg: 270.27691650390625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 4.1751, loss_val: nan, pos_over_neg: 184.11370849609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 4.1649, loss_val: nan, pos_over_neg: 208.76820373535156 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 4.171, loss_val: nan, pos_over_neg: 217.06434631347656 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 4.1706, loss_val: nan, pos_over_neg: 149.4275360107422 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 4.1312, loss_val: nan, pos_over_neg: 140.29086303710938 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 4.1619, loss_val: nan, pos_over_neg: 133.0029296875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 4.1477, loss_val: nan, pos_over_neg: 132.70762634277344 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 4.0758, loss_val: nan, pos_over_neg: 182.5474395751953 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 4.11, loss_val: nan, pos_over_neg: 199.0355987548828 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 4.1689, loss_val: nan, pos_over_neg: 148.81094360351562 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 4.1148, loss_val: nan, pos_over_neg: 194.5315399169922 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 4.1761, loss_val: nan, pos_over_neg: 172.0135040283203 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 4.186, loss_val: nan, pos_over_neg: 149.52845764160156 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 4.142, loss_val: nan, pos_over_neg: 104.56089782714844 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 4.0793, loss_val: nan, pos_over_neg: 169.89466857910156 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 4.1082, loss_val: nan, pos_over_neg: 157.68482971191406 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 4.1415, loss_val: nan, pos_over_neg: 117.89432525634766 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 4.1481, loss_val: nan, pos_over_neg: 119.28107452392578 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 4.1356, loss_val: nan, pos_over_neg: 237.5593719482422 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 4.1136, loss_val: nan, pos_over_neg: 165.4720001220703 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 4.1432, loss_val: nan, pos_over_neg: 129.6504669189453 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 4.1854, loss_val: nan, pos_over_neg: 131.64964294433594 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 4.1381, loss_val: nan, pos_over_neg: 125.59272766113281 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 4.1433, loss_val: nan, pos_over_neg: 145.04002380371094 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 4.1299, loss_val: nan, pos_over_neg: 161.08787536621094 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 4.1205, loss_val: nan, pos_over_neg: 159.60165405273438 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 4.1394, loss_val: nan, pos_over_neg: 228.31626892089844 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 4.159, loss_val: nan, pos_over_neg: 164.66021728515625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 4.0802, loss_val: nan, pos_over_neg: 185.143798828125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 4.1217, loss_val: nan, pos_over_neg: 174.10934448242188 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 4.1656, loss_val: nan, pos_over_neg: 146.2204132080078 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 4.0967, loss_val: nan, pos_over_neg: 211.04466247558594 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 4.1111, loss_val: nan, pos_over_neg: 232.84561157226562 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 4.1393, loss_val: nan, pos_over_neg: 199.9132843017578 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 4.1179, loss_val: nan, pos_over_neg: 159.25491333007812 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 4.103, loss_val: nan, pos_over_neg: 183.8504180908203 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 4.1407, loss_val: nan, pos_over_neg: 147.82923889160156 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 4.0958, loss_val: nan, pos_over_neg: 158.70184326171875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 4.1369, loss_val: nan, pos_over_neg: 140.99795532226562 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 4.1137, loss_val: nan, pos_over_neg: 130.5084991455078 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 4.1108, loss_val: nan, pos_over_neg: 297.9116516113281 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 4.1235, loss_val: nan, pos_over_neg: 198.70660400390625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 4.1451, loss_val: nan, pos_over_neg: 225.69955444335938 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 4.1239, loss_val: nan, pos_over_neg: 265.8674621582031 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 4.1343, loss_val: nan, pos_over_neg: 176.1457061767578 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 4.119, loss_val: nan, pos_over_neg: 125.1623764038086 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 4.0812, loss_val: nan, pos_over_neg: 210.93927001953125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 4.1408, loss_val: nan, pos_over_neg: 137.33729553222656 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 4.0947, loss_val: nan, pos_over_neg: 258.9129333496094 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 4.1299, loss_val: nan, pos_over_neg: 296.0765075683594 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 4.0956, loss_val: nan, pos_over_neg: 293.271728515625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 4.1005, loss_val: nan, pos_over_neg: 232.46560668945312 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 4.1506, loss_val: nan, pos_over_neg: 188.412353515625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 4.1441, loss_val: nan, pos_over_neg: 183.75729370117188 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 4.1048, loss_val: nan, pos_over_neg: 156.7905731201172 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 4.1068, loss_val: nan, pos_over_neg: 176.37481689453125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 4.1444, loss_val: nan, pos_over_neg: 212.04110717773438 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 4.089, loss_val: nan, pos_over_neg: 317.9455871582031 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 4.0932, loss_val: nan, pos_over_neg: 175.16741943359375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 4.1024, loss_val: nan, pos_over_neg: 246.1646728515625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 4.0807, loss_val: nan, pos_over_neg: 198.6979217529297 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 4.1252, loss_val: nan, pos_over_neg: 114.02122497558594 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 4.0573, loss_val: nan, pos_over_neg: 146.83212280273438 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 4.1438, loss_val: nan, pos_over_neg: 144.76101684570312 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 4.087, loss_val: nan, pos_over_neg: 200.0033721923828 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 4.0922, loss_val: nan, pos_over_neg: 243.249755859375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 4.0931, loss_val: nan, pos_over_neg: 217.0397491455078 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 4.0696, loss_val: nan, pos_over_neg: 265.30859375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 4.0923, loss_val: nan, pos_over_neg: 256.4494934082031 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 4.0704, loss_val: nan, pos_over_neg: 260.3043212890625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 4.1126, loss_val: nan, pos_over_neg: 228.56866455078125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 4.0659, loss_val: nan, pos_over_neg: 145.63427734375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 4.0703, loss_val: nan, pos_over_neg: 175.91094970703125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 4.1046, loss_val: nan, pos_over_neg: 184.14788818359375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 4.0402, loss_val: nan, pos_over_neg: 223.10638427734375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 4.1019, loss_val: nan, pos_over_neg: 203.82083129882812 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 4.0924, loss_val: nan, pos_over_neg: 145.38597106933594 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 4.1303, loss_val: nan, pos_over_neg: 184.97886657714844 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 4.095, loss_val: nan, pos_over_neg: 185.37989807128906 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 4.1325, loss_val: nan, pos_over_neg: 133.263916015625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 4.1286, loss_val: nan, pos_over_neg: 120.35292053222656 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 4.0382, loss_val: nan, pos_over_neg: 201.1746368408203 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 4.0527, loss_val: nan, pos_over_neg: 209.18182373046875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 4.0983, loss_val: nan, pos_over_neg: 128.9643096923828 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 4.0769, loss_val: nan, pos_over_neg: 160.86465454101562 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 4.0962, loss_val: nan, pos_over_neg: 154.975830078125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 4.1404, loss_val: nan, pos_over_neg: 158.86920166015625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 4.0888, loss_val: nan, pos_over_neg: 168.5359649658203 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 4.0457, loss_val: nan, pos_over_neg: 172.48883056640625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 4.1684, loss_val: nan, pos_over_neg: 109.71697998046875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 4.0918, loss_val: nan, pos_over_neg: 159.4154510498047 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 4.0595, loss_val: nan, pos_over_neg: 224.1882781982422 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 4.1237, loss_val: nan, pos_over_neg: 152.2755889892578 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 4.0597, loss_val: nan, pos_over_neg: 214.53562927246094 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 4.0749, loss_val: nan, pos_over_neg: 164.51431274414062 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 4.0883, loss_val: nan, pos_over_neg: 120.67511749267578 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 4.1509, loss_val: nan, pos_over_neg: 129.81967163085938 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 4.0767, loss_val: nan, pos_over_neg: 149.76234436035156 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 4.1095, loss_val: nan, pos_over_neg: 138.29574584960938 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 4.1292, loss_val: nan, pos_over_neg: 162.4740447998047 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 4.1006, loss_val: nan, pos_over_neg: 194.01698303222656 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 4.1102, loss_val: nan, pos_over_neg: 148.36221313476562 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 4.0811, loss_val: nan, pos_over_neg: 145.66148376464844 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 4.0852, loss_val: nan, pos_over_neg: 176.68096923828125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 4.1202, loss_val: nan, pos_over_neg: 151.6559600830078 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 4.0736, loss_val: nan, pos_over_neg: 145.9170684814453 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 4.052, loss_val: nan, pos_over_neg: 170.04934692382812 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 4.0748, loss_val: nan, pos_over_neg: 171.4230499267578 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 4.0163, loss_val: nan, pos_over_neg: 249.318603515625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 4.0521, loss_val: nan, pos_over_neg: 219.52188110351562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 4.1091, loss_val: nan, pos_over_neg: 152.3200225830078 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 4.0888, loss_val: nan, pos_over_neg: 187.5725860595703 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 4.0869, loss_val: nan, pos_over_neg: 198.84539794921875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 4.0335, loss_val: nan, pos_over_neg: 191.45228576660156 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 4.0952, loss_val: nan, pos_over_neg: 167.4955291748047 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 4.0819, loss_val: nan, pos_over_neg: 144.7252960205078 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 4.0757, loss_val: nan, pos_over_neg: 249.1234130859375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 4.0705, loss_val: nan, pos_over_neg: 181.17808532714844 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 4.0228, loss_val: nan, pos_over_neg: 291.9968566894531 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 4.0859, loss_val: nan, pos_over_neg: 389.66650390625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 4.0477, loss_val: nan, pos_over_neg: 212.17674255371094 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 4.0746, loss_val: nan, pos_over_neg: 155.94290161132812 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 4.06, loss_val: nan, pos_over_neg: 177.74606323242188 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 4.0654, loss_val: nan, pos_over_neg: 129.85385131835938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 4.0696, loss_val: nan, pos_over_neg: 192.67648315429688 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 4.1078, loss_val: nan, pos_over_neg: 182.8885040283203 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 4.0551, loss_val: nan, pos_over_neg: 294.04571533203125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 4.0859, loss_val: nan, pos_over_neg: 296.1553649902344 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 4.0996, loss_val: nan, pos_over_neg: 181.0212860107422 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 4.0603, loss_val: nan, pos_over_neg: 226.81190490722656 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 4.0729, loss_val: nan, pos_over_neg: 242.1851348876953 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 4.0595, loss_val: nan, pos_over_neg: 166.27304077148438 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 4.083, loss_val: nan, pos_over_neg: 209.15370178222656 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 4.0736, loss_val: nan, pos_over_neg: 216.58628845214844 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 4.0498, loss_val: nan, pos_over_neg: 221.27227783203125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 4.0701, loss_val: nan, pos_over_neg: 178.4580535888672 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 4.0544, loss_val: nan, pos_over_neg: 156.70843505859375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 4.0325, loss_val: nan, pos_over_neg: 181.55972290039062 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 4.0462, loss_val: nan, pos_over_neg: 181.22386169433594 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 4.0874, loss_val: nan, pos_over_neg: 166.486572265625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 4.0996, loss_val: nan, pos_over_neg: 164.21543884277344 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 4.0253, loss_val: nan, pos_over_neg: 345.8648681640625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 4.1122, loss_val: nan, pos_over_neg: 204.5574188232422 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 4.0881, loss_val: nan, pos_over_neg: 168.3410186767578 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 4.0632, loss_val: nan, pos_over_neg: 152.87017822265625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 4.062, loss_val: nan, pos_over_neg: 111.50467681884766 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 4.1295, loss_val: nan, pos_over_neg: 126.52845001220703 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 4.0941, loss_val: nan, pos_over_neg: 183.92384338378906 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 4.0537, loss_val: nan, pos_over_neg: 233.4520263671875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 4.0658, loss_val: nan, pos_over_neg: 273.3432922363281 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 4.1032, loss_val: nan, pos_over_neg: 174.29901123046875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 4.0657, loss_val: nan, pos_over_neg: 206.9175262451172 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 4.1049, loss_val: nan, pos_over_neg: 164.87835693359375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 4.0677, loss_val: nan, pos_over_neg: 158.2624969482422 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 4.0288, loss_val: nan, pos_over_neg: 127.85740661621094 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 4.1045, loss_val: nan, pos_over_neg: 119.19974517822266 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 4.0508, loss_val: nan, pos_over_neg: 138.51829528808594 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 4.0692, loss_val: nan, pos_over_neg: 210.15980529785156 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 4.0319, loss_val: nan, pos_over_neg: 339.45068359375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 4.0538, loss_val: nan, pos_over_neg: 202.1729736328125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 4.0427, loss_val: nan, pos_over_neg: 202.18844604492188 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 4.0521, loss_val: nan, pos_over_neg: 166.9287567138672 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 4.0644, loss_val: nan, pos_over_neg: 147.16932678222656 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 4.0865, loss_val: nan, pos_over_neg: 183.35182189941406 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 4.0808, loss_val: nan, pos_over_neg: 154.8529815673828 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 4.0141, loss_val: nan, pos_over_neg: 179.0621337890625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 4.0968, loss_val: nan, pos_over_neg: 148.37249755859375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 4.1109, loss_val: nan, pos_over_neg: 135.5157928466797 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 4.0619, loss_val: nan, pos_over_neg: 180.3437042236328 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 4.0243, loss_val: nan, pos_over_neg: 267.9679870605469 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 4.0428, loss_val: nan, pos_over_neg: 267.5064697265625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 4.0803, loss_val: nan, pos_over_neg: 175.869873046875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 4.0677, loss_val: nan, pos_over_neg: 156.0225067138672 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [15:28<77389:09:33, 928.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.9996, loss_val: nan, pos_over_neg: 221.7789764404297 lr: 0.00031623\n",
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 4.0963, loss_val: nan, pos_over_neg: 112.41771697998047 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 4.0751, loss_val: nan, pos_over_neg: 138.1513671875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 4.0033, loss_val: nan, pos_over_neg: 195.693603515625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 4.0682, loss_val: nan, pos_over_neg: 197.64073181152344 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 4.051, loss_val: nan, pos_over_neg: 216.97677612304688 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 4.079, loss_val: nan, pos_over_neg: 178.913330078125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 4.0528, loss_val: nan, pos_over_neg: 261.31243896484375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 4.0514, loss_val: nan, pos_over_neg: 199.71282958984375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 4.1074, loss_val: nan, pos_over_neg: 208.39236450195312 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.9882, loss_val: nan, pos_over_neg: 309.50616455078125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 4.0446, loss_val: nan, pos_over_neg: 230.28387451171875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 4.1204, loss_val: nan, pos_over_neg: 140.3318634033203 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 4.067, loss_val: nan, pos_over_neg: 199.54183959960938 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 4.0463, loss_val: nan, pos_over_neg: 153.79132080078125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 4.043, loss_val: nan, pos_over_neg: 128.05308532714844 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 4.0504, loss_val: nan, pos_over_neg: 174.55218505859375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 4.042, loss_val: nan, pos_over_neg: 252.39112854003906 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 4.0581, loss_val: nan, pos_over_neg: 198.71987915039062 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 4.0448, loss_val: nan, pos_over_neg: 208.5980224609375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 4.0134, loss_val: nan, pos_over_neg: 252.85665893554688 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 4.0507, loss_val: nan, pos_over_neg: 278.89031982421875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 4.0385, loss_val: nan, pos_over_neg: 206.884521484375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 4.0365, loss_val: nan, pos_over_neg: 147.33680725097656 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 4.0202, loss_val: nan, pos_over_neg: 176.80508422851562 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 4.0668, loss_val: nan, pos_over_neg: 180.1009063720703 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 4.0638, loss_val: nan, pos_over_neg: 175.14195251464844 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 4.0559, loss_val: nan, pos_over_neg: 189.75909423828125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 4.0459, loss_val: nan, pos_over_neg: 261.65753173828125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 4.056, loss_val: nan, pos_over_neg: 373.2056579589844 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 4.0678, loss_val: nan, pos_over_neg: 192.68650817871094 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 4.0211, loss_val: nan, pos_over_neg: 180.15028381347656 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 4.0243, loss_val: nan, pos_over_neg: 193.0492706298828 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 4.0659, loss_val: nan, pos_over_neg: 199.8202362060547 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 4.0194, loss_val: nan, pos_over_neg: 163.15660095214844 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 4.038, loss_val: nan, pos_over_neg: 181.6900634765625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 4.0808, loss_val: nan, pos_over_neg: 176.4973602294922 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 4.0271, loss_val: nan, pos_over_neg: 189.44015502929688 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 4.0257, loss_val: nan, pos_over_neg: 184.72439575195312 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 4.0371, loss_val: nan, pos_over_neg: 133.76365661621094 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 4.0822, loss_val: nan, pos_over_neg: 177.38912963867188 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 4.0503, loss_val: nan, pos_over_neg: 225.45550537109375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 4.0278, loss_val: nan, pos_over_neg: 240.87643432617188 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 4.0447, loss_val: nan, pos_over_neg: 200.41259765625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.9936, loss_val: nan, pos_over_neg: 183.0924835205078 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 4.0249, loss_val: nan, pos_over_neg: 166.57252502441406 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 4.0246, loss_val: nan, pos_over_neg: 222.59698486328125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 4.0288, loss_val: nan, pos_over_neg: 191.15115356445312 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 4.0448, loss_val: nan, pos_over_neg: 164.43917846679688 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 4.0779, loss_val: nan, pos_over_neg: 119.41305541992188 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 4.0345, loss_val: nan, pos_over_neg: 175.26458740234375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 4.0555, loss_val: nan, pos_over_neg: 193.99224853515625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 4.0056, loss_val: nan, pos_over_neg: 203.68263244628906 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 4.0431, loss_val: nan, pos_over_neg: 208.10678100585938 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 4.0117, loss_val: nan, pos_over_neg: 271.3167419433594 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 4.0091, loss_val: nan, pos_over_neg: 230.58074951171875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.9944, loss_val: nan, pos_over_neg: 208.7578125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.9843, loss_val: nan, pos_over_neg: 234.328857421875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 4.0995, loss_val: nan, pos_over_neg: 114.60820007324219 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.9588, loss_val: nan, pos_over_neg: 175.39097595214844 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 4.0542, loss_val: nan, pos_over_neg: 183.17710876464844 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 4.0762, loss_val: nan, pos_over_neg: 119.39871215820312 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 4.039, loss_val: nan, pos_over_neg: 161.2099609375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 4.0705, loss_val: nan, pos_over_neg: 188.7696990966797 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 4.0603, loss_val: nan, pos_over_neg: 261.2398376464844 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 4.0482, loss_val: nan, pos_over_neg: 172.6772003173828 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 4.0136, loss_val: nan, pos_over_neg: 203.7626495361328 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.975, loss_val: nan, pos_over_neg: 140.3064727783203 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 4.012, loss_val: nan, pos_over_neg: 192.4451446533203 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 4.0586, loss_val: nan, pos_over_neg: 121.14617919921875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 4.0643, loss_val: nan, pos_over_neg: 132.84677124023438 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 4.0204, loss_val: nan, pos_over_neg: 163.4199981689453 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 4.0202, loss_val: nan, pos_over_neg: 217.98721313476562 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 4.0178, loss_val: nan, pos_over_neg: 307.7709655761719 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 4.0737, loss_val: nan, pos_over_neg: 161.533447265625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 4.0584, loss_val: nan, pos_over_neg: 165.5275115966797 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 4.0394, loss_val: nan, pos_over_neg: 185.46426391601562 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 4.0565, loss_val: nan, pos_over_neg: 267.5020446777344 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 4.0011, loss_val: nan, pos_over_neg: 171.47271728515625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 4.064, loss_val: nan, pos_over_neg: 169.937255859375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 4.0529, loss_val: nan, pos_over_neg: 199.77317810058594 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.9582, loss_val: nan, pos_over_neg: 170.49586486816406 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 4.1006, loss_val: nan, pos_over_neg: 133.96961975097656 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 4.0187, loss_val: nan, pos_over_neg: 160.84664916992188 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 4.0053, loss_val: nan, pos_over_neg: 252.11875915527344 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 4.0343, loss_val: nan, pos_over_neg: 170.6884307861328 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.999, loss_val: nan, pos_over_neg: 242.40420532226562 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 4.0476, loss_val: nan, pos_over_neg: 214.46438598632812 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.9781, loss_val: nan, pos_over_neg: 241.25970458984375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 4.0018, loss_val: nan, pos_over_neg: 186.7926483154297 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 4.0123, loss_val: nan, pos_over_neg: 148.65916442871094 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.9878, loss_val: nan, pos_over_neg: 221.2959442138672 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.9755, loss_val: nan, pos_over_neg: 253.55686950683594 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 4.0159, loss_val: nan, pos_over_neg: 265.4414978027344 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 4.065, loss_val: nan, pos_over_neg: 187.30519104003906 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.9938, loss_val: nan, pos_over_neg: 246.34922790527344 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 4.0064, loss_val: nan, pos_over_neg: 224.84503173828125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.977, loss_val: nan, pos_over_neg: 294.35546875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.981, loss_val: nan, pos_over_neg: 238.90924072265625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 4.0707, loss_val: nan, pos_over_neg: 181.87681579589844 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 4.0348, loss_val: nan, pos_over_neg: 256.24169921875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.9645, loss_val: nan, pos_over_neg: 269.6471252441406 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 4.0265, loss_val: nan, pos_over_neg: 260.7807922363281 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.9776, loss_val: nan, pos_over_neg: 280.5641174316406 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 4.0255, loss_val: nan, pos_over_neg: 238.8651580810547 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 4.0766, loss_val: nan, pos_over_neg: 161.20980834960938 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.981, loss_val: nan, pos_over_neg: 210.65045166015625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 4.0136, loss_val: nan, pos_over_neg: 290.2890930175781 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 4.0643, loss_val: nan, pos_over_neg: 184.8959503173828 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 4.0228, loss_val: nan, pos_over_neg: 160.48748779296875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.9675, loss_val: nan, pos_over_neg: 192.26608276367188 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 4.0006, loss_val: nan, pos_over_neg: 189.7114715576172 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.9567, loss_val: nan, pos_over_neg: 267.4639587402344 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 4.0142, loss_val: nan, pos_over_neg: 166.17092895507812 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.9811, loss_val: nan, pos_over_neg: 178.5142822265625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 4.0404, loss_val: nan, pos_over_neg: 124.22213745117188 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.9689, loss_val: nan, pos_over_neg: 229.5302276611328 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.9956, loss_val: nan, pos_over_neg: 169.38388061523438 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 4.0596, loss_val: nan, pos_over_neg: 114.46159362792969 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 4.0405, loss_val: nan, pos_over_neg: 127.30522155761719 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 4.039, loss_val: nan, pos_over_neg: 130.6105499267578 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 4.0417, loss_val: nan, pos_over_neg: 125.56425476074219 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 4.0444, loss_val: nan, pos_over_neg: 162.94944763183594 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 4.0191, loss_val: nan, pos_over_neg: 189.1344451904297 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.9613, loss_val: nan, pos_over_neg: 196.9045867919922 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 4.0146, loss_val: nan, pos_over_neg: 208.56735229492188 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 4.0312, loss_val: nan, pos_over_neg: 238.29310607910156 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 4.0068, loss_val: nan, pos_over_neg: 222.5938262939453 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 4.0955, loss_val: nan, pos_over_neg: 144.24925231933594 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 4.0517, loss_val: nan, pos_over_neg: 188.66920471191406 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 4.0727, loss_val: nan, pos_over_neg: 125.97460174560547 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.9994, loss_val: nan, pos_over_neg: 211.35498046875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 4.0355, loss_val: nan, pos_over_neg: 261.88677978515625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 192.25991821289062 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 4.0098, loss_val: nan, pos_over_neg: 180.90505981445312 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.9512, loss_val: nan, pos_over_neg: 225.91030883789062 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 4.0483, loss_val: nan, pos_over_neg: 196.65798950195312 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.9816, loss_val: nan, pos_over_neg: 202.70416259765625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 4.0468, loss_val: nan, pos_over_neg: 169.7174072265625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 4.0377, loss_val: nan, pos_over_neg: 234.4770050048828 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 4.0141, loss_val: nan, pos_over_neg: 189.86520385742188 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.9898, loss_val: nan, pos_over_neg: 217.6759490966797 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.9878, loss_val: nan, pos_over_neg: 235.61451721191406 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.9865, loss_val: nan, pos_over_neg: 185.51046752929688 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 210.3914794921875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 4.0024, loss_val: nan, pos_over_neg: 229.41368103027344 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 4.0077, loss_val: nan, pos_over_neg: 176.83917236328125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 4.0373, loss_val: nan, pos_over_neg: 242.25209045410156 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 4.0182, loss_val: nan, pos_over_neg: 229.31692504882812 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 4.0162, loss_val: nan, pos_over_neg: 317.2164306640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 4.0039, loss_val: nan, pos_over_neg: 190.52122497558594 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 4.0113, loss_val: nan, pos_over_neg: 237.70997619628906 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 4.046, loss_val: nan, pos_over_neg: 129.36329650878906 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.9962, loss_val: nan, pos_over_neg: 168.93177795410156 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 4.0155, loss_val: nan, pos_over_neg: 148.0856170654297 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.962, loss_val: nan, pos_over_neg: 423.7794494628906 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 4.0257, loss_val: nan, pos_over_neg: 293.8595275878906 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.9505, loss_val: nan, pos_over_neg: 588.5679321289062 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 4.003, loss_val: nan, pos_over_neg: 364.4644775390625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 4.0153, loss_val: nan, pos_over_neg: 192.9986572265625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.9898, loss_val: nan, pos_over_neg: 150.1815948486328 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.9905, loss_val: nan, pos_over_neg: 203.9814453125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 4.038, loss_val: nan, pos_over_neg: 133.10629272460938 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 4.0099, loss_val: nan, pos_over_neg: 159.5006561279297 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.9714, loss_val: nan, pos_over_neg: 218.79849243164062 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 4.0749, loss_val: nan, pos_over_neg: 250.74501037597656 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 4.0292, loss_val: nan, pos_over_neg: 184.6592254638672 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 4.0072, loss_val: nan, pos_over_neg: 232.7879180908203 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.9936, loss_val: nan, pos_over_neg: 319.8846435546875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.985, loss_val: nan, pos_over_neg: 302.5916442871094 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.975, loss_val: nan, pos_over_neg: 161.9424285888672 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.9825, loss_val: nan, pos_over_neg: 257.3614196777344 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.9563, loss_val: nan, pos_over_neg: 287.06591796875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 4.0095, loss_val: nan, pos_over_neg: 147.1535186767578 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 4.0224, loss_val: nan, pos_over_neg: 141.18820190429688 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 4.0485, loss_val: nan, pos_over_neg: 156.7227783203125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.966, loss_val: nan, pos_over_neg: 228.31883239746094 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 4.0094, loss_val: nan, pos_over_neg: 180.6307830810547 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.9605, loss_val: nan, pos_over_neg: 200.87619018554688 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.9966, loss_val: nan, pos_over_neg: 237.48681640625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 4.0268, loss_val: nan, pos_over_neg: 253.6780242919922 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.9899, loss_val: nan, pos_over_neg: 252.90643310546875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.9638, loss_val: nan, pos_over_neg: 274.5105285644531 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.9823, loss_val: nan, pos_over_neg: 368.3680114746094 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 4.0437, loss_val: nan, pos_over_neg: 189.60174560546875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 4.0077, loss_val: nan, pos_over_neg: 256.1462707519531 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 4.008, loss_val: nan, pos_over_neg: 213.4305419921875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.9701, loss_val: nan, pos_over_neg: 123.47411346435547 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 4.0132, loss_val: nan, pos_over_neg: 109.6984634399414 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.9971, loss_val: nan, pos_over_neg: 266.47113037109375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 4.0161, loss_val: nan, pos_over_neg: 271.9169006347656 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.98, loss_val: nan, pos_over_neg: 205.30039978027344 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 4.0794, loss_val: nan, pos_over_neg: 184.53848266601562 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 4.0434, loss_val: nan, pos_over_neg: 263.0166931152344 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.9835, loss_val: nan, pos_over_neg: 297.3238220214844 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 4.005, loss_val: nan, pos_over_neg: 162.89865112304688 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 4.0433, loss_val: nan, pos_over_neg: 136.49278259277344 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.9789, loss_val: nan, pos_over_neg: 167.988037109375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 4.0385, loss_val: nan, pos_over_neg: 143.16778564453125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 4.043, loss_val: nan, pos_over_neg: 173.52780151367188 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.9465, loss_val: nan, pos_over_neg: 183.40640258789062 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 4.0005, loss_val: nan, pos_over_neg: 167.52870178222656 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.9877, loss_val: nan, pos_over_neg: 200.79351806640625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.9558, loss_val: nan, pos_over_neg: 224.76150512695312 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.9932, loss_val: nan, pos_over_neg: 228.6406707763672 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 4.0016, loss_val: nan, pos_over_neg: 167.92047119140625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 4.0075, loss_val: nan, pos_over_neg: 220.87564086914062 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 4.0307, loss_val: nan, pos_over_neg: 142.03126525878906 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.9906, loss_val: nan, pos_over_neg: 121.41543579101562 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.9633, loss_val: nan, pos_over_neg: 210.24375915527344 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.9778, loss_val: nan, pos_over_neg: 168.8290252685547 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 4.0322, loss_val: nan, pos_over_neg: 145.18429565429688 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 4.004, loss_val: nan, pos_over_neg: 249.0492706298828 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 4.0056, loss_val: nan, pos_over_neg: 224.74000549316406 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.9745, loss_val: nan, pos_over_neg: 220.72462463378906 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.9896, loss_val: nan, pos_over_neg: 195.78941345214844 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.9951, loss_val: nan, pos_over_neg: 215.09161376953125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.9804, loss_val: nan, pos_over_neg: 134.40560913085938 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 4.0049, loss_val: nan, pos_over_neg: 140.05905151367188 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.9532, loss_val: nan, pos_over_neg: 211.053955078125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 4.026, loss_val: nan, pos_over_neg: 171.02825927734375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.9998, loss_val: nan, pos_over_neg: 153.5211181640625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.975, loss_val: nan, pos_over_neg: 347.39691162109375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 321.3844299316406 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.994, loss_val: nan, pos_over_neg: 174.0184783935547 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.9947, loss_val: nan, pos_over_neg: 166.6689910888672 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.9994, loss_val: nan, pos_over_neg: 218.9142303466797 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.9834, loss_val: nan, pos_over_neg: 272.128173828125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 4.0193, loss_val: nan, pos_over_neg: 175.2191162109375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.9827, loss_val: nan, pos_over_neg: 282.2998962402344 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.9979, loss_val: nan, pos_over_neg: 227.6578369140625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 4.0233, loss_val: nan, pos_over_neg: 175.36988830566406 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.9618, loss_val: nan, pos_over_neg: 242.95436096191406 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.9706, loss_val: nan, pos_over_neg: 204.9944610595703 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 4.0033, loss_val: nan, pos_over_neg: 209.63172912597656 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.9838, loss_val: nan, pos_over_neg: 207.65382385253906 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.9355, loss_val: nan, pos_over_neg: 243.03692626953125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.9247, loss_val: nan, pos_over_neg: 338.11474609375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 4.0082, loss_val: nan, pos_over_neg: 231.89205932617188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.934, loss_val: nan, pos_over_neg: 262.4878234863281 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.9615, loss_val: nan, pos_over_neg: 233.302001953125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.942, loss_val: nan, pos_over_neg: 320.3873596191406 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.968, loss_val: nan, pos_over_neg: 170.65370178222656 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.9979, loss_val: nan, pos_over_neg: 192.84237670898438 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.9539, loss_val: nan, pos_over_neg: 244.11734008789062 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.9676, loss_val: nan, pos_over_neg: 214.4748992919922 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.953, loss_val: nan, pos_over_neg: 223.97479248046875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 4.0027, loss_val: nan, pos_over_neg: 264.52862548828125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.9357, loss_val: nan, pos_over_neg: 256.27386474609375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.9637, loss_val: nan, pos_over_neg: 224.43910217285156 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 4.014, loss_val: nan, pos_over_neg: 234.41441345214844 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.9284, loss_val: nan, pos_over_neg: 263.39801025390625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.982, loss_val: nan, pos_over_neg: 179.10601806640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.9353, loss_val: nan, pos_over_neg: 283.9739990234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.9714, loss_val: nan, pos_over_neg: 233.9114990234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.9415, loss_val: nan, pos_over_neg: 265.80548095703125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.9613, loss_val: nan, pos_over_neg: 291.8268737792969 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 4.0263, loss_val: nan, pos_over_neg: 210.47691345214844 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.9635, loss_val: nan, pos_over_neg: 204.33399963378906 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.9697, loss_val: nan, pos_over_neg: 223.47544860839844 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 4.0037, loss_val: nan, pos_over_neg: 155.40541076660156 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.9592, loss_val: nan, pos_over_neg: 209.62704467773438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.9885, loss_val: nan, pos_over_neg: 215.18624877929688 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.9899, loss_val: nan, pos_over_neg: 207.8566131591797 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.9832, loss_val: nan, pos_over_neg: 282.18988037109375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.9614, loss_val: nan, pos_over_neg: 212.20755004882812 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.9614, loss_val: nan, pos_over_neg: 194.88253784179688 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.9687, loss_val: nan, pos_over_neg: 191.62550354003906 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.9844, loss_val: nan, pos_over_neg: 168.4776153564453 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 4.0081, loss_val: nan, pos_over_neg: 140.70944213867188 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.9668, loss_val: nan, pos_over_neg: 115.16858673095703 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.9584, loss_val: nan, pos_over_neg: 253.80763244628906 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.9468, loss_val: nan, pos_over_neg: 212.9427490234375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 4.0097, loss_val: nan, pos_over_neg: 178.9651336669922 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.952, loss_val: nan, pos_over_neg: 223.28982543945312 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.949, loss_val: nan, pos_over_neg: 317.64154052734375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.9328, loss_val: nan, pos_over_neg: 207.7347869873047 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.9688, loss_val: nan, pos_over_neg: 233.83441162109375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.9412, loss_val: nan, pos_over_neg: 351.97216796875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.9571, loss_val: nan, pos_over_neg: 167.70401000976562 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.9319, loss_val: nan, pos_over_neg: 247.95523071289062 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.9427, loss_val: nan, pos_over_neg: 270.14581298828125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.9743, loss_val: nan, pos_over_neg: 239.9726104736328 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 4.0062, loss_val: nan, pos_over_neg: 172.127685546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.9897, loss_val: nan, pos_over_neg: 205.3607940673828 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.9552, loss_val: nan, pos_over_neg: 266.9256286621094 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 246.70462036132812 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.9948, loss_val: nan, pos_over_neg: 322.64227294921875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.953, loss_val: nan, pos_over_neg: 213.44216918945312 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.9798, loss_val: nan, pos_over_neg: 275.4090270996094 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.9228, loss_val: nan, pos_over_neg: 261.0829772949219 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.9834, loss_val: nan, pos_over_neg: 147.26126098632812 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.9696, loss_val: nan, pos_over_neg: 160.30137634277344 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.9843, loss_val: nan, pos_over_neg: 158.24388122558594 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.9649, loss_val: nan, pos_over_neg: 215.5576629638672 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.9692, loss_val: nan, pos_over_neg: 338.1499938964844 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.9774, loss_val: nan, pos_over_neg: 258.8385925292969 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.9547, loss_val: nan, pos_over_neg: 256.5045471191406 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.9709, loss_val: nan, pos_over_neg: 217.18243408203125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.9266, loss_val: nan, pos_over_neg: 363.7442321777344 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.9907, loss_val: nan, pos_over_neg: 234.66189575195312 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.9286, loss_val: nan, pos_over_neg: 388.7286682128906 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 4.0033, loss_val: nan, pos_over_neg: 214.04685974121094 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.9609, loss_val: nan, pos_over_neg: 246.14578247070312 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 4.0307, loss_val: nan, pos_over_neg: 176.7694549560547 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.9679, loss_val: nan, pos_over_neg: 164.87283325195312 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.9867, loss_val: nan, pos_over_neg: 176.0623016357422 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.9396, loss_val: nan, pos_over_neg: 249.606689453125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.9417, loss_val: nan, pos_over_neg: 286.9873962402344 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.9313, loss_val: nan, pos_over_neg: 305.8504333496094 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 4.0198, loss_val: nan, pos_over_neg: 138.60043334960938 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.9705, loss_val: nan, pos_over_neg: 196.8988494873047 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.9425, loss_val: nan, pos_over_neg: 278.0460205078125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.9343, loss_val: nan, pos_over_neg: 205.5790252685547 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.9571, loss_val: nan, pos_over_neg: 212.00621032714844 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.9767, loss_val: nan, pos_over_neg: 286.9431457519531 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.9874, loss_val: nan, pos_over_neg: 223.46791076660156 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 4.0141, loss_val: nan, pos_over_neg: 228.7568359375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.9403, loss_val: nan, pos_over_neg: 218.8631591796875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.9636, loss_val: nan, pos_over_neg: 289.7846374511719 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.9729, loss_val: nan, pos_over_neg: 220.00169372558594 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.9556, loss_val: nan, pos_over_neg: 196.40609741210938 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.9343, loss_val: nan, pos_over_neg: 215.4943389892578 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.9202, loss_val: nan, pos_over_neg: 230.02194213867188 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.9735, loss_val: nan, pos_over_neg: 263.185302734375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.9511, loss_val: nan, pos_over_neg: 302.34814453125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 243.93002319335938 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.9372, loss_val: nan, pos_over_neg: 323.1265869140625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 319.6649475097656 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.9856, loss_val: nan, pos_over_neg: 171.27392578125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.9556, loss_val: nan, pos_over_neg: 222.7398223876953 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.9462, loss_val: nan, pos_over_neg: 283.1199035644531 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.9134, loss_val: nan, pos_over_neg: 200.6659698486328 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.9254, loss_val: nan, pos_over_neg: 159.15945434570312 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.9748, loss_val: nan, pos_over_neg: 177.94639587402344 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.9833, loss_val: nan, pos_over_neg: 216.43304443359375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.9489, loss_val: nan, pos_over_neg: 237.931884765625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 4.001, loss_val: nan, pos_over_neg: 238.9929962158203 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.9556, loss_val: nan, pos_over_neg: 193.48333740234375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.9635, loss_val: nan, pos_over_neg: 186.44821166992188 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 4.0224, loss_val: nan, pos_over_neg: 130.62881469726562 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.9695, loss_val: nan, pos_over_neg: 233.20042419433594 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.9659, loss_val: nan, pos_over_neg: 224.93775939941406 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.9653, loss_val: nan, pos_over_neg: 284.46112060546875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.9397, loss_val: nan, pos_over_neg: 365.7318115234375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.9109, loss_val: nan, pos_over_neg: 349.929931640625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.9907, loss_val: nan, pos_over_neg: 232.71951293945312 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.9312, loss_val: nan, pos_over_neg: 175.53598022460938 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.9533, loss_val: nan, pos_over_neg: 256.3316345214844 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.9388, loss_val: nan, pos_over_neg: 277.31884765625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.9105, loss_val: nan, pos_over_neg: 308.85302734375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.9201, loss_val: nan, pos_over_neg: 428.268310546875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.9342, loss_val: nan, pos_over_neg: 237.2279052734375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.9596, loss_val: nan, pos_over_neg: 216.57073974609375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.9703, loss_val: nan, pos_over_neg: 208.10479736328125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.9169, loss_val: nan, pos_over_neg: 296.3356018066406 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.93, loss_val: nan, pos_over_neg: 243.0839080810547 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.945, loss_val: nan, pos_over_neg: 255.37567138671875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.9902, loss_val: nan, pos_over_neg: 160.72824096679688 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.9602, loss_val: nan, pos_over_neg: 230.1638641357422 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.9589, loss_val: nan, pos_over_neg: 221.93081665039062 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.9739, loss_val: nan, pos_over_neg: 219.05816650390625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.9532, loss_val: nan, pos_over_neg: 184.8765411376953 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.981, loss_val: nan, pos_over_neg: 191.56863403320312 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.9081, loss_val: nan, pos_over_neg: 248.81382751464844 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.9542, loss_val: nan, pos_over_neg: 146.93775939941406 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.9579, loss_val: nan, pos_over_neg: 202.53199768066406 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.9572, loss_val: nan, pos_over_neg: 211.41262817382812 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.9612, loss_val: nan, pos_over_neg: 181.17945861816406 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.9409, loss_val: nan, pos_over_neg: 218.02992248535156 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.9673, loss_val: nan, pos_over_neg: 274.91790771484375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.978, loss_val: nan, pos_over_neg: 261.21063232421875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.9498, loss_val: nan, pos_over_neg: 201.28829956054688 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.9711, loss_val: nan, pos_over_neg: 227.8343963623047 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.9146, loss_val: nan, pos_over_neg: 214.7361602783203 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.9476, loss_val: nan, pos_over_neg: 367.4652099609375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.9799, loss_val: nan, pos_over_neg: 216.53973388671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.996, loss_val: nan, pos_over_neg: 197.3218994140625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.9442, loss_val: nan, pos_over_neg: 154.2814483642578 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.9091, loss_val: nan, pos_over_neg: 150.24282836914062 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.8942, loss_val: nan, pos_over_neg: 338.7989501953125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.9472, loss_val: nan, pos_over_neg: 298.61651611328125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.9371, loss_val: nan, pos_over_neg: 266.05548095703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.9523, loss_val: nan, pos_over_neg: 375.62744140625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.9354, loss_val: nan, pos_over_neg: 398.2438659667969 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.923, loss_val: nan, pos_over_neg: 247.1939697265625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.9455, loss_val: nan, pos_over_neg: 252.2963104248047 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.8807, loss_val: nan, pos_over_neg: 247.57215881347656 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.945, loss_val: nan, pos_over_neg: 130.7951202392578 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 4.0354, loss_val: nan, pos_over_neg: 118.49072265625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.948, loss_val: nan, pos_over_neg: 287.661865234375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.9193, loss_val: nan, pos_over_neg: 298.4088134765625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.8874, loss_val: nan, pos_over_neg: 281.0148010253906 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.8881, loss_val: nan, pos_over_neg: 422.1561584472656 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.9187, loss_val: nan, pos_over_neg: 449.50665283203125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.9157, loss_val: nan, pos_over_neg: 279.0487365722656 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.9432, loss_val: nan, pos_over_neg: 223.47705078125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.913, loss_val: nan, pos_over_neg: 196.19053649902344 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.9377, loss_val: nan, pos_over_neg: 167.17840576171875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.9953, loss_val: nan, pos_over_neg: 161.81442260742188 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.9168, loss_val: nan, pos_over_neg: 223.92913818359375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.9256, loss_val: nan, pos_over_neg: 261.7574157714844 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.9506, loss_val: nan, pos_over_neg: 237.68772888183594 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.9555, loss_val: nan, pos_over_neg: 286.9146423339844 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.88, loss_val: nan, pos_over_neg: 305.9083557128906 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.9322, loss_val: nan, pos_over_neg: 174.6605987548828 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.9479, loss_val: nan, pos_over_neg: 180.38339233398438 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.8957, loss_val: nan, pos_over_neg: 269.39080810546875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.9494, loss_val: nan, pos_over_neg: 180.82098388671875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.9287, loss_val: nan, pos_over_neg: 246.68324279785156 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.9414, loss_val: nan, pos_over_neg: 260.9998474121094 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.958, loss_val: nan, pos_over_neg: 281.4775695800781 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.8752, loss_val: nan, pos_over_neg: 289.1718444824219 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.9471, loss_val: nan, pos_over_neg: 344.3370056152344 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.936, loss_val: nan, pos_over_neg: 265.1825866699219 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.9602, loss_val: nan, pos_over_neg: 181.94166564941406 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.8998, loss_val: nan, pos_over_neg: 204.52166748046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.9084, loss_val: nan, pos_over_neg: 212.38446044921875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.9746, loss_val: nan, pos_over_neg: 172.6927947998047 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.9157, loss_val: nan, pos_over_neg: 169.3082733154297 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.9429, loss_val: nan, pos_over_neg: 207.0204620361328 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.9057, loss_val: nan, pos_over_neg: 348.916748046875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.8854, loss_val: nan, pos_over_neg: 517.9896850585938 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.8775, loss_val: nan, pos_over_neg: 374.5400695800781 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.9497, loss_val: nan, pos_over_neg: 282.4171142578125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.9651, loss_val: nan, pos_over_neg: 246.33267211914062 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.8987, loss_val: nan, pos_over_neg: 303.4515075683594 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.9068, loss_val: nan, pos_over_neg: 186.35870361328125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.9488, loss_val: nan, pos_over_neg: 199.8321075439453 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.929, loss_val: nan, pos_over_neg: 215.93917846679688 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.9015, loss_val: nan, pos_over_neg: 222.74732971191406 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.8884, loss_val: nan, pos_over_neg: 381.7820739746094 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.9068, loss_val: nan, pos_over_neg: 509.5981750488281 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.8972, loss_val: nan, pos_over_neg: 280.56549072265625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.924, loss_val: nan, pos_over_neg: 188.4185028076172 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.9528, loss_val: nan, pos_over_neg: 212.01841735839844 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.9348, loss_val: nan, pos_over_neg: 213.55374145507812 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.9099, loss_val: nan, pos_over_neg: 281.744873046875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.9184, loss_val: nan, pos_over_neg: 273.79913330078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 312.6374206542969 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.9197, loss_val: nan, pos_over_neg: 223.40621948242188 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.9116, loss_val: nan, pos_over_neg: 186.47097778320312 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.9166, loss_val: nan, pos_over_neg: 264.03564453125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.9594, loss_val: nan, pos_over_neg: 241.0374755859375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.9165, loss_val: nan, pos_over_neg: 251.83743286132812 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.911, loss_val: nan, pos_over_neg: 195.92465209960938 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.927, loss_val: nan, pos_over_neg: 297.7460021972656 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.9335, loss_val: nan, pos_over_neg: 194.1823272705078 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.9441, loss_val: nan, pos_over_neg: 161.22373962402344 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 224.3306427001953 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.9736, loss_val: nan, pos_over_neg: 182.75013732910156 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.8894, loss_val: nan, pos_over_neg: 226.64102172851562 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.9268, loss_val: nan, pos_over_neg: 253.3168487548828 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.8961, loss_val: nan, pos_over_neg: 360.6479797363281 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.882, loss_val: nan, pos_over_neg: 367.38946533203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.9418, loss_val: nan, pos_over_neg: 229.30694580078125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.9572, loss_val: nan, pos_over_neg: 164.23858642578125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.9823, loss_val: nan, pos_over_neg: 215.50624084472656 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.8927, loss_val: nan, pos_over_neg: 218.40643310546875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.9519, loss_val: nan, pos_over_neg: 146.11349487304688 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.935, loss_val: nan, pos_over_neg: 179.1057586669922 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.9592, loss_val: nan, pos_over_neg: 230.9459991455078 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.9379, loss_val: nan, pos_over_neg: 216.68252563476562 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.9416, loss_val: nan, pos_over_neg: 281.2300109863281 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.9715, loss_val: nan, pos_over_neg: 333.36383056640625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.9444, loss_val: nan, pos_over_neg: 209.8025360107422 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.8811, loss_val: nan, pos_over_neg: 364.3860778808594 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.8869, loss_val: nan, pos_over_neg: 288.4967346191406 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 158.9062957763672 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.9458, loss_val: nan, pos_over_neg: 192.59347534179688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.9419, loss_val: nan, pos_over_neg: 150.7613525390625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.9369, loss_val: nan, pos_over_neg: 247.6509246826172 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.9078, loss_val: nan, pos_over_neg: 249.9729766845703 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.9918, loss_val: nan, pos_over_neg: 260.23101806640625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.9143, loss_val: nan, pos_over_neg: 263.0994873046875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.9385, loss_val: nan, pos_over_neg: 247.94639587402344 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.9615, loss_val: nan, pos_over_neg: 128.00880432128906 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.8857, loss_val: nan, pos_over_neg: 176.08657836914062 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.8895, loss_val: nan, pos_over_neg: 158.02236938476562 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.9047, loss_val: nan, pos_over_neg: 239.2915802001953 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.8979, loss_val: nan, pos_over_neg: 252.5166015625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.9196, loss_val: nan, pos_over_neg: 198.19039916992188 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.8964, loss_val: nan, pos_over_neg: 273.4361877441406 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.9009, loss_val: nan, pos_over_neg: 237.40931701660156 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.9503, loss_val: nan, pos_over_neg: 219.8643341064453 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.9091, loss_val: nan, pos_over_neg: 183.54083251953125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.8607, loss_val: nan, pos_over_neg: 195.59112548828125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.9115, loss_val: nan, pos_over_neg: 200.26815795898438 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.9121, loss_val: nan, pos_over_neg: 181.05996704101562 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.9531, loss_val: nan, pos_over_neg: 195.7936248779297 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.9154, loss_val: nan, pos_over_neg: 192.95465087890625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.9444, loss_val: nan, pos_over_neg: 300.9416198730469 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.9078, loss_val: nan, pos_over_neg: 201.18759155273438 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.9323, loss_val: nan, pos_over_neg: 214.57725524902344 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.9493, loss_val: nan, pos_over_neg: 206.91542053222656 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.9713, loss_val: nan, pos_over_neg: 213.4031982421875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.8835, loss_val: nan, pos_over_neg: 158.5213623046875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.9433, loss_val: nan, pos_over_neg: 191.0919952392578 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.9137, loss_val: nan, pos_over_neg: 186.82325744628906 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.9352, loss_val: nan, pos_over_neg: 170.718017578125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.915, loss_val: nan, pos_over_neg: 258.36920166015625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.9548, loss_val: nan, pos_over_neg: 210.2112579345703 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.8902, loss_val: nan, pos_over_neg: 212.1627960205078 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.876, loss_val: nan, pos_over_neg: 229.4886016845703 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.9036, loss_val: nan, pos_over_neg: 245.04090881347656 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.9142, loss_val: nan, pos_over_neg: 193.51588439941406 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.8824, loss_val: nan, pos_over_neg: 281.2635498046875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.9687, loss_val: nan, pos_over_neg: 149.94412231445312 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.933, loss_val: nan, pos_over_neg: 191.60169982910156 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.9364, loss_val: nan, pos_over_neg: 368.34619140625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.953, loss_val: nan, pos_over_neg: 326.00311279296875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.948, loss_val: nan, pos_over_neg: 209.0756378173828 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.9176, loss_val: nan, pos_over_neg: 210.937255859375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.9251, loss_val: nan, pos_over_neg: 213.68516540527344 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.9038, loss_val: nan, pos_over_neg: 228.24386596679688 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.9059, loss_val: nan, pos_over_neg: 215.27740478515625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.8695, loss_val: nan, pos_over_neg: 323.8419189453125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.9039, loss_val: nan, pos_over_neg: 568.5950927734375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 441.780029296875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.9415, loss_val: nan, pos_over_neg: 335.4244384765625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.9114, loss_val: nan, pos_over_neg: 241.11582946777344 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.9485, loss_val: nan, pos_over_neg: 160.173095703125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.923, loss_val: nan, pos_over_neg: 243.81382751464844 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.9299, loss_val: nan, pos_over_neg: 252.4474639892578 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.8746, loss_val: nan, pos_over_neg: 428.7364501953125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.9229, loss_val: nan, pos_over_neg: 218.3242645263672 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.8699, loss_val: nan, pos_over_neg: 341.5213623046875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.899, loss_val: nan, pos_over_neg: 353.9512939453125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.8843, loss_val: nan, pos_over_neg: 278.81719970703125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.8927, loss_val: nan, pos_over_neg: 263.1732177734375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.9027, loss_val: nan, pos_over_neg: 286.5960388183594 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.8781, loss_val: nan, pos_over_neg: 368.94757080078125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.9567, loss_val: nan, pos_over_neg: 252.05104064941406 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.8997, loss_val: nan, pos_over_neg: 359.98944091796875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.8723, loss_val: nan, pos_over_neg: 863.92724609375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.8683, loss_val: nan, pos_over_neg: 350.53277587890625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.9451, loss_val: nan, pos_over_neg: 227.48353576660156 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 219.4013671875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.8556, loss_val: nan, pos_over_neg: 672.804443359375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.8619, loss_val: nan, pos_over_neg: 402.506103515625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.887, loss_val: nan, pos_over_neg: 300.6282958984375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.8977, loss_val: nan, pos_over_neg: 387.9320373535156 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.904, loss_val: nan, pos_over_neg: 330.8018493652344 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.8579, loss_val: nan, pos_over_neg: 287.25469970703125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.9034, loss_val: nan, pos_over_neg: 287.6539611816406 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.9649, loss_val: nan, pos_over_neg: 180.08485412597656 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.9077, loss_val: nan, pos_over_neg: 264.7176818847656 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.9656, loss_val: nan, pos_over_neg: 180.3629150390625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.9113, loss_val: nan, pos_over_neg: 222.7172088623047 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.9235, loss_val: nan, pos_over_neg: 206.38894653320312 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.9294, loss_val: nan, pos_over_neg: 202.43600463867188 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.9538, loss_val: nan, pos_over_neg: 233.64308166503906 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.8954, loss_val: nan, pos_over_neg: 294.2734680175781 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.8741, loss_val: nan, pos_over_neg: 249.22442626953125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.9194, loss_val: nan, pos_over_neg: 224.74142456054688 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.8949, loss_val: nan, pos_over_neg: 184.80404663085938 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.8875, loss_val: nan, pos_over_neg: 332.86474609375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.9308, loss_val: nan, pos_over_neg: 186.0360870361328 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.922, loss_val: nan, pos_over_neg: 183.9574432373047 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.9241, loss_val: nan, pos_over_neg: 256.26910400390625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.9589, loss_val: nan, pos_over_neg: 229.33685302734375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.9185, loss_val: nan, pos_over_neg: 198.0929412841797 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.9189, loss_val: nan, pos_over_neg: 275.3895263671875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.9981, loss_val: nan, pos_over_neg: 212.6114044189453 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.9014, loss_val: nan, pos_over_neg: 205.80274963378906 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.8744, loss_val: nan, pos_over_neg: 190.436279296875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.8795, loss_val: nan, pos_over_neg: 164.5907440185547 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.9055, loss_val: nan, pos_over_neg: 173.49468994140625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.9063, loss_val: nan, pos_over_neg: 297.9858093261719 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.9048, loss_val: nan, pos_over_neg: 268.0704345703125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.9382, loss_val: nan, pos_over_neg: 229.8020782470703 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.8817, loss_val: nan, pos_over_neg: 251.9730987548828 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.9391, loss_val: nan, pos_over_neg: 235.79844665527344 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.9084, loss_val: nan, pos_over_neg: 264.3775634765625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.9059, loss_val: nan, pos_over_neg: 209.27923583984375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 164.46875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.895, loss_val: nan, pos_over_neg: 155.14857482910156 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.9396, loss_val: nan, pos_over_neg: 227.4146270751953 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.9009, loss_val: nan, pos_over_neg: 273.44146728515625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.8788, loss_val: nan, pos_over_neg: 262.0380859375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.9288, loss_val: nan, pos_over_neg: 168.1529998779297 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.8766, loss_val: nan, pos_over_neg: 278.3274841308594 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.9415, loss_val: nan, pos_over_neg: 404.10784912109375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.8937, loss_val: nan, pos_over_neg: 378.69610595703125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.8915, loss_val: nan, pos_over_neg: 283.6424255371094 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.9264, loss_val: nan, pos_over_neg: 211.19393920898438 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.8996, loss_val: nan, pos_over_neg: 194.60768127441406 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.9439, loss_val: nan, pos_over_neg: 165.96026611328125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.9408, loss_val: nan, pos_over_neg: 154.8984375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.9138, loss_val: nan, pos_over_neg: 183.5700225830078 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.8774, loss_val: nan, pos_over_neg: 321.5781555175781 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.9053, loss_val: nan, pos_over_neg: 244.03314208984375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.8831, loss_val: nan, pos_over_neg: 289.16278076171875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.8607, loss_val: nan, pos_over_neg: 288.2675476074219 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.8947, loss_val: nan, pos_over_neg: 244.18161010742188 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.917, loss_val: nan, pos_over_neg: 182.75204467773438 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.884, loss_val: nan, pos_over_neg: 278.2235107421875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.9005, loss_val: nan, pos_over_neg: 178.44786071777344 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.9166, loss_val: nan, pos_over_neg: 211.806396484375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.8565, loss_val: nan, pos_over_neg: 266.6071472167969 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.865, loss_val: nan, pos_over_neg: 276.1311340332031 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.9217, loss_val: nan, pos_over_neg: 226.68704223632812 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.9529, loss_val: nan, pos_over_neg: 195.58836364746094 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.9102, loss_val: nan, pos_over_neg: 331.4153137207031 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.8964, loss_val: nan, pos_over_neg: 274.99114990234375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.894, loss_val: nan, pos_over_neg: 264.785400390625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.9176, loss_val: nan, pos_over_neg: 269.4089050292969 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.8981, loss_val: nan, pos_over_neg: 236.82672119140625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.9246, loss_val: nan, pos_over_neg: 176.5669708251953 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.875, loss_val: nan, pos_over_neg: 246.10556030273438 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.8933, loss_val: nan, pos_over_neg: 221.0997314453125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.9345, loss_val: nan, pos_over_neg: 150.1873779296875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.8539, loss_val: nan, pos_over_neg: 348.0118103027344 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.9244, loss_val: nan, pos_over_neg: 283.7414855957031 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.8608, loss_val: nan, pos_over_neg: 314.0223083496094 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.9197, loss_val: nan, pos_over_neg: 226.6104278564453 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.9171, loss_val: nan, pos_over_neg: 193.5044403076172 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.8955, loss_val: nan, pos_over_neg: 191.75953674316406 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.8991, loss_val: nan, pos_over_neg: 237.1844024658203 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 241.0448455810547 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.903, loss_val: nan, pos_over_neg: 328.6744689941406 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.9266, loss_val: nan, pos_over_neg: 217.03822326660156 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.8924, loss_val: nan, pos_over_neg: 275.9599914550781 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.8834, loss_val: nan, pos_over_neg: 291.3635559082031 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.8786, loss_val: nan, pos_over_neg: 242.9215545654297 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.8962, loss_val: nan, pos_over_neg: 224.8543243408203 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 265.2295837402344 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.9071, loss_val: nan, pos_over_neg: 295.6871032714844 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.9186, loss_val: nan, pos_over_neg: 220.6226806640625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.9296, loss_val: nan, pos_over_neg: 196.10037231445312 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.8632, loss_val: nan, pos_over_neg: 265.9005126953125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.8229, loss_val: nan, pos_over_neg: 646.7432250976562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.9011, loss_val: nan, pos_over_neg: 198.3144073486328 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.9281, loss_val: nan, pos_over_neg: 216.2119140625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.9245, loss_val: nan, pos_over_neg: 235.71820068359375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.8669, loss_val: nan, pos_over_neg: 209.52906799316406 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.8754, loss_val: nan, pos_over_neg: 273.02105712890625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.8894, loss_val: nan, pos_over_neg: 280.3934020996094 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.8751, loss_val: nan, pos_over_neg: 217.46261596679688 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.8215, loss_val: nan, pos_over_neg: 226.66720581054688 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.8852, loss_val: nan, pos_over_neg: 223.5946044921875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.8489, loss_val: nan, pos_over_neg: 403.4197082519531 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.9107, loss_val: nan, pos_over_neg: 227.74697875976562 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 281.79913330078125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.8689, loss_val: nan, pos_over_neg: 242.79103088378906 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.9058, loss_val: nan, pos_over_neg: 257.8647766113281 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.8992, loss_val: nan, pos_over_neg: 229.66424560546875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.8719, loss_val: nan, pos_over_neg: 239.90762329101562 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.8661, loss_val: nan, pos_over_neg: 323.8076477050781 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.9431, loss_val: nan, pos_over_neg: 234.44796752929688 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.8467, loss_val: nan, pos_over_neg: 389.6493225097656 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.8702, loss_val: nan, pos_over_neg: 307.3715515136719 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.8821, loss_val: nan, pos_over_neg: 280.6854248046875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.9024, loss_val: nan, pos_over_neg: 229.42391967773438 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.9225, loss_val: nan, pos_over_neg: 187.43292236328125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.8971, loss_val: nan, pos_over_neg: 349.419921875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.8958, loss_val: nan, pos_over_neg: 272.7337646484375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.9142, loss_val: nan, pos_over_neg: 324.5082092285156 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.8845, loss_val: nan, pos_over_neg: 290.6151428222656 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.8768, loss_val: nan, pos_over_neg: 352.6650695800781 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.8748, loss_val: nan, pos_over_neg: 244.30450439453125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.8859, loss_val: nan, pos_over_neg: 240.78451538085938 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.873, loss_val: nan, pos_over_neg: 247.70132446289062 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.8902, loss_val: nan, pos_over_neg: 488.33154296875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.8822, loss_val: nan, pos_over_neg: 194.93280029296875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.8531, loss_val: nan, pos_over_neg: 238.9601593017578 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.9402, loss_val: nan, pos_over_neg: 161.12855529785156 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.9107, loss_val: nan, pos_over_neg: 245.22686767578125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.8701, loss_val: nan, pos_over_neg: 208.6628875732422 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.9059, loss_val: nan, pos_over_neg: 245.5679168701172 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.8655, loss_val: nan, pos_over_neg: 226.31932067871094 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.8605, loss_val: nan, pos_over_neg: 310.9681701660156 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.8476, loss_val: nan, pos_over_neg: 255.8282928466797 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.8756, loss_val: nan, pos_over_neg: 240.32762145996094 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.9036, loss_val: nan, pos_over_neg: 223.05477905273438 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.9102, loss_val: nan, pos_over_neg: 166.97752380371094 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.918, loss_val: nan, pos_over_neg: 276.83538818359375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.8784, loss_val: nan, pos_over_neg: 222.7279815673828 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.95, loss_val: nan, pos_over_neg: 186.79226684570312 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.8966, loss_val: nan, pos_over_neg: 212.9441375732422 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.8779, loss_val: nan, pos_over_neg: 200.5795135498047 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.8815, loss_val: nan, pos_over_neg: 223.50509643554688 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.9177, loss_val: nan, pos_over_neg: 203.79608154296875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.8799, loss_val: nan, pos_over_neg: 335.56793212890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.9235, loss_val: nan, pos_over_neg: 277.09906005859375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.9474, loss_val: nan, pos_over_neg: 292.025146484375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.8945, loss_val: nan, pos_over_neg: 262.29443359375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.8852, loss_val: nan, pos_over_neg: 199.08544921875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.8657, loss_val: nan, pos_over_neg: 335.6011047363281 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.8695, loss_val: nan, pos_over_neg: 304.7548828125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.9479, loss_val: nan, pos_over_neg: 144.86940002441406 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.8583, loss_val: nan, pos_over_neg: 165.77195739746094 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.9061, loss_val: nan, pos_over_neg: 201.7676239013672 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.8578, loss_val: nan, pos_over_neg: 393.1675109863281 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.878, loss_val: nan, pos_over_neg: 319.75482177734375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [30:34<76280:22:43, 915.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.8625, loss_val: nan, pos_over_neg: 303.62420654296875 lr: 0.00031623\n",
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 3.8476, loss_val: nan, pos_over_neg: 245.13450622558594 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.9036, loss_val: nan, pos_over_neg: 286.5736999511719 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.891, loss_val: nan, pos_over_neg: 213.7187957763672 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.8562, loss_val: nan, pos_over_neg: 218.32717895507812 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.8885, loss_val: nan, pos_over_neg: 150.40284729003906 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.8679, loss_val: nan, pos_over_neg: 165.22605895996094 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.8922, loss_val: nan, pos_over_neg: 216.0522918701172 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.8552, loss_val: nan, pos_over_neg: 275.8893127441406 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.8881, loss_val: nan, pos_over_neg: 310.1696472167969 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.8678, loss_val: nan, pos_over_neg: 353.7990417480469 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.915, loss_val: nan, pos_over_neg: 215.43275451660156 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.8674, loss_val: nan, pos_over_neg: 250.6500244140625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 310.417236328125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.9146, loss_val: nan, pos_over_neg: 186.69729614257812 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.8588, loss_val: nan, pos_over_neg: 252.53179931640625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.8775, loss_val: nan, pos_over_neg: 204.65863037109375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.8604, loss_val: nan, pos_over_neg: 226.1489715576172 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.9109, loss_val: nan, pos_over_neg: 216.05703735351562 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.897, loss_val: nan, pos_over_neg: 221.11769104003906 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.8987, loss_val: nan, pos_over_neg: 239.4251251220703 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.8418, loss_val: nan, pos_over_neg: 229.08546447753906 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.8549, loss_val: nan, pos_over_neg: 252.20327758789062 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.8391, loss_val: nan, pos_over_neg: 315.2811279296875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.9223, loss_val: nan, pos_over_neg: 220.0380401611328 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.9043, loss_val: nan, pos_over_neg: 161.9703369140625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.8667, loss_val: nan, pos_over_neg: 252.74134826660156 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.8773, loss_val: nan, pos_over_neg: 195.3065185546875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.8725, loss_val: nan, pos_over_neg: 233.38241577148438 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 345.8094482421875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.9044, loss_val: nan, pos_over_neg: 223.69090270996094 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.8841, loss_val: nan, pos_over_neg: 295.0347900390625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.8594, loss_val: nan, pos_over_neg: 247.7198486328125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.8741, loss_val: nan, pos_over_neg: 249.5768280029297 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.9051, loss_val: nan, pos_over_neg: 218.2513427734375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.8544, loss_val: nan, pos_over_neg: 215.40707397460938 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.8826, loss_val: nan, pos_over_neg: 225.719482421875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.8738, loss_val: nan, pos_over_neg: 228.12246704101562 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.8837, loss_val: nan, pos_over_neg: 284.1001892089844 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.8637, loss_val: nan, pos_over_neg: 209.68035888671875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.8889, loss_val: nan, pos_over_neg: 257.6700439453125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.9124, loss_val: nan, pos_over_neg: 305.0787353515625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.8844, loss_val: nan, pos_over_neg: 315.8232421875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.9102, loss_val: nan, pos_over_neg: 231.03555297851562 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.9378, loss_val: nan, pos_over_neg: 211.0391387939453 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.9071, loss_val: nan, pos_over_neg: 227.72499084472656 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.8547, loss_val: nan, pos_over_neg: 472.4664001464844 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.844, loss_val: nan, pos_over_neg: 187.37274169921875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.9079, loss_val: nan, pos_over_neg: 179.15863037109375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.872, loss_val: nan, pos_over_neg: 296.4273986816406 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.8189, loss_val: nan, pos_over_neg: 413.98101806640625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.9078, loss_val: nan, pos_over_neg: 252.9897918701172 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 367.5583801269531 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 262.33416748046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.8435, loss_val: nan, pos_over_neg: 287.7015075683594 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.8957, loss_val: nan, pos_over_neg: 242.9165802001953 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.856, loss_val: nan, pos_over_neg: 377.7248229980469 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.8666, loss_val: nan, pos_over_neg: 334.718505859375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.8638, loss_val: nan, pos_over_neg: 237.76939392089844 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.9334, loss_val: nan, pos_over_neg: 207.13018798828125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.8398, loss_val: nan, pos_over_neg: 280.5793151855469 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.8837, loss_val: nan, pos_over_neg: 275.939453125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.8388, loss_val: nan, pos_over_neg: 339.2631530761719 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.8641, loss_val: nan, pos_over_neg: 392.18212890625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.8789, loss_val: nan, pos_over_neg: 491.9550476074219 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.8849, loss_val: nan, pos_over_neg: 632.4902954101562 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 380.8419494628906 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.8531, loss_val: nan, pos_over_neg: 292.3041687011719 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.8323, loss_val: nan, pos_over_neg: 413.2450256347656 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.8449, loss_val: nan, pos_over_neg: 175.04119873046875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.8866, loss_val: nan, pos_over_neg: 198.8292694091797 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.8572, loss_val: nan, pos_over_neg: 282.2693176269531 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.8506, loss_val: nan, pos_over_neg: 297.3099670410156 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.8755, loss_val: nan, pos_over_neg: 504.6128234863281 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.9003, loss_val: nan, pos_over_neg: 302.76165771484375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.8998, loss_val: nan, pos_over_neg: 368.9239196777344 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.8692, loss_val: nan, pos_over_neg: 428.08001708984375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.8295, loss_val: nan, pos_over_neg: 336.1988830566406 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.9377, loss_val: nan, pos_over_neg: 184.0572052001953 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.8695, loss_val: nan, pos_over_neg: 215.32846069335938 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.9063, loss_val: nan, pos_over_neg: 203.8086395263672 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.8774, loss_val: nan, pos_over_neg: 259.7011413574219 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.8879, loss_val: nan, pos_over_neg: 300.6598815917969 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.8918, loss_val: nan, pos_over_neg: 377.97906494140625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.8715, loss_val: nan, pos_over_neg: 337.7567138671875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.8876, loss_val: nan, pos_over_neg: 280.8128967285156 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.8514, loss_val: nan, pos_over_neg: 327.1434631347656 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.8592, loss_val: nan, pos_over_neg: 212.78211975097656 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.8688, loss_val: nan, pos_over_neg: 310.32025146484375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.8813, loss_val: nan, pos_over_neg: 215.57150268554688 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.8815, loss_val: nan, pos_over_neg: 247.48634338378906 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.8994, loss_val: nan, pos_over_neg: 406.01385498046875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.8882, loss_val: nan, pos_over_neg: 299.51788330078125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.8754, loss_val: nan, pos_over_neg: 310.9133605957031 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.8538, loss_val: nan, pos_over_neg: 321.1236572265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.8434, loss_val: nan, pos_over_neg: 238.99937438964844 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.8762, loss_val: nan, pos_over_neg: 277.17559814453125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.8866, loss_val: nan, pos_over_neg: 195.02711486816406 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.8943, loss_val: nan, pos_over_neg: 241.89930725097656 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.8666, loss_val: nan, pos_over_neg: 309.337158203125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.9005, loss_val: nan, pos_over_neg: 172.95826721191406 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.9098, loss_val: nan, pos_over_neg: 210.25323486328125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 456.8231506347656 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.8651, loss_val: nan, pos_over_neg: 256.6800537109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.8805, loss_val: nan, pos_over_neg: 171.29039001464844 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.8502, loss_val: nan, pos_over_neg: 217.61729431152344 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.8409, loss_val: nan, pos_over_neg: 269.75762939453125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.9252, loss_val: nan, pos_over_neg: 142.8802490234375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.8682, loss_val: nan, pos_over_neg: 229.01084899902344 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.8661, loss_val: nan, pos_over_neg: 413.5323181152344 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.8323, loss_val: nan, pos_over_neg: 297.168212890625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.8893, loss_val: nan, pos_over_neg: 177.4922332763672 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.8725, loss_val: nan, pos_over_neg: 179.207763671875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.8349, loss_val: nan, pos_over_neg: 294.7912292480469 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.928, loss_val: nan, pos_over_neg: 182.71359252929688 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.8487, loss_val: nan, pos_over_neg: 177.9574737548828 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.8734, loss_val: nan, pos_over_neg: 225.25096130371094 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.8609, loss_val: nan, pos_over_neg: 287.7615661621094 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.8883, loss_val: nan, pos_over_neg: 172.4877471923828 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.8754, loss_val: nan, pos_over_neg: 196.73460388183594 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.8929, loss_val: nan, pos_over_neg: 227.94752502441406 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 364.5342712402344 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.857, loss_val: nan, pos_over_neg: 313.4095764160156 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.883, loss_val: nan, pos_over_neg: 176.2300262451172 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 245.13125610351562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.8969, loss_val: nan, pos_over_neg: 255.8207550048828 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.8839, loss_val: nan, pos_over_neg: 198.3905792236328 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.8899, loss_val: nan, pos_over_neg: 192.62545776367188 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.8029, loss_val: nan, pos_over_neg: 238.39674377441406 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.8556, loss_val: nan, pos_over_neg: 202.02281188964844 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.9188, loss_val: nan, pos_over_neg: 217.5774383544922 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.8763, loss_val: nan, pos_over_neg: 449.5115051269531 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.8923, loss_val: nan, pos_over_neg: 318.64849853515625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.8872, loss_val: nan, pos_over_neg: 187.53346252441406 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.8672, loss_val: nan, pos_over_neg: 222.24093627929688 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.8839, loss_val: nan, pos_over_neg: 274.98895263671875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.7879, loss_val: nan, pos_over_neg: 430.5592041015625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.8659, loss_val: nan, pos_over_neg: 259.5276794433594 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.8157, loss_val: nan, pos_over_neg: 313.40521240234375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.8087, loss_val: nan, pos_over_neg: 215.29437255859375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.8298, loss_val: nan, pos_over_neg: 206.67919921875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.8546, loss_val: nan, pos_over_neg: 211.70989990234375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.8808, loss_val: nan, pos_over_neg: 246.21807861328125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.8537, loss_val: nan, pos_over_neg: 186.97232055664062 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.8492, loss_val: nan, pos_over_neg: 246.66079711914062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.8319, loss_val: nan, pos_over_neg: 286.5468444824219 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.8845, loss_val: nan, pos_over_neg: 290.0943603515625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.8467, loss_val: nan, pos_over_neg: 223.84841918945312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.9003, loss_val: nan, pos_over_neg: 215.49673461914062 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.832, loss_val: nan, pos_over_neg: 274.4154052734375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.8516, loss_val: nan, pos_over_neg: 384.0875549316406 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.8565, loss_val: nan, pos_over_neg: 301.9141540527344 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.8691, loss_val: nan, pos_over_neg: 203.4160614013672 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.8751, loss_val: nan, pos_over_neg: 166.10301208496094 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.9038, loss_val: nan, pos_over_neg: 161.2859344482422 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.8554, loss_val: nan, pos_over_neg: 234.44973754882812 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.8503, loss_val: nan, pos_over_neg: 228.91954040527344 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.8647, loss_val: nan, pos_over_neg: 450.912353515625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.8747, loss_val: nan, pos_over_neg: 220.88706970214844 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.9053, loss_val: nan, pos_over_neg: 224.37710571289062 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.8756, loss_val: nan, pos_over_neg: 227.61122131347656 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.8891, loss_val: nan, pos_over_neg: 186.6553192138672 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.8562, loss_val: nan, pos_over_neg: 241.8508758544922 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.8745, loss_val: nan, pos_over_neg: 306.6225891113281 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.8946, loss_val: nan, pos_over_neg: 297.6779479980469 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.8859, loss_val: nan, pos_over_neg: 263.6553649902344 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.8216, loss_val: nan, pos_over_neg: 330.96807861328125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.852, loss_val: nan, pos_over_neg: 268.2021179199219 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.9442, loss_val: nan, pos_over_neg: 178.0982208251953 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.8854, loss_val: nan, pos_over_neg: 199.62295532226562 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.8959, loss_val: nan, pos_over_neg: 180.68765258789062 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.8948, loss_val: nan, pos_over_neg: 259.6221618652344 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.8014, loss_val: nan, pos_over_neg: 368.15185546875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.8724, loss_val: nan, pos_over_neg: 195.72618103027344 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.8336, loss_val: nan, pos_over_neg: 235.7403106689453 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.8765, loss_val: nan, pos_over_neg: 190.42176818847656 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.8467, loss_val: nan, pos_over_neg: 223.81150817871094 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 408.5474548339844 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.835, loss_val: nan, pos_over_neg: 358.1683349609375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.8481, loss_val: nan, pos_over_neg: 274.9899597167969 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.8528, loss_val: nan, pos_over_neg: 317.0840759277344 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.8299, loss_val: nan, pos_over_neg: 237.26365661621094 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.8577, loss_val: nan, pos_over_neg: 256.5249938964844 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.8423, loss_val: nan, pos_over_neg: 194.95896911621094 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 338.968017578125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.7984, loss_val: nan, pos_over_neg: 306.7968444824219 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.854, loss_val: nan, pos_over_neg: 249.1943359375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.8796, loss_val: nan, pos_over_neg: 290.4973449707031 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.8739, loss_val: nan, pos_over_neg: 264.43450927734375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.8948, loss_val: nan, pos_over_neg: 219.96112060546875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.8512, loss_val: nan, pos_over_neg: 298.3538513183594 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.8587, loss_val: nan, pos_over_neg: 243.03988647460938 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.8537, loss_val: nan, pos_over_neg: 229.5957794189453 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.9167, loss_val: nan, pos_over_neg: 191.05149841308594 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.8433, loss_val: nan, pos_over_neg: 350.10687255859375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.8441, loss_val: nan, pos_over_neg: 310.5562438964844 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.8257, loss_val: nan, pos_over_neg: 568.1717529296875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.8422, loss_val: nan, pos_over_neg: 305.7298278808594 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.8309, loss_val: nan, pos_over_neg: 300.0946044921875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.8503, loss_val: nan, pos_over_neg: 289.37469482421875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.8259, loss_val: nan, pos_over_neg: 315.9267578125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.856, loss_val: nan, pos_over_neg: 222.46975708007812 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.8726, loss_val: nan, pos_over_neg: 251.6707763671875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 308.23822021484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.8539, loss_val: nan, pos_over_neg: 260.1308288574219 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.8342, loss_val: nan, pos_over_neg: 312.845947265625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.8374, loss_val: nan, pos_over_neg: 218.65460205078125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.8376, loss_val: nan, pos_over_neg: 260.5998840332031 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.8032, loss_val: nan, pos_over_neg: 313.89898681640625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.8282, loss_val: nan, pos_over_neg: 306.4175720214844 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.9432, loss_val: nan, pos_over_neg: 166.96144104003906 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.877, loss_val: nan, pos_over_neg: 178.93902587890625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.841, loss_val: nan, pos_over_neg: 218.63864135742188 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.8634, loss_val: nan, pos_over_neg: 190.7662811279297 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.8372, loss_val: nan, pos_over_neg: 292.0452575683594 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 296.9571838378906 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.8596, loss_val: nan, pos_over_neg: 264.9922180175781 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.8701, loss_val: nan, pos_over_neg: 218.82374572753906 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.8408, loss_val: nan, pos_over_neg: 304.67352294921875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.8531, loss_val: nan, pos_over_neg: 259.748779296875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.8802, loss_val: nan, pos_over_neg: 165.548828125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.8149, loss_val: nan, pos_over_neg: 255.9958953857422 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.7701, loss_val: nan, pos_over_neg: 338.68658447265625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.8239, loss_val: nan, pos_over_neg: 292.7626037597656 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 282.1746520996094 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.862, loss_val: nan, pos_over_neg: 181.91259765625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.8646, loss_val: nan, pos_over_neg: 262.2158203125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.836, loss_val: nan, pos_over_neg: 221.3657989501953 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.8155, loss_val: nan, pos_over_neg: 282.1365051269531 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.8576, loss_val: nan, pos_over_neg: 242.69549560546875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.8624, loss_val: nan, pos_over_neg: 194.06161499023438 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.8124, loss_val: nan, pos_over_neg: 216.3621063232422 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.9029, loss_val: nan, pos_over_neg: 162.63673400878906 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.8309, loss_val: nan, pos_over_neg: 242.93026733398438 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 329.570068359375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.829, loss_val: nan, pos_over_neg: 335.91949462890625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.8598, loss_val: nan, pos_over_neg: 289.8766174316406 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.8744, loss_val: nan, pos_over_neg: 312.1318664550781 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.8431, loss_val: nan, pos_over_neg: 388.5057373046875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.8408, loss_val: nan, pos_over_neg: 292.2530822753906 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.8065, loss_val: nan, pos_over_neg: 323.7998046875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 328.4460754394531 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.8698, loss_val: nan, pos_over_neg: 216.33457946777344 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.859, loss_val: nan, pos_over_neg: 270.3245544433594 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.8522, loss_val: nan, pos_over_neg: 217.19802856445312 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.8523, loss_val: nan, pos_over_neg: 328.5987243652344 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.8364, loss_val: nan, pos_over_neg: 211.90573120117188 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.8611, loss_val: nan, pos_over_neg: 243.45094299316406 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.8128, loss_val: nan, pos_over_neg: 493.08306884765625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.8421, loss_val: nan, pos_over_neg: 224.13809204101562 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.8813, loss_val: nan, pos_over_neg: 199.79006958007812 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.8559, loss_val: nan, pos_over_neg: 212.1915740966797 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.8686, loss_val: nan, pos_over_neg: 176.79429626464844 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.8213, loss_val: nan, pos_over_neg: 335.45733642578125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.8455, loss_val: nan, pos_over_neg: 237.08631896972656 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.8888, loss_val: nan, pos_over_neg: 453.2814636230469 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.8657, loss_val: nan, pos_over_neg: 320.5154724121094 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.8762, loss_val: nan, pos_over_neg: 211.31211853027344 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.8594, loss_val: nan, pos_over_neg: 239.92645263671875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.8197, loss_val: nan, pos_over_neg: 202.3922119140625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 298.75750732421875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.8302, loss_val: nan, pos_over_neg: 302.88507080078125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.8163, loss_val: nan, pos_over_neg: 296.5511474609375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.8513, loss_val: nan, pos_over_neg: 203.35931396484375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.8506, loss_val: nan, pos_over_neg: 227.4951934814453 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.7867, loss_val: nan, pos_over_neg: 340.47186279296875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.8991, loss_val: nan, pos_over_neg: 252.70938110351562 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.886, loss_val: nan, pos_over_neg: 199.4976348876953 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.8188, loss_val: nan, pos_over_neg: 254.45877075195312 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.8992, loss_val: nan, pos_over_neg: 326.88641357421875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.8963, loss_val: nan, pos_over_neg: 235.77516174316406 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.8267, loss_val: nan, pos_over_neg: 311.3829040527344 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.831, loss_val: nan, pos_over_neg: 273.135498046875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.8541, loss_val: nan, pos_over_neg: 238.27215576171875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.8226, loss_val: nan, pos_over_neg: 315.7250061035156 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.8133, loss_val: nan, pos_over_neg: 249.11312866210938 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.8575, loss_val: nan, pos_over_neg: 389.5962219238281 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.842, loss_val: nan, pos_over_neg: 304.38787841796875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.926, loss_val: nan, pos_over_neg: 227.45477294921875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.8294, loss_val: nan, pos_over_neg: 262.76934814453125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.8523, loss_val: nan, pos_over_neg: 173.1874237060547 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.8356, loss_val: nan, pos_over_neg: 239.72079467773438 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.8936, loss_val: nan, pos_over_neg: 205.5712432861328 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.8758, loss_val: nan, pos_over_neg: 208.58973693847656 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.8553, loss_val: nan, pos_over_neg: 276.6075439453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.8232, loss_val: nan, pos_over_neg: 320.5782775878906 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.8603, loss_val: nan, pos_over_neg: 307.5253601074219 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.8359, loss_val: nan, pos_over_neg: 374.5881652832031 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.8506, loss_val: nan, pos_over_neg: 294.9326477050781 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.8178, loss_val: nan, pos_over_neg: 287.1100158691406 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.8498, loss_val: nan, pos_over_neg: 224.7838897705078 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.8637, loss_val: nan, pos_over_neg: 184.77745056152344 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.8697, loss_val: nan, pos_over_neg: 191.37924194335938 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.8339, loss_val: nan, pos_over_neg: 218.5263671875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.8294, loss_val: nan, pos_over_neg: 298.6062316894531 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.8053, loss_val: nan, pos_over_neg: 429.98486328125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.837, loss_val: nan, pos_over_neg: 424.6324157714844 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 290.4598083496094 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.8285, loss_val: nan, pos_over_neg: 335.6246337890625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.8588, loss_val: nan, pos_over_neg: 292.6851806640625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.8353, loss_val: nan, pos_over_neg: 266.9654541015625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.8726, loss_val: nan, pos_over_neg: 221.98019409179688 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.8714, loss_val: nan, pos_over_neg: 203.58116149902344 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.815, loss_val: nan, pos_over_neg: 244.20806884765625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.799, loss_val: nan, pos_over_neg: 312.7400817871094 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.844, loss_val: nan, pos_over_neg: 310.357666015625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.8589, loss_val: nan, pos_over_neg: 207.7249755859375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.833, loss_val: nan, pos_over_neg: 254.28143310546875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.8375, loss_val: nan, pos_over_neg: 269.45526123046875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.8651, loss_val: nan, pos_over_neg: 271.5517272949219 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.8561, loss_val: nan, pos_over_neg: 228.8964385986328 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.9103, loss_val: nan, pos_over_neg: 237.42474365234375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 243.324462890625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.8536, loss_val: nan, pos_over_neg: 247.3065948486328 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.8353, loss_val: nan, pos_over_neg: 217.66522216796875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.8435, loss_val: nan, pos_over_neg: 181.79339599609375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.9039, loss_val: nan, pos_over_neg: 163.3597412109375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.8415, loss_val: nan, pos_over_neg: 301.0270690917969 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.8527, loss_val: nan, pos_over_neg: 220.5707550048828 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.87, loss_val: nan, pos_over_neg: 208.77798461914062 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.8529, loss_val: nan, pos_over_neg: 367.3934631347656 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.8555, loss_val: nan, pos_over_neg: 339.5401611328125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.8366, loss_val: nan, pos_over_neg: 274.20849609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.8416, loss_val: nan, pos_over_neg: 221.42933654785156 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.877, loss_val: nan, pos_over_neg: 147.51100158691406 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.8512, loss_val: nan, pos_over_neg: 176.88253784179688 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.828, loss_val: nan, pos_over_neg: 207.30604553222656 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.8525, loss_val: nan, pos_over_neg: 155.77960205078125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 270.7065124511719 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.8213, loss_val: nan, pos_over_neg: 207.24331665039062 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.8264, loss_val: nan, pos_over_neg: 375.0809020996094 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.8207, loss_val: nan, pos_over_neg: 437.9550476074219 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.8166, loss_val: nan, pos_over_neg: 353.1620788574219 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.8238, loss_val: nan, pos_over_neg: 326.96087646484375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.8437, loss_val: nan, pos_over_neg: 257.6627502441406 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.8526, loss_val: nan, pos_over_neg: 262.5015869140625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.8302, loss_val: nan, pos_over_neg: 282.31695556640625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.881, loss_val: nan, pos_over_neg: 178.7660675048828 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.8564, loss_val: nan, pos_over_neg: 153.0628204345703 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.7785, loss_val: nan, pos_over_neg: 315.211181640625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.8021, loss_val: nan, pos_over_neg: 301.46514892578125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.8315, loss_val: nan, pos_over_neg: 347.5308532714844 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.8788, loss_val: nan, pos_over_neg: 333.0992736816406 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.8069, loss_val: nan, pos_over_neg: 275.15753173828125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.862, loss_val: nan, pos_over_neg: 246.35733032226562 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.8345, loss_val: nan, pos_over_neg: 243.4476776123047 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.8017, loss_val: nan, pos_over_neg: 245.40219116210938 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.8938, loss_val: nan, pos_over_neg: 184.9346466064453 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.8433, loss_val: nan, pos_over_neg: 226.58050537109375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.8194, loss_val: nan, pos_over_neg: 218.6371307373047 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.8395, loss_val: nan, pos_over_neg: 166.08865356445312 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.8026, loss_val: nan, pos_over_neg: 255.3095703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.8172, loss_val: nan, pos_over_neg: 341.3767395019531 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.8542, loss_val: nan, pos_over_neg: 332.3042297363281 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.8407, loss_val: nan, pos_over_neg: 227.87197875976562 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.7973, loss_val: nan, pos_over_neg: 182.76193237304688 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.8934, loss_val: nan, pos_over_neg: 272.251708984375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.8429, loss_val: nan, pos_over_neg: 278.6011962890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.8346, loss_val: nan, pos_over_neg: 199.96832275390625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.8719, loss_val: nan, pos_over_neg: 201.86036682128906 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.8093, loss_val: nan, pos_over_neg: 340.295166015625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 285.4422302246094 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.8062, loss_val: nan, pos_over_neg: 287.4851989746094 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 206.50328063964844 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.8518, loss_val: nan, pos_over_neg: 281.9948425292969 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.8139, loss_val: nan, pos_over_neg: 272.1289367675781 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.8153, loss_val: nan, pos_over_neg: 213.43313598632812 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 231.96011352539062 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.8563, loss_val: nan, pos_over_neg: 258.04498291015625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.8649, loss_val: nan, pos_over_neg: 217.12538146972656 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.7977, loss_val: nan, pos_over_neg: 319.786376953125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.8045, loss_val: nan, pos_over_neg: 489.8693542480469 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.846, loss_val: nan, pos_over_neg: 257.6734619140625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.8411, loss_val: nan, pos_over_neg: 336.2718811035156 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.8526, loss_val: nan, pos_over_neg: 256.4574279785156 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.836, loss_val: nan, pos_over_neg: 266.01708984375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.8396, loss_val: nan, pos_over_neg: 213.02496337890625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.8458, loss_val: nan, pos_over_neg: 263.99493408203125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.8887, loss_val: nan, pos_over_neg: 162.5700225830078 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.8356, loss_val: nan, pos_over_neg: 230.32115173339844 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.811, loss_val: nan, pos_over_neg: 307.0504150390625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.8233, loss_val: nan, pos_over_neg: 343.4734191894531 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.8619, loss_val: nan, pos_over_neg: 322.8238830566406 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.8184, loss_val: nan, pos_over_neg: 497.4403076171875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.8395, loss_val: nan, pos_over_neg: 392.6290588378906 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.8615, loss_val: nan, pos_over_neg: 489.1824951171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.8375, loss_val: nan, pos_over_neg: 189.76458740234375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.8637, loss_val: nan, pos_over_neg: 227.66517639160156 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.9068, loss_val: nan, pos_over_neg: 166.98886108398438 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.8083, loss_val: nan, pos_over_neg: 312.3795166015625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.9143, loss_val: nan, pos_over_neg: 216.9675750732422 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.8399, loss_val: nan, pos_over_neg: 312.49462890625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.799, loss_val: nan, pos_over_neg: 399.10870361328125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.8464, loss_val: nan, pos_over_neg: 382.8584899902344 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.8386, loss_val: nan, pos_over_neg: 292.8972473144531 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.8639, loss_val: nan, pos_over_neg: 241.0807647705078 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.8783, loss_val: nan, pos_over_neg: 266.61968994140625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.8371, loss_val: nan, pos_over_neg: 320.14361572265625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.8733, loss_val: nan, pos_over_neg: 268.08038330078125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.8758, loss_val: nan, pos_over_neg: 233.24908447265625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.8787, loss_val: nan, pos_over_neg: 234.65780639648438 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.8489, loss_val: nan, pos_over_neg: 329.5196838378906 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.8463, loss_val: nan, pos_over_neg: 261.51593017578125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.825, loss_val: nan, pos_over_neg: 252.4266357421875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.8489, loss_val: nan, pos_over_neg: 421.9867248535156 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.885, loss_val: nan, pos_over_neg: 448.2823791503906 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.8477, loss_val: nan, pos_over_neg: 367.55224609375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.8076, loss_val: nan, pos_over_neg: 293.8203125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.8609, loss_val: nan, pos_over_neg: 324.0709228515625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.8592, loss_val: nan, pos_over_neg: 305.8775634765625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.871, loss_val: nan, pos_over_neg: 327.4587097167969 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.8602, loss_val: nan, pos_over_neg: 308.3258056640625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.82, loss_val: nan, pos_over_neg: 306.7513732910156 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.8405, loss_val: nan, pos_over_neg: 434.6748352050781 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.7779, loss_val: nan, pos_over_neg: 388.64752197265625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.8625, loss_val: nan, pos_over_neg: 177.04844665527344 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.8118, loss_val: nan, pos_over_neg: 283.7088317871094 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.8397, loss_val: nan, pos_over_neg: 285.54473876953125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.8292, loss_val: nan, pos_over_neg: 333.83740234375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.8433, loss_val: nan, pos_over_neg: 282.97222900390625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.8237, loss_val: nan, pos_over_neg: 307.2719421386719 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.8121, loss_val: nan, pos_over_neg: 327.4861145019531 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.8221, loss_val: nan, pos_over_neg: 283.36279296875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.8401, loss_val: nan, pos_over_neg: 280.4598388671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.7868, loss_val: nan, pos_over_neg: 343.3614807128906 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.812, loss_val: nan, pos_over_neg: 285.5901794433594 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.8346, loss_val: nan, pos_over_neg: 244.04690551757812 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.8336, loss_val: nan, pos_over_neg: 223.780517578125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.8146, loss_val: nan, pos_over_neg: 386.97686767578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.8169, loss_val: nan, pos_over_neg: 270.85076904296875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.8387, loss_val: nan, pos_over_neg: 230.8587188720703 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.7903, loss_val: nan, pos_over_neg: 334.5086364746094 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.8224, loss_val: nan, pos_over_neg: 297.5584716796875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.8922, loss_val: nan, pos_over_neg: 203.1728515625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 196.016845703125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.8288, loss_val: nan, pos_over_neg: 270.282470703125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.8526, loss_val: nan, pos_over_neg: 263.68377685546875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.7971, loss_val: nan, pos_over_neg: 307.16217041015625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.8367, loss_val: nan, pos_over_neg: 232.15904235839844 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.8264, loss_val: nan, pos_over_neg: 388.0388488769531 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.905, loss_val: nan, pos_over_neg: 199.02401733398438 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.8617, loss_val: nan, pos_over_neg: 166.45986938476562 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.8054, loss_val: nan, pos_over_neg: 208.8103485107422 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.8287, loss_val: nan, pos_over_neg: 188.29541015625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.8298, loss_val: nan, pos_over_neg: 293.702392578125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.8855, loss_val: nan, pos_over_neg: 311.3549499511719 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.7719, loss_val: nan, pos_over_neg: 842.970947265625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.8635, loss_val: nan, pos_over_neg: 283.1932067871094 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.8161, loss_val: nan, pos_over_neg: 237.02784729003906 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.8853, loss_val: nan, pos_over_neg: 203.73915100097656 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.8453, loss_val: nan, pos_over_neg: 219.90818786621094 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.85, loss_val: nan, pos_over_neg: 171.53721618652344 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.8415, loss_val: nan, pos_over_neg: 219.75897216796875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.8207, loss_val: nan, pos_over_neg: 259.492919921875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.825, loss_val: nan, pos_over_neg: 216.85060119628906 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.8518, loss_val: nan, pos_over_neg: 204.17054748535156 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.8208, loss_val: nan, pos_over_neg: 199.2643585205078 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.8239, loss_val: nan, pos_over_neg: 287.756103515625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.8357, loss_val: nan, pos_over_neg: 395.15069580078125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.8032, loss_val: nan, pos_over_neg: 328.26690673828125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.7973, loss_val: nan, pos_over_neg: 383.6549072265625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.7849, loss_val: nan, pos_over_neg: 367.7019348144531 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.8223, loss_val: nan, pos_over_neg: 235.0341339111328 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.8438, loss_val: nan, pos_over_neg: 173.5829620361328 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.8439, loss_val: nan, pos_over_neg: 157.11952209472656 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.79, loss_val: nan, pos_over_neg: 277.8437805175781 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.8578, loss_val: nan, pos_over_neg: 195.07577514648438 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.8151, loss_val: nan, pos_over_neg: 303.1978454589844 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.9009, loss_val: nan, pos_over_neg: 241.37733459472656 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.8255, loss_val: nan, pos_over_neg: 329.58990478515625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.8152, loss_val: nan, pos_over_neg: 262.0706481933594 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.7894, loss_val: nan, pos_over_neg: 539.6552124023438 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.8055, loss_val: nan, pos_over_neg: 333.1564636230469 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.8367, loss_val: nan, pos_over_neg: 173.80975341796875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.8072, loss_val: nan, pos_over_neg: 185.5673370361328 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.7726, loss_val: nan, pos_over_neg: 381.8671569824219 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.807, loss_val: nan, pos_over_neg: 226.05995178222656 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.85, loss_val: nan, pos_over_neg: 210.5319061279297 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.8129, loss_val: nan, pos_over_neg: 295.9051818847656 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.803, loss_val: nan, pos_over_neg: 223.94329833984375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.8366, loss_val: nan, pos_over_neg: 185.5615692138672 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 229.63961791992188 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.7777, loss_val: nan, pos_over_neg: 261.7022705078125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 184.62127685546875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.8237, loss_val: nan, pos_over_neg: 197.86782836914062 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.7903, loss_val: nan, pos_over_neg: 193.24050903320312 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.8208, loss_val: nan, pos_over_neg: 256.10870361328125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.7787, loss_val: nan, pos_over_neg: 257.5866394042969 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.7923, loss_val: nan, pos_over_neg: 294.4319152832031 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.849, loss_val: nan, pos_over_neg: 215.27777099609375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.8191, loss_val: nan, pos_over_neg: 196.6386260986328 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.8038, loss_val: nan, pos_over_neg: 247.75192260742188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.8185, loss_val: nan, pos_over_neg: 211.24929809570312 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.8041, loss_val: nan, pos_over_neg: 244.29371643066406 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.7887, loss_val: nan, pos_over_neg: 226.0410919189453 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.8068, loss_val: nan, pos_over_neg: 227.53787231445312 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.817, loss_val: nan, pos_over_neg: 309.0514831542969 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.8652, loss_val: nan, pos_over_neg: 219.6771697998047 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.8067, loss_val: nan, pos_over_neg: 439.2980041503906 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 255.52989196777344 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.8337, loss_val: nan, pos_over_neg: 206.6256866455078 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.8499, loss_val: nan, pos_over_neg: 229.62533569335938 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.8542, loss_val: nan, pos_over_neg: 279.5398864746094 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.8333, loss_val: nan, pos_over_neg: 240.1730499267578 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.7881, loss_val: nan, pos_over_neg: 313.0172119140625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.8286, loss_val: nan, pos_over_neg: 263.21234130859375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.8005, loss_val: nan, pos_over_neg: 371.6527099609375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 330.530029296875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.7823, loss_val: nan, pos_over_neg: 347.8323974609375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 298.5451965332031 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.8732, loss_val: nan, pos_over_neg: 188.35458374023438 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.8096, loss_val: nan, pos_over_neg: 184.01824951171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 226.71774291992188 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.8025, loss_val: nan, pos_over_neg: 303.18023681640625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.8459, loss_val: nan, pos_over_neg: 199.54046630859375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.7966, loss_val: nan, pos_over_neg: 229.8966064453125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.9019, loss_val: nan, pos_over_neg: 183.6881866455078 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.8555, loss_val: nan, pos_over_neg: 239.1269989013672 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.7833, loss_val: nan, pos_over_neg: 387.37384033203125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.8179, loss_val: nan, pos_over_neg: 385.3443603515625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.8305, loss_val: nan, pos_over_neg: 280.4610595703125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.862, loss_val: nan, pos_over_neg: 182.69586181640625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.7854, loss_val: nan, pos_over_neg: 258.62396240234375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.7887, loss_val: nan, pos_over_neg: 242.28482055664062 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.8227, loss_val: nan, pos_over_neg: 209.74884033203125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.811, loss_val: nan, pos_over_neg: 246.64979553222656 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.8305, loss_val: nan, pos_over_neg: 275.01373291015625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.8371, loss_val: nan, pos_over_neg: 270.44940185546875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.8389, loss_val: nan, pos_over_neg: 253.05006408691406 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.8325, loss_val: nan, pos_over_neg: 300.074462890625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.8045, loss_val: nan, pos_over_neg: 307.67999267578125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.8542, loss_val: nan, pos_over_neg: 253.33758544921875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.8269, loss_val: nan, pos_over_neg: 322.88519287109375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.8136, loss_val: nan, pos_over_neg: 226.8080291748047 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.8275, loss_val: nan, pos_over_neg: 211.0928497314453 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.7983, loss_val: nan, pos_over_neg: 345.7970275878906 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.795, loss_val: nan, pos_over_neg: 350.5366516113281 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.7976, loss_val: nan, pos_over_neg: 276.9833068847656 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.8212, loss_val: nan, pos_over_neg: 233.31268310546875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.8154, loss_val: nan, pos_over_neg: 249.34156799316406 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.8631, loss_val: nan, pos_over_neg: 283.1585998535156 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.8582, loss_val: nan, pos_over_neg: 262.9666442871094 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.8267, loss_val: nan, pos_over_neg: 230.93589782714844 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.8452, loss_val: nan, pos_over_neg: 206.3364715576172 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.8521, loss_val: nan, pos_over_neg: 192.72787475585938 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.7923, loss_val: nan, pos_over_neg: 223.2559814453125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 238.55661010742188 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.8226, loss_val: nan, pos_over_neg: 260.21661376953125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.8707, loss_val: nan, pos_over_neg: 222.99087524414062 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.8253, loss_val: nan, pos_over_neg: 389.40948486328125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.8239, loss_val: nan, pos_over_neg: 504.7148742675781 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.8601, loss_val: nan, pos_over_neg: 526.0347290039062 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 252.392578125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.8183, loss_val: nan, pos_over_neg: 262.55889892578125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.8293, loss_val: nan, pos_over_neg: 228.3836669921875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.8505, loss_val: nan, pos_over_neg: 180.48301696777344 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.8223, loss_val: nan, pos_over_neg: 204.96571350097656 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.8485, loss_val: nan, pos_over_neg: 159.85549926757812 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.8491, loss_val: nan, pos_over_neg: 267.4423828125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.8291, loss_val: nan, pos_over_neg: 326.3599853515625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.8498, loss_val: nan, pos_over_neg: 359.40472412109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.7987, loss_val: nan, pos_over_neg: 500.03131103515625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.8357, loss_val: nan, pos_over_neg: 419.67474365234375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.8381, loss_val: nan, pos_over_neg: 627.7339477539062 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.8363, loss_val: nan, pos_over_neg: 380.55029296875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.8525, loss_val: nan, pos_over_neg: 232.91639709472656 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.8368, loss_val: nan, pos_over_neg: 208.60531616210938 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 175.93734741210938 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.8091, loss_val: nan, pos_over_neg: 289.83551025390625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.8488, loss_val: nan, pos_over_neg: 324.43310546875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 291.3175354003906 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.8487, loss_val: nan, pos_over_neg: 304.8495788574219 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 356.0062255859375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.807, loss_val: nan, pos_over_neg: 276.0626220703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.8345, loss_val: nan, pos_over_neg: 273.3647155761719 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.7693, loss_val: nan, pos_over_neg: 380.62884521484375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.8116, loss_val: nan, pos_over_neg: 397.0034484863281 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.8353, loss_val: nan, pos_over_neg: 297.4153137207031 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.8004, loss_val: nan, pos_over_neg: 326.423828125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.8173, loss_val: nan, pos_over_neg: 332.995849609375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.8326, loss_val: nan, pos_over_neg: 228.9451141357422 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.8153, loss_val: nan, pos_over_neg: 444.3958435058594 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.829, loss_val: nan, pos_over_neg: 324.2630310058594 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.8478, loss_val: nan, pos_over_neg: 343.6329650878906 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 355.40936279296875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.785, loss_val: nan, pos_over_neg: 272.90838623046875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.815, loss_val: nan, pos_over_neg: 293.57965087890625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.8062, loss_val: nan, pos_over_neg: 316.0260925292969 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.8075, loss_val: nan, pos_over_neg: 248.75099182128906 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.7651, loss_val: nan, pos_over_neg: 323.973876953125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.8557, loss_val: nan, pos_over_neg: 329.87298583984375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.7785, loss_val: nan, pos_over_neg: 333.210693359375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.8138, loss_val: nan, pos_over_neg: 227.9231719970703 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.8432, loss_val: nan, pos_over_neg: 233.5361328125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.8552, loss_val: nan, pos_over_neg: 247.54727172851562 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.842, loss_val: nan, pos_over_neg: 284.0240173339844 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.7775, loss_val: nan, pos_over_neg: 413.12255859375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.8671, loss_val: nan, pos_over_neg: 192.3709716796875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.8324, loss_val: nan, pos_over_neg: 200.01150512695312 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.8383, loss_val: nan, pos_over_neg: 217.473876953125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.7987, loss_val: nan, pos_over_neg: 232.0354766845703 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.8169, loss_val: nan, pos_over_neg: 300.15557861328125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.8221, loss_val: nan, pos_over_neg: 404.8358459472656 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.7597, loss_val: nan, pos_over_neg: 439.8192138671875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.7597, loss_val: nan, pos_over_neg: 301.65130615234375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.8076, loss_val: nan, pos_over_neg: 245.74624633789062 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.821, loss_val: nan, pos_over_neg: 267.930419921875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.841, loss_val: nan, pos_over_neg: 192.82614135742188 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.818, loss_val: nan, pos_over_neg: 277.51409912109375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.8753, loss_val: nan, pos_over_neg: 196.14678955078125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.803, loss_val: nan, pos_over_neg: 234.87667846679688 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.8377, loss_val: nan, pos_over_neg: 271.43731689453125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.8217, loss_val: nan, pos_over_neg: 314.2900390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.8229, loss_val: nan, pos_over_neg: 305.1815185546875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.8267, loss_val: nan, pos_over_neg: 482.7270202636719 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.8155, loss_val: nan, pos_over_neg: 295.3874206542969 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.8125, loss_val: nan, pos_over_neg: 212.42147827148438 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.8349, loss_val: nan, pos_over_neg: 316.2769775390625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.7941, loss_val: nan, pos_over_neg: 406.8260803222656 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.8017, loss_val: nan, pos_over_neg: 229.00628662109375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.8095, loss_val: nan, pos_over_neg: 296.69036865234375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.8008, loss_val: nan, pos_over_neg: 269.49005126953125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.8092, loss_val: nan, pos_over_neg: 221.0374755859375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.8185, loss_val: nan, pos_over_neg: 216.73098754882812 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.8223, loss_val: nan, pos_over_neg: 275.7848815917969 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.8287, loss_val: nan, pos_over_neg: 252.07476806640625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.8197, loss_val: nan, pos_over_neg: 489.53424072265625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.8101, loss_val: nan, pos_over_neg: 306.4071960449219 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.8244, loss_val: nan, pos_over_neg: 381.5764465332031 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.8658, loss_val: nan, pos_over_neg: 264.5736389160156 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.8115, loss_val: nan, pos_over_neg: 382.2082824707031 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.7575, loss_val: nan, pos_over_neg: 631.1054077148438 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.7814, loss_val: nan, pos_over_neg: 268.9562072753906 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.8188, loss_val: nan, pos_over_neg: 244.8085174560547 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.7968, loss_val: nan, pos_over_neg: 334.2288818359375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.8044, loss_val: nan, pos_over_neg: 257.08984375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.8316, loss_val: nan, pos_over_neg: 251.0112762451172 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.8434, loss_val: nan, pos_over_neg: 338.77606201171875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.8326, loss_val: nan, pos_over_neg: 264.7123107910156 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.8067, loss_val: nan, pos_over_neg: 385.99298095703125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.7718, loss_val: nan, pos_over_neg: 534.3185424804688 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.8054, loss_val: nan, pos_over_neg: 531.8067016601562 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.8059, loss_val: nan, pos_over_neg: 266.52423095703125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.7977, loss_val: nan, pos_over_neg: 237.51898193359375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.7501, loss_val: nan, pos_over_neg: 309.2857971191406 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.8152, loss_val: nan, pos_over_neg: 253.6996612548828 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.7985, loss_val: nan, pos_over_neg: 226.75514221191406 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.8042, loss_val: nan, pos_over_neg: 291.687744140625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.8205, loss_val: nan, pos_over_neg: 421.4762878417969 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.8838, loss_val: nan, pos_over_neg: 230.43756103515625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.7784, loss_val: nan, pos_over_neg: 278.65093994140625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.8462, loss_val: nan, pos_over_neg: 616.7770385742188 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.7971, loss_val: nan, pos_over_neg: 339.1677551269531 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.7863, loss_val: nan, pos_over_neg: 335.2221984863281 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.8323, loss_val: nan, pos_over_neg: 195.85276794433594 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.8296, loss_val: nan, pos_over_neg: 197.39529418945312 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.8093, loss_val: nan, pos_over_neg: 251.94119262695312 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.8226, loss_val: nan, pos_over_neg: 211.818115234375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.8553, loss_val: nan, pos_over_neg: 236.53395080566406 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.7746, loss_val: nan, pos_over_neg: 401.3123779296875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.7704, loss_val: nan, pos_over_neg: 392.592529296875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.8189, loss_val: nan, pos_over_neg: 333.41876220703125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.812, loss_val: nan, pos_over_neg: 213.79678344726562 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.8058, loss_val: nan, pos_over_neg: 319.5152282714844 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.788, loss_val: nan, pos_over_neg: 351.2662353515625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 214.7406463623047 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.7859, loss_val: nan, pos_over_neg: 286.736328125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.7853, loss_val: nan, pos_over_neg: 277.26507568359375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.82, loss_val: nan, pos_over_neg: 260.74554443359375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.8388, loss_val: nan, pos_over_neg: 230.37619018554688 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.878, loss_val: nan, pos_over_neg: 244.82510375976562 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.8564, loss_val: nan, pos_over_neg: 366.33056640625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.7889, loss_val: nan, pos_over_neg: 492.93341064453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.7815, loss_val: nan, pos_over_neg: 334.52691650390625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.8299, loss_val: nan, pos_over_neg: 229.69464111328125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.8097, loss_val: nan, pos_over_neg: 368.5022888183594 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.7689, loss_val: nan, pos_over_neg: 360.2242736816406 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.8185, loss_val: nan, pos_over_neg: 291.8673400878906 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.8081, loss_val: nan, pos_over_neg: 281.55230712890625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.8084, loss_val: nan, pos_over_neg: 231.33709716796875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.8234, loss_val: nan, pos_over_neg: 218.33480834960938 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.8285, loss_val: nan, pos_over_neg: 187.3742218017578 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.8651, loss_val: nan, pos_over_neg: 238.22731018066406 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.814, loss_val: nan, pos_over_neg: 382.0672912597656 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.8219, loss_val: nan, pos_over_neg: 289.5968322753906 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.8669, loss_val: nan, pos_over_neg: 215.62380981445312 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.799, loss_val: nan, pos_over_neg: 237.4386749267578 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.7982, loss_val: nan, pos_over_neg: 257.3575134277344 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.8046, loss_val: nan, pos_over_neg: 180.44940185546875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.7647, loss_val: nan, pos_over_neg: 235.47952270507812 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.7598, loss_val: nan, pos_over_neg: 335.26165771484375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.7917, loss_val: nan, pos_over_neg: 227.70089721679688 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.8136, loss_val: nan, pos_over_neg: 279.851318359375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.8669, loss_val: nan, pos_over_neg: 299.751220703125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.7814, loss_val: nan, pos_over_neg: 414.8132019042969 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [45:33<75647:01:19, 907.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.8427, loss_val: nan, pos_over_neg: 289.8622131347656 lr: 0.00031623\n",
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 3.794, loss_val: nan, pos_over_neg: 295.88214111328125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.7838, loss_val: nan, pos_over_neg: 302.9064025878906 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 213.50436401367188 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.819, loss_val: nan, pos_over_neg: 196.8424835205078 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.8313, loss_val: nan, pos_over_neg: 268.9628601074219 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.8448, loss_val: nan, pos_over_neg: 272.85076904296875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.8047, loss_val: nan, pos_over_neg: 288.224365234375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.8382, loss_val: nan, pos_over_neg: 229.23497009277344 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.7792, loss_val: nan, pos_over_neg: 311.0002136230469 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.824, loss_val: nan, pos_over_neg: 435.96453857421875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.778, loss_val: nan, pos_over_neg: 338.532470703125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.8072, loss_val: nan, pos_over_neg: 182.05709838867188 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.7748, loss_val: nan, pos_over_neg: 243.0225067138672 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.7835, loss_val: nan, pos_over_neg: 410.4554138183594 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.813, loss_val: nan, pos_over_neg: 198.419189453125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.7459, loss_val: nan, pos_over_neg: 242.34841918945312 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.8023, loss_val: nan, pos_over_neg: 252.21080017089844 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.8589, loss_val: nan, pos_over_neg: 245.53453063964844 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.8358, loss_val: nan, pos_over_neg: 235.1217498779297 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.7897, loss_val: nan, pos_over_neg: 376.8635559082031 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.7961, loss_val: nan, pos_over_neg: 480.657470703125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.7511, loss_val: nan, pos_over_neg: 564.3472290039062 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.751, loss_val: nan, pos_over_neg: 387.5786437988281 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.8417, loss_val: nan, pos_over_neg: 253.80889892578125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.8256, loss_val: nan, pos_over_neg: 208.60107421875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.786, loss_val: nan, pos_over_neg: 356.7837219238281 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 294.55792236328125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.8057, loss_val: nan, pos_over_neg: 418.5142822265625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.8331, loss_val: nan, pos_over_neg: 329.3244934082031 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.7819, loss_val: nan, pos_over_neg: 384.3797607421875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.8523, loss_val: nan, pos_over_neg: 272.92034912109375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.8174, loss_val: nan, pos_over_neg: 198.563720703125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.7953, loss_val: nan, pos_over_neg: 248.86376953125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 379.7228698730469 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.8828, loss_val: nan, pos_over_neg: 256.9696044921875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.7746, loss_val: nan, pos_over_neg: 336.7984924316406 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.8251, loss_val: nan, pos_over_neg: 279.4476318359375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.7802, loss_val: nan, pos_over_neg: 304.4076232910156 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.8435, loss_val: nan, pos_over_neg: 214.85418701171875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.8389, loss_val: nan, pos_over_neg: 211.34872436523438 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.8233, loss_val: nan, pos_over_neg: 220.65760803222656 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.8006, loss_val: nan, pos_over_neg: 294.6739501953125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 291.0843811035156 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.7854, loss_val: nan, pos_over_neg: 296.1626281738281 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 358.6330871582031 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 858.52392578125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 253.12832641601562 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.8164, loss_val: nan, pos_over_neg: 191.6322021484375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.8227, loss_val: nan, pos_over_neg: 212.7113494873047 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.7498, loss_val: nan, pos_over_neg: 359.853515625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.7891, loss_val: nan, pos_over_neg: 366.140869140625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.7707, loss_val: nan, pos_over_neg: 390.443359375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.8235, loss_val: nan, pos_over_neg: 251.13816833496094 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.8796, loss_val: nan, pos_over_neg: 213.45748901367188 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.7883, loss_val: nan, pos_over_neg: 266.69195556640625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.8619, loss_val: nan, pos_over_neg: 220.4544677734375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.8224, loss_val: nan, pos_over_neg: 325.2600402832031 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.787, loss_val: nan, pos_over_neg: 458.29168701171875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.7988, loss_val: nan, pos_over_neg: 258.0777282714844 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.819, loss_val: nan, pos_over_neg: 349.9372863769531 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.8186, loss_val: nan, pos_over_neg: 464.8681335449219 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 312.142578125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 269.0358581542969 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.8083, loss_val: nan, pos_over_neg: 259.0728759765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.7842, loss_val: nan, pos_over_neg: 320.6876220703125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.83, loss_val: nan, pos_over_neg: 206.01893615722656 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.7803, loss_val: nan, pos_over_neg: 249.10040283203125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.8122, loss_val: nan, pos_over_neg: 281.44549560546875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.8354, loss_val: nan, pos_over_neg: 228.85301208496094 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.8069, loss_val: nan, pos_over_neg: 447.55615234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.8365, loss_val: nan, pos_over_neg: 359.2591247558594 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.8643, loss_val: nan, pos_over_neg: 264.53533935546875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.8038, loss_val: nan, pos_over_neg: 317.8584289550781 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.7864, loss_val: nan, pos_over_neg: 284.2900695800781 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.8094, loss_val: nan, pos_over_neg: 321.2665710449219 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.8156, loss_val: nan, pos_over_neg: 252.88824462890625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.8447, loss_val: nan, pos_over_neg: 300.9579772949219 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.775, loss_val: nan, pos_over_neg: 286.6335144042969 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.8115, loss_val: nan, pos_over_neg: 268.72515869140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.7587, loss_val: nan, pos_over_neg: 291.0101013183594 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.7551, loss_val: nan, pos_over_neg: 201.3573760986328 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.7977, loss_val: nan, pos_over_neg: 210.72479248046875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.8017, loss_val: nan, pos_over_neg: 215.72962951660156 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.7679, loss_val: nan, pos_over_neg: 221.93026733398438 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.8004, loss_val: nan, pos_over_neg: 309.4131774902344 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.8033, loss_val: nan, pos_over_neg: 325.7950744628906 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.7542, loss_val: nan, pos_over_neg: 311.0486145019531 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 469.1271667480469 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.8208, loss_val: nan, pos_over_neg: 460.9183654785156 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.8104, loss_val: nan, pos_over_neg: 251.84620666503906 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.8018, loss_val: nan, pos_over_neg: 207.38356018066406 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.7998, loss_val: nan, pos_over_neg: 178.6997528076172 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.7769, loss_val: nan, pos_over_neg: 277.3250427246094 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.7705, loss_val: nan, pos_over_neg: 266.1351013183594 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.7781, loss_val: nan, pos_over_neg: 241.36595153808594 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.8179, loss_val: nan, pos_over_neg: 225.25762939453125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.7998, loss_val: nan, pos_over_neg: 283.2557678222656 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.7844, loss_val: nan, pos_over_neg: 274.9494934082031 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.7507, loss_val: nan, pos_over_neg: 332.6349182128906 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.7648, loss_val: nan, pos_over_neg: 403.9270935058594 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.8054, loss_val: nan, pos_over_neg: 174.5106658935547 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.8, loss_val: nan, pos_over_neg: 227.9585723876953 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.7948, loss_val: nan, pos_over_neg: 254.23817443847656 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.7673, loss_val: nan, pos_over_neg: 326.3268737792969 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 275.1398010253906 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.8567, loss_val: nan, pos_over_neg: 279.6637268066406 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 239.29391479492188 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.7751, loss_val: nan, pos_over_neg: 279.36944580078125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.7957, loss_val: nan, pos_over_neg: 191.7875213623047 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.8171, loss_val: nan, pos_over_neg: 175.55604553222656 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.8113, loss_val: nan, pos_over_neg: 215.69155883789062 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.8148, loss_val: nan, pos_over_neg: 263.70904541015625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.8043, loss_val: nan, pos_over_neg: 267.5162048339844 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.8072, loss_val: nan, pos_over_neg: 240.46351623535156 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.7936, loss_val: nan, pos_over_neg: 341.1548156738281 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.7701, loss_val: nan, pos_over_neg: 299.2070617675781 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.7984, loss_val: nan, pos_over_neg: 488.2325134277344 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 636.6417846679688 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.7985, loss_val: nan, pos_over_neg: 238.93832397460938 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.868, loss_val: nan, pos_over_neg: 255.94207763671875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.7918, loss_val: nan, pos_over_neg: 317.1543273925781 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 245.32933044433594 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.8123, loss_val: nan, pos_over_neg: 237.76280212402344 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.8403, loss_val: nan, pos_over_neg: 147.58457946777344 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.822, loss_val: nan, pos_over_neg: 212.15560913085938 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.8312, loss_val: nan, pos_over_neg: 296.8253479003906 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.7822, loss_val: nan, pos_over_neg: 378.8669738769531 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.7704, loss_val: nan, pos_over_neg: 364.6276550292969 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.7885, loss_val: nan, pos_over_neg: 267.5802001953125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.7849, loss_val: nan, pos_over_neg: 297.3738708496094 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.8016, loss_val: nan, pos_over_neg: 392.5330505371094 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.7256, loss_val: nan, pos_over_neg: 542.2239990234375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.7682, loss_val: nan, pos_over_neg: 302.55792236328125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.8254, loss_val: nan, pos_over_neg: 260.6675109863281 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.8033, loss_val: nan, pos_over_neg: 312.9090576171875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.7648, loss_val: nan, pos_over_neg: 280.3392639160156 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.7606, loss_val: nan, pos_over_neg: 358.64654541015625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.7645, loss_val: nan, pos_over_neg: 505.54400634765625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.8008, loss_val: nan, pos_over_neg: 249.768798828125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.7159, loss_val: nan, pos_over_neg: 250.24832153320312 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.7651, loss_val: nan, pos_over_neg: 319.7049865722656 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.7796, loss_val: nan, pos_over_neg: 304.6645812988281 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.7733, loss_val: nan, pos_over_neg: 271.0357666015625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.7605, loss_val: nan, pos_over_neg: 247.8632354736328 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.7532, loss_val: nan, pos_over_neg: 240.234619140625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.7699, loss_val: nan, pos_over_neg: 264.609130859375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.745, loss_val: nan, pos_over_neg: 308.1875305175781 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.8158, loss_val: nan, pos_over_neg: 149.9822998046875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.8166, loss_val: nan, pos_over_neg: 211.09083557128906 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.8107, loss_val: nan, pos_over_neg: 352.5204772949219 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.7745, loss_val: nan, pos_over_neg: 531.9242553710938 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.7391, loss_val: nan, pos_over_neg: 470.54541015625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.7846, loss_val: nan, pos_over_neg: 294.3782043457031 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.8203, loss_val: nan, pos_over_neg: 279.4535217285156 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.774, loss_val: nan, pos_over_neg: 245.24514770507812 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.8268, loss_val: nan, pos_over_neg: 179.7133026123047 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.7885, loss_val: nan, pos_over_neg: 247.1866455078125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.7887, loss_val: nan, pos_over_neg: 307.0576477050781 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.7984, loss_val: nan, pos_over_neg: 328.35906982421875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.7956, loss_val: nan, pos_over_neg: 296.9027099609375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.801, loss_val: nan, pos_over_neg: 318.8642883300781 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.7809, loss_val: nan, pos_over_neg: 399.603759765625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.8164, loss_val: nan, pos_over_neg: 250.13636779785156 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.8181, loss_val: nan, pos_over_neg: 225.49520874023438 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.7949, loss_val: nan, pos_over_neg: 343.754150390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.7644, loss_val: nan, pos_over_neg: 309.33355712890625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.771, loss_val: nan, pos_over_neg: 304.63372802734375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 379.95501708984375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.8286, loss_val: nan, pos_over_neg: 320.77734375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.7837, loss_val: nan, pos_over_neg: 237.4208526611328 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.8441, loss_val: nan, pos_over_neg: 270.76239013671875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.7486, loss_val: nan, pos_over_neg: 436.3339538574219 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.8139, loss_val: nan, pos_over_neg: 221.4510498046875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.8253, loss_val: nan, pos_over_neg: 256.2939758300781 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.8285, loss_val: nan, pos_over_neg: 272.687744140625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.7713, loss_val: nan, pos_over_neg: 336.9833984375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.8495, loss_val: nan, pos_over_neg: 211.77670288085938 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.7959, loss_val: nan, pos_over_neg: 387.15264892578125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.7664, loss_val: nan, pos_over_neg: 535.6737060546875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.8378, loss_val: nan, pos_over_neg: 300.1812438964844 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.7893, loss_val: nan, pos_over_neg: 257.417236328125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.8202, loss_val: nan, pos_over_neg: 347.7665710449219 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.7793, loss_val: nan, pos_over_neg: 391.2130126953125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.8324, loss_val: nan, pos_over_neg: 282.51837158203125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.7777, loss_val: nan, pos_over_neg: 248.89027404785156 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.7391, loss_val: nan, pos_over_neg: 237.38900756835938 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.8126, loss_val: nan, pos_over_neg: 214.53781127929688 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 257.821044921875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.8098, loss_val: nan, pos_over_neg: 389.0534973144531 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.739, loss_val: nan, pos_over_neg: 402.9608154296875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.8002, loss_val: nan, pos_over_neg: 271.1934509277344 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.8408, loss_val: nan, pos_over_neg: 264.38104248046875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.7415, loss_val: nan, pos_over_neg: 484.9814758300781 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.7448, loss_val: nan, pos_over_neg: 478.40594482421875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.7338, loss_val: nan, pos_over_neg: 292.0966491699219 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.806, loss_val: nan, pos_over_neg: 234.84237670898438 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.7606, loss_val: nan, pos_over_neg: 329.30474853515625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.8, loss_val: nan, pos_over_neg: 254.36285400390625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.7822, loss_val: nan, pos_over_neg: 183.5077362060547 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.7712, loss_val: nan, pos_over_neg: 210.77334594726562 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.81, loss_val: nan, pos_over_neg: 177.22996520996094 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.8612, loss_val: nan, pos_over_neg: 262.1737976074219 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.7623, loss_val: nan, pos_over_neg: 275.9133605957031 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.7834, loss_val: nan, pos_over_neg: 281.457763671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.7617, loss_val: nan, pos_over_neg: 460.2453308105469 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.8298, loss_val: nan, pos_over_neg: 228.78094482421875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.8127, loss_val: nan, pos_over_neg: 164.52528381347656 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.8212, loss_val: nan, pos_over_neg: 286.72711181640625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 297.5798034667969 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.7482, loss_val: nan, pos_over_neg: 430.5473937988281 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.8043, loss_val: nan, pos_over_neg: 247.12847900390625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.7857, loss_val: nan, pos_over_neg: 258.2945251464844 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 320.9974060058594 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.7767, loss_val: nan, pos_over_neg: 375.3151550292969 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.8115, loss_val: nan, pos_over_neg: 361.3353271484375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.7874, loss_val: nan, pos_over_neg: 245.71920776367188 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.8432, loss_val: nan, pos_over_neg: 240.2360076904297 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 245.72068786621094 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.8024, loss_val: nan, pos_over_neg: 250.6623992919922 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 241.43292236328125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.8067, loss_val: nan, pos_over_neg: 312.4429626464844 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.7489, loss_val: nan, pos_over_neg: 241.93280029296875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.8073, loss_val: nan, pos_over_neg: 271.3479309082031 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.7882, loss_val: nan, pos_over_neg: 379.96649169921875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.7859, loss_val: nan, pos_over_neg: 319.22650146484375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.8062, loss_val: nan, pos_over_neg: 198.9276885986328 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.7949, loss_val: nan, pos_over_neg: 266.47186279296875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.7727, loss_val: nan, pos_over_neg: 270.2079772949219 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.7576, loss_val: nan, pos_over_neg: 284.0656433105469 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.7625, loss_val: nan, pos_over_neg: 248.28697204589844 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.8325, loss_val: nan, pos_over_neg: 232.31158447265625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.8176, loss_val: nan, pos_over_neg: 289.9819030761719 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.7661, loss_val: nan, pos_over_neg: 408.676025390625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.7866, loss_val: nan, pos_over_neg: 316.3757629394531 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.8074, loss_val: nan, pos_over_neg: 251.02708435058594 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.7347, loss_val: nan, pos_over_neg: 396.5458068847656 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.8359, loss_val: nan, pos_over_neg: 447.56707763671875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.7674, loss_val: nan, pos_over_neg: 574.3699951171875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.7849, loss_val: nan, pos_over_neg: 259.245361328125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.7705, loss_val: nan, pos_over_neg: 240.23458862304688 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.7736, loss_val: nan, pos_over_neg: 207.2765655517578 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.8251, loss_val: nan, pos_over_neg: 172.97463989257812 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.7959, loss_val: nan, pos_over_neg: 200.54730224609375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.8097, loss_val: nan, pos_over_neg: 233.15756225585938 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.7968, loss_val: nan, pos_over_neg: 314.14410400390625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.7715, loss_val: nan, pos_over_neg: 389.4173889160156 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.7952, loss_val: nan, pos_over_neg: 314.06463623046875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.8171, loss_val: nan, pos_over_neg: 270.8255920410156 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.7855, loss_val: nan, pos_over_neg: 217.5838623046875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.8133, loss_val: nan, pos_over_neg: 269.13720703125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.7554, loss_val: nan, pos_over_neg: 297.1560363769531 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.8089, loss_val: nan, pos_over_neg: 186.75660705566406 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.846, loss_val: nan, pos_over_neg: 165.90866088867188 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.7707, loss_val: nan, pos_over_neg: 283.3751525878906 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.7779, loss_val: nan, pos_over_neg: 262.7146301269531 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.8126, loss_val: nan, pos_over_neg: 280.31182861328125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.7886, loss_val: nan, pos_over_neg: 261.18621826171875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.8227, loss_val: nan, pos_over_neg: 258.8714599609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.8143, loss_val: nan, pos_over_neg: 168.09976196289062 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.8235, loss_val: nan, pos_over_neg: 241.03628540039062 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.7485, loss_val: nan, pos_over_neg: 363.15643310546875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.781, loss_val: nan, pos_over_neg: 326.0255432128906 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.8272, loss_val: nan, pos_over_neg: 200.5943145751953 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.79, loss_val: nan, pos_over_neg: 225.79296875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.7734, loss_val: nan, pos_over_neg: 295.22314453125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.7752, loss_val: nan, pos_over_neg: 290.6461181640625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.7218, loss_val: nan, pos_over_neg: 399.4794006347656 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.792, loss_val: nan, pos_over_neg: 286.8309631347656 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.7901, loss_val: nan, pos_over_neg: 446.45806884765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.8005, loss_val: nan, pos_over_neg: 329.7384033203125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.7863, loss_val: nan, pos_over_neg: 373.6593017578125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.7617, loss_val: nan, pos_over_neg: 367.5206298828125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.7779, loss_val: nan, pos_over_neg: 308.5648193359375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.7595, loss_val: nan, pos_over_neg: 374.75775146484375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.7545, loss_val: nan, pos_over_neg: 297.8882751464844 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.822, loss_val: nan, pos_over_neg: 347.5898742675781 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.8445, loss_val: nan, pos_over_neg: 310.3875427246094 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.8035, loss_val: nan, pos_over_neg: 313.65814208984375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.7589, loss_val: nan, pos_over_neg: 376.10162353515625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.785, loss_val: nan, pos_over_neg: 315.1599426269531 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.7603, loss_val: nan, pos_over_neg: 594.9396362304688 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.8085, loss_val: nan, pos_over_neg: 281.9167785644531 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 271.3182067871094 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 339.51666259765625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.8099, loss_val: nan, pos_over_neg: 339.4482421875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.7926, loss_val: nan, pos_over_neg: 411.470458984375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.7544, loss_val: nan, pos_over_neg: 418.4507751464844 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.8383, loss_val: nan, pos_over_neg: 184.87110900878906 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.8311, loss_val: nan, pos_over_neg: 263.89044189453125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.7639, loss_val: nan, pos_over_neg: 307.6481628417969 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.7624, loss_val: nan, pos_over_neg: 285.79644775390625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.7755, loss_val: nan, pos_over_neg: 203.45220947265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.8127, loss_val: nan, pos_over_neg: 222.3380889892578 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.7672, loss_val: nan, pos_over_neg: 333.5277099609375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.7702, loss_val: nan, pos_over_neg: 372.8938293457031 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.7823, loss_val: nan, pos_over_neg: 350.0481262207031 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.8034, loss_val: nan, pos_over_neg: 278.88232421875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.7654, loss_val: nan, pos_over_neg: 311.9019775390625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 347.2854919433594 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.7973, loss_val: nan, pos_over_neg: 244.15829467773438 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.7641, loss_val: nan, pos_over_neg: 273.1832275390625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.7632, loss_val: nan, pos_over_neg: 283.8975830078125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.7917, loss_val: nan, pos_over_neg: 309.75555419921875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.7586, loss_val: nan, pos_over_neg: 400.6754455566406 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.8083, loss_val: nan, pos_over_neg: 202.7401885986328 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.7745, loss_val: nan, pos_over_neg: 357.78717041015625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.7964, loss_val: nan, pos_over_neg: 258.4133605957031 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.7691, loss_val: nan, pos_over_neg: 257.151611328125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.7726, loss_val: nan, pos_over_neg: 368.3163757324219 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.7658, loss_val: nan, pos_over_neg: 505.21234130859375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.8125, loss_val: nan, pos_over_neg: 233.7439422607422 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.7713, loss_val: nan, pos_over_neg: 256.2889099121094 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.7816, loss_val: nan, pos_over_neg: 271.5064697265625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.8, loss_val: nan, pos_over_neg: 247.2621307373047 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 270.56646728515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.8241, loss_val: nan, pos_over_neg: 228.50836181640625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.7986, loss_val: nan, pos_over_neg: 345.724853515625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.7955, loss_val: nan, pos_over_neg: 243.77520751953125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 307.6580810546875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.825, loss_val: nan, pos_over_neg: 481.7354736328125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.7496, loss_val: nan, pos_over_neg: 428.143798828125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.8821, loss_val: nan, pos_over_neg: 215.45606994628906 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.761, loss_val: nan, pos_over_neg: 333.8077087402344 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.7805, loss_val: nan, pos_over_neg: 215.97584533691406 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.8052, loss_val: nan, pos_over_neg: 174.7040557861328 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.7672, loss_val: nan, pos_over_neg: 271.9977111816406 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.8274, loss_val: nan, pos_over_neg: 210.8756103515625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 207.19192504882812 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.832, loss_val: nan, pos_over_neg: 256.5815734863281 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.8193, loss_val: nan, pos_over_neg: 445.1197204589844 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.7622, loss_val: nan, pos_over_neg: 465.4480895996094 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.8441, loss_val: nan, pos_over_neg: 230.4772186279297 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.7638, loss_val: nan, pos_over_neg: 270.8027648925781 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.7579, loss_val: nan, pos_over_neg: 204.3052520751953 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.7648, loss_val: nan, pos_over_neg: 198.38157653808594 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.8134, loss_val: nan, pos_over_neg: 225.7476043701172 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.7747, loss_val: nan, pos_over_neg: 308.2073974609375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.8103, loss_val: nan, pos_over_neg: 188.35464477539062 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.7823, loss_val: nan, pos_over_neg: 195.78419494628906 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.8056, loss_val: nan, pos_over_neg: 288.93634033203125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.8316, loss_val: nan, pos_over_neg: 261.5552673339844 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.7863, loss_val: nan, pos_over_neg: 242.4186553955078 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.7756, loss_val: nan, pos_over_neg: 235.4467010498047 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.7996, loss_val: nan, pos_over_neg: 302.410400390625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.7797, loss_val: nan, pos_over_neg: 257.27960205078125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.7769, loss_val: nan, pos_over_neg: 258.7266540527344 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.7502, loss_val: nan, pos_over_neg: 251.35545349121094 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.7789, loss_val: nan, pos_over_neg: 350.9972229003906 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.8117, loss_val: nan, pos_over_neg: 228.8565216064453 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 237.14390563964844 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.7208, loss_val: nan, pos_over_neg: 301.0207824707031 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.7627, loss_val: nan, pos_over_neg: 284.0831604003906 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.7681, loss_val: nan, pos_over_neg: 287.3839416503906 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.8292, loss_val: nan, pos_over_neg: 192.12705993652344 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.7852, loss_val: nan, pos_over_neg: 224.67115783691406 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.7685, loss_val: nan, pos_over_neg: 227.81492614746094 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.7883, loss_val: nan, pos_over_neg: 236.45359802246094 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.7396, loss_val: nan, pos_over_neg: 390.2938537597656 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.756, loss_val: nan, pos_over_neg: 462.27655029296875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.7595, loss_val: nan, pos_over_neg: 691.5354614257812 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.8144, loss_val: nan, pos_over_neg: 332.1661376953125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.7821, loss_val: nan, pos_over_neg: 413.57928466796875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.8199, loss_val: nan, pos_over_neg: 301.7701721191406 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.758, loss_val: nan, pos_over_neg: 281.6473388671875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.7315, loss_val: nan, pos_over_neg: 346.1054992675781 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.8032, loss_val: nan, pos_over_neg: 209.7632598876953 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.769, loss_val: nan, pos_over_neg: 252.42152404785156 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.7506, loss_val: nan, pos_over_neg: 244.40054321289062 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.7558, loss_val: nan, pos_over_neg: 293.61492919921875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.8399, loss_val: nan, pos_over_neg: 222.34800720214844 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 279.96038818359375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.7517, loss_val: nan, pos_over_neg: 242.16036987304688 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.7617, loss_val: nan, pos_over_neg: 464.1312561035156 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.7454, loss_val: nan, pos_over_neg: 427.1199645996094 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 429.7066345214844 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.8009, loss_val: nan, pos_over_neg: 229.70257568359375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.7472, loss_val: nan, pos_over_neg: 279.3770446777344 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.7639, loss_val: nan, pos_over_neg: 337.5641784667969 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 328.80755615234375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.7577, loss_val: nan, pos_over_neg: 390.4163818359375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.7499, loss_val: nan, pos_over_neg: 320.0862121582031 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 281.54254150390625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.7653, loss_val: nan, pos_over_neg: 285.4245300292969 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.7415, loss_val: nan, pos_over_neg: 386.703369140625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.8061, loss_val: nan, pos_over_neg: 186.39163208007812 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.7957, loss_val: nan, pos_over_neg: 193.93959045410156 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.7804, loss_val: nan, pos_over_neg: 338.2210693359375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.8161, loss_val: nan, pos_over_neg: 233.07716369628906 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.7978, loss_val: nan, pos_over_neg: 287.3915100097656 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.7788, loss_val: nan, pos_over_neg: 263.70849609375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.7279, loss_val: nan, pos_over_neg: 405.7523498535156 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.7959, loss_val: nan, pos_over_neg: 313.8825988769531 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.7906, loss_val: nan, pos_over_neg: 357.2621154785156 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 520.2626342773438 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.7742, loss_val: nan, pos_over_neg: 349.9598693847656 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.81, loss_val: nan, pos_over_neg: 236.73524475097656 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.7815, loss_val: nan, pos_over_neg: 344.4580383300781 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.7621, loss_val: nan, pos_over_neg: 282.7490234375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.8167, loss_val: nan, pos_over_neg: 193.52325439453125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.7443, loss_val: nan, pos_over_neg: 264.9131774902344 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.7901, loss_val: nan, pos_over_neg: 242.53878784179688 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.7679, loss_val: nan, pos_over_neg: 305.6241149902344 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.7931, loss_val: nan, pos_over_neg: 492.0404968261719 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.751, loss_val: nan, pos_over_neg: 410.0878601074219 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.7656, loss_val: nan, pos_over_neg: 351.77978515625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.7532, loss_val: nan, pos_over_neg: 359.0734558105469 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.7865, loss_val: nan, pos_over_neg: 315.84564208984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.8074, loss_val: nan, pos_over_neg: 262.7340393066406 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.7826, loss_val: nan, pos_over_neg: 218.47132873535156 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.7597, loss_val: nan, pos_over_neg: 299.2037353515625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.7912, loss_val: nan, pos_over_neg: 181.74386596679688 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.7652, loss_val: nan, pos_over_neg: 290.2050476074219 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.7704, loss_val: nan, pos_over_neg: 406.0316467285156 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.7914, loss_val: nan, pos_over_neg: 308.4678649902344 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.7546, loss_val: nan, pos_over_neg: 341.30084228515625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.7872, loss_val: nan, pos_over_neg: 302.8843994140625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.7614, loss_val: nan, pos_over_neg: 246.2308807373047 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.7492, loss_val: nan, pos_over_neg: 354.4964294433594 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.7831, loss_val: nan, pos_over_neg: 208.35562133789062 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.7618, loss_val: nan, pos_over_neg: 294.181884765625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.7932, loss_val: nan, pos_over_neg: 213.14561462402344 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.7622, loss_val: nan, pos_over_neg: 250.64093017578125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.8031, loss_val: nan, pos_over_neg: 345.4603271484375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.7735, loss_val: nan, pos_over_neg: 332.9822082519531 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.7847, loss_val: nan, pos_over_neg: 334.7190856933594 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.8019, loss_val: nan, pos_over_neg: 320.8753356933594 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.7606, loss_val: nan, pos_over_neg: 448.3074645996094 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.7627, loss_val: nan, pos_over_neg: 288.79864501953125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.7884, loss_val: nan, pos_over_neg: 279.8744201660156 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.7469, loss_val: nan, pos_over_neg: 318.18951416015625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.7766, loss_val: nan, pos_over_neg: 369.6774597167969 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.7706, loss_val: nan, pos_over_neg: 243.2762451171875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.7816, loss_val: nan, pos_over_neg: 253.8350372314453 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.7528, loss_val: nan, pos_over_neg: 412.27606201171875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 407.5205383300781 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.7885, loss_val: nan, pos_over_neg: 229.5199432373047 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.7366, loss_val: nan, pos_over_neg: 427.7287902832031 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.7541, loss_val: nan, pos_over_neg: 348.0278015136719 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.8508, loss_val: nan, pos_over_neg: 274.2384033203125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 336.0167541503906 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.7394, loss_val: nan, pos_over_neg: 353.2908630371094 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.7636, loss_val: nan, pos_over_neg: 305.94940185546875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.7464, loss_val: nan, pos_over_neg: 421.9898376464844 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.7682, loss_val: nan, pos_over_neg: 361.5343322753906 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.7607, loss_val: nan, pos_over_neg: 253.99636840820312 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.74, loss_val: nan, pos_over_neg: 324.9497375488281 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.8163, loss_val: nan, pos_over_neg: 239.20579528808594 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.7802, loss_val: nan, pos_over_neg: 263.31201171875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.7858, loss_val: nan, pos_over_neg: 454.61749267578125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.8224, loss_val: nan, pos_over_neg: 352.47247314453125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.7569, loss_val: nan, pos_over_neg: 231.4729766845703 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.7806, loss_val: nan, pos_over_neg: 310.9482116699219 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.7722, loss_val: nan, pos_over_neg: 290.0652770996094 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.7562, loss_val: nan, pos_over_neg: 443.51177978515625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.7274, loss_val: nan, pos_over_neg: 463.4477233886719 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.7697, loss_val: nan, pos_over_neg: 321.43145751953125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.7314, loss_val: nan, pos_over_neg: 387.922607421875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 391.5321350097656 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.794, loss_val: nan, pos_over_neg: 389.0991516113281 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.7776, loss_val: nan, pos_over_neg: 277.6184387207031 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.7495, loss_val: nan, pos_over_neg: 276.402587890625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 434.32550048828125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.7611, loss_val: nan, pos_over_neg: 355.47467041015625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 274.8268127441406 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.7594, loss_val: nan, pos_over_neg: 213.48260498046875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.7767, loss_val: nan, pos_over_neg: 186.08518981933594 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.8026, loss_val: nan, pos_over_neg: 257.0565490722656 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.8211, loss_val: nan, pos_over_neg: 408.2749328613281 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.748, loss_val: nan, pos_over_neg: 546.2862548828125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.7867, loss_val: nan, pos_over_neg: 363.28179931640625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.8707, loss_val: nan, pos_over_neg: 242.49305725097656 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.7424, loss_val: nan, pos_over_neg: 424.2070007324219 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.7556, loss_val: nan, pos_over_neg: 193.28689575195312 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.8129, loss_val: nan, pos_over_neg: 211.7454071044922 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.7352, loss_val: nan, pos_over_neg: 249.4198455810547 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.7686, loss_val: nan, pos_over_neg: 152.5634307861328 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.7355, loss_val: nan, pos_over_neg: 210.255126953125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.7619, loss_val: nan, pos_over_neg: 221.57943725585938 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.7416, loss_val: nan, pos_over_neg: 502.7109375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.7641, loss_val: nan, pos_over_neg: 297.7080993652344 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.7442, loss_val: nan, pos_over_neg: 217.1233367919922 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 259.6412048339844 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.7794, loss_val: nan, pos_over_neg: 326.72442626953125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.82, loss_val: nan, pos_over_neg: 276.076904296875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.7541, loss_val: nan, pos_over_neg: 231.16845703125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.7635, loss_val: nan, pos_over_neg: 211.68267822265625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.7419, loss_val: nan, pos_over_neg: 266.81915283203125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.8206, loss_val: nan, pos_over_neg: 174.8321075439453 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.7283, loss_val: nan, pos_over_neg: 369.9917907714844 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.7525, loss_val: nan, pos_over_neg: 348.43292236328125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.7357, loss_val: nan, pos_over_neg: 361.3591613769531 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.7137, loss_val: nan, pos_over_neg: 474.9680480957031 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.7691, loss_val: nan, pos_over_neg: 226.18222045898438 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.734, loss_val: nan, pos_over_neg: 265.2393798828125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.7664, loss_val: nan, pos_over_neg: 264.6138916015625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.7833, loss_val: nan, pos_over_neg: 270.64520263671875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.7901, loss_val: nan, pos_over_neg: 230.7956085205078 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.778, loss_val: nan, pos_over_neg: 268.7877197265625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.8064, loss_val: nan, pos_over_neg: 194.10914611816406 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 339.0021057128906 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.8181, loss_val: nan, pos_over_neg: 198.37538146972656 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.7873, loss_val: nan, pos_over_neg: 283.5775451660156 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.7489, loss_val: nan, pos_over_neg: 339.8053894042969 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.7666, loss_val: nan, pos_over_neg: 386.4793701171875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.7717, loss_val: nan, pos_over_neg: 362.161865234375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.7559, loss_val: nan, pos_over_neg: 334.2381591796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.796, loss_val: nan, pos_over_neg: 224.01748657226562 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.755, loss_val: nan, pos_over_neg: 294.79095458984375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.7968, loss_val: nan, pos_over_neg: 200.4781494140625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.7933, loss_val: nan, pos_over_neg: 181.7267303466797 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.7315, loss_val: nan, pos_over_neg: 200.4088592529297 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.7519, loss_val: nan, pos_over_neg: 218.32254028320312 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.7667, loss_val: nan, pos_over_neg: 333.139404296875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.7781, loss_val: nan, pos_over_neg: 307.1213073730469 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.7131, loss_val: nan, pos_over_neg: 303.6333923339844 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.7873, loss_val: nan, pos_over_neg: 352.97265625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.7499, loss_val: nan, pos_over_neg: 338.2966613769531 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.763, loss_val: nan, pos_over_neg: 379.313720703125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.7792, loss_val: nan, pos_over_neg: 286.50762939453125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.7497, loss_val: nan, pos_over_neg: 309.7909240722656 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.7567, loss_val: nan, pos_over_neg: 262.71234130859375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.84, loss_val: nan, pos_over_neg: 206.56178283691406 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.7343, loss_val: nan, pos_over_neg: 306.0132751464844 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.8186, loss_val: nan, pos_over_neg: 253.0830078125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.7333, loss_val: nan, pos_over_neg: 285.2317810058594 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.7741, loss_val: nan, pos_over_neg: 222.16683959960938 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.7822, loss_val: nan, pos_over_neg: 255.18838500976562 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.7884, loss_val: nan, pos_over_neg: 389.0178527832031 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.7492, loss_val: nan, pos_over_neg: 394.1777648925781 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.7855, loss_val: nan, pos_over_neg: 326.4167785644531 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.739, loss_val: nan, pos_over_neg: 361.63067626953125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.7981, loss_val: nan, pos_over_neg: 430.7336730957031 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.76, loss_val: nan, pos_over_neg: 434.1353759765625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.7175, loss_val: nan, pos_over_neg: 323.3250732421875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.7922, loss_val: nan, pos_over_neg: 241.78390502929688 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.8155, loss_val: nan, pos_over_neg: 400.2135925292969 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.7689, loss_val: nan, pos_over_neg: 215.19508361816406 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.7716, loss_val: nan, pos_over_neg: 209.01113891601562 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.8081, loss_val: nan, pos_over_neg: 242.52923583984375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.7784, loss_val: nan, pos_over_neg: 350.6002502441406 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.7715, loss_val: nan, pos_over_neg: 299.2355041503906 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 492.0743713378906 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.7034, loss_val: nan, pos_over_neg: 394.1957092285156 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.7859, loss_val: nan, pos_over_neg: 235.3999481201172 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.7182, loss_val: nan, pos_over_neg: 411.1957702636719 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.7614, loss_val: nan, pos_over_neg: 302.5465393066406 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.7936, loss_val: nan, pos_over_neg: 317.5660705566406 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.7829, loss_val: nan, pos_over_neg: 276.0685119628906 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.7121, loss_val: nan, pos_over_neg: 328.48468017578125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.7749, loss_val: nan, pos_over_neg: 261.6740417480469 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.7348, loss_val: nan, pos_over_neg: 285.5565185546875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.7278, loss_val: nan, pos_over_neg: 589.0728149414062 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.7889, loss_val: nan, pos_over_neg: 238.52276611328125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.7979, loss_val: nan, pos_over_neg: 292.71368408203125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 285.84625244140625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.7666, loss_val: nan, pos_over_neg: 238.2789764404297 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.7523, loss_val: nan, pos_over_neg: 343.1401062011719 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.7668, loss_val: nan, pos_over_neg: 414.3035583496094 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.7474, loss_val: nan, pos_over_neg: 285.6683044433594 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.7656, loss_val: nan, pos_over_neg: 243.57736206054688 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.754, loss_val: nan, pos_over_neg: 409.56646728515625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.7562, loss_val: nan, pos_over_neg: 483.17059326171875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.7282, loss_val: nan, pos_over_neg: 235.63320922851562 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.7599, loss_val: nan, pos_over_neg: 308.96868896484375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.7607, loss_val: nan, pos_over_neg: 212.5819549560547 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.7891, loss_val: nan, pos_over_neg: 285.1258544921875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.7226, loss_val: nan, pos_over_neg: 329.5724182128906 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.7527, loss_val: nan, pos_over_neg: 376.1911926269531 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 274.2929992675781 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.7016, loss_val: nan, pos_over_neg: 287.3404235839844 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.8201, loss_val: nan, pos_over_neg: 218.39683532714844 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.7741, loss_val: nan, pos_over_neg: 170.61056518554688 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.7377, loss_val: nan, pos_over_neg: 247.76580810546875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.7443, loss_val: nan, pos_over_neg: 217.41668701171875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.7998, loss_val: nan, pos_over_neg: 279.8092041015625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 305.7989501953125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.8042, loss_val: nan, pos_over_neg: 266.96881103515625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.767, loss_val: nan, pos_over_neg: 277.09136962890625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.7381, loss_val: nan, pos_over_neg: 433.83770751953125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.7316, loss_val: nan, pos_over_neg: 405.2833251953125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.7578, loss_val: nan, pos_over_neg: 271.6911926269531 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 349.6580505371094 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.7328, loss_val: nan, pos_over_neg: 270.5780944824219 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.7706, loss_val: nan, pos_over_neg: 287.54766845703125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.7279, loss_val: nan, pos_over_neg: 284.98504638671875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.7792, loss_val: nan, pos_over_neg: 181.9259490966797 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.7385, loss_val: nan, pos_over_neg: 237.83876037597656 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.7521, loss_val: nan, pos_over_neg: 317.841796875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.7371, loss_val: nan, pos_over_neg: 449.26422119140625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.7543, loss_val: nan, pos_over_neg: 360.1706848144531 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.729, loss_val: nan, pos_over_neg: 432.7265930175781 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.7498, loss_val: nan, pos_over_neg: 341.3494567871094 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.7583, loss_val: nan, pos_over_neg: 219.4364776611328 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.7374, loss_val: nan, pos_over_neg: 324.2054443359375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.7416, loss_val: nan, pos_over_neg: 310.2062683105469 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.7214, loss_val: nan, pos_over_neg: 320.4078674316406 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.7585, loss_val: nan, pos_over_neg: 286.965576171875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.7213, loss_val: nan, pos_over_neg: 501.3798522949219 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.7413, loss_val: nan, pos_over_neg: 531.5780639648438 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.7369, loss_val: nan, pos_over_neg: 271.7023620605469 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.7913, loss_val: nan, pos_over_neg: 270.26605224609375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.748, loss_val: nan, pos_over_neg: 342.8481140136719 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 356.2622985839844 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.7282, loss_val: nan, pos_over_neg: 370.9801330566406 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.7692, loss_val: nan, pos_over_neg: 377.42022705078125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.7858, loss_val: nan, pos_over_neg: 381.5550231933594 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.7862, loss_val: nan, pos_over_neg: 283.9819030761719 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.7714, loss_val: nan, pos_over_neg: 296.4984130859375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.7322, loss_val: nan, pos_over_neg: 269.38128662109375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.7602, loss_val: nan, pos_over_neg: 241.15133666992188 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.7425, loss_val: nan, pos_over_neg: 303.6997985839844 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.7657, loss_val: nan, pos_over_neg: 275.8221435546875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.7362, loss_val: nan, pos_over_neg: 314.05078125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.7768, loss_val: nan, pos_over_neg: 351.0836181640625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.7523, loss_val: nan, pos_over_neg: 508.0894470214844 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.7793, loss_val: nan, pos_over_neg: 259.7814636230469 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 328.8482971191406 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.793, loss_val: nan, pos_over_neg: 320.79486083984375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.7551, loss_val: nan, pos_over_neg: 309.6120300292969 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.7757, loss_val: nan, pos_over_neg: 220.55813598632812 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.789, loss_val: nan, pos_over_neg: 188.89993286132812 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.7378, loss_val: nan, pos_over_neg: 268.61077880859375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.7118, loss_val: nan, pos_over_neg: 409.8376159667969 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.7454, loss_val: nan, pos_over_neg: 315.3074645996094 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.718, loss_val: nan, pos_over_neg: 272.6823425292969 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 258.58795166015625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.783, loss_val: nan, pos_over_neg: 280.0304870605469 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.7364, loss_val: nan, pos_over_neg: 384.9459533691406 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.726, loss_val: nan, pos_over_neg: 484.44561767578125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.7561, loss_val: nan, pos_over_neg: 205.66220092773438 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.7886, loss_val: nan, pos_over_neg: 227.3909454345703 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.7707, loss_val: nan, pos_over_neg: 247.62957763671875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.7805, loss_val: nan, pos_over_neg: 199.92367553710938 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.7404, loss_val: nan, pos_over_neg: 195.88450622558594 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.8066, loss_val: nan, pos_over_neg: 270.5015563964844 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.7454, loss_val: nan, pos_over_neg: 431.6625061035156 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.8217, loss_val: nan, pos_over_neg: 249.38978576660156 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 376.5284423828125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.7593, loss_val: nan, pos_over_neg: 278.58941650390625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.7736, loss_val: nan, pos_over_neg: 269.1993408203125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.7385, loss_val: nan, pos_over_neg: 310.31512451171875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.7699, loss_val: nan, pos_over_neg: 231.38990783691406 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 268.7850341796875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.7671, loss_val: nan, pos_over_neg: 301.36566162109375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.7521, loss_val: nan, pos_over_neg: 348.11907958984375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.7132, loss_val: nan, pos_over_neg: 689.0738525390625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.7457, loss_val: nan, pos_over_neg: 391.5179443359375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.7414, loss_val: nan, pos_over_neg: 323.46429443359375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.7442, loss_val: nan, pos_over_neg: 327.3860778808594 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.7943, loss_val: nan, pos_over_neg: 341.44873046875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.7499, loss_val: nan, pos_over_neg: 370.0207214355469 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.804, loss_val: nan, pos_over_neg: 218.1515350341797 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.7481, loss_val: nan, pos_over_neg: 307.72784423828125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.7455, loss_val: nan, pos_over_neg: 306.93756103515625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.7507, loss_val: nan, pos_over_neg: 319.6921081542969 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.7599, loss_val: nan, pos_over_neg: 397.3211975097656 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.7684, loss_val: nan, pos_over_neg: 412.2569580078125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.7784, loss_val: nan, pos_over_neg: 294.38507080078125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.7896, loss_val: nan, pos_over_neg: 322.5496826171875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.8127, loss_val: nan, pos_over_neg: 184.77378845214844 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.7551, loss_val: nan, pos_over_neg: 312.5472717285156 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.8045, loss_val: nan, pos_over_neg: 243.79965209960938 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.7521, loss_val: nan, pos_over_neg: 225.65762329101562 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.733, loss_val: nan, pos_over_neg: 207.88404846191406 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.7147, loss_val: nan, pos_over_neg: 337.33062744140625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.725, loss_val: nan, pos_over_neg: 253.00759887695312 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.7545, loss_val: nan, pos_over_neg: 325.8362731933594 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.7117, loss_val: nan, pos_over_neg: 384.3579406738281 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.7527, loss_val: nan, pos_over_neg: 401.1494445800781 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.7381, loss_val: nan, pos_over_neg: 495.1802673339844 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.7268, loss_val: nan, pos_over_neg: 256.677001953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.7566, loss_val: nan, pos_over_neg: 298.1705017089844 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.7847, loss_val: nan, pos_over_neg: 231.56602478027344 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.7548, loss_val: nan, pos_over_neg: 338.0553894042969 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.7327, loss_val: nan, pos_over_neg: 301.40185546875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.7667, loss_val: nan, pos_over_neg: 252.15843200683594 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.7882, loss_val: nan, pos_over_neg: 211.10720825195312 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.7502, loss_val: nan, pos_over_neg: 371.0350646972656 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.789, loss_val: nan, pos_over_neg: 193.93850708007812 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.7557, loss_val: nan, pos_over_neg: 214.054443359375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.7554, loss_val: nan, pos_over_neg: 258.37738037109375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.7994, loss_val: nan, pos_over_neg: 264.9349670410156 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.708, loss_val: nan, pos_over_neg: 293.8326721191406 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.7335, loss_val: nan, pos_over_neg: 388.3293762207031 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.7411, loss_val: nan, pos_over_neg: 342.04681396484375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.7414, loss_val: nan, pos_over_neg: 264.5093994140625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.7247, loss_val: nan, pos_over_neg: 430.4404296875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.7557, loss_val: nan, pos_over_neg: 282.530517578125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.7963, loss_val: nan, pos_over_neg: 209.8352813720703 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.7208, loss_val: nan, pos_over_neg: 402.25421142578125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.7389, loss_val: nan, pos_over_neg: 377.4642333984375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.7516, loss_val: nan, pos_over_neg: 378.57122802734375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.7133, loss_val: nan, pos_over_neg: 260.50732421875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.7866, loss_val: nan, pos_over_neg: 277.31024169921875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:00:37<75514:52:31, 906.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.8115, loss_val: nan, pos_over_neg: 313.9820861816406 lr: 0.00031623\n",
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 3.7789, loss_val: nan, pos_over_neg: 176.9964599609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.7588, loss_val: nan, pos_over_neg: 318.3659973144531 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.7385, loss_val: nan, pos_over_neg: 345.0010681152344 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.7714, loss_val: nan, pos_over_neg: 450.0994567871094 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 369.3482666015625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 257.606689453125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.7787, loss_val: nan, pos_over_neg: 218.99575805664062 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.7549, loss_val: nan, pos_over_neg: 229.75196838378906 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 246.35531616210938 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.7624, loss_val: nan, pos_over_neg: 378.1249084472656 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.7439, loss_val: nan, pos_over_neg: 269.8341064453125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.7482, loss_val: nan, pos_over_neg: 236.40594482421875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.7272, loss_val: nan, pos_over_neg: 318.55438232421875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.7143, loss_val: nan, pos_over_neg: 314.3919982910156 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.758, loss_val: nan, pos_over_neg: 294.5865783691406 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.7214, loss_val: nan, pos_over_neg: 359.805419921875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.758, loss_val: nan, pos_over_neg: 254.45729064941406 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.7883, loss_val: nan, pos_over_neg: 256.9250183105469 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.744, loss_val: nan, pos_over_neg: 299.29876708984375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.7634, loss_val: nan, pos_over_neg: 566.66162109375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.7463, loss_val: nan, pos_over_neg: 304.266845703125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.7617, loss_val: nan, pos_over_neg: 297.4165954589844 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.7611, loss_val: nan, pos_over_neg: 325.0312194824219 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.7808, loss_val: nan, pos_over_neg: 237.7838134765625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.7392, loss_val: nan, pos_over_neg: 209.59536743164062 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.7832, loss_val: nan, pos_over_neg: 347.2644958496094 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.7705, loss_val: nan, pos_over_neg: 567.6474609375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.781, loss_val: nan, pos_over_neg: 337.4952087402344 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.7417, loss_val: nan, pos_over_neg: 511.9944152832031 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.7731, loss_val: nan, pos_over_neg: 353.443603515625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.7657, loss_val: nan, pos_over_neg: 359.76300048828125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.7419, loss_val: nan, pos_over_neg: 264.56707763671875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.7767, loss_val: nan, pos_over_neg: 181.84181213378906 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.7462, loss_val: nan, pos_over_neg: 254.8111114501953 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.7647, loss_val: nan, pos_over_neg: 256.4312744140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.8148, loss_val: nan, pos_over_neg: 220.12603759765625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.8015, loss_val: nan, pos_over_neg: 243.7693634033203 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.7382, loss_val: nan, pos_over_neg: 313.02325439453125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.7258, loss_val: nan, pos_over_neg: 499.1988525390625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.7596, loss_val: nan, pos_over_neg: 272.1709899902344 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.7406, loss_val: nan, pos_over_neg: 313.1670227050781 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.7518, loss_val: nan, pos_over_neg: 255.0760040283203 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.7676, loss_val: nan, pos_over_neg: 315.093994140625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.7476, loss_val: nan, pos_over_neg: 225.45758056640625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.7821, loss_val: nan, pos_over_neg: 208.738037109375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6892, loss_val: nan, pos_over_neg: 325.85369873046875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.7215, loss_val: nan, pos_over_neg: 419.57611083984375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 405.0630187988281 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.7363, loss_val: nan, pos_over_neg: 385.1831359863281 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.7364, loss_val: nan, pos_over_neg: 269.552734375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.7398, loss_val: nan, pos_over_neg: 356.9813537597656 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.7432, loss_val: nan, pos_over_neg: 340.8686218261719 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.7402, loss_val: nan, pos_over_neg: 263.4015197753906 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.7217, loss_val: nan, pos_over_neg: 334.1842346191406 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.7257, loss_val: nan, pos_over_neg: 190.01988220214844 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.7588, loss_val: nan, pos_over_neg: 263.2074890136719 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.8264, loss_val: nan, pos_over_neg: 200.5633087158203 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.7196, loss_val: nan, pos_over_neg: 330.5388488769531 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.7588, loss_val: nan, pos_over_neg: 233.75296020507812 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.7545, loss_val: nan, pos_over_neg: 304.56683349609375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.7525, loss_val: nan, pos_over_neg: 326.0709228515625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.7537, loss_val: nan, pos_over_neg: 377.7519836425781 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.8054, loss_val: nan, pos_over_neg: 290.26800537109375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.7574, loss_val: nan, pos_over_neg: 327.16259765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.7635, loss_val: nan, pos_over_neg: 273.8152770996094 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.7366, loss_val: nan, pos_over_neg: 388.92987060546875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.7195, loss_val: nan, pos_over_neg: 350.5081787109375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6965, loss_val: nan, pos_over_neg: 437.8698425292969 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.7259, loss_val: nan, pos_over_neg: 357.1876220703125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.7485, loss_val: nan, pos_over_neg: 271.0845031738281 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.7802, loss_val: nan, pos_over_neg: 315.93975830078125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.7309, loss_val: nan, pos_over_neg: 415.15380859375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.7627, loss_val: nan, pos_over_neg: 485.5255432128906 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.7454, loss_val: nan, pos_over_neg: 311.2953186035156 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.7193, loss_val: nan, pos_over_neg: 415.58966064453125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.7697, loss_val: nan, pos_over_neg: 308.0594177246094 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.7744, loss_val: nan, pos_over_neg: 247.113037109375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.7223, loss_val: nan, pos_over_neg: 224.97885131835938 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.7046, loss_val: nan, pos_over_neg: 392.7740478515625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.7307, loss_val: nan, pos_over_neg: 407.2425231933594 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.7658, loss_val: nan, pos_over_neg: 252.7333221435547 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.7452, loss_val: nan, pos_over_neg: 554.7628784179688 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.7629, loss_val: nan, pos_over_neg: 278.25970458984375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.7944, loss_val: nan, pos_over_neg: 283.8421325683594 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.7693, loss_val: nan, pos_over_neg: 261.28582763671875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.7069, loss_val: nan, pos_over_neg: 412.3565979003906 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.7554, loss_val: nan, pos_over_neg: 279.22015380859375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.7338, loss_val: nan, pos_over_neg: 556.732666015625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.7682, loss_val: nan, pos_over_neg: 321.1670227050781 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6987, loss_val: nan, pos_over_neg: 479.9525451660156 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.7591, loss_val: nan, pos_over_neg: 252.38714599609375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.7755, loss_val: nan, pos_over_neg: 275.6583251953125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.7602, loss_val: nan, pos_over_neg: 325.5867919921875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.7186, loss_val: nan, pos_over_neg: 379.3158264160156 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.7624, loss_val: nan, pos_over_neg: 363.7857360839844 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.7236, loss_val: nan, pos_over_neg: 493.3127746582031 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.7187, loss_val: nan, pos_over_neg: 597.203369140625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.7351, loss_val: nan, pos_over_neg: 369.0369873046875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.7332, loss_val: nan, pos_over_neg: 355.2441101074219 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.7569, loss_val: nan, pos_over_neg: 273.5787353515625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.7794, loss_val: nan, pos_over_neg: 312.3067321777344 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.7405, loss_val: nan, pos_over_neg: 241.9169158935547 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.7685, loss_val: nan, pos_over_neg: 241.9711456298828 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.7309, loss_val: nan, pos_over_neg: 421.4075012207031 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.7258, loss_val: nan, pos_over_neg: 422.4744567871094 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.7649, loss_val: nan, pos_over_neg: 273.1083068847656 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.747, loss_val: nan, pos_over_neg: 382.917236328125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.7566, loss_val: nan, pos_over_neg: 277.2742614746094 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.7615, loss_val: nan, pos_over_neg: 286.1956787109375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.7364, loss_val: nan, pos_over_neg: 304.9637145996094 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.7382, loss_val: nan, pos_over_neg: 294.9974670410156 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.7296, loss_val: nan, pos_over_neg: 286.0842590332031 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.7555, loss_val: nan, pos_over_neg: 196.65191650390625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.7334, loss_val: nan, pos_over_neg: 289.5353698730469 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.6995, loss_val: nan, pos_over_neg: 382.7614440917969 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.757, loss_val: nan, pos_over_neg: 323.24566650390625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.7605, loss_val: nan, pos_over_neg: 336.55206298828125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.7673, loss_val: nan, pos_over_neg: 246.12179565429688 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.7193, loss_val: nan, pos_over_neg: 306.4425354003906 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.7435, loss_val: nan, pos_over_neg: 330.9799499511719 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 454.20489501953125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.7448, loss_val: nan, pos_over_neg: 319.1398620605469 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.7369, loss_val: nan, pos_over_neg: 214.18023681640625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.7316, loss_val: nan, pos_over_neg: 290.90130615234375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.7508, loss_val: nan, pos_over_neg: 308.9344482421875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.7496, loss_val: nan, pos_over_neg: 400.73193359375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.7172, loss_val: nan, pos_over_neg: 341.62640380859375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.7602, loss_val: nan, pos_over_neg: 400.0460205078125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.7628, loss_val: nan, pos_over_neg: 342.0039367675781 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.7556, loss_val: nan, pos_over_neg: 263.795654296875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.7375, loss_val: nan, pos_over_neg: 307.1985168457031 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.7703, loss_val: nan, pos_over_neg: 301.06085205078125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.7509, loss_val: nan, pos_over_neg: 276.2593688964844 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.7909, loss_val: nan, pos_over_neg: 199.023193359375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.7381, loss_val: nan, pos_over_neg: 405.7326965332031 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.753, loss_val: nan, pos_over_neg: 362.0383605957031 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.7548, loss_val: nan, pos_over_neg: 258.7778015136719 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.7774, loss_val: nan, pos_over_neg: 510.4572448730469 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.7119, loss_val: nan, pos_over_neg: 496.4222106933594 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.7543, loss_val: nan, pos_over_neg: 350.62255859375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.7326, loss_val: nan, pos_over_neg: 315.384521484375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.7414, loss_val: nan, pos_over_neg: 359.4698181152344 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.7663, loss_val: nan, pos_over_neg: 244.2823486328125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 349.8410949707031 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.7253, loss_val: nan, pos_over_neg: 437.84661865234375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.7435, loss_val: nan, pos_over_neg: 434.54425048828125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.7481, loss_val: nan, pos_over_neg: 359.3434143066406 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.7419, loss_val: nan, pos_over_neg: 340.6302795410156 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.7365, loss_val: nan, pos_over_neg: 261.2817077636719 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.7638, loss_val: nan, pos_over_neg: 262.3940734863281 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.7799, loss_val: nan, pos_over_neg: 249.1671600341797 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.7368, loss_val: nan, pos_over_neg: 197.59121704101562 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.7027, loss_val: nan, pos_over_neg: 415.4861755371094 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.7374, loss_val: nan, pos_over_neg: 362.7728271484375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.7162, loss_val: nan, pos_over_neg: 337.0431823730469 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.77, loss_val: nan, pos_over_neg: 185.5466766357422 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.7624, loss_val: nan, pos_over_neg: 221.32028198242188 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.7207, loss_val: nan, pos_over_neg: 510.5972595214844 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.7645, loss_val: nan, pos_over_neg: 376.2843017578125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.7704, loss_val: nan, pos_over_neg: 252.121826171875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.7532, loss_val: nan, pos_over_neg: 290.1700744628906 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.7234, loss_val: nan, pos_over_neg: 362.4341125488281 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.7195, loss_val: nan, pos_over_neg: 204.545166015625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.7141, loss_val: nan, pos_over_neg: 314.963623046875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.7369, loss_val: nan, pos_over_neg: 293.10809326171875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.7131, loss_val: nan, pos_over_neg: 493.25933837890625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.7588, loss_val: nan, pos_over_neg: 450.6475830078125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.7357, loss_val: nan, pos_over_neg: 556.6405639648438 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.7497, loss_val: nan, pos_over_neg: 479.4341125488281 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.7352, loss_val: nan, pos_over_neg: 508.0511169433594 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.748, loss_val: nan, pos_over_neg: 203.30416870117188 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.7133, loss_val: nan, pos_over_neg: 226.52297973632812 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.7727, loss_val: nan, pos_over_neg: 273.177734375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.7867, loss_val: nan, pos_over_neg: 234.3324737548828 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.7611, loss_val: nan, pos_over_neg: 237.58328247070312 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.7419, loss_val: nan, pos_over_neg: 281.9690246582031 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.7286, loss_val: nan, pos_over_neg: 578.761962890625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 434.72052001953125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.7425, loss_val: nan, pos_over_neg: 410.9536437988281 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.7147, loss_val: nan, pos_over_neg: 353.5016174316406 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 250.87619018554688 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.7891, loss_val: nan, pos_over_neg: 238.91616821289062 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.7576, loss_val: nan, pos_over_neg: 245.4398193359375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.7097, loss_val: nan, pos_over_neg: 307.33319091796875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6916, loss_val: nan, pos_over_neg: 308.0730895996094 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.7385, loss_val: nan, pos_over_neg: 325.4005126953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.7354, loss_val: nan, pos_over_neg: 271.58697509765625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.7205, loss_val: nan, pos_over_neg: 368.8116455078125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.717, loss_val: nan, pos_over_neg: 383.606689453125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.7636, loss_val: nan, pos_over_neg: 293.3816833496094 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.7217, loss_val: nan, pos_over_neg: 295.73406982421875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.7425, loss_val: nan, pos_over_neg: 237.53675842285156 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.7528, loss_val: nan, pos_over_neg: 402.3023986816406 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.7546, loss_val: nan, pos_over_neg: 301.760009765625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.7413, loss_val: nan, pos_over_neg: 259.7478942871094 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.7354, loss_val: nan, pos_over_neg: 387.5913391113281 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.7256, loss_val: nan, pos_over_neg: 389.5269470214844 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.7107, loss_val: nan, pos_over_neg: 374.0704650878906 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 227.66610717773438 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.729, loss_val: nan, pos_over_neg: 236.66090393066406 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.7291, loss_val: nan, pos_over_neg: 359.02978515625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.7376, loss_val: nan, pos_over_neg: 288.179931640625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.7371, loss_val: nan, pos_over_neg: 256.2732849121094 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.7205, loss_val: nan, pos_over_neg: 381.5369873046875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.748, loss_val: nan, pos_over_neg: 424.1566467285156 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.7384, loss_val: nan, pos_over_neg: 393.1105651855469 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.7544, loss_val: nan, pos_over_neg: 262.9666748046875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.7785, loss_val: nan, pos_over_neg: 231.10604858398438 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.7179, loss_val: nan, pos_over_neg: 492.7242736816406 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.7345, loss_val: nan, pos_over_neg: 418.7048645019531 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.7169, loss_val: nan, pos_over_neg: 287.4501953125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.7271, loss_val: nan, pos_over_neg: 277.88153076171875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.7255, loss_val: nan, pos_over_neg: 345.7179260253906 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.7388, loss_val: nan, pos_over_neg: 340.03009033203125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.7782, loss_val: nan, pos_over_neg: 240.51907348632812 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.764, loss_val: nan, pos_over_neg: 281.5791931152344 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.7564, loss_val: nan, pos_over_neg: 316.93505859375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6906, loss_val: nan, pos_over_neg: 435.0757141113281 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.7896, loss_val: nan, pos_over_neg: 307.9095764160156 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.7222, loss_val: nan, pos_over_neg: 320.3927001953125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.7646, loss_val: nan, pos_over_neg: 227.72384643554688 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.7721, loss_val: nan, pos_over_neg: 224.8287811279297 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.7823, loss_val: nan, pos_over_neg: 298.54583740234375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.7317, loss_val: nan, pos_over_neg: 268.4899597167969 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.7754, loss_val: nan, pos_over_neg: 290.95458984375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.77, loss_val: nan, pos_over_neg: 395.50653076171875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.8024, loss_val: nan, pos_over_neg: 229.2756805419922 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.736, loss_val: nan, pos_over_neg: 333.1874694824219 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.7551, loss_val: nan, pos_over_neg: 262.6877746582031 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.7352, loss_val: nan, pos_over_neg: 330.3553771972656 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.7737, loss_val: nan, pos_over_neg: 361.8570251464844 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.7314, loss_val: nan, pos_over_neg: 411.2882385253906 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.7263, loss_val: nan, pos_over_neg: 366.6281433105469 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.7475, loss_val: nan, pos_over_neg: 295.2892150878906 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.7576, loss_val: nan, pos_over_neg: 267.45281982421875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.7556, loss_val: nan, pos_over_neg: 290.3100891113281 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.7294, loss_val: nan, pos_over_neg: 360.7121276855469 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.8003, loss_val: nan, pos_over_neg: 281.5610046386719 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.7433, loss_val: nan, pos_over_neg: 249.51515197753906 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.6999, loss_val: nan, pos_over_neg: 320.5228271484375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.7578, loss_val: nan, pos_over_neg: 279.2843322753906 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.7456, loss_val: nan, pos_over_neg: 300.9715576171875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.7323, loss_val: nan, pos_over_neg: 344.8517150878906 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.7662, loss_val: nan, pos_over_neg: 235.27789306640625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 227.20498657226562 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.7212, loss_val: nan, pos_over_neg: 450.63739013671875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.7493, loss_val: nan, pos_over_neg: 370.65960693359375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.7304, loss_val: nan, pos_over_neg: 287.587890625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 286.92529296875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.7221, loss_val: nan, pos_over_neg: 399.6662902832031 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.7398, loss_val: nan, pos_over_neg: 312.9980773925781 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.7104, loss_val: nan, pos_over_neg: 384.92547607421875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.7514, loss_val: nan, pos_over_neg: 274.47998046875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.7125, loss_val: nan, pos_over_neg: 341.19879150390625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.7411, loss_val: nan, pos_over_neg: 312.18646240234375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.7129, loss_val: nan, pos_over_neg: 259.1343078613281 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.7418, loss_val: nan, pos_over_neg: 419.91015625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.7374, loss_val: nan, pos_over_neg: 307.8948974609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.7548, loss_val: nan, pos_over_neg: 311.96282958984375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.7408, loss_val: nan, pos_over_neg: 286.2586975097656 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.733, loss_val: nan, pos_over_neg: 323.4773864746094 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.7676, loss_val: nan, pos_over_neg: 226.95030212402344 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.7301, loss_val: nan, pos_over_neg: 333.7353515625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.7226, loss_val: nan, pos_over_neg: 593.65087890625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.7343, loss_val: nan, pos_over_neg: 542.52587890625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.7296, loss_val: nan, pos_over_neg: 517.7938842773438 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.7185, loss_val: nan, pos_over_neg: 376.6000671386719 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.7284, loss_val: nan, pos_over_neg: 350.8536376953125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.7368, loss_val: nan, pos_over_neg: 371.930419921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.7237, loss_val: nan, pos_over_neg: 242.2526092529297 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.7222, loss_val: nan, pos_over_neg: 289.1212463378906 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.7108, loss_val: nan, pos_over_neg: 231.77818298339844 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.757, loss_val: nan, pos_over_neg: 217.7492218017578 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.751, loss_val: nan, pos_over_neg: 332.7056579589844 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.7298, loss_val: nan, pos_over_neg: 311.6175231933594 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.7815, loss_val: nan, pos_over_neg: 231.3651123046875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.7541, loss_val: nan, pos_over_neg: 261.65936279296875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.7605, loss_val: nan, pos_over_neg: 305.429931640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.7691, loss_val: nan, pos_over_neg: 346.0228576660156 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.7513, loss_val: nan, pos_over_neg: 253.44032287597656 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.7405, loss_val: nan, pos_over_neg: 238.6410369873047 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 247.9071044921875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.76, loss_val: nan, pos_over_neg: 316.5270690917969 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.7277, loss_val: nan, pos_over_neg: 250.2313995361328 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.775, loss_val: nan, pos_over_neg: 251.42559814453125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.7452, loss_val: nan, pos_over_neg: 293.2410583496094 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.7469, loss_val: nan, pos_over_neg: 287.46734619140625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.7701, loss_val: nan, pos_over_neg: 230.61155700683594 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.7289, loss_val: nan, pos_over_neg: 351.6141052246094 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.7784, loss_val: nan, pos_over_neg: 190.5211639404297 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.7707, loss_val: nan, pos_over_neg: 222.1962890625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.7918, loss_val: nan, pos_over_neg: 303.3277282714844 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.7267, loss_val: nan, pos_over_neg: 332.7105407714844 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.7317, loss_val: nan, pos_over_neg: 293.0167236328125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.7168, loss_val: nan, pos_over_neg: 446.97149658203125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.7623, loss_val: nan, pos_over_neg: 313.8588562011719 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6797, loss_val: nan, pos_over_neg: 387.7672424316406 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.7902, loss_val: nan, pos_over_neg: 256.246826171875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.7159, loss_val: nan, pos_over_neg: 426.8100280761719 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.7167, loss_val: nan, pos_over_neg: 295.4919738769531 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.7381, loss_val: nan, pos_over_neg: 240.11892700195312 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 416.0836181640625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.7415, loss_val: nan, pos_over_neg: 473.3232116699219 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.757, loss_val: nan, pos_over_neg: 197.38519287109375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.7292, loss_val: nan, pos_over_neg: 313.4748229980469 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.8117, loss_val: nan, pos_over_neg: 162.70118713378906 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 316.9659118652344 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.7872, loss_val: nan, pos_over_neg: 287.5668029785156 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.7057, loss_val: nan, pos_over_neg: 337.1281433105469 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6804, loss_val: nan, pos_over_neg: 623.12109375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.7649, loss_val: nan, pos_over_neg: 347.5460510253906 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.7259, loss_val: nan, pos_over_neg: 391.1840515136719 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.7358, loss_val: nan, pos_over_neg: 416.85870361328125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.7742, loss_val: nan, pos_over_neg: 357.4482421875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6973, loss_val: nan, pos_over_neg: 239.0460205078125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.8272, loss_val: nan, pos_over_neg: 148.40077209472656 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.7578, loss_val: nan, pos_over_neg: 328.2998046875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.758, loss_val: nan, pos_over_neg: 335.85772705078125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.7555, loss_val: nan, pos_over_neg: 300.44964599609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.7992, loss_val: nan, pos_over_neg: 306.7599182128906 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.7614, loss_val: nan, pos_over_neg: 312.62249755859375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.7409, loss_val: nan, pos_over_neg: 388.760009765625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.7029, loss_val: nan, pos_over_neg: 360.04241943359375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.7713, loss_val: nan, pos_over_neg: 332.90802001953125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.7621, loss_val: nan, pos_over_neg: 269.4676818847656 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.7293, loss_val: nan, pos_over_neg: 354.3113708496094 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.7717, loss_val: nan, pos_over_neg: 321.14111328125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.7511, loss_val: nan, pos_over_neg: 334.8943176269531 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.7141, loss_val: nan, pos_over_neg: 435.04248046875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.7399, loss_val: nan, pos_over_neg: 242.7772674560547 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.7544, loss_val: nan, pos_over_neg: 237.38087463378906 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.7493, loss_val: nan, pos_over_neg: 254.29159545898438 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.7228, loss_val: nan, pos_over_neg: 330.002685546875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.7131, loss_val: nan, pos_over_neg: 321.7135009765625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 284.944091796875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6737, loss_val: nan, pos_over_neg: 497.9712219238281 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.7626, loss_val: nan, pos_over_neg: 321.7024230957031 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.7094, loss_val: nan, pos_over_neg: 341.5897521972656 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.7374, loss_val: nan, pos_over_neg: 308.3677978515625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.7324, loss_val: nan, pos_over_neg: 423.9252014160156 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.6976, loss_val: nan, pos_over_neg: 427.7269287109375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.7475, loss_val: nan, pos_over_neg: 265.3271179199219 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.7441, loss_val: nan, pos_over_neg: 252.51995849609375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.7328, loss_val: nan, pos_over_neg: 373.3457336425781 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.769, loss_val: nan, pos_over_neg: 346.018310546875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.7959, loss_val: nan, pos_over_neg: 306.4720458984375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6759, loss_val: nan, pos_over_neg: 653.9248657226562 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.7471, loss_val: nan, pos_over_neg: 435.3946533203125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.7205, loss_val: nan, pos_over_neg: 464.72259521484375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.7299, loss_val: nan, pos_over_neg: 346.9386901855469 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.7299, loss_val: nan, pos_over_neg: 351.88031005859375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 295.6729736328125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.7387, loss_val: nan, pos_over_neg: 231.9810333251953 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.7053, loss_val: nan, pos_over_neg: 291.8833923339844 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.7293, loss_val: nan, pos_over_neg: 330.80712890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 344.82000732421875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.727, loss_val: nan, pos_over_neg: 443.64190673828125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6913, loss_val: nan, pos_over_neg: 616.7240600585938 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.7082, loss_val: nan, pos_over_neg: 411.05291748046875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.7189, loss_val: nan, pos_over_neg: 471.59881591796875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.7289, loss_val: nan, pos_over_neg: 313.49176025390625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.6978, loss_val: nan, pos_over_neg: 454.41058349609375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.7538, loss_val: nan, pos_over_neg: 333.0999755859375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.743, loss_val: nan, pos_over_neg: 334.9970703125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.7362, loss_val: nan, pos_over_neg: 265.6007385253906 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.7021, loss_val: nan, pos_over_neg: 315.651123046875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.7183, loss_val: nan, pos_over_neg: 341.45574951171875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.7568, loss_val: nan, pos_over_neg: 317.607177734375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.6837, loss_val: nan, pos_over_neg: 502.82708740234375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.698, loss_val: nan, pos_over_neg: 357.4838562011719 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.7243, loss_val: nan, pos_over_neg: 312.8741149902344 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.7688, loss_val: nan, pos_over_neg: 413.64886474609375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.7106, loss_val: nan, pos_over_neg: 469.3108825683594 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.7019, loss_val: nan, pos_over_neg: 345.95965576171875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.7294, loss_val: nan, pos_over_neg: 386.95037841796875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.7105, loss_val: nan, pos_over_neg: 416.5689392089844 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.7756, loss_val: nan, pos_over_neg: 265.4239501953125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.7414, loss_val: nan, pos_over_neg: 204.1124725341797 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.7173, loss_val: nan, pos_over_neg: 221.33346557617188 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.772, loss_val: nan, pos_over_neg: 284.2768859863281 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.7222, loss_val: nan, pos_over_neg: 365.9746398925781 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.7334, loss_val: nan, pos_over_neg: 428.9379577636719 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.738, loss_val: nan, pos_over_neg: 570.6316528320312 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.7291, loss_val: nan, pos_over_neg: 394.48785400390625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.7196, loss_val: nan, pos_over_neg: 351.8621826171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.7142, loss_val: nan, pos_over_neg: 471.1533203125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.7253, loss_val: nan, pos_over_neg: 289.98895263671875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.7657, loss_val: nan, pos_over_neg: 290.23876953125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.7388, loss_val: nan, pos_over_neg: 274.7398376464844 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.7242, loss_val: nan, pos_over_neg: 323.39703369140625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.7555, loss_val: nan, pos_over_neg: 328.8394775390625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.7102, loss_val: nan, pos_over_neg: 328.5448913574219 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.781, loss_val: nan, pos_over_neg: 241.48715209960938 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.7547, loss_val: nan, pos_over_neg: 276.5664367675781 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.717, loss_val: nan, pos_over_neg: 483.1098327636719 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.7826, loss_val: nan, pos_over_neg: 223.83316040039062 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.7245, loss_val: nan, pos_over_neg: 288.9493713378906 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.7242, loss_val: nan, pos_over_neg: 323.8409729003906 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.7358, loss_val: nan, pos_over_neg: 321.52325439453125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.7582, loss_val: nan, pos_over_neg: 271.0944519042969 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.7393, loss_val: nan, pos_over_neg: 289.0976257324219 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.7221, loss_val: nan, pos_over_neg: 320.0009765625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.732, loss_val: nan, pos_over_neg: 326.8717346191406 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.7794, loss_val: nan, pos_over_neg: 304.1393127441406 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.7189, loss_val: nan, pos_over_neg: 428.4543151855469 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.7211, loss_val: nan, pos_over_neg: 462.0933837890625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.7293, loss_val: nan, pos_over_neg: 370.9636535644531 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.784, loss_val: nan, pos_over_neg: 388.9639587402344 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6774, loss_val: nan, pos_over_neg: 292.3665771484375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.7958, loss_val: nan, pos_over_neg: 157.12564086914062 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6956, loss_val: nan, pos_over_neg: 267.94390869140625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.7066, loss_val: nan, pos_over_neg: 291.56732177734375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.7109, loss_val: nan, pos_over_neg: 468.89697265625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6958, loss_val: nan, pos_over_neg: 433.77386474609375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.7177, loss_val: nan, pos_over_neg: 312.1448974609375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.7504, loss_val: nan, pos_over_neg: 305.7334289550781 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.7809, loss_val: nan, pos_over_neg: 254.78302001953125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.7212, loss_val: nan, pos_over_neg: 445.1822814941406 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.7073, loss_val: nan, pos_over_neg: 306.61163330078125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.7218, loss_val: nan, pos_over_neg: 265.5942687988281 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.7139, loss_val: nan, pos_over_neg: 355.8385925292969 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.7291, loss_val: nan, pos_over_neg: 249.74449157714844 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 289.795654296875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.7067, loss_val: nan, pos_over_neg: 445.5196838378906 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.7361, loss_val: nan, pos_over_neg: 304.9148864746094 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.7109, loss_val: nan, pos_over_neg: 377.9056396484375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.7199, loss_val: nan, pos_over_neg: 364.5003662109375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.7191, loss_val: nan, pos_over_neg: 281.53173828125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.7053, loss_val: nan, pos_over_neg: 329.71026611328125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.754, loss_val: nan, pos_over_neg: 222.55694580078125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.7563, loss_val: nan, pos_over_neg: 286.25762939453125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.7718, loss_val: nan, pos_over_neg: 334.56170654296875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.7375, loss_val: nan, pos_over_neg: 375.108642578125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.7532, loss_val: nan, pos_over_neg: 296.3544616699219 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.7299, loss_val: nan, pos_over_neg: 323.19195556640625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.7483, loss_val: nan, pos_over_neg: 355.45947265625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.7272, loss_val: nan, pos_over_neg: 465.53955078125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.7824, loss_val: nan, pos_over_neg: 234.4434814453125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.7724, loss_val: nan, pos_over_neg: 241.954345703125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.7621, loss_val: nan, pos_over_neg: 300.64678955078125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.7234, loss_val: nan, pos_over_neg: 349.41583251953125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.7035, loss_val: nan, pos_over_neg: 344.5853576660156 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.7079, loss_val: nan, pos_over_neg: 337.8016052246094 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.7168, loss_val: nan, pos_over_neg: 560.3761596679688 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6857, loss_val: nan, pos_over_neg: 443.34100341796875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.7409, loss_val: nan, pos_over_neg: 402.4656982421875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.7135, loss_val: nan, pos_over_neg: 422.6502990722656 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.7132, loss_val: nan, pos_over_neg: 442.1780090332031 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.6889, loss_val: nan, pos_over_neg: 422.161376953125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 210.9666748046875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.7396, loss_val: nan, pos_over_neg: 215.43589782714844 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.7231, loss_val: nan, pos_over_neg: 310.9267883300781 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.7804, loss_val: nan, pos_over_neg: 307.9498291015625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.7135, loss_val: nan, pos_over_neg: 310.9754333496094 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.7032, loss_val: nan, pos_over_neg: 364.4184265136719 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.7486, loss_val: nan, pos_over_neg: 304.8342590332031 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.7457, loss_val: nan, pos_over_neg: 222.482177734375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.7357, loss_val: nan, pos_over_neg: 244.05755615234375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.7641, loss_val: nan, pos_over_neg: 312.0784606933594 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.7302, loss_val: nan, pos_over_neg: 252.77928161621094 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.7188, loss_val: nan, pos_over_neg: 226.61814880371094 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.7533, loss_val: nan, pos_over_neg: 343.6745910644531 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.7121, loss_val: nan, pos_over_neg: 348.98553466796875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.7225, loss_val: nan, pos_over_neg: 269.4414978027344 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.7575, loss_val: nan, pos_over_neg: 460.0743408203125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.7428, loss_val: nan, pos_over_neg: 363.70989990234375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.6914, loss_val: nan, pos_over_neg: 518.9757080078125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.7482, loss_val: nan, pos_over_neg: 433.01287841796875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.7676, loss_val: nan, pos_over_neg: 245.39109802246094 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.7408, loss_val: nan, pos_over_neg: 341.00482177734375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.7814, loss_val: nan, pos_over_neg: 309.9859313964844 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.7078, loss_val: nan, pos_over_neg: 416.928466796875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6881, loss_val: nan, pos_over_neg: 377.86920166015625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.7792, loss_val: nan, pos_over_neg: 263.6911315917969 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.7187, loss_val: nan, pos_over_neg: 328.6606140136719 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.7064, loss_val: nan, pos_over_neg: 372.49884033203125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.7351, loss_val: nan, pos_over_neg: 300.1894226074219 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.6896, loss_val: nan, pos_over_neg: 544.4149169921875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.7101, loss_val: nan, pos_over_neg: 278.1914978027344 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.7568, loss_val: nan, pos_over_neg: 378.07659912109375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.7356, loss_val: nan, pos_over_neg: 498.0713195800781 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.7506, loss_val: nan, pos_over_neg: 343.7870788574219 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.7059, loss_val: nan, pos_over_neg: 304.2691650390625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.7402, loss_val: nan, pos_over_neg: 238.0403594970703 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.773, loss_val: nan, pos_over_neg: 174.29315185546875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.7651, loss_val: nan, pos_over_neg: 194.26834106445312 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.7369, loss_val: nan, pos_over_neg: 183.95184326171875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6881, loss_val: nan, pos_over_neg: 387.93096923828125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.7439, loss_val: nan, pos_over_neg: 352.9974365234375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.6929, loss_val: nan, pos_over_neg: 491.7372131347656 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.7622, loss_val: nan, pos_over_neg: 303.8089599609375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.7555, loss_val: nan, pos_over_neg: 335.87896728515625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 370.6681823730469 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.731, loss_val: nan, pos_over_neg: 481.3614501953125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.7592, loss_val: nan, pos_over_neg: 214.7270965576172 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.7406, loss_val: nan, pos_over_neg: 248.14918518066406 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.7549, loss_val: nan, pos_over_neg: 177.7020721435547 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.7369, loss_val: nan, pos_over_neg: 188.39292907714844 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.6767, loss_val: nan, pos_over_neg: 265.9306335449219 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.7421, loss_val: nan, pos_over_neg: 371.3948669433594 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.7173, loss_val: nan, pos_over_neg: 515.1715698242188 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6831, loss_val: nan, pos_over_neg: 517.8077392578125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.7501, loss_val: nan, pos_over_neg: 440.22576904296875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.7269, loss_val: nan, pos_over_neg: 353.8858337402344 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.7416, loss_val: nan, pos_over_neg: 303.8303527832031 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.6865, loss_val: nan, pos_over_neg: 388.37615966796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.7406, loss_val: nan, pos_over_neg: 365.8371887207031 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.756, loss_val: nan, pos_over_neg: 213.5821533203125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.7547, loss_val: nan, pos_over_neg: 319.966064453125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.7434, loss_val: nan, pos_over_neg: 260.5098571777344 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.6967, loss_val: nan, pos_over_neg: 389.206787109375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.741, loss_val: nan, pos_over_neg: 401.2510070800781 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.7432, loss_val: nan, pos_over_neg: 278.44744873046875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.7301, loss_val: nan, pos_over_neg: 310.49761962890625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.745, loss_val: nan, pos_over_neg: 336.536865234375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.7397, loss_val: nan, pos_over_neg: 383.3680725097656 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6673, loss_val: nan, pos_over_neg: 1240.180908203125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.713, loss_val: nan, pos_over_neg: 448.53326416015625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.7831, loss_val: nan, pos_over_neg: 241.7114715576172 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.7307, loss_val: nan, pos_over_neg: 282.14898681640625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.723, loss_val: nan, pos_over_neg: 280.8756103515625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.7433, loss_val: nan, pos_over_neg: 273.1596374511719 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.7621, loss_val: nan, pos_over_neg: 260.88739013671875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.739, loss_val: nan, pos_over_neg: 306.62738037109375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.7861, loss_val: nan, pos_over_neg: 341.8636169433594 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.7453, loss_val: nan, pos_over_neg: 414.8619384765625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6948, loss_val: nan, pos_over_neg: 558.2964477539062 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.7102, loss_val: nan, pos_over_neg: 329.1330871582031 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.7257, loss_val: nan, pos_over_neg: 288.560791015625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.7315, loss_val: nan, pos_over_neg: 323.42352294921875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.7136, loss_val: nan, pos_over_neg: 240.1731719970703 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6923, loss_val: nan, pos_over_neg: 341.4952087402344 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.7147, loss_val: nan, pos_over_neg: 314.3526306152344 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.7658, loss_val: nan, pos_over_neg: 371.0307312011719 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.7443, loss_val: nan, pos_over_neg: 360.3961486816406 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 506.1842956542969 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.737, loss_val: nan, pos_over_neg: 544.9208984375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.7573, loss_val: nan, pos_over_neg: 361.1366882324219 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.7243, loss_val: nan, pos_over_neg: 373.4789733886719 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.7058, loss_val: nan, pos_over_neg: 462.6198425292969 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.738, loss_val: nan, pos_over_neg: 358.2083740234375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.7131, loss_val: nan, pos_over_neg: 396.9599609375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.7465, loss_val: nan, pos_over_neg: 279.81097412109375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.8186, loss_val: nan, pos_over_neg: 199.45782470703125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.7262, loss_val: nan, pos_over_neg: 434.4649353027344 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.7408, loss_val: nan, pos_over_neg: 459.58599853515625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.7501, loss_val: nan, pos_over_neg: 277.77935791015625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.7441, loss_val: nan, pos_over_neg: 304.635498046875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6845, loss_val: nan, pos_over_neg: 286.1408386230469 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.7159, loss_val: nan, pos_over_neg: 511.487548828125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.719, loss_val: nan, pos_over_neg: 372.90972900390625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.7005, loss_val: nan, pos_over_neg: 420.2781677246094 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.7423, loss_val: nan, pos_over_neg: 320.7646179199219 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.7205, loss_val: nan, pos_over_neg: 329.12506103515625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.7374, loss_val: nan, pos_over_neg: 214.07391357421875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.6924, loss_val: nan, pos_over_neg: 312.2030944824219 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.7717, loss_val: nan, pos_over_neg: 275.6647644042969 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.7175, loss_val: nan, pos_over_neg: 324.3117980957031 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.7117, loss_val: nan, pos_over_neg: 411.2787780761719 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.7332, loss_val: nan, pos_over_neg: 448.11810302734375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.7107, loss_val: nan, pos_over_neg: 311.7079772949219 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.6987, loss_val: nan, pos_over_neg: 289.0404052734375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.744, loss_val: nan, pos_over_neg: 307.2189025878906 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.7003, loss_val: nan, pos_over_neg: 386.6381530761719 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.7277, loss_val: nan, pos_over_neg: 313.6953125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.7471, loss_val: nan, pos_over_neg: 241.8770294189453 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.7725, loss_val: nan, pos_over_neg: 173.82354736328125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.6989, loss_val: nan, pos_over_neg: 270.6551513671875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 500.47454833984375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.7502, loss_val: nan, pos_over_neg: 294.6963806152344 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.6776, loss_val: nan, pos_over_neg: 503.3395690917969 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.7546, loss_val: nan, pos_over_neg: 269.93585205078125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.736, loss_val: nan, pos_over_neg: 381.74322509765625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.6991, loss_val: nan, pos_over_neg: 520.7412109375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.7697, loss_val: nan, pos_over_neg: 287.58880615234375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.738, loss_val: nan, pos_over_neg: 329.25274658203125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.7093, loss_val: nan, pos_over_neg: 370.2423095703125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.7504, loss_val: nan, pos_over_neg: 238.40016174316406 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.7535, loss_val: nan, pos_over_neg: 461.631103515625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.7366, loss_val: nan, pos_over_neg: 255.3372344970703 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.7457, loss_val: nan, pos_over_neg: 261.2308654785156 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.7001, loss_val: nan, pos_over_neg: 435.3763732910156 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.7315, loss_val: nan, pos_over_neg: 328.9644470214844 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.7209, loss_val: nan, pos_over_neg: 329.348388671875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.7317, loss_val: nan, pos_over_neg: 313.0496520996094 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.7387, loss_val: nan, pos_over_neg: 217.78936767578125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6905, loss_val: nan, pos_over_neg: 392.45428466796875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.6933, loss_val: nan, pos_over_neg: 435.4603271484375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.7207, loss_val: nan, pos_over_neg: 340.8507080078125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.7191, loss_val: nan, pos_over_neg: 316.3297119140625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.7137, loss_val: nan, pos_over_neg: 306.5926208496094 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.7696, loss_val: nan, pos_over_neg: 350.54656982421875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 311.69415283203125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.7184, loss_val: nan, pos_over_neg: 282.25714111328125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.7235, loss_val: nan, pos_over_neg: 235.30148315429688 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.7648, loss_val: nan, pos_over_neg: 269.6057434082031 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.7479, loss_val: nan, pos_over_neg: 269.33294677734375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.7934, loss_val: nan, pos_over_neg: 280.2643127441406 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.7644, loss_val: nan, pos_over_neg: 277.7824401855469 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.7172, loss_val: nan, pos_over_neg: 298.33111572265625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.7115, loss_val: nan, pos_over_neg: 387.365234375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.7371, loss_val: nan, pos_over_neg: 225.93283081054688 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.7302, loss_val: nan, pos_over_neg: 384.83770751953125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.7405, loss_val: nan, pos_over_neg: 348.4114990234375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.757, loss_val: nan, pos_over_neg: 292.1595458984375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.7314, loss_val: nan, pos_over_neg: 312.3146057128906 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.8144, loss_val: nan, pos_over_neg: 236.51669311523438 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.7977, loss_val: nan, pos_over_neg: 350.78656005859375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6987, loss_val: nan, pos_over_neg: 593.69775390625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6944, loss_val: nan, pos_over_neg: 400.79327392578125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.7376, loss_val: nan, pos_over_neg: 272.76702880859375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.7307, loss_val: nan, pos_over_neg: 287.58111572265625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.7195, loss_val: nan, pos_over_neg: 309.8885803222656 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.6931, loss_val: nan, pos_over_neg: 470.071044921875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.7584, loss_val: nan, pos_over_neg: 246.91261291503906 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.7541, loss_val: nan, pos_over_neg: 343.848876953125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.6885, loss_val: nan, pos_over_neg: 550.644287109375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.7277, loss_val: nan, pos_over_neg: 482.9559020996094 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.7063, loss_val: nan, pos_over_neg: 729.9535522460938 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.765, loss_val: nan, pos_over_neg: 270.6859436035156 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.6993, loss_val: nan, pos_over_neg: 242.59335327148438 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.7625, loss_val: nan, pos_over_neg: 309.929931640625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.7951, loss_val: nan, pos_over_neg: 252.61465454101562 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.7534, loss_val: nan, pos_over_neg: 324.2826232910156 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6892, loss_val: nan, pos_over_neg: 463.918701171875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.7154, loss_val: nan, pos_over_neg: 439.3390197753906 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.7359, loss_val: nan, pos_over_neg: 564.34326171875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.7108, loss_val: nan, pos_over_neg: 401.4037780761719 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.7088, loss_val: nan, pos_over_neg: 588.2803955078125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.7373, loss_val: nan, pos_over_neg: 250.3895263671875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.7292, loss_val: nan, pos_over_neg: 354.3294982910156 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 462.9182434082031 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.7507, loss_val: nan, pos_over_neg: 362.0084228515625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.7402, loss_val: nan, pos_over_neg: 290.2034606933594 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.7221, loss_val: nan, pos_over_neg: 375.36163330078125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.7356, loss_val: nan, pos_over_neg: 361.0888977050781 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.7602, loss_val: nan, pos_over_neg: 214.2960205078125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.7688, loss_val: nan, pos_over_neg: 317.5531921386719 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6709, loss_val: nan, pos_over_neg: 368.4935302734375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.7129, loss_val: nan, pos_over_neg: 478.13494873046875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.7438, loss_val: nan, pos_over_neg: 395.2176208496094 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.7189, loss_val: nan, pos_over_neg: 476.9794616699219 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.7681, loss_val: nan, pos_over_neg: 245.49989318847656 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.7167, loss_val: nan, pos_over_neg: 265.9158935546875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.7291, loss_val: nan, pos_over_neg: 256.7300720214844 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.7244, loss_val: nan, pos_over_neg: 317.1576843261719 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.7691, loss_val: nan, pos_over_neg: 161.4648895263672 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.7295, loss_val: nan, pos_over_neg: 226.7863006591797 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.672, loss_val: nan, pos_over_neg: 486.8833923339844 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.7127, loss_val: nan, pos_over_neg: 543.7684326171875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.7741, loss_val: nan, pos_over_neg: 383.90997314453125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.7546, loss_val: nan, pos_over_neg: 297.5787048339844 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.712, loss_val: nan, pos_over_neg: 314.1762390136719 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.705, loss_val: nan, pos_over_neg: 341.349853515625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.7176, loss_val: nan, pos_over_neg: 283.6733093261719 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6794, loss_val: nan, pos_over_neg: 298.23828125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.7299, loss_val: nan, pos_over_neg: 214.50390625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.7348, loss_val: nan, pos_over_neg: 262.776611328125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.7331, loss_val: nan, pos_over_neg: 353.8714904785156 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.7883, loss_val: nan, pos_over_neg: 282.9122619628906 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.7413, loss_val: nan, pos_over_neg: 294.5560607910156 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.7085, loss_val: nan, pos_over_neg: 352.8467102050781 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.7045, loss_val: nan, pos_over_neg: 312.9325866699219 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.7086, loss_val: nan, pos_over_neg: 285.21746826171875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 211.67269897460938 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.7042, loss_val: nan, pos_over_neg: 384.72222900390625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.7154, loss_val: nan, pos_over_neg: 264.3678283691406 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.7035, loss_val: nan, pos_over_neg: 354.74072265625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.7169, loss_val: nan, pos_over_neg: 341.5498962402344 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.7063, loss_val: nan, pos_over_neg: 321.4892883300781 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6982, loss_val: nan, pos_over_neg: 357.41522216796875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.7446, loss_val: nan, pos_over_neg: 342.5284118652344 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.7115, loss_val: nan, pos_over_neg: 287.61077880859375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.755, loss_val: nan, pos_over_neg: 370.11151123046875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.7489, loss_val: nan, pos_over_neg: 296.9295349121094 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.7096, loss_val: nan, pos_over_neg: 331.9525146484375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.7465, loss_val: nan, pos_over_neg: 346.24261474609375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.7526, loss_val: nan, pos_over_neg: 226.17137145996094 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.7147, loss_val: nan, pos_over_neg: 268.1835021972656 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.7434, loss_val: nan, pos_over_neg: 379.8626708984375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.7134, loss_val: nan, pos_over_neg: 300.29937744140625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.7177, loss_val: nan, pos_over_neg: 552.197509765625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.7623, loss_val: nan, pos_over_neg: 304.5913391113281 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.7436, loss_val: nan, pos_over_neg: 485.21368408203125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.7101, loss_val: nan, pos_over_neg: 437.00482177734375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.7007, loss_val: nan, pos_over_neg: 341.4095764160156 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.7064, loss_val: nan, pos_over_neg: 444.37890625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.7334, loss_val: nan, pos_over_neg: 432.5099182128906 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.7453, loss_val: nan, pos_over_neg: 245.13343811035156 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.7481, loss_val: nan, pos_over_neg: 296.9863586425781 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.7124, loss_val: nan, pos_over_neg: 404.2177429199219 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.7423, loss_val: nan, pos_over_neg: 418.3905944824219 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.7535, loss_val: nan, pos_over_neg: 206.01007080078125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.7461, loss_val: nan, pos_over_neg: 202.70248413085938 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:15:41<75455:24:29, 905.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.7135, loss_val: nan, pos_over_neg: 305.20379638671875 lr: 0.00031623\n",
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 3.7016, loss_val: nan, pos_over_neg: 512.1918334960938 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.7261, loss_val: nan, pos_over_neg: 370.0746765136719 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.685, loss_val: nan, pos_over_neg: 473.0284118652344 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.6914, loss_val: nan, pos_over_neg: 587.31982421875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6888, loss_val: nan, pos_over_neg: 450.1361999511719 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.7032, loss_val: nan, pos_over_neg: 518.4842529296875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.664, loss_val: nan, pos_over_neg: 558.9545288085938 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.725, loss_val: nan, pos_over_neg: 270.1484375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6645, loss_val: nan, pos_over_neg: 277.1529846191406 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6915, loss_val: nan, pos_over_neg: 235.7049102783203 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6874, loss_val: nan, pos_over_neg: 298.56982421875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.7686, loss_val: nan, pos_over_neg: 203.9420166015625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.6843, loss_val: nan, pos_over_neg: 314.3125305175781 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.7225, loss_val: nan, pos_over_neg: 341.3934020996094 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.7263, loss_val: nan, pos_over_neg: 299.3315734863281 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.7297, loss_val: nan, pos_over_neg: 622.4166259765625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.7172, loss_val: nan, pos_over_neg: 578.8540649414062 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.6973, loss_val: nan, pos_over_neg: 507.3136291503906 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.7405, loss_val: nan, pos_over_neg: 263.9359130859375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.7465, loss_val: nan, pos_over_neg: 273.7559814453125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.7276, loss_val: nan, pos_over_neg: 355.9035949707031 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.6748, loss_val: nan, pos_over_neg: 366.405029296875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.737, loss_val: nan, pos_over_neg: 276.5072021484375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.7217, loss_val: nan, pos_over_neg: 298.7000427246094 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.7077, loss_val: nan, pos_over_neg: 351.1209411621094 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.7217, loss_val: nan, pos_over_neg: 298.4411926269531 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.7681, loss_val: nan, pos_over_neg: 206.7009735107422 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.6995, loss_val: nan, pos_over_neg: 383.4977111816406 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.7275, loss_val: nan, pos_over_neg: 350.0825500488281 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.7274, loss_val: nan, pos_over_neg: 237.35446166992188 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.7276, loss_val: nan, pos_over_neg: 376.6877136230469 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.714, loss_val: nan, pos_over_neg: 433.7617492675781 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.7215, loss_val: nan, pos_over_neg: 487.146240234375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.744, loss_val: nan, pos_over_neg: 390.3952941894531 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.7177, loss_val: nan, pos_over_neg: 316.6423645019531 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.7321, loss_val: nan, pos_over_neg: 310.97283935546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.7264, loss_val: nan, pos_over_neg: 235.0513916015625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.7184, loss_val: nan, pos_over_neg: 259.9159240722656 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6786, loss_val: nan, pos_over_neg: 301.8177185058594 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.6953, loss_val: nan, pos_over_neg: 413.4744873046875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.7693, loss_val: nan, pos_over_neg: 215.55917358398438 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.7243, loss_val: nan, pos_over_neg: 243.46432495117188 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.7099, loss_val: nan, pos_over_neg: 325.40216064453125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.6889, loss_val: nan, pos_over_neg: 338.0284729003906 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.7111, loss_val: nan, pos_over_neg: 331.09368896484375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.7024, loss_val: nan, pos_over_neg: 509.9797058105469 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.74, loss_val: nan, pos_over_neg: 272.9665222167969 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.7368, loss_val: nan, pos_over_neg: 343.5606384277344 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.717, loss_val: nan, pos_over_neg: 303.33294677734375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.7322, loss_val: nan, pos_over_neg: 252.67404174804688 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.7114, loss_val: nan, pos_over_neg: 307.3683776855469 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.6995, loss_val: nan, pos_over_neg: 403.2413330078125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.6875, loss_val: nan, pos_over_neg: 326.9463806152344 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.7159, loss_val: nan, pos_over_neg: 379.37628173828125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6634, loss_val: nan, pos_over_neg: 360.49810791015625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.6861, loss_val: nan, pos_over_neg: 500.7856750488281 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.712, loss_val: nan, pos_over_neg: 320.1880798339844 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.7217, loss_val: nan, pos_over_neg: 261.8292541503906 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.7179, loss_val: nan, pos_over_neg: 216.7494659423828 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.7675, loss_val: nan, pos_over_neg: 212.30516052246094 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.7461, loss_val: nan, pos_over_neg: 203.1608428955078 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.7203, loss_val: nan, pos_over_neg: 484.8403625488281 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.6966, loss_val: nan, pos_over_neg: 329.6465759277344 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.7281, loss_val: nan, pos_over_neg: 297.3948059082031 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.7125, loss_val: nan, pos_over_neg: 317.2637023925781 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.7389, loss_val: nan, pos_over_neg: 293.08050537109375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.7305, loss_val: nan, pos_over_neg: 254.29000854492188 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6989, loss_val: nan, pos_over_neg: 246.3822021484375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.7342, loss_val: nan, pos_over_neg: 339.7351379394531 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.7121, loss_val: nan, pos_over_neg: 354.2222595214844 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.7457, loss_val: nan, pos_over_neg: 224.5950164794922 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.7286, loss_val: nan, pos_over_neg: 243.50033569335938 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.7298, loss_val: nan, pos_over_neg: 251.930419921875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.7045, loss_val: nan, pos_over_neg: 379.59423828125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6806, loss_val: nan, pos_over_neg: 341.70941162109375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.7329, loss_val: nan, pos_over_neg: 187.979248046875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.7275, loss_val: nan, pos_over_neg: 293.3442687988281 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 214.62673950195312 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.7259, loss_val: nan, pos_over_neg: 203.68003845214844 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6864, loss_val: nan, pos_over_neg: 299.9691162109375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.6978, loss_val: nan, pos_over_neg: 338.4749450683594 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.7055, loss_val: nan, pos_over_neg: 229.7350311279297 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.7433, loss_val: nan, pos_over_neg: 275.55596923828125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.7877, loss_val: nan, pos_over_neg: 176.63487243652344 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.7001, loss_val: nan, pos_over_neg: 196.95770263671875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6794, loss_val: nan, pos_over_neg: 246.5049285888672 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.7406, loss_val: nan, pos_over_neg: 352.9283447265625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.7132, loss_val: nan, pos_over_neg: 333.9944152832031 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.7057, loss_val: nan, pos_over_neg: 298.55889892578125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.7446, loss_val: nan, pos_over_neg: 265.2701110839844 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.7403, loss_val: nan, pos_over_neg: 349.7839660644531 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6753, loss_val: nan, pos_over_neg: 371.31671142578125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6816, loss_val: nan, pos_over_neg: 518.046875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.6931, loss_val: nan, pos_over_neg: 484.1922912597656 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6784, loss_val: nan, pos_over_neg: 391.4564514160156 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.7526, loss_val: nan, pos_over_neg: 325.3617858886719 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.7267, loss_val: nan, pos_over_neg: 305.94189453125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.6995, loss_val: nan, pos_over_neg: 274.2240905761719 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.6878, loss_val: nan, pos_over_neg: 222.657470703125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.7588, loss_val: nan, pos_over_neg: 197.41888427734375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.689, loss_val: nan, pos_over_neg: 331.07635498046875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.7059, loss_val: nan, pos_over_neg: 497.21893310546875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.7384, loss_val: nan, pos_over_neg: 392.24554443359375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.7116, loss_val: nan, pos_over_neg: 331.1166687011719 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.705, loss_val: nan, pos_over_neg: 441.3896179199219 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.7298, loss_val: nan, pos_over_neg: 350.6348876953125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.7394, loss_val: nan, pos_over_neg: 294.5078125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.7455, loss_val: nan, pos_over_neg: 347.20794677734375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.687, loss_val: nan, pos_over_neg: 277.349365234375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6756, loss_val: nan, pos_over_neg: 266.9957580566406 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6454, loss_val: nan, pos_over_neg: 478.23394775390625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6983, loss_val: nan, pos_over_neg: 346.7336730957031 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.7396, loss_val: nan, pos_over_neg: 263.6352844238281 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.7304, loss_val: nan, pos_over_neg: 340.87957763671875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.7172, loss_val: nan, pos_over_neg: 355.0497131347656 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.6964, loss_val: nan, pos_over_neg: 313.6867370605469 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.7095, loss_val: nan, pos_over_neg: 288.38616943359375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.7102, loss_val: nan, pos_over_neg: 282.5494384765625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.6884, loss_val: nan, pos_over_neg: 467.4468078613281 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.7615, loss_val: nan, pos_over_neg: 382.388916015625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 576.559326171875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.7386, loss_val: nan, pos_over_neg: 382.71954345703125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6957, loss_val: nan, pos_over_neg: 617.2158203125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 415.5035400390625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.705, loss_val: nan, pos_over_neg: 524.5450439453125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6632, loss_val: nan, pos_over_neg: 532.4744262695312 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.7089, loss_val: nan, pos_over_neg: 303.5193176269531 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.6997, loss_val: nan, pos_over_neg: 342.4397277832031 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.7282, loss_val: nan, pos_over_neg: 229.04214477539062 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.7038, loss_val: nan, pos_over_neg: 233.8537139892578 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.7382, loss_val: nan, pos_over_neg: 263.3746337890625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.7034, loss_val: nan, pos_over_neg: 396.4347229003906 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.6999, loss_val: nan, pos_over_neg: 536.166259765625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6888, loss_val: nan, pos_over_neg: 339.6154479980469 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.7575, loss_val: nan, pos_over_neg: 267.500244140625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.737, loss_val: nan, pos_over_neg: 202.00872802734375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.7134, loss_val: nan, pos_over_neg: 237.11148071289062 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.7244, loss_val: nan, pos_over_neg: 452.2350769042969 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.7619, loss_val: nan, pos_over_neg: 434.89599609375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.6906, loss_val: nan, pos_over_neg: 249.61251831054688 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.7279, loss_val: nan, pos_over_neg: 247.8950958251953 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.7163, loss_val: nan, pos_over_neg: 221.51931762695312 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.7223, loss_val: nan, pos_over_neg: 434.833251953125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.7196, loss_val: nan, pos_over_neg: 279.8865661621094 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.7117, loss_val: nan, pos_over_neg: 525.2308959960938 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.7275, loss_val: nan, pos_over_neg: 433.45294189453125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.719, loss_val: nan, pos_over_neg: 391.3132019042969 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.7384, loss_val: nan, pos_over_neg: 435.45172119140625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.7228, loss_val: nan, pos_over_neg: 299.9294128417969 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.7075, loss_val: nan, pos_over_neg: 235.06198120117188 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.7241, loss_val: nan, pos_over_neg: 217.6558074951172 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.7499, loss_val: nan, pos_over_neg: 213.80731201171875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.7149, loss_val: nan, pos_over_neg: 276.3182373046875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.7637, loss_val: nan, pos_over_neg: 191.7845458984375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.7417, loss_val: nan, pos_over_neg: 267.8713073730469 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.7693, loss_val: nan, pos_over_neg: 492.167724609375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6972, loss_val: nan, pos_over_neg: 418.4775390625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.7013, loss_val: nan, pos_over_neg: 321.22528076171875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.7184, loss_val: nan, pos_over_neg: 535.3305053710938 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.7154, loss_val: nan, pos_over_neg: 282.8901062011719 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.734, loss_val: nan, pos_over_neg: 224.33935546875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.7319, loss_val: nan, pos_over_neg: 327.25799560546875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.6889, loss_val: nan, pos_over_neg: 340.12738037109375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.7297, loss_val: nan, pos_over_neg: 263.0709533691406 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6867, loss_val: nan, pos_over_neg: 358.6131286621094 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.7244, loss_val: nan, pos_over_neg: 247.99148559570312 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.7481, loss_val: nan, pos_over_neg: 225.06585693359375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.6719, loss_val: nan, pos_over_neg: 380.8194274902344 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.7415, loss_val: nan, pos_over_neg: 318.8921203613281 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.746, loss_val: nan, pos_over_neg: 360.3870849609375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.7188, loss_val: nan, pos_over_neg: 338.92547607421875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6811, loss_val: nan, pos_over_neg: 344.55657958984375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.7445, loss_val: nan, pos_over_neg: 292.4981689453125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.7115, loss_val: nan, pos_over_neg: 334.5408630371094 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.7402, loss_val: nan, pos_over_neg: 260.3805847167969 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.7049, loss_val: nan, pos_over_neg: 301.1126708984375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.7023, loss_val: nan, pos_over_neg: 327.9112243652344 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.7139, loss_val: nan, pos_over_neg: 375.3742980957031 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.7098, loss_val: nan, pos_over_neg: 385.7947998046875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6956, loss_val: nan, pos_over_neg: 289.42181396484375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.7288, loss_val: nan, pos_over_neg: 367.1493225097656 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.6926, loss_val: nan, pos_over_neg: 331.4521179199219 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.7277, loss_val: nan, pos_over_neg: 267.4988098144531 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.7162, loss_val: nan, pos_over_neg: 409.3917541503906 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.7007, loss_val: nan, pos_over_neg: 372.1183166503906 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.765, loss_val: nan, pos_over_neg: 284.9666442871094 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.706, loss_val: nan, pos_over_neg: 332.37030029296875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.7533, loss_val: nan, pos_over_neg: 225.32411193847656 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.7165, loss_val: nan, pos_over_neg: 342.2985534667969 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.6981, loss_val: nan, pos_over_neg: 284.1192321777344 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.7419, loss_val: nan, pos_over_neg: 249.4923858642578 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.6921, loss_val: nan, pos_over_neg: 312.7753601074219 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.7436, loss_val: nan, pos_over_neg: 317.7232666015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 395.7839660644531 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.7227, loss_val: nan, pos_over_neg: 386.0408630371094 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.7688, loss_val: nan, pos_over_neg: 269.6847229003906 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.7426, loss_val: nan, pos_over_neg: 269.64263916015625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.6952, loss_val: nan, pos_over_neg: 222.96749877929688 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.7229, loss_val: nan, pos_over_neg: 246.06605529785156 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6854, loss_val: nan, pos_over_neg: 278.07470703125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6744, loss_val: nan, pos_over_neg: 381.3467712402344 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.7282, loss_val: nan, pos_over_neg: 269.98394775390625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.7361, loss_val: nan, pos_over_neg: 389.80828857421875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.7254, loss_val: nan, pos_over_neg: 253.02313232421875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.7136, loss_val: nan, pos_over_neg: 465.3014831542969 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.7609, loss_val: nan, pos_over_neg: 295.989501953125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.7004, loss_val: nan, pos_over_neg: 375.0381774902344 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.723, loss_val: nan, pos_over_neg: 362.6843566894531 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.7305, loss_val: nan, pos_over_neg: 323.628173828125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.723, loss_val: nan, pos_over_neg: 416.6590270996094 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6845, loss_val: nan, pos_over_neg: 277.25579833984375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.7372, loss_val: nan, pos_over_neg: 257.7066650390625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.7198, loss_val: nan, pos_over_neg: 301.1590576171875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.7904, loss_val: nan, pos_over_neg: 219.8994903564453 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.6858, loss_val: nan, pos_over_neg: 263.1809997558594 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.7164, loss_val: nan, pos_over_neg: 302.1167297363281 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.7076, loss_val: nan, pos_over_neg: 283.6468505859375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6569, loss_val: nan, pos_over_neg: 383.3814392089844 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.6933, loss_val: nan, pos_over_neg: 281.4208984375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.6695, loss_val: nan, pos_over_neg: 346.10296630859375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.7555, loss_val: nan, pos_over_neg: 331.8404846191406 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.7083, loss_val: nan, pos_over_neg: 243.7955780029297 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.7543, loss_val: nan, pos_over_neg: 254.1269073486328 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6898, loss_val: nan, pos_over_neg: 359.2477722167969 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.7318, loss_val: nan, pos_over_neg: 250.8037567138672 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6979, loss_val: nan, pos_over_neg: 251.29290771484375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.7237, loss_val: nan, pos_over_neg: 225.33853149414062 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.7124, loss_val: nan, pos_over_neg: 326.87115478515625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.7085, loss_val: nan, pos_over_neg: 297.5064392089844 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6877, loss_val: nan, pos_over_neg: 329.33966064453125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.701, loss_val: nan, pos_over_neg: 408.41473388671875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 312.9782409667969 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.7083, loss_val: nan, pos_over_neg: 367.37615966796875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.7035, loss_val: nan, pos_over_neg: 512.2324829101562 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 507.0865783691406 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.685, loss_val: nan, pos_over_neg: 374.2081604003906 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.7341, loss_val: nan, pos_over_neg: 385.79254150390625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.7363, loss_val: nan, pos_over_neg: 290.87286376953125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.7243, loss_val: nan, pos_over_neg: 233.472412109375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.712, loss_val: nan, pos_over_neg: 251.7525634765625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.7123, loss_val: nan, pos_over_neg: 362.7151794433594 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.7003, loss_val: nan, pos_over_neg: 291.35076904296875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.6985, loss_val: nan, pos_over_neg: 331.2436828613281 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.713, loss_val: nan, pos_over_neg: 396.30126953125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.7121, loss_val: nan, pos_over_neg: 566.4720458984375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.7328, loss_val: nan, pos_over_neg: 363.6560974121094 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.7018, loss_val: nan, pos_over_neg: 448.0375671386719 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6899, loss_val: nan, pos_over_neg: 323.5138854980469 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.7261, loss_val: nan, pos_over_neg: 360.2147521972656 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.7052, loss_val: nan, pos_over_neg: 469.56304931640625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.7327, loss_val: nan, pos_over_neg: 301.65777587890625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.7799, loss_val: nan, pos_over_neg: 206.37005615234375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.7344, loss_val: nan, pos_over_neg: 213.20123291015625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.7331, loss_val: nan, pos_over_neg: 333.3872985839844 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.7134, loss_val: nan, pos_over_neg: 393.7366027832031 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.722, loss_val: nan, pos_over_neg: 340.69500732421875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.7205, loss_val: nan, pos_over_neg: 325.56927490234375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6992, loss_val: nan, pos_over_neg: 346.72979736328125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.7298, loss_val: nan, pos_over_neg: 282.3035888671875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.7093, loss_val: nan, pos_over_neg: 418.9936828613281 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.7273, loss_val: nan, pos_over_neg: 325.05328369140625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 499.7137756347656 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6913, loss_val: nan, pos_over_neg: 554.9278564453125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.7378, loss_val: nan, pos_over_neg: 397.539306640625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.7212, loss_val: nan, pos_over_neg: 306.8140563964844 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.7526, loss_val: nan, pos_over_neg: 233.96514892578125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.7014, loss_val: nan, pos_over_neg: 216.5233154296875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.7489, loss_val: nan, pos_over_neg: 308.9444274902344 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.7365, loss_val: nan, pos_over_neg: 279.4747619628906 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.7117, loss_val: nan, pos_over_neg: 346.3846740722656 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.7097, loss_val: nan, pos_over_neg: 412.85113525390625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.7359, loss_val: nan, pos_over_neg: 459.8209228515625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.7421, loss_val: nan, pos_over_neg: 327.3287353515625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.7235, loss_val: nan, pos_over_neg: 300.88641357421875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.7019, loss_val: nan, pos_over_neg: 321.89105224609375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.7183, loss_val: nan, pos_over_neg: 291.5484313964844 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6957, loss_val: nan, pos_over_neg: 434.1332092285156 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.7155, loss_val: nan, pos_over_neg: 343.3249206542969 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.7621, loss_val: nan, pos_over_neg: 343.5408935546875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6803, loss_val: nan, pos_over_neg: 342.61663818359375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6976, loss_val: nan, pos_over_neg: 349.0144958496094 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.7047, loss_val: nan, pos_over_neg: 330.96075439453125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.7099, loss_val: nan, pos_over_neg: 401.21484375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.7505, loss_val: nan, pos_over_neg: 208.42201232910156 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.7337, loss_val: nan, pos_over_neg: 335.864990234375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6808, loss_val: nan, pos_over_neg: 366.2015380859375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.7574, loss_val: nan, pos_over_neg: 302.8145751953125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.7206, loss_val: nan, pos_over_neg: 307.9228515625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.7052, loss_val: nan, pos_over_neg: 272.0429992675781 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6941, loss_val: nan, pos_over_neg: 445.21820068359375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.7114, loss_val: nan, pos_over_neg: 445.4529113769531 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.7137, loss_val: nan, pos_over_neg: 390.7980041503906 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.7273, loss_val: nan, pos_over_neg: 286.0487976074219 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6954, loss_val: nan, pos_over_neg: 667.7196655273438 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6882, loss_val: nan, pos_over_neg: 434.6700134277344 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.7232, loss_val: nan, pos_over_neg: 498.9123840332031 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.7345, loss_val: nan, pos_over_neg: 319.329833984375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.7407, loss_val: nan, pos_over_neg: 409.95904541015625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.6698, loss_val: nan, pos_over_neg: 478.2014465332031 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6857, loss_val: nan, pos_over_neg: 464.5871276855469 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6688, loss_val: nan, pos_over_neg: 418.5018615722656 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.713, loss_val: nan, pos_over_neg: 326.0007019042969 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.672, loss_val: nan, pos_over_neg: 625.4703979492188 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 267.34991455078125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.7351, loss_val: nan, pos_over_neg: 249.54747009277344 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.7089, loss_val: nan, pos_over_neg: 286.7194519042969 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.7119, loss_val: nan, pos_over_neg: 375.9215087890625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.8133, loss_val: nan, pos_over_neg: 213.49526977539062 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.7151, loss_val: nan, pos_over_neg: 222.3181610107422 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6975, loss_val: nan, pos_over_neg: 342.36700439453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6979, loss_val: nan, pos_over_neg: 378.77899169921875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6915, loss_val: nan, pos_over_neg: 296.4483337402344 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.7041, loss_val: nan, pos_over_neg: 228.3695526123047 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.6772, loss_val: nan, pos_over_neg: 386.1252136230469 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.7065, loss_val: nan, pos_over_neg: 345.2790222167969 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 333.1550598144531 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.7071, loss_val: nan, pos_over_neg: 320.62103271484375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.702, loss_val: nan, pos_over_neg: 360.9818420410156 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.7091, loss_val: nan, pos_over_neg: 252.53619384765625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6688, loss_val: nan, pos_over_neg: 296.8078308105469 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.7351, loss_val: nan, pos_over_neg: 284.1602783203125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.724, loss_val: nan, pos_over_neg: 261.8976135253906 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.748, loss_val: nan, pos_over_neg: 384.541748046875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 291.3135681152344 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.7161, loss_val: nan, pos_over_neg: 230.20016479492188 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.7072, loss_val: nan, pos_over_neg: 282.1824645996094 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.6765, loss_val: nan, pos_over_neg: 349.53582763671875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6821, loss_val: nan, pos_over_neg: 454.0838317871094 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.7026, loss_val: nan, pos_over_neg: 363.45123291015625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.7097, loss_val: nan, pos_over_neg: 252.7037811279297 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.723, loss_val: nan, pos_over_neg: 268.9147033691406 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6998, loss_val: nan, pos_over_neg: 311.9468078613281 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.7036, loss_val: nan, pos_over_neg: 358.38519287109375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.6605, loss_val: nan, pos_over_neg: 391.6954040527344 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.7045, loss_val: nan, pos_over_neg: 265.5792236328125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6927, loss_val: nan, pos_over_neg: 333.2137451171875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.7096, loss_val: nan, pos_over_neg: 345.4488525390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.6976, loss_val: nan, pos_over_neg: 329.7002868652344 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.693, loss_val: nan, pos_over_neg: 321.1378173828125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.7808, loss_val: nan, pos_over_neg: 233.92901611328125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.7027, loss_val: nan, pos_over_neg: 293.9560546875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.7088, loss_val: nan, pos_over_neg: 257.8017272949219 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.7283, loss_val: nan, pos_over_neg: 327.8641662597656 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.6924, loss_val: nan, pos_over_neg: 323.11883544921875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.7507, loss_val: nan, pos_over_neg: 187.42982482910156 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 265.1461486816406 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6722, loss_val: nan, pos_over_neg: 587.158935546875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.6997, loss_val: nan, pos_over_neg: 282.7568664550781 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.7306, loss_val: nan, pos_over_neg: 384.0589294433594 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.7509, loss_val: nan, pos_over_neg: 238.24221801757812 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.7288, loss_val: nan, pos_over_neg: 375.9998474121094 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.7264, loss_val: nan, pos_over_neg: 298.8653869628906 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.744, loss_val: nan, pos_over_neg: 246.19288635253906 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.7447, loss_val: nan, pos_over_neg: 240.56741333007812 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.7237, loss_val: nan, pos_over_neg: 371.1767883300781 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 384.97198486328125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.7276, loss_val: nan, pos_over_neg: 374.99664306640625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6901, loss_val: nan, pos_over_neg: 555.5526733398438 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.7282, loss_val: nan, pos_over_neg: 369.3908386230469 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.7374, loss_val: nan, pos_over_neg: 310.52435302734375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.6796, loss_val: nan, pos_over_neg: 258.4140930175781 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.7345, loss_val: nan, pos_over_neg: 227.6449432373047 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6974, loss_val: nan, pos_over_neg: 292.91290283203125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 281.8919677734375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6785, loss_val: nan, pos_over_neg: 460.0385437011719 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.6835, loss_val: nan, pos_over_neg: 298.766357421875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.7044, loss_val: nan, pos_over_neg: 371.1899719238281 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.7288, loss_val: nan, pos_over_neg: 315.9032287597656 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.6905, loss_val: nan, pos_over_neg: 384.95086669921875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.7315, loss_val: nan, pos_over_neg: 278.13360595703125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.6973, loss_val: nan, pos_over_neg: 301.0201110839844 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.754, loss_val: nan, pos_over_neg: 244.66754150390625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.7565, loss_val: nan, pos_over_neg: 181.55259704589844 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.7324, loss_val: nan, pos_over_neg: 351.42291259765625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6855, loss_val: nan, pos_over_neg: 337.7261657714844 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.7165, loss_val: nan, pos_over_neg: 304.24163818359375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.714, loss_val: nan, pos_over_neg: 360.29571533203125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6742, loss_val: nan, pos_over_neg: 398.3742370605469 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.6784, loss_val: nan, pos_over_neg: 348.361083984375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.708, loss_val: nan, pos_over_neg: 254.8538055419922 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.7589, loss_val: nan, pos_over_neg: 246.83815002441406 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6642, loss_val: nan, pos_over_neg: 297.7982177734375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.7317, loss_val: nan, pos_over_neg: 329.64703369140625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.7098, loss_val: nan, pos_over_neg: 303.83258056640625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.7253, loss_val: nan, pos_over_neg: 234.93984985351562 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.7342, loss_val: nan, pos_over_neg: 341.6148376464844 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.7541, loss_val: nan, pos_over_neg: 226.46737670898438 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.7193, loss_val: nan, pos_over_neg: 215.65142822265625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.7265, loss_val: nan, pos_over_neg: 200.2361297607422 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.6782, loss_val: nan, pos_over_neg: 459.93499755859375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6816, loss_val: nan, pos_over_neg: 275.4026794433594 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.7401, loss_val: nan, pos_over_neg: 286.9307556152344 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.6917, loss_val: nan, pos_over_neg: 436.05621337890625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.7633, loss_val: nan, pos_over_neg: 259.9317321777344 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.725, loss_val: nan, pos_over_neg: 318.34515380859375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6659, loss_val: nan, pos_over_neg: 373.6668701171875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6896, loss_val: nan, pos_over_neg: 341.8876953125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.6944, loss_val: nan, pos_over_neg: 239.1407470703125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.7297, loss_val: nan, pos_over_neg: 264.60791015625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6975, loss_val: nan, pos_over_neg: 313.98992919921875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.7019, loss_val: nan, pos_over_neg: 279.2096252441406 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.6771, loss_val: nan, pos_over_neg: 409.7973937988281 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 294.78424072265625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.6944, loss_val: nan, pos_over_neg: 242.76528930664062 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.7434, loss_val: nan, pos_over_neg: 176.4426727294922 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 338.6110534667969 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.7071, loss_val: nan, pos_over_neg: 440.6341857910156 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.6757, loss_val: nan, pos_over_neg: 422.3321228027344 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 279.9703063964844 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.712, loss_val: nan, pos_over_neg: 381.56964111328125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.7079, loss_val: nan, pos_over_neg: 321.6575927734375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.6937, loss_val: nan, pos_over_neg: 299.7293395996094 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.6603, loss_val: nan, pos_over_neg: 363.11895751953125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.7244, loss_val: nan, pos_over_neg: 400.8121032714844 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.6891, loss_val: nan, pos_over_neg: 284.5648498535156 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.7078, loss_val: nan, pos_over_neg: 306.30023193359375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.7353, loss_val: nan, pos_over_neg: 212.7135467529297 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.7294, loss_val: nan, pos_over_neg: 278.7112121582031 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.7199, loss_val: nan, pos_over_neg: 245.6880340576172 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.7572, loss_val: nan, pos_over_neg: 208.63426208496094 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.7316, loss_val: nan, pos_over_neg: 279.9505310058594 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.7182, loss_val: nan, pos_over_neg: 294.3575744628906 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.6901, loss_val: nan, pos_over_neg: 703.0211791992188 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.7364, loss_val: nan, pos_over_neg: 662.6932983398438 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.7098, loss_val: nan, pos_over_neg: 471.924072265625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.7388, loss_val: nan, pos_over_neg: 347.62274169921875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.7172, loss_val: nan, pos_over_neg: 360.88531494140625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.7658, loss_val: nan, pos_over_neg: 294.5407409667969 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.7229, loss_val: nan, pos_over_neg: 220.7118377685547 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.7325, loss_val: nan, pos_over_neg: 207.68740844726562 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.716, loss_val: nan, pos_over_neg: 293.1632080078125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.7713, loss_val: nan, pos_over_neg: 229.6355743408203 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.7232, loss_val: nan, pos_over_neg: 313.1822204589844 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.7382, loss_val: nan, pos_over_neg: 211.585693359375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.7062, loss_val: nan, pos_over_neg: 257.9632568359375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.7129, loss_val: nan, pos_over_neg: 231.66563415527344 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 360.5393981933594 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6983, loss_val: nan, pos_over_neg: 338.7898254394531 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.7288, loss_val: nan, pos_over_neg: 509.7603454589844 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.7489, loss_val: nan, pos_over_neg: 380.3498840332031 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.694, loss_val: nan, pos_over_neg: 418.8990173339844 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 333.83636474609375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.6885, loss_val: nan, pos_over_neg: 299.8855285644531 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.7316, loss_val: nan, pos_over_neg: 283.2740478515625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.7199, loss_val: nan, pos_over_neg: 264.4371337890625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6773, loss_val: nan, pos_over_neg: 330.33197021484375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.73, loss_val: nan, pos_over_neg: 227.25682067871094 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.6959, loss_val: nan, pos_over_neg: 296.1046142578125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.7241, loss_val: nan, pos_over_neg: 255.01339721679688 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.7066, loss_val: nan, pos_over_neg: 801.9473876953125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 362.7059020996094 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.7268, loss_val: nan, pos_over_neg: 301.64410400390625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.6754, loss_val: nan, pos_over_neg: 275.32537841796875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.7353, loss_val: nan, pos_over_neg: 309.2182312011719 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.7139, loss_val: nan, pos_over_neg: 412.0209045410156 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.6919, loss_val: nan, pos_over_neg: 309.5096435546875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.7128, loss_val: nan, pos_over_neg: 276.5376892089844 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.7413, loss_val: nan, pos_over_neg: 266.9493713378906 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.6389, loss_val: nan, pos_over_neg: 495.3540954589844 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.7252, loss_val: nan, pos_over_neg: 271.9150085449219 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.6872, loss_val: nan, pos_over_neg: 316.27655029296875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.7003, loss_val: nan, pos_over_neg: 318.5019226074219 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6836, loss_val: nan, pos_over_neg: 390.7882080078125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.7264, loss_val: nan, pos_over_neg: 326.5714416503906 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.7437, loss_val: nan, pos_over_neg: 197.4254150390625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.7337, loss_val: nan, pos_over_neg: 253.34056091308594 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.6841, loss_val: nan, pos_over_neg: 316.3796081542969 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.6911, loss_val: nan, pos_over_neg: 508.71630859375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.7521, loss_val: nan, pos_over_neg: 200.9700164794922 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6595, loss_val: nan, pos_over_neg: 335.417724609375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.7251, loss_val: nan, pos_over_neg: 354.7693176269531 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.726, loss_val: nan, pos_over_neg: 358.08734130859375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6947, loss_val: nan, pos_over_neg: 352.7947692871094 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6875, loss_val: nan, pos_over_neg: 322.51416015625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.6824, loss_val: nan, pos_over_neg: 394.3104248046875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 584.709716796875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6999, loss_val: nan, pos_over_neg: 314.6451721191406 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.7064, loss_val: nan, pos_over_neg: 333.1656799316406 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.6777, loss_val: nan, pos_over_neg: 377.18621826171875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.7331, loss_val: nan, pos_over_neg: 311.27996826171875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.7206, loss_val: nan, pos_over_neg: 336.6041564941406 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.742, loss_val: nan, pos_over_neg: 278.8902893066406 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.7286, loss_val: nan, pos_over_neg: 242.75535583496094 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.7078, loss_val: nan, pos_over_neg: 236.0910186767578 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.7236, loss_val: nan, pos_over_neg: 300.98974609375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.6865, loss_val: nan, pos_over_neg: 343.14385986328125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.7198, loss_val: nan, pos_over_neg: 362.90008544921875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.726, loss_val: nan, pos_over_neg: 466.6086120605469 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.739, loss_val: nan, pos_over_neg: 400.2264709472656 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.7298, loss_val: nan, pos_over_neg: 609.4148559570312 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.6994, loss_val: nan, pos_over_neg: 388.4022521972656 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.6864, loss_val: nan, pos_over_neg: 322.14508056640625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6825, loss_val: nan, pos_over_neg: 448.47174072265625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6666, loss_val: nan, pos_over_neg: 276.37225341796875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.6739, loss_val: nan, pos_over_neg: 321.912109375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.6958, loss_val: nan, pos_over_neg: 309.39190673828125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6929, loss_val: nan, pos_over_neg: 270.1374816894531 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.7083, loss_val: nan, pos_over_neg: 319.09027099609375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.6938, loss_val: nan, pos_over_neg: 252.8949737548828 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 376.4741516113281 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.7387, loss_val: nan, pos_over_neg: 397.2994689941406 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.7153, loss_val: nan, pos_over_neg: 401.6952209472656 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.6878, loss_val: nan, pos_over_neg: 628.6588745117188 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.7019, loss_val: nan, pos_over_neg: 308.8761291503906 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.721, loss_val: nan, pos_over_neg: 348.1159973144531 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.7073, loss_val: nan, pos_over_neg: 567.748291015625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6863, loss_val: nan, pos_over_neg: 316.8757019042969 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6706, loss_val: nan, pos_over_neg: 341.18402099609375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.689, loss_val: nan, pos_over_neg: 264.5208740234375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.7017, loss_val: nan, pos_over_neg: 251.1870880126953 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.723, loss_val: nan, pos_over_neg: 389.7818298339844 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6939, loss_val: nan, pos_over_neg: 572.1203002929688 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.7125, loss_val: nan, pos_over_neg: 376.14776611328125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 618.2612915039062 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.6802, loss_val: nan, pos_over_neg: 510.3659362792969 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6974, loss_val: nan, pos_over_neg: 286.25897216796875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.7204, loss_val: nan, pos_over_neg: 307.9783020019531 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.7077, loss_val: nan, pos_over_neg: 318.0138854980469 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.7556, loss_val: nan, pos_over_neg: 254.9219512939453 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6855, loss_val: nan, pos_over_neg: 315.273681640625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.7083, loss_val: nan, pos_over_neg: 307.79083251953125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6479, loss_val: nan, pos_over_neg: 418.9245910644531 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.712, loss_val: nan, pos_over_neg: 485.2327575683594 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.7158, loss_val: nan, pos_over_neg: 343.8682861328125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.6881, loss_val: nan, pos_over_neg: 522.7149658203125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.7117, loss_val: nan, pos_over_neg: 387.2359313964844 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.7062, loss_val: nan, pos_over_neg: 319.1172790527344 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6893, loss_val: nan, pos_over_neg: 425.32958984375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.7322, loss_val: nan, pos_over_neg: 320.7873229980469 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6837, loss_val: nan, pos_over_neg: 427.40118408203125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.6992, loss_val: nan, pos_over_neg: 386.27203369140625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.7143, loss_val: nan, pos_over_neg: 262.86260986328125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.7431, loss_val: nan, pos_over_neg: 319.9189147949219 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.7054, loss_val: nan, pos_over_neg: 326.8958740234375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6625, loss_val: nan, pos_over_neg: 660.3961791992188 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.6653, loss_val: nan, pos_over_neg: 436.6075134277344 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.733, loss_val: nan, pos_over_neg: 324.773681640625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.6975, loss_val: nan, pos_over_neg: 358.2049560546875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.692, loss_val: nan, pos_over_neg: 409.2648010253906 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6747, loss_val: nan, pos_over_neg: 607.2619018554688 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.7023, loss_val: nan, pos_over_neg: 484.51708984375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.6984, loss_val: nan, pos_over_neg: 376.1636047363281 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.7037, loss_val: nan, pos_over_neg: 335.2662658691406 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.696, loss_val: nan, pos_over_neg: 279.412109375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 262.0260314941406 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.7157, loss_val: nan, pos_over_neg: 210.69842529296875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.7129, loss_val: nan, pos_over_neg: 293.753173828125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 271.833740234375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6631, loss_val: nan, pos_over_neg: 346.1552429199219 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.7175, loss_val: nan, pos_over_neg: 252.7843475341797 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6965, loss_val: nan, pos_over_neg: 296.58734130859375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6942, loss_val: nan, pos_over_neg: 376.3232727050781 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6706, loss_val: nan, pos_over_neg: 433.7049865722656 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 290.8740234375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.7218, loss_val: nan, pos_over_neg: 223.101318359375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.6878, loss_val: nan, pos_over_neg: 300.211181640625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.6538, loss_val: nan, pos_over_neg: 385.64898681640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.7408, loss_val: nan, pos_over_neg: 222.20889282226562 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.6905, loss_val: nan, pos_over_neg: 334.7804870605469 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.7345, loss_val: nan, pos_over_neg: 380.39862060546875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.7117, loss_val: nan, pos_over_neg: 352.0274658203125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.7065, loss_val: nan, pos_over_neg: 441.0449523925781 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.7147, loss_val: nan, pos_over_neg: 422.1782531738281 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.707, loss_val: nan, pos_over_neg: 663.9119873046875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.7155, loss_val: nan, pos_over_neg: 465.9346923828125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6804, loss_val: nan, pos_over_neg: 441.1996154785156 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.7155, loss_val: nan, pos_over_neg: 387.4606628417969 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.7291, loss_val: nan, pos_over_neg: 225.39553833007812 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.7136, loss_val: nan, pos_over_neg: 316.12060546875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.7319, loss_val: nan, pos_over_neg: 236.00955200195312 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.7132, loss_val: nan, pos_over_neg: 247.6205291748047 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 462.1190490722656 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.7086, loss_val: nan, pos_over_neg: 321.5283508300781 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.6592, loss_val: nan, pos_over_neg: 334.0670471191406 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6747, loss_val: nan, pos_over_neg: 506.5151062011719 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.7029, loss_val: nan, pos_over_neg: 474.1541442871094 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.7172, loss_val: nan, pos_over_neg: 383.3746032714844 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6788, loss_val: nan, pos_over_neg: 406.7128601074219 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.6644, loss_val: nan, pos_over_neg: 470.6508483886719 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.6961, loss_val: nan, pos_over_neg: 300.8720397949219 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.6796, loss_val: nan, pos_over_neg: 300.7045593261719 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6813, loss_val: nan, pos_over_neg: 305.0459899902344 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 342.9137878417969 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.6875, loss_val: nan, pos_over_neg: 228.35198974609375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.712, loss_val: nan, pos_over_neg: 243.9028778076172 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 408.7184753417969 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6963, loss_val: nan, pos_over_neg: 332.34271240234375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.7295, loss_val: nan, pos_over_neg: 319.4100646972656 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.6974, loss_val: nan, pos_over_neg: 327.05499267578125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.7555, loss_val: nan, pos_over_neg: 290.2665100097656 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.6982, loss_val: nan, pos_over_neg: 235.74795532226562 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.6871, loss_val: nan, pos_over_neg: 311.4853210449219 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.756, loss_val: nan, pos_over_neg: 237.140380859375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.7653, loss_val: nan, pos_over_neg: 218.4012451171875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.7222, loss_val: nan, pos_over_neg: 332.7685852050781 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.704, loss_val: nan, pos_over_neg: 386.4057922363281 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.7501, loss_val: nan, pos_over_neg: 551.0824584960938 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 453.96240234375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.6983, loss_val: nan, pos_over_neg: 314.8760681152344 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.6924, loss_val: nan, pos_over_neg: 484.2124328613281 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.6775, loss_val: nan, pos_over_neg: 513.689453125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.6538, loss_val: nan, pos_over_neg: 556.0476684570312 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.7081, loss_val: nan, pos_over_neg: 274.69677734375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.7165, loss_val: nan, pos_over_neg: 262.12158203125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.7015, loss_val: nan, pos_over_neg: 252.5209503173828 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.6904, loss_val: nan, pos_over_neg: 322.6995544433594 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.7615, loss_val: nan, pos_over_neg: 196.33560180664062 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.6945, loss_val: nan, pos_over_neg: 446.21063232421875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.7002, loss_val: nan, pos_over_neg: 462.7555847167969 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.7086, loss_val: nan, pos_over_neg: 422.5473937988281 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.7008, loss_val: nan, pos_over_neg: 624.6701049804688 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.6953, loss_val: nan, pos_over_neg: 576.9418334960938 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6993, loss_val: nan, pos_over_neg: 534.4514770507812 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.6871, loss_val: nan, pos_over_neg: 370.0903625488281 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.7607, loss_val: nan, pos_over_neg: 154.067626953125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.6941, loss_val: nan, pos_over_neg: 242.88626098632812 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.6659, loss_val: nan, pos_over_neg: 339.1387023925781 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.7077, loss_val: nan, pos_over_neg: 288.0328674316406 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.6758, loss_val: nan, pos_over_neg: 491.2530517578125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.7322, loss_val: nan, pos_over_neg: 343.046875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.6857, loss_val: nan, pos_over_neg: 489.8297119140625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.688, loss_val: nan, pos_over_neg: 338.3023681640625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.7328, loss_val: nan, pos_over_neg: 336.65625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.6853, loss_val: nan, pos_over_neg: 365.05267333984375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.7139, loss_val: nan, pos_over_neg: 264.7680969238281 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.7398, loss_val: nan, pos_over_neg: 195.8844451904297 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.6878, loss_val: nan, pos_over_neg: 281.790283203125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.7212, loss_val: nan, pos_over_neg: 279.36279296875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6431, loss_val: nan, pos_over_neg: 664.1754150390625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.7033, loss_val: nan, pos_over_neg: 309.4962158203125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.687, loss_val: nan, pos_over_neg: 462.8531799316406 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.7263, loss_val: nan, pos_over_neg: 353.76251220703125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.723, loss_val: nan, pos_over_neg: 330.0391845703125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6938, loss_val: nan, pos_over_neg: 470.7957458496094 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6968, loss_val: nan, pos_over_neg: 333.20672607421875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.7017, loss_val: nan, pos_over_neg: 273.9166259765625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 320.8333740234375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.6858, loss_val: nan, pos_over_neg: 418.44561767578125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6741, loss_val: nan, pos_over_neg: 309.4577331542969 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.7223, loss_val: nan, pos_over_neg: 351.5251159667969 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.7418, loss_val: nan, pos_over_neg: 347.6018981933594 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.7277, loss_val: nan, pos_over_neg: 446.09686279296875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.6456, loss_val: nan, pos_over_neg: 651.5407104492188 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.6921, loss_val: nan, pos_over_neg: 472.21636962890625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.7088, loss_val: nan, pos_over_neg: 288.5172424316406 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.7761, loss_val: nan, pos_over_neg: 228.9089813232422 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.702, loss_val: nan, pos_over_neg: 356.5953063964844 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.7002, loss_val: nan, pos_over_neg: 412.07720947265625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.6774, loss_val: nan, pos_over_neg: 372.2122497558594 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.6606, loss_val: nan, pos_over_neg: 539.6842041015625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.6963, loss_val: nan, pos_over_neg: 323.4842529296875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.6985, loss_val: nan, pos_over_neg: 346.6455993652344 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.6813, loss_val: nan, pos_over_neg: 381.2516174316406 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.7386, loss_val: nan, pos_over_neg: 344.88665771484375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6553, loss_val: nan, pos_over_neg: 546.548095703125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.675, loss_val: nan, pos_over_neg: 530.808349609375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.703, loss_val: nan, pos_over_neg: 370.5353088378906 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6883, loss_val: nan, pos_over_neg: 434.6064758300781 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6983, loss_val: nan, pos_over_neg: 310.2471618652344 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6952, loss_val: nan, pos_over_neg: 299.48260498046875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6919, loss_val: nan, pos_over_neg: 350.61944580078125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.6811, loss_val: nan, pos_over_neg: 304.7514343261719 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.6948, loss_val: nan, pos_over_neg: 290.1236267089844 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.724, loss_val: nan, pos_over_neg: 174.44644165039062 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 596.3094482421875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.7232, loss_val: nan, pos_over_neg: 303.7160949707031 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.7352, loss_val: nan, pos_over_neg: 293.072509765625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.6685, loss_val: nan, pos_over_neg: 413.75341796875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.7229, loss_val: nan, pos_over_neg: 228.88404846191406 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.713, loss_val: nan, pos_over_neg: 284.2541198730469 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6949, loss_val: nan, pos_over_neg: 261.903076171875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 495.32781982421875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.6711, loss_val: nan, pos_over_neg: 317.7123718261719 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.6802, loss_val: nan, pos_over_neg: 351.17657470703125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.7263, loss_val: nan, pos_over_neg: 401.73956298828125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.7571, loss_val: nan, pos_over_neg: 339.7445983886719 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6929, loss_val: nan, pos_over_neg: 450.9420471191406 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.7171, loss_val: nan, pos_over_neg: 330.94635009765625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.6508, loss_val: nan, pos_over_neg: 653.1180419921875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.693, loss_val: nan, pos_over_neg: 345.9532775878906 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.7023, loss_val: nan, pos_over_neg: 346.9056701660156 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6879, loss_val: nan, pos_over_neg: 430.57073974609375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6709, loss_val: nan, pos_over_neg: 460.8780822753906 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.6579, loss_val: nan, pos_over_neg: 320.7438049316406 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.6792, loss_val: nan, pos_over_neg: 306.40087890625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.7063, loss_val: nan, pos_over_neg: 356.5368347167969 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.7049, loss_val: nan, pos_over_neg: 395.5231628417969 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.6791, loss_val: nan, pos_over_neg: 544.9078979492188 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.7055, loss_val: nan, pos_over_neg: 489.449951171875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.6873, loss_val: nan, pos_over_neg: 307.6058349609375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.7503, loss_val: nan, pos_over_neg: 288.8885803222656 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6857, loss_val: nan, pos_over_neg: 264.36102294921875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.7234, loss_val: nan, pos_over_neg: 277.8388366699219 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6994, loss_val: nan, pos_over_neg: 378.2461242675781 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [1:30:46<75443:16:37, 905.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.6972, loss_val: nan, pos_over_neg: 237.25430297851562 lr: 0.00031623\n",
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 3.7103, loss_val: nan, pos_over_neg: 314.2431945800781 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.7032, loss_val: nan, pos_over_neg: 320.0061340332031 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.7349, loss_val: nan, pos_over_neg: 266.8605651855469 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 622.6722412109375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.7196, loss_val: nan, pos_over_neg: 308.4683837890625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.7342, loss_val: nan, pos_over_neg: 274.7271423339844 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 439.6731872558594 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.7694, loss_val: nan, pos_over_neg: 248.04119873046875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6781, loss_val: nan, pos_over_neg: 332.2933349609375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6949, loss_val: nan, pos_over_neg: 282.96051025390625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6882, loss_val: nan, pos_over_neg: 304.5990905761719 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.684, loss_val: nan, pos_over_neg: 253.761962890625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.7229, loss_val: nan, pos_over_neg: 201.46128845214844 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.6864, loss_val: nan, pos_over_neg: 371.51226806640625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6627, loss_val: nan, pos_over_neg: 595.585205078125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.6913, loss_val: nan, pos_over_neg: 436.32696533203125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.6888, loss_val: nan, pos_over_neg: 409.7139892578125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.6942, loss_val: nan, pos_over_neg: 517.0379638671875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.704, loss_val: nan, pos_over_neg: 337.1206359863281 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.7012, loss_val: nan, pos_over_neg: 328.9965515136719 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6874, loss_val: nan, pos_over_neg: 355.49896240234375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 325.7750244140625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.685, loss_val: nan, pos_over_neg: 327.7637939453125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 270.9244079589844 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6651, loss_val: nan, pos_over_neg: 244.7548065185547 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.7245, loss_val: nan, pos_over_neg: 396.1895446777344 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.69, loss_val: nan, pos_over_neg: 506.5309143066406 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.6809, loss_val: nan, pos_over_neg: 430.69482421875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 323.7666015625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.7218, loss_val: nan, pos_over_neg: 519.96630859375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6435, loss_val: nan, pos_over_neg: 683.267578125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.6758, loss_val: nan, pos_over_neg: 385.95758056640625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.7059, loss_val: nan, pos_over_neg: 250.35731506347656 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.7148, loss_val: nan, pos_over_neg: 200.75619506835938 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.6824, loss_val: nan, pos_over_neg: 332.07891845703125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.6806, loss_val: nan, pos_over_neg: 303.35357666015625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6774, loss_val: nan, pos_over_neg: 409.56219482421875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.6817, loss_val: nan, pos_over_neg: 463.35125732421875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6809, loss_val: nan, pos_over_neg: 470.2194519042969 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.694, loss_val: nan, pos_over_neg: 431.1710205078125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.7116, loss_val: nan, pos_over_neg: 487.53802490234375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 1293.9381103515625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.7231, loss_val: nan, pos_over_neg: 499.13934326171875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.6568, loss_val: nan, pos_over_neg: 442.1949157714844 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.7105, loss_val: nan, pos_over_neg: 275.7700500488281 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6371, loss_val: nan, pos_over_neg: 407.0373229980469 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.7023, loss_val: nan, pos_over_neg: 333.93035888671875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6637, loss_val: nan, pos_over_neg: 339.80029296875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.7145, loss_val: nan, pos_over_neg: 307.366943359375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.7583, loss_val: nan, pos_over_neg: 287.6165466308594 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.7461, loss_val: nan, pos_over_neg: 346.8897705078125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.7352, loss_val: nan, pos_over_neg: 277.8064880371094 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.6774, loss_val: nan, pos_over_neg: 327.16845703125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.7028, loss_val: nan, pos_over_neg: 374.4987487792969 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6668, loss_val: nan, pos_over_neg: 345.1658935546875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.7177, loss_val: nan, pos_over_neg: 284.88873291015625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.7348, loss_val: nan, pos_over_neg: 378.3433532714844 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.7228, loss_val: nan, pos_over_neg: 259.1673583984375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.6768, loss_val: nan, pos_over_neg: 371.3152160644531 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.778, loss_val: nan, pos_over_neg: 245.38677978515625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.736, loss_val: nan, pos_over_neg: 224.50772094726562 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.71, loss_val: nan, pos_over_neg: 264.902099609375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.7017, loss_val: nan, pos_over_neg: 275.9090270996094 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6621, loss_val: nan, pos_over_neg: 395.752197265625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 574.1110229492188 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.7151, loss_val: nan, pos_over_neg: 315.9794616699219 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.7228, loss_val: nan, pos_over_neg: 240.31381225585938 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6967, loss_val: nan, pos_over_neg: 490.0107727050781 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.7235, loss_val: nan, pos_over_neg: 435.8136291503906 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.69, loss_val: nan, pos_over_neg: 268.5858459472656 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.7136, loss_val: nan, pos_over_neg: 243.9004669189453 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.6434, loss_val: nan, pos_over_neg: 336.4601745605469 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.7143, loss_val: nan, pos_over_neg: 289.73748779296875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.7214, loss_val: nan, pos_over_neg: 325.7319641113281 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.7111, loss_val: nan, pos_over_neg: 351.6152648925781 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.6766, loss_val: nan, pos_over_neg: 346.8238525390625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.641, loss_val: nan, pos_over_neg: 869.1022338867188 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.6808, loss_val: nan, pos_over_neg: 385.2328186035156 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.7091, loss_val: nan, pos_over_neg: 303.7515869140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6638, loss_val: nan, pos_over_neg: 376.26007080078125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.7201, loss_val: nan, pos_over_neg: 246.84925842285156 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.6966, loss_val: nan, pos_over_neg: 310.9111022949219 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.705, loss_val: nan, pos_over_neg: 260.0147705078125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.6807, loss_val: nan, pos_over_neg: 441.15521240234375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.7126, loss_val: nan, pos_over_neg: 353.0292053222656 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6867, loss_val: nan, pos_over_neg: 369.31829833984375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.6626, loss_val: nan, pos_over_neg: 405.8425598144531 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.6632, loss_val: nan, pos_over_neg: 558.376220703125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.7174, loss_val: nan, pos_over_neg: 408.3896179199219 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6682, loss_val: nan, pos_over_neg: 342.50927734375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.7038, loss_val: nan, pos_over_neg: 241.66000366210938 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.7153, loss_val: nan, pos_over_neg: 264.9705505371094 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.7048, loss_val: nan, pos_over_neg: 342.8655700683594 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.7139, loss_val: nan, pos_over_neg: 234.9657440185547 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.7557, loss_val: nan, pos_over_neg: 197.55165100097656 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.7175, loss_val: nan, pos_over_neg: 278.53961181640625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.6829, loss_val: nan, pos_over_neg: 340.21551513671875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.665, loss_val: nan, pos_over_neg: 350.7643127441406 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.6688, loss_val: nan, pos_over_neg: 652.7610473632812 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.701, loss_val: nan, pos_over_neg: 569.44775390625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.7331, loss_val: nan, pos_over_neg: 368.4217529296875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6837, loss_val: nan, pos_over_neg: 315.5575866699219 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.7464, loss_val: nan, pos_over_neg: 165.02452087402344 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.7334, loss_val: nan, pos_over_neg: 266.751953125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.7055, loss_val: nan, pos_over_neg: 283.772216796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.6904, loss_val: nan, pos_over_neg: 336.1446228027344 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.6605, loss_val: nan, pos_over_neg: 360.4351501464844 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.744, loss_val: nan, pos_over_neg: 234.03729248046875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6928, loss_val: nan, pos_over_neg: 304.5572204589844 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6861, loss_val: nan, pos_over_neg: 504.5085144042969 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6574, loss_val: nan, pos_over_neg: 562.1669311523438 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 522.5673217773438 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.6844, loss_val: nan, pos_over_neg: 477.5375671386719 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 268.2704162597656 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.7002, loss_val: nan, pos_over_neg: 270.5090026855469 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.6676, loss_val: nan, pos_over_neg: 311.58880615234375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6856, loss_val: nan, pos_over_neg: 276.8410339355469 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6846, loss_val: nan, pos_over_neg: 366.5555419921875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.6803, loss_val: nan, pos_over_neg: 285.4706115722656 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.6849, loss_val: nan, pos_over_neg: 297.2740478515625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.717, loss_val: nan, pos_over_neg: 375.1007995605469 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.7073, loss_val: nan, pos_over_neg: 520.6971435546875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6698, loss_val: nan, pos_over_neg: 428.5054016113281 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.7209, loss_val: nan, pos_over_neg: 336.28594970703125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.703, loss_val: nan, pos_over_neg: 542.042724609375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6795, loss_val: nan, pos_over_neg: 326.71533203125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.7341, loss_val: nan, pos_over_neg: 246.41123962402344 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.7095, loss_val: nan, pos_over_neg: 240.71246337890625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.6798, loss_val: nan, pos_over_neg: 294.73980712890625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.6684, loss_val: nan, pos_over_neg: 490.5550537109375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.7207, loss_val: nan, pos_over_neg: 291.448974609375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.71, loss_val: nan, pos_over_neg: 373.57861328125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.7711, loss_val: nan, pos_over_neg: 387.1260681152344 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6796, loss_val: nan, pos_over_neg: 363.40728759765625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 424.3063659667969 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.738, loss_val: nan, pos_over_neg: 300.8172302246094 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.6885, loss_val: nan, pos_over_neg: 401.2478332519531 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.7168, loss_val: nan, pos_over_neg: 297.13446044921875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.7081, loss_val: nan, pos_over_neg: 407.1861572265625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 439.0448913574219 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6899, loss_val: nan, pos_over_neg: 288.1146545410156 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.7423, loss_val: nan, pos_over_neg: 292.9902038574219 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.695, loss_val: nan, pos_over_neg: 417.8069152832031 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.7604, loss_val: nan, pos_over_neg: 332.9076843261719 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.69, loss_val: nan, pos_over_neg: 395.4574890136719 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.693, loss_val: nan, pos_over_neg: 284.34149169921875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.6759, loss_val: nan, pos_over_neg: 426.65728759765625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.68, loss_val: nan, pos_over_neg: 322.376220703125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.6933, loss_val: nan, pos_over_neg: 241.87518310546875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.7401, loss_val: nan, pos_over_neg: 330.52142333984375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.6756, loss_val: nan, pos_over_neg: 396.7032775878906 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.6715, loss_val: nan, pos_over_neg: 481.2564392089844 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6714, loss_val: nan, pos_over_neg: 437.17437744140625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 452.59820556640625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.7537, loss_val: nan, pos_over_neg: 301.0538024902344 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 324.7615051269531 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6853, loss_val: nan, pos_over_neg: 379.3338317871094 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.6828, loss_val: nan, pos_over_neg: 280.0191345214844 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.6903, loss_val: nan, pos_over_neg: 334.6039733886719 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6624, loss_val: nan, pos_over_neg: 349.49566650390625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.7114, loss_val: nan, pos_over_neg: 283.2132568359375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.7063, loss_val: nan, pos_over_neg: 351.9425354003906 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.7438, loss_val: nan, pos_over_neg: 275.9506530761719 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.7112, loss_val: nan, pos_over_neg: 373.9858093261719 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 473.17498779296875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.6746, loss_val: nan, pos_over_neg: 489.7587585449219 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.676, loss_val: nan, pos_over_neg: 374.391357421875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.7153, loss_val: nan, pos_over_neg: 568.0939331054688 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 442.6893005371094 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.6472, loss_val: nan, pos_over_neg: 453.69317626953125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.6885, loss_val: nan, pos_over_neg: 332.928466796875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6997, loss_val: nan, pos_over_neg: 296.7035217285156 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.7251, loss_val: nan, pos_over_neg: 242.81600952148438 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.7095, loss_val: nan, pos_over_neg: 434.0005798339844 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.6814, loss_val: nan, pos_over_neg: 500.4878234863281 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.6889, loss_val: nan, pos_over_neg: 759.4891967773438 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6828, loss_val: nan, pos_over_neg: 451.33221435546875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 530.597900390625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6832, loss_val: nan, pos_over_neg: 420.3071594238281 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 344.2113342285156 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6868, loss_val: nan, pos_over_neg: 295.23431396484375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.701, loss_val: nan, pos_over_neg: 305.72723388671875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.7026, loss_val: nan, pos_over_neg: 364.7765808105469 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.7142, loss_val: nan, pos_over_neg: 317.4908142089844 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6562, loss_val: nan, pos_over_neg: 383.7028503417969 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.6968, loss_val: nan, pos_over_neg: 340.92755126953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.723, loss_val: nan, pos_over_neg: 334.22760009765625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.7034, loss_val: nan, pos_over_neg: 311.70550537109375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.7444, loss_val: nan, pos_over_neg: 318.81829833984375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.6272, loss_val: nan, pos_over_neg: 532.7301025390625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.6659, loss_val: nan, pos_over_neg: 550.2392578125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.6931, loss_val: nan, pos_over_neg: 242.46340942382812 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.6514, loss_val: nan, pos_over_neg: 352.0580139160156 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.6775, loss_val: nan, pos_over_neg: 348.00750732421875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.6969, loss_val: nan, pos_over_neg: 268.6524353027344 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.6874, loss_val: nan, pos_over_neg: 262.7102355957031 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.6644, loss_val: nan, pos_over_neg: 284.21929931640625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 300.28729248046875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.7054, loss_val: nan, pos_over_neg: 212.18545532226562 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6859, loss_val: nan, pos_over_neg: 217.61932373046875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.7017, loss_val: nan, pos_over_neg: 282.4227294921875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.6492, loss_val: nan, pos_over_neg: 762.3306274414062 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 369.16912841796875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.6757, loss_val: nan, pos_over_neg: 421.217529296875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.6758, loss_val: nan, pos_over_neg: 413.40667724609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.7042, loss_val: nan, pos_over_neg: 400.8599853515625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.6631, loss_val: nan, pos_over_neg: 470.21307373046875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.7468, loss_val: nan, pos_over_neg: 250.57769775390625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.6901, loss_val: nan, pos_over_neg: 264.83221435546875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.6987, loss_val: nan, pos_over_neg: 363.677001953125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6823, loss_val: nan, pos_over_neg: 264.8511962890625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6652, loss_val: nan, pos_over_neg: 537.2911987304688 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.6984, loss_val: nan, pos_over_neg: 407.6733703613281 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.7083, loss_val: nan, pos_over_neg: 213.26026916503906 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.7056, loss_val: nan, pos_over_neg: 251.34066772460938 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.719, loss_val: nan, pos_over_neg: 420.0140380859375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.6783, loss_val: nan, pos_over_neg: 595.4513549804688 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.7037, loss_val: nan, pos_over_neg: 338.5618591308594 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.7041, loss_val: nan, pos_over_neg: 330.460693359375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.6763, loss_val: nan, pos_over_neg: 630.4158935546875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 523.9948120117188 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.7102, loss_val: nan, pos_over_neg: 279.1577453613281 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.7119, loss_val: nan, pos_over_neg: 292.74029541015625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.7173, loss_val: nan, pos_over_neg: 302.49676513671875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 354.4165344238281 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6896, loss_val: nan, pos_over_neg: 373.89678955078125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6563, loss_val: nan, pos_over_neg: 469.9638366699219 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6635, loss_val: nan, pos_over_neg: 995.2277221679688 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.7153, loss_val: nan, pos_over_neg: 440.2930603027344 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6714, loss_val: nan, pos_over_neg: 537.510498046875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.7146, loss_val: nan, pos_over_neg: 367.0738525390625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.6423, loss_val: nan, pos_over_neg: 357.4808654785156 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.6234, loss_val: nan, pos_over_neg: 396.38531494140625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.6864, loss_val: nan, pos_over_neg: 297.9020080566406 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.677, loss_val: nan, pos_over_neg: 259.2178649902344 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.7039, loss_val: nan, pos_over_neg: 288.0165100097656 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.6784, loss_val: nan, pos_over_neg: 334.4140625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.6958, loss_val: nan, pos_over_neg: 472.6939392089844 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.7092, loss_val: nan, pos_over_neg: 248.9288330078125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 639.0100708007812 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.7189, loss_val: nan, pos_over_neg: 194.4088134765625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.6835, loss_val: nan, pos_over_neg: 313.6817626953125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.6918, loss_val: nan, pos_over_neg: 320.33221435546875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.7159, loss_val: nan, pos_over_neg: 365.0362854003906 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.7182, loss_val: nan, pos_over_neg: 241.38856506347656 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6786, loss_val: nan, pos_over_neg: 318.9559631347656 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.737, loss_val: nan, pos_over_neg: 246.7525634765625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6978, loss_val: nan, pos_over_neg: 379.7540588378906 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6839, loss_val: nan, pos_over_neg: 431.51031494140625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 452.96826171875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6476, loss_val: nan, pos_over_neg: 558.7164306640625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.7098, loss_val: nan, pos_over_neg: 380.6536865234375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.657, loss_val: nan, pos_over_neg: 444.4283142089844 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.7031, loss_val: nan, pos_over_neg: 382.3385009765625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6708, loss_val: nan, pos_over_neg: 358.6167907714844 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6575, loss_val: nan, pos_over_neg: 286.3277587890625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.712, loss_val: nan, pos_over_neg: 224.90017700195312 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.7685, loss_val: nan, pos_over_neg: 165.27235412597656 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.7099, loss_val: nan, pos_over_neg: 260.7386474609375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 615.7597045898438 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.7224, loss_val: nan, pos_over_neg: 497.57891845703125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6964, loss_val: nan, pos_over_neg: 653.6006469726562 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6869, loss_val: nan, pos_over_neg: 507.85504150390625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.7329, loss_val: nan, pos_over_neg: 487.5321960449219 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.6971, loss_val: nan, pos_over_neg: 474.9085388183594 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.7305, loss_val: nan, pos_over_neg: 274.3441467285156 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 391.47552490234375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.6906, loss_val: nan, pos_over_neg: 230.71383666992188 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.6757, loss_val: nan, pos_over_neg: 452.46112060546875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.6669, loss_val: nan, pos_over_neg: 324.7337646484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.698, loss_val: nan, pos_over_neg: 435.2242126464844 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.6529, loss_val: nan, pos_over_neg: 374.0378723144531 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 343.52313232421875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.6792, loss_val: nan, pos_over_neg: 303.1622619628906 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6888, loss_val: nan, pos_over_neg: 746.6950073242188 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.7054, loss_val: nan, pos_over_neg: 374.5030517578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 381.15216064453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.7007, loss_val: nan, pos_over_neg: 298.3065185546875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.6989, loss_val: nan, pos_over_neg: 299.91619873046875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.7248, loss_val: nan, pos_over_neg: 244.2198028564453 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.659, loss_val: nan, pos_over_neg: 318.32928466796875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.6944, loss_val: nan, pos_over_neg: 266.7293701171875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6628, loss_val: nan, pos_over_neg: 276.1475830078125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 596.233642578125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.6769, loss_val: nan, pos_over_neg: 484.6294250488281 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6686, loss_val: nan, pos_over_neg: 503.84478759765625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.7155, loss_val: nan, pos_over_neg: 423.7319641113281 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.7089, loss_val: nan, pos_over_neg: 311.9994812011719 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.6638, loss_val: nan, pos_over_neg: 413.4462890625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6858, loss_val: nan, pos_over_neg: 382.02099609375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.7237, loss_val: nan, pos_over_neg: 246.7296905517578 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.6631, loss_val: nan, pos_over_neg: 520.4125366210938 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.7093, loss_val: nan, pos_over_neg: 311.7381286621094 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.685, loss_val: nan, pos_over_neg: 285.94976806640625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.7288, loss_val: nan, pos_over_neg: 242.80787658691406 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.7155, loss_val: nan, pos_over_neg: 275.1474914550781 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6591, loss_val: nan, pos_over_neg: 297.3116760253906 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.6932, loss_val: nan, pos_over_neg: 463.0197448730469 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.7695, loss_val: nan, pos_over_neg: 257.65948486328125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6898, loss_val: nan, pos_over_neg: 471.5797119140625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6915, loss_val: nan, pos_over_neg: 566.9776000976562 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.6809, loss_val: nan, pos_over_neg: 508.29443359375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.6653, loss_val: nan, pos_over_neg: 631.1880493164062 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6855, loss_val: nan, pos_over_neg: 409.182373046875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.7149, loss_val: nan, pos_over_neg: 304.7340087890625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.7081, loss_val: nan, pos_over_neg: 290.8270568847656 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.7242, loss_val: nan, pos_over_neg: 264.0057373046875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.7028, loss_val: nan, pos_over_neg: 352.7417907714844 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.6751, loss_val: nan, pos_over_neg: 638.5845336914062 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.7119, loss_val: nan, pos_over_neg: 527.3164672851562 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6984, loss_val: nan, pos_over_neg: 635.3903198242188 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.7003, loss_val: nan, pos_over_neg: 594.006591796875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.6841, loss_val: nan, pos_over_neg: 508.4611511230469 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.7231, loss_val: nan, pos_over_neg: 316.4378967285156 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.7211, loss_val: nan, pos_over_neg: 329.6964111328125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.7548, loss_val: nan, pos_over_neg: 235.16986083984375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.7211, loss_val: nan, pos_over_neg: 213.8250274658203 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.6956, loss_val: nan, pos_over_neg: 275.0928039550781 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6632, loss_val: nan, pos_over_neg: 417.7922668457031 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.7181, loss_val: nan, pos_over_neg: 311.17315673828125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6874, loss_val: nan, pos_over_neg: 412.28997802734375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 505.0745849609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.7204, loss_val: nan, pos_over_neg: 490.7032775878906 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.6875, loss_val: nan, pos_over_neg: 578.4730224609375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.6993, loss_val: nan, pos_over_neg: 536.7166748046875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 417.3514404296875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 251.25978088378906 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6691, loss_val: nan, pos_over_neg: 298.0251770019531 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 335.5558166503906 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 283.66888427734375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.6697, loss_val: nan, pos_over_neg: 211.20729064941406 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6529, loss_val: nan, pos_over_neg: 350.0806884765625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.7021, loss_val: nan, pos_over_neg: 535.3936157226562 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.706, loss_val: nan, pos_over_neg: 376.18310546875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.7487, loss_val: nan, pos_over_neg: 270.1250915527344 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 277.5199890136719 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.7531, loss_val: nan, pos_over_neg: 267.1166076660156 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 905.6294555664062 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.6791, loss_val: nan, pos_over_neg: 305.4385070800781 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6692, loss_val: nan, pos_over_neg: 273.3601379394531 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.7113, loss_val: nan, pos_over_neg: 306.86767578125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 337.6698303222656 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.7177, loss_val: nan, pos_over_neg: 353.10174560546875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.6753, loss_val: nan, pos_over_neg: 330.571044921875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.6123, loss_val: nan, pos_over_neg: 619.3540649414062 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.7388, loss_val: nan, pos_over_neg: 411.62457275390625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.682, loss_val: nan, pos_over_neg: 489.0445861816406 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.6986, loss_val: nan, pos_over_neg: 476.7041015625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.718, loss_val: nan, pos_over_neg: 250.54469299316406 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.747, loss_val: nan, pos_over_neg: 221.44464111328125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.6779, loss_val: nan, pos_over_neg: 344.3377685546875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 299.7815856933594 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6727, loss_val: nan, pos_over_neg: 293.92352294921875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.7291, loss_val: nan, pos_over_neg: 339.62493896484375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.7297, loss_val: nan, pos_over_neg: 297.3296813964844 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 958.1551513671875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6536, loss_val: nan, pos_over_neg: 592.165283203125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6661, loss_val: nan, pos_over_neg: 544.9829711914062 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.6757, loss_val: nan, pos_over_neg: 579.5173950195312 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 407.2492370605469 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.7077, loss_val: nan, pos_over_neg: 244.7013702392578 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 370.71826171875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6763, loss_val: nan, pos_over_neg: 590.8743896484375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.6843, loss_val: nan, pos_over_neg: 468.0850830078125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6608, loss_val: nan, pos_over_neg: 444.7572937011719 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.7077, loss_val: nan, pos_over_neg: 342.25482177734375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.6696, loss_val: nan, pos_over_neg: 469.1382141113281 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 678.789306640625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.647, loss_val: nan, pos_over_neg: 414.86126708984375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.6594, loss_val: nan, pos_over_neg: 587.5802612304688 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.7273, loss_val: nan, pos_over_neg: 383.803466796875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.6893, loss_val: nan, pos_over_neg: 359.94891357421875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.6627, loss_val: nan, pos_over_neg: 346.2217102050781 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.6769, loss_val: nan, pos_over_neg: 364.6917724609375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6841, loss_val: nan, pos_over_neg: 392.2272033691406 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.683, loss_val: nan, pos_over_neg: 276.9922180175781 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.7182, loss_val: nan, pos_over_neg: 245.6009521484375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6724, loss_val: nan, pos_over_neg: 397.39752197265625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.7404, loss_val: nan, pos_over_neg: 290.71234130859375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.6878, loss_val: nan, pos_over_neg: 326.981201171875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.6689, loss_val: nan, pos_over_neg: 577.4827270507812 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.7156, loss_val: nan, pos_over_neg: 354.2281188964844 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6234, loss_val: nan, pos_over_neg: 464.6427917480469 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 537.501953125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.7135, loss_val: nan, pos_over_neg: 372.4306335449219 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.6848, loss_val: nan, pos_over_neg: 304.2688293457031 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.7, loss_val: nan, pos_over_neg: 330.67889404296875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6559, loss_val: nan, pos_over_neg: 358.349609375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.6661, loss_val: nan, pos_over_neg: 792.081787109375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.6737, loss_val: nan, pos_over_neg: 515.6611328125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 707.6519775390625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6558, loss_val: nan, pos_over_neg: 500.214599609375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.6831, loss_val: nan, pos_over_neg: 302.0242919921875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6591, loss_val: nan, pos_over_neg: 362.02923583984375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 448.21527099609375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6819, loss_val: nan, pos_over_neg: 307.38885498046875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6889, loss_val: nan, pos_over_neg: 248.46864318847656 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.7188, loss_val: nan, pos_over_neg: 219.56509399414062 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.714, loss_val: nan, pos_over_neg: 393.8847351074219 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6539, loss_val: nan, pos_over_neg: 529.5541381835938 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 541.2992553710938 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.6954, loss_val: nan, pos_over_neg: 355.6219787597656 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.6819, loss_val: nan, pos_over_neg: 406.4241943359375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.6507, loss_val: nan, pos_over_neg: 440.4290771484375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.6743, loss_val: nan, pos_over_neg: 376.89947509765625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6609, loss_val: nan, pos_over_neg: 316.23785400390625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.6773, loss_val: nan, pos_over_neg: 448.9783935546875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.6271, loss_val: nan, pos_over_neg: 426.93682861328125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 285.57147216796875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.7182, loss_val: nan, pos_over_neg: 237.0725555419922 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 391.2645568847656 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.6828, loss_val: nan, pos_over_neg: 448.85675048828125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.6942, loss_val: nan, pos_over_neg: 313.0240478515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.678, loss_val: nan, pos_over_neg: 477.8774719238281 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.7102, loss_val: nan, pos_over_neg: 327.52972412109375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.6587, loss_val: nan, pos_over_neg: 735.9746704101562 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.6761, loss_val: nan, pos_over_neg: 495.0940856933594 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.6882, loss_val: nan, pos_over_neg: 392.9451599121094 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.6893, loss_val: nan, pos_over_neg: 388.259765625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.6617, loss_val: nan, pos_over_neg: 400.3028564453125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6576, loss_val: nan, pos_over_neg: 484.97186279296875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.6903, loss_val: nan, pos_over_neg: 446.636474609375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.6441, loss_val: nan, pos_over_neg: 375.2461853027344 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.6568, loss_val: nan, pos_over_neg: 274.1028747558594 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.6543, loss_val: nan, pos_over_neg: 386.7991027832031 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.6954, loss_val: nan, pos_over_neg: 314.3532409667969 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.6705, loss_val: nan, pos_over_neg: 335.9766540527344 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6929, loss_val: nan, pos_over_neg: 343.46441650390625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 492.8446960449219 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.6847, loss_val: nan, pos_over_neg: 314.45355224609375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.6709, loss_val: nan, pos_over_neg: 439.0060729980469 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 580.442626953125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.6291, loss_val: nan, pos_over_neg: 569.659423828125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.709, loss_val: nan, pos_over_neg: 279.4794616699219 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.6882, loss_val: nan, pos_over_neg: 467.09002685546875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.6584, loss_val: nan, pos_over_neg: 455.6964416503906 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.713, loss_val: nan, pos_over_neg: 387.5045471191406 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6877, loss_val: nan, pos_over_neg: 295.3681335449219 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.6856, loss_val: nan, pos_over_neg: 436.6759338378906 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.6386, loss_val: nan, pos_over_neg: 458.2268371582031 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.6849, loss_val: nan, pos_over_neg: 466.81964111328125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.6353, loss_val: nan, pos_over_neg: 361.9514465332031 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.6805, loss_val: nan, pos_over_neg: 529.9180297851562 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.6626, loss_val: nan, pos_over_neg: 456.700927734375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 427.81427001953125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 588.0119018554688 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 390.172607421875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.7296, loss_val: nan, pos_over_neg: 278.2054138183594 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.6862, loss_val: nan, pos_over_neg: 360.92083740234375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 541.1846923828125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.717, loss_val: nan, pos_over_neg: 272.8835754394531 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.6559, loss_val: nan, pos_over_neg: 446.18048095703125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.6762, loss_val: nan, pos_over_neg: 408.30999755859375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.7126, loss_val: nan, pos_over_neg: 240.72024536132812 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.7115, loss_val: nan, pos_over_neg: 381.46307373046875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.7076, loss_val: nan, pos_over_neg: 420.2557678222656 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.6649, loss_val: nan, pos_over_neg: 354.8260498046875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.6866, loss_val: nan, pos_over_neg: 480.8197937011719 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.7244, loss_val: nan, pos_over_neg: 342.05902099609375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.7028, loss_val: nan, pos_over_neg: 316.0572814941406 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.6867, loss_val: nan, pos_over_neg: 374.2991943359375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 492.5665283203125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6682, loss_val: nan, pos_over_neg: 451.1754455566406 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.6814, loss_val: nan, pos_over_neg: 403.1866760253906 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.7128, loss_val: nan, pos_over_neg: 322.8221130371094 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.7099, loss_val: nan, pos_over_neg: 431.92547607421875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.6711, loss_val: nan, pos_over_neg: 362.8547668457031 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.6763, loss_val: nan, pos_over_neg: 559.9569091796875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.6765, loss_val: nan, pos_over_neg: 518.440185546875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6928, loss_val: nan, pos_over_neg: 411.7232971191406 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 350.1807556152344 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.6786, loss_val: nan, pos_over_neg: 362.45721435546875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6751, loss_val: nan, pos_over_neg: 506.1060485839844 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6838, loss_val: nan, pos_over_neg: 408.1234130859375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.6852, loss_val: nan, pos_over_neg: 429.6532897949219 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6686, loss_val: nan, pos_over_neg: 311.0838317871094 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.707, loss_val: nan, pos_over_neg: 327.7748107910156 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.7555, loss_val: nan, pos_over_neg: 199.9064178466797 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.6608, loss_val: nan, pos_over_neg: 451.0653991699219 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.6791, loss_val: nan, pos_over_neg: 361.412353515625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.7208, loss_val: nan, pos_over_neg: 301.95269775390625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.7054, loss_val: nan, pos_over_neg: 338.6932373046875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.7198, loss_val: nan, pos_over_neg: 360.38385009765625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.6776, loss_val: nan, pos_over_neg: 363.3965148925781 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.6778, loss_val: nan, pos_over_neg: 416.0396728515625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.6815, loss_val: nan, pos_over_neg: 430.12738037109375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.6928, loss_val: nan, pos_over_neg: 318.4736328125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6521, loss_val: nan, pos_over_neg: 570.6583251953125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.6781, loss_val: nan, pos_over_neg: 361.2457580566406 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.6906, loss_val: nan, pos_over_neg: 325.8670349121094 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 524.9078979492188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.7178, loss_val: nan, pos_over_neg: 380.20831298828125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6448, loss_val: nan, pos_over_neg: 739.1017456054688 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6648, loss_val: nan, pos_over_neg: 331.0765686035156 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.6414, loss_val: nan, pos_over_neg: 356.3056335449219 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.699, loss_val: nan, pos_over_neg: 266.13934326171875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6629, loss_val: nan, pos_over_neg: 367.7176513671875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.6945, loss_val: nan, pos_over_neg: 380.1288146972656 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.6662, loss_val: nan, pos_over_neg: 333.30010986328125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6566, loss_val: nan, pos_over_neg: 489.4493408203125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.7289, loss_val: nan, pos_over_neg: 332.87652587890625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6805, loss_val: nan, pos_over_neg: 417.8759460449219 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.6785, loss_val: nan, pos_over_neg: 431.7286376953125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.6737, loss_val: nan, pos_over_neg: 339.9302978515625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.7223, loss_val: nan, pos_over_neg: 463.0578308105469 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.7176, loss_val: nan, pos_over_neg: 390.9892578125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6296, loss_val: nan, pos_over_neg: 520.7758178710938 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6903, loss_val: nan, pos_over_neg: 374.2689208984375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6667, loss_val: nan, pos_over_neg: 391.9658203125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.6953, loss_val: nan, pos_over_neg: 336.510009765625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.6625, loss_val: nan, pos_over_neg: 328.3485107421875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6676, loss_val: nan, pos_over_neg: 286.1543884277344 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.6878, loss_val: nan, pos_over_neg: 279.6434020996094 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.7026, loss_val: nan, pos_over_neg: 208.44998168945312 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.6767, loss_val: nan, pos_over_neg: 276.9754638671875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.7256, loss_val: nan, pos_over_neg: 243.4523468017578 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 410.576416015625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6981, loss_val: nan, pos_over_neg: 303.81878662109375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.6772, loss_val: nan, pos_over_neg: 332.8926086425781 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6872, loss_val: nan, pos_over_neg: 285.77789306640625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.6713, loss_val: nan, pos_over_neg: 414.96343994140625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6722, loss_val: nan, pos_over_neg: 401.0324401855469 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.6507, loss_val: nan, pos_over_neg: 327.30224609375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6767, loss_val: nan, pos_over_neg: 305.48199462890625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.6735, loss_val: nan, pos_over_neg: 300.2959899902344 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.6975, loss_val: nan, pos_over_neg: 236.86093139648438 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6504, loss_val: nan, pos_over_neg: 363.9390869140625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.665, loss_val: nan, pos_over_neg: 368.9380798339844 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.6807, loss_val: nan, pos_over_neg: 546.9492797851562 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.7191, loss_val: nan, pos_over_neg: 308.8398742675781 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.7064, loss_val: nan, pos_over_neg: 348.1353759765625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 463.4383850097656 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 571.3848876953125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.669, loss_val: nan, pos_over_neg: 345.3555908203125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6527, loss_val: nan, pos_over_neg: 246.28094482421875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.7004, loss_val: nan, pos_over_neg: 242.81520080566406 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.689, loss_val: nan, pos_over_neg: 317.8481750488281 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.6632, loss_val: nan, pos_over_neg: 388.54205322265625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 311.5556335449219 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 594.3561401367188 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.6676, loss_val: nan, pos_over_neg: 500.9484558105469 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.7345, loss_val: nan, pos_over_neg: 227.08685302734375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6932, loss_val: nan, pos_over_neg: 292.7439880371094 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.6538, loss_val: nan, pos_over_neg: 585.2311401367188 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.706, loss_val: nan, pos_over_neg: 235.22454833984375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.6831, loss_val: nan, pos_over_neg: 286.4942626953125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.6544, loss_val: nan, pos_over_neg: 441.9648132324219 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6589, loss_val: nan, pos_over_neg: 460.7955627441406 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 412.227294921875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.6555, loss_val: nan, pos_over_neg: 313.26763916015625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.679, loss_val: nan, pos_over_neg: 436.5340576171875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 618.6061401367188 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6965, loss_val: nan, pos_over_neg: 362.7841796875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.6781, loss_val: nan, pos_over_neg: 424.17047119140625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.6731, loss_val: nan, pos_over_neg: 320.75146484375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.6809, loss_val: nan, pos_over_neg: 398.7992248535156 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.7068, loss_val: nan, pos_over_neg: 407.0776062011719 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.6748, loss_val: nan, pos_over_neg: 474.2828063964844 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.7127, loss_val: nan, pos_over_neg: 356.8102111816406 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.7164, loss_val: nan, pos_over_neg: 299.92974853515625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.7357, loss_val: nan, pos_over_neg: 284.5750427246094 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.7093, loss_val: nan, pos_over_neg: 298.12518310546875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.6773, loss_val: nan, pos_over_neg: 318.5942077636719 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 441.4928894042969 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.7258, loss_val: nan, pos_over_neg: 377.5723571777344 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6982, loss_val: nan, pos_over_neg: 374.3310241699219 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.6977, loss_val: nan, pos_over_neg: 441.7249450683594 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.7037, loss_val: nan, pos_over_neg: 377.1417541503906 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6698, loss_val: nan, pos_over_neg: 416.6490173339844 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.7003, loss_val: nan, pos_over_neg: 288.9461669921875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.7141, loss_val: nan, pos_over_neg: 313.4134216308594 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6659, loss_val: nan, pos_over_neg: 663.914306640625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.6373, loss_val: nan, pos_over_neg: 442.22705078125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.6791, loss_val: nan, pos_over_neg: 353.54180908203125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6513, loss_val: nan, pos_over_neg: 376.7893981933594 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.6732, loss_val: nan, pos_over_neg: 420.84844970703125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.679, loss_val: nan, pos_over_neg: 274.46697998046875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6568, loss_val: nan, pos_over_neg: 415.1732482910156 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 433.7830505371094 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.7084, loss_val: nan, pos_over_neg: 342.1667175292969 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.7187, loss_val: nan, pos_over_neg: 275.9983825683594 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 549.4100952148438 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.6965, loss_val: nan, pos_over_neg: 474.4319763183594 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.6403, loss_val: nan, pos_over_neg: 578.7407836914062 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.6737, loss_val: nan, pos_over_neg: 441.44537353515625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.6866, loss_val: nan, pos_over_neg: 454.5224609375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6807, loss_val: nan, pos_over_neg: 342.5360107421875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.6554, loss_val: nan, pos_over_neg: 420.3984375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.7047, loss_val: nan, pos_over_neg: 264.98504638671875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.6736, loss_val: nan, pos_over_neg: 367.6153259277344 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.6713, loss_val: nan, pos_over_neg: 483.9012145996094 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.6777, loss_val: nan, pos_over_neg: 508.7536315917969 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.7162, loss_val: nan, pos_over_neg: 296.0701904296875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.6565, loss_val: nan, pos_over_neg: 761.8539428710938 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 628.2095947265625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.677, loss_val: nan, pos_over_neg: 406.576904296875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6802, loss_val: nan, pos_over_neg: 386.3443603515625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6887, loss_val: nan, pos_over_neg: 312.5401611328125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.6657, loss_val: nan, pos_over_neg: 337.2139892578125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.6615, loss_val: nan, pos_over_neg: 342.32745361328125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.6898, loss_val: nan, pos_over_neg: 462.2293395996094 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.7376, loss_val: nan, pos_over_neg: 359.7189025878906 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.7253, loss_val: nan, pos_over_neg: 339.18243408203125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.6925, loss_val: nan, pos_over_neg: 347.5761413574219 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.6634, loss_val: nan, pos_over_neg: 516.4950561523438 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.673, loss_val: nan, pos_over_neg: 462.8753967285156 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.6712, loss_val: nan, pos_over_neg: 649.534912109375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.6921, loss_val: nan, pos_over_neg: 305.3470764160156 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6787, loss_val: nan, pos_over_neg: 450.6875305175781 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 278.67578125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.7349, loss_val: nan, pos_over_neg: 240.80838012695312 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.6947, loss_val: nan, pos_over_neg: 263.2885437011719 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6706, loss_val: nan, pos_over_neg: 293.8868713378906 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.7237, loss_val: nan, pos_over_neg: 304.9780578613281 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.6963, loss_val: nan, pos_over_neg: 309.0477600097656 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 331.67864990234375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.696, loss_val: nan, pos_over_neg: 364.3146667480469 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.6942, loss_val: nan, pos_over_neg: 475.2408752441406 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.7049, loss_val: nan, pos_over_neg: 307.23736572265625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6967, loss_val: nan, pos_over_neg: 398.6203918457031 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.7015, loss_val: nan, pos_over_neg: 437.5946960449219 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 517.2992553710938 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.7317, loss_val: nan, pos_over_neg: 209.24884033203125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.6791, loss_val: nan, pos_over_neg: 280.1743469238281 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6813, loss_val: nan, pos_over_neg: 291.7037353515625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.732, loss_val: nan, pos_over_neg: 250.42784118652344 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 417.9678955078125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.6315, loss_val: nan, pos_over_neg: 436.38470458984375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6612, loss_val: nan, pos_over_neg: 461.527099609375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.6696, loss_val: nan, pos_over_neg: 308.58697509765625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6676, loss_val: nan, pos_over_neg: 415.4419250488281 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.7096, loss_val: nan, pos_over_neg: 369.9779968261719 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.6979, loss_val: nan, pos_over_neg: 401.34893798828125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6592, loss_val: nan, pos_over_neg: 485.3446350097656 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6883, loss_val: nan, pos_over_neg: 271.0093078613281 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.6553, loss_val: nan, pos_over_neg: 418.6989440917969 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.7035, loss_val: nan, pos_over_neg: 227.45501708984375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.6569, loss_val: nan, pos_over_neg: 238.07489013671875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6451, loss_val: nan, pos_over_neg: 263.1006164550781 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.6973, loss_val: nan, pos_over_neg: 352.5780334472656 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.6706, loss_val: nan, pos_over_neg: 257.5512390136719 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.6996, loss_val: nan, pos_over_neg: 315.1026611328125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.6656, loss_val: nan, pos_over_neg: 332.9311828613281 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.6848, loss_val: nan, pos_over_neg: 428.04425048828125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6649, loss_val: nan, pos_over_neg: 330.18896484375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.6634, loss_val: nan, pos_over_neg: 381.9111633300781 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.6947, loss_val: nan, pos_over_neg: 243.3175506591797 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.6862, loss_val: nan, pos_over_neg: 320.2601623535156 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 270.4717102050781 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.7068, loss_val: nan, pos_over_neg: 283.4935302734375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.682, loss_val: nan, pos_over_neg: 323.4569396972656 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.7094, loss_val: nan, pos_over_neg: 200.00160217285156 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.676, loss_val: nan, pos_over_neg: 293.52874755859375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.6496, loss_val: nan, pos_over_neg: 482.42181396484375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6793, loss_val: nan, pos_over_neg: 400.4347839355469 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 369.2971496582031 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.6375, loss_val: nan, pos_over_neg: 565.8181762695312 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6334, loss_val: nan, pos_over_neg: 612.2607421875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6962, loss_val: nan, pos_over_neg: 384.7164306640625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6704, loss_val: nan, pos_over_neg: 271.261474609375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6559, loss_val: nan, pos_over_neg: 314.5178527832031 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.6932, loss_val: nan, pos_over_neg: 364.06951904296875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.692, loss_val: nan, pos_over_neg: 292.869140625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6721, loss_val: nan, pos_over_neg: 371.28009033203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.6841, loss_val: nan, pos_over_neg: 247.2153778076172 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 590.3833618164062 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.6578, loss_val: nan, pos_over_neg: 378.5920104980469 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.6239, loss_val: nan, pos_over_neg: 589.5233154296875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.6555, loss_val: nan, pos_over_neg: 719.9035034179688 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.6893, loss_val: nan, pos_over_neg: 461.4186706542969 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6675, loss_val: nan, pos_over_neg: 585.8584594726562 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6328, loss_val: nan, pos_over_neg: 550.2991943359375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.6934, loss_val: nan, pos_over_neg: 290.263916015625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.6676, loss_val: nan, pos_over_neg: 295.60028076171875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.7239, loss_val: nan, pos_over_neg: 246.00709533691406 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.7064, loss_val: nan, pos_over_neg: 304.4344177246094 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6681, loss_val: nan, pos_over_neg: 844.8690795898438 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.7075, loss_val: nan, pos_over_neg: 446.16827392578125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.6572, loss_val: nan, pos_over_neg: 460.8825378417969 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.6703, loss_val: nan, pos_over_neg: 946.8197631835938 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.663, loss_val: nan, pos_over_neg: 1017.7015380859375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6959, loss_val: nan, pos_over_neg: 375.5536804199219 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6722, loss_val: nan, pos_over_neg: 357.7368469238281 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.696, loss_val: nan, pos_over_neg: 277.7112121582031 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.731, loss_val: nan, pos_over_neg: 238.0531005859375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.6824, loss_val: nan, pos_over_neg: 294.4237976074219 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.66, loss_val: nan, pos_over_neg: 420.03326416015625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.6923, loss_val: nan, pos_over_neg: 310.81298828125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.694, loss_val: nan, pos_over_neg: 308.5411376953125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.6615, loss_val: nan, pos_over_neg: 466.5028381347656 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.6732, loss_val: nan, pos_over_neg: 417.17132568359375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6823, loss_val: nan, pos_over_neg: 457.302734375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6741, loss_val: nan, pos_over_neg: 507.7410888671875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6551, loss_val: nan, pos_over_neg: 270.5946960449219 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [1:45:46<75292:31:01, 903.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.6908, loss_val: nan, pos_over_neg: 372.93060302734375 lr: 0.00031623\n",
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 3.7344, loss_val: nan, pos_over_neg: 241.61790466308594 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.6693, loss_val: nan, pos_over_neg: 339.0138244628906 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 416.8945007324219 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 487.5563659667969 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6524, loss_val: nan, pos_over_neg: 382.6783447265625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.6737, loss_val: nan, pos_over_neg: 411.1014709472656 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.6726, loss_val: nan, pos_over_neg: 406.54931640625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.6699, loss_val: nan, pos_over_neg: 401.423828125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6575, loss_val: nan, pos_over_neg: 444.0557861328125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6736, loss_val: nan, pos_over_neg: 776.8717651367188 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6812, loss_val: nan, pos_over_neg: 771.9869995117188 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.6863, loss_val: nan, pos_over_neg: 367.9517517089844 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.6708, loss_val: nan, pos_over_neg: 379.7791748046875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.6723, loss_val: nan, pos_over_neg: 346.0928039550781 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6516, loss_val: nan, pos_over_neg: 404.2623596191406 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.6639, loss_val: nan, pos_over_neg: 286.0267333984375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.7079, loss_val: nan, pos_over_neg: 250.5625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.6868, loss_val: nan, pos_over_neg: 325.3846130371094 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.7255, loss_val: nan, pos_over_neg: 296.302978515625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.6869, loss_val: nan, pos_over_neg: 372.7268981933594 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.7082, loss_val: nan, pos_over_neg: 337.4272766113281 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.6469, loss_val: nan, pos_over_neg: 577.1287841796875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.7084, loss_val: nan, pos_over_neg: 327.2975769042969 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.6776, loss_val: nan, pos_over_neg: 351.46685791015625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.699, loss_val: nan, pos_over_neg: 357.3070983886719 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.6962, loss_val: nan, pos_over_neg: 298.4169616699219 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.69, loss_val: nan, pos_over_neg: 296.2698059082031 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.6358, loss_val: nan, pos_over_neg: 284.6474304199219 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6983, loss_val: nan, pos_over_neg: 301.0536193847656 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 438.5574951171875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.7256, loss_val: nan, pos_over_neg: 286.55511474609375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.6617, loss_val: nan, pos_over_neg: 351.5232849121094 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.6974, loss_val: nan, pos_over_neg: 289.96966552734375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6765, loss_val: nan, pos_over_neg: 389.6231384277344 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 447.12359619140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.645, loss_val: nan, pos_over_neg: 359.9063720703125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 439.6822509765625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.6787, loss_val: nan, pos_over_neg: 475.8748474121094 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6412, loss_val: nan, pos_over_neg: 383.36114501953125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.7273, loss_val: nan, pos_over_neg: 347.95361328125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6476, loss_val: nan, pos_over_neg: 538.2747192382812 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.6787, loss_val: nan, pos_over_neg: 444.5964050292969 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.6792, loss_val: nan, pos_over_neg: 291.04156494140625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.663, loss_val: nan, pos_over_neg: 369.2646789550781 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.6696, loss_val: nan, pos_over_neg: 343.67547607421875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6988, loss_val: nan, pos_over_neg: 227.2636260986328 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.6473, loss_val: nan, pos_over_neg: 325.86944580078125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6684, loss_val: nan, pos_over_neg: 397.9312744140625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.6755, loss_val: nan, pos_over_neg: 421.6595764160156 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.6572, loss_val: nan, pos_over_neg: 341.8960876464844 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 284.8914794921875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.6565, loss_val: nan, pos_over_neg: 454.6856384277344 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.653, loss_val: nan, pos_over_neg: 348.00494384765625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.6423, loss_val: nan, pos_over_neg: 294.2980041503906 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6675, loss_val: nan, pos_over_neg: 494.201171875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.6303, loss_val: nan, pos_over_neg: 392.5007019042969 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.6885, loss_val: nan, pos_over_neg: 172.8824005126953 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.6895, loss_val: nan, pos_over_neg: 222.49462890625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.6501, loss_val: nan, pos_over_neg: 236.14968872070312 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.6421, loss_val: nan, pos_over_neg: 280.4259948730469 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.6974, loss_val: nan, pos_over_neg: 261.056884765625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.6716, loss_val: nan, pos_over_neg: 367.4353332519531 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.634, loss_val: nan, pos_over_neg: 402.4410705566406 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.669, loss_val: nan, pos_over_neg: 506.23870849609375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.7169, loss_val: nan, pos_over_neg: 309.7226867675781 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 489.9608459472656 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 481.2890930175781 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6554, loss_val: nan, pos_over_neg: 309.4935302734375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.6498, loss_val: nan, pos_over_neg: 313.4161071777344 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.7067, loss_val: nan, pos_over_neg: 240.79307556152344 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6583, loss_val: nan, pos_over_neg: 290.9114990234375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 286.34234619140625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.6548, loss_val: nan, pos_over_neg: 308.6163330078125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.6589, loss_val: nan, pos_over_neg: 308.38238525390625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 279.0904235839844 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 392.85870361328125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.7131, loss_val: nan, pos_over_neg: 219.5228729248047 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.6478, loss_val: nan, pos_over_neg: 381.9514465332031 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.6616, loss_val: nan, pos_over_neg: 458.3135681152344 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6496, loss_val: nan, pos_over_neg: 396.18878173828125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.7181, loss_val: nan, pos_over_neg: 320.70159912109375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.6888, loss_val: nan, pos_over_neg: 354.02166748046875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.7033, loss_val: nan, pos_over_neg: 285.04083251953125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.6735, loss_val: nan, pos_over_neg: 275.14129638671875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.6636, loss_val: nan, pos_over_neg: 312.8824462890625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 399.97784423828125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.6739, loss_val: nan, pos_over_neg: 349.10345458984375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.6637, loss_val: nan, pos_over_neg: 379.848876953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.6501, loss_val: nan, pos_over_neg: 486.2018737792969 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6951, loss_val: nan, pos_over_neg: 277.77606201171875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.677, loss_val: nan, pos_over_neg: 278.08203125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6439, loss_val: nan, pos_over_neg: 342.6232604980469 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 355.34710693359375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.663, loss_val: nan, pos_over_neg: 391.4588317871094 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.7421, loss_val: nan, pos_over_neg: 276.777099609375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.7349, loss_val: nan, pos_over_neg: 241.4939422607422 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 461.72613525390625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.7106, loss_val: nan, pos_over_neg: 285.7648620605469 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.6847, loss_val: nan, pos_over_neg: 345.2907409667969 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.6735, loss_val: nan, pos_over_neg: 311.0827331542969 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6567, loss_val: nan, pos_over_neg: 435.6855773925781 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6699, loss_val: nan, pos_over_neg: 564.4413452148438 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.6752, loss_val: nan, pos_over_neg: 315.4962463378906 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.6636, loss_val: nan, pos_over_neg: 351.4054260253906 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.7122, loss_val: nan, pos_over_neg: 266.47882080078125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.6848, loss_val: nan, pos_over_neg: 410.1873474121094 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.7179, loss_val: nan, pos_over_neg: 332.3787841796875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 323.44915771484375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 514.9129638671875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6295, loss_val: nan, pos_over_neg: 349.63739013671875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6918, loss_val: nan, pos_over_neg: 372.7032775878906 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 348.058837890625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.6388, loss_val: nan, pos_over_neg: 522.4467163085938 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.66, loss_val: nan, pos_over_neg: 406.60491943359375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.6656, loss_val: nan, pos_over_neg: 348.708984375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.654, loss_val: nan, pos_over_neg: 395.64984130859375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6768, loss_val: nan, pos_over_neg: 337.05126953125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6665, loss_val: nan, pos_over_neg: 490.76519775390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.7057, loss_val: nan, pos_over_neg: 490.1553039550781 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.6536, loss_val: nan, pos_over_neg: 607.8709106445312 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 343.9622497558594 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.7171, loss_val: nan, pos_over_neg: 209.84617614746094 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6481, loss_val: nan, pos_over_neg: 346.89263916015625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6946, loss_val: nan, pos_over_neg: 254.13243103027344 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 443.72906494140625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.671, loss_val: nan, pos_over_neg: 577.6915283203125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 261.1425476074219 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.6354, loss_val: nan, pos_over_neg: 376.19134521484375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.6936, loss_val: nan, pos_over_neg: 283.2913513183594 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.6561, loss_val: nan, pos_over_neg: 421.84185791015625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.675, loss_val: nan, pos_over_neg: 355.857421875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.7129, loss_val: nan, pos_over_neg: 387.5616149902344 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.6412, loss_val: nan, pos_over_neg: 425.2738037109375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6181, loss_val: nan, pos_over_neg: 393.60723876953125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.7131, loss_val: nan, pos_over_neg: 299.8244934082031 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6532, loss_val: nan, pos_over_neg: 380.21405029296875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.7188, loss_val: nan, pos_over_neg: 251.2477264404297 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.6437, loss_val: nan, pos_over_neg: 352.1791076660156 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.6844, loss_val: nan, pos_over_neg: 347.2535400390625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.7011, loss_val: nan, pos_over_neg: 219.0768280029297 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6706, loss_val: nan, pos_over_neg: 278.9317626953125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.6985, loss_val: nan, pos_over_neg: 359.28192138671875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 445.5640869140625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 438.3739929199219 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.648, loss_val: nan, pos_over_neg: 368.3158264160156 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.6943, loss_val: nan, pos_over_neg: 281.017578125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.6653, loss_val: nan, pos_over_neg: 265.7403259277344 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.6851, loss_val: nan, pos_over_neg: 348.0306701660156 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.7108, loss_val: nan, pos_over_neg: 452.68597412109375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.6654, loss_val: nan, pos_over_neg: 323.0350646972656 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.6677, loss_val: nan, pos_over_neg: 328.55517578125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.6945, loss_val: nan, pos_over_neg: 331.8382873535156 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6602, loss_val: nan, pos_over_neg: 307.9525146484375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.7069, loss_val: nan, pos_over_neg: 430.18017578125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.6458, loss_val: nan, pos_over_neg: 385.0970764160156 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 358.58392333984375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6493, loss_val: nan, pos_over_neg: 353.8424377441406 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.6861, loss_val: nan, pos_over_neg: 269.58135986328125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.73, loss_val: nan, pos_over_neg: 340.28521728515625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6704, loss_val: nan, pos_over_neg: 479.010498046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.6919, loss_val: nan, pos_over_neg: 490.912841796875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.7066, loss_val: nan, pos_over_neg: 391.891357421875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.6585, loss_val: nan, pos_over_neg: 639.2428588867188 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.6592, loss_val: nan, pos_over_neg: 304.6375427246094 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6679, loss_val: nan, pos_over_neg: 449.22900390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.6467, loss_val: nan, pos_over_neg: 572.566162109375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.6256, loss_val: nan, pos_over_neg: 607.354248046875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.648, loss_val: nan, pos_over_neg: 313.4443054199219 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.6809, loss_val: nan, pos_over_neg: 359.7010192871094 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.7004, loss_val: nan, pos_over_neg: 325.49603271484375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.7193, loss_val: nan, pos_over_neg: 241.2107391357422 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6855, loss_val: nan, pos_over_neg: 327.9399108886719 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.6566, loss_val: nan, pos_over_neg: 305.6179504394531 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.6785, loss_val: nan, pos_over_neg: 422.1128234863281 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 536.6547241210938 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.6572, loss_val: nan, pos_over_neg: 433.2743225097656 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6576, loss_val: nan, pos_over_neg: 488.6751708984375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.6662, loss_val: nan, pos_over_neg: 473.44549560546875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 530.4656982421875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6544, loss_val: nan, pos_over_neg: 362.6595153808594 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6355, loss_val: nan, pos_over_neg: 443.6102600097656 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.6996, loss_val: nan, pos_over_neg: 297.62615966796875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.6581, loss_val: nan, pos_over_neg: 377.7574157714844 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.6581, loss_val: nan, pos_over_neg: 332.3837890625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6085, loss_val: nan, pos_over_neg: 485.4183349609375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.7038, loss_val: nan, pos_over_neg: 377.7997131347656 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 475.0790100097656 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 770.0795288085938 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.6939, loss_val: nan, pos_over_neg: 333.9436950683594 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.6494, loss_val: nan, pos_over_neg: 459.1174621582031 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.66, loss_val: nan, pos_over_neg: 352.3233337402344 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.7265, loss_val: nan, pos_over_neg: 300.6948547363281 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.6552, loss_val: nan, pos_over_neg: 481.00439453125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.6558, loss_val: nan, pos_over_neg: 427.3539123535156 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.6811, loss_val: nan, pos_over_neg: 353.66845703125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.6949, loss_val: nan, pos_over_neg: 314.52569580078125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.7062, loss_val: nan, pos_over_neg: 379.71356201171875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.6642, loss_val: nan, pos_over_neg: 327.93084716796875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.6739, loss_val: nan, pos_over_neg: 524.0662841796875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6317, loss_val: nan, pos_over_neg: 824.8211669921875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6859, loss_val: nan, pos_over_neg: 235.5529327392578 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.7039, loss_val: nan, pos_over_neg: 291.1202087402344 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6615, loss_val: nan, pos_over_neg: 270.627197265625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.6807, loss_val: nan, pos_over_neg: 323.9201965332031 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 398.3468933105469 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 481.65020751953125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.7027, loss_val: nan, pos_over_neg: 317.0218200683594 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.6587, loss_val: nan, pos_over_neg: 266.3521423339844 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.6534, loss_val: nan, pos_over_neg: 407.3164367675781 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.6875, loss_val: nan, pos_over_neg: 476.7080078125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 553.2031860351562 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6553, loss_val: nan, pos_over_neg: 612.498779296875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.7075, loss_val: nan, pos_over_neg: 246.7171630859375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.6603, loss_val: nan, pos_over_neg: 312.033935546875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 322.4111022949219 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.6627, loss_val: nan, pos_over_neg: 361.61041259765625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.6508, loss_val: nan, pos_over_neg: 400.18438720703125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.673, loss_val: nan, pos_over_neg: 310.8366394042969 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.6518, loss_val: nan, pos_over_neg: 523.4717407226562 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 369.4622802734375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 315.6961364746094 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.6938, loss_val: nan, pos_over_neg: 321.64501953125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.6499, loss_val: nan, pos_over_neg: 456.93157958984375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.5809, loss_val: nan, pos_over_neg: 557.2839965820312 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.6745, loss_val: nan, pos_over_neg: 323.7386169433594 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6662, loss_val: nan, pos_over_neg: 256.5338439941406 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.656, loss_val: nan, pos_over_neg: 291.84197998046875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6592, loss_val: nan, pos_over_neg: 414.6839294433594 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6819, loss_val: nan, pos_over_neg: 394.2789611816406 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 511.590087890625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.6963, loss_val: nan, pos_over_neg: 300.2370300292969 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.6886, loss_val: nan, pos_over_neg: 385.0608215332031 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.6918, loss_val: nan, pos_over_neg: 331.5869445800781 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.6635, loss_val: nan, pos_over_neg: 340.83953857421875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6895, loss_val: nan, pos_over_neg: 413.2930908203125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6615, loss_val: nan, pos_over_neg: 575.7606811523438 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.6855, loss_val: nan, pos_over_neg: 349.1407470703125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.7204, loss_val: nan, pos_over_neg: 186.21958923339844 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.6783, loss_val: nan, pos_over_neg: 284.02685546875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.698, loss_val: nan, pos_over_neg: 269.03778076171875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.7084, loss_val: nan, pos_over_neg: 299.6972351074219 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 539.6168212890625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.6481, loss_val: nan, pos_over_neg: 549.7571411132812 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.6466, loss_val: nan, pos_over_neg: 748.4690551757812 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.6609, loss_val: nan, pos_over_neg: 496.524658203125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6347, loss_val: nan, pos_over_neg: 379.1448669433594 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.6652, loss_val: nan, pos_over_neg: 459.3083190917969 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6726, loss_val: nan, pos_over_neg: 459.8713684082031 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6842, loss_val: nan, pos_over_neg: 345.4215393066406 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.6963, loss_val: nan, pos_over_neg: 252.21139526367188 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6782, loss_val: nan, pos_over_neg: 397.49468994140625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.6693, loss_val: nan, pos_over_neg: 530.598876953125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6685, loss_val: nan, pos_over_neg: 359.9819641113281 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.6689, loss_val: nan, pos_over_neg: 298.0279235839844 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6531, loss_val: nan, pos_over_neg: 391.00457763671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 535.7367553710938 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.6907, loss_val: nan, pos_over_neg: 344.613525390625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6439, loss_val: nan, pos_over_neg: 569.300537109375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 521.6084594726562 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.6975, loss_val: nan, pos_over_neg: 317.9315490722656 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.705, loss_val: nan, pos_over_neg: 459.86541748046875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6488, loss_val: nan, pos_over_neg: 464.8278503417969 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6771, loss_val: nan, pos_over_neg: 390.5392150878906 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.6243, loss_val: nan, pos_over_neg: 616.4818115234375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.6391, loss_val: nan, pos_over_neg: 454.76348876953125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.6761, loss_val: nan, pos_over_neg: 298.1049499511719 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6708, loss_val: nan, pos_over_neg: 303.4631652832031 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.6136, loss_val: nan, pos_over_neg: 472.0578308105469 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.6353, loss_val: nan, pos_over_neg: 291.6767578125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.6556, loss_val: nan, pos_over_neg: 316.1463928222656 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.6631, loss_val: nan, pos_over_neg: 370.7663269042969 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.6809, loss_val: nan, pos_over_neg: 405.18499755859375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.7193, loss_val: nan, pos_over_neg: 245.1171875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 484.9132080078125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6953, loss_val: nan, pos_over_neg: 332.7935791015625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 403.5509338378906 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6661, loss_val: nan, pos_over_neg: 560.6350708007812 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.6504, loss_val: nan, pos_over_neg: 401.0378723144531 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 479.80926513671875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6591, loss_val: nan, pos_over_neg: 364.2873229980469 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 395.83673095703125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.6831, loss_val: nan, pos_over_neg: 426.4031066894531 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6751, loss_val: nan, pos_over_neg: 333.0500183105469 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6558, loss_val: nan, pos_over_neg: 340.0871887207031 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.7465, loss_val: nan, pos_over_neg: 175.6944122314453 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6755, loss_val: nan, pos_over_neg: 395.1687316894531 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.7119, loss_val: nan, pos_over_neg: 232.44972229003906 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.6407, loss_val: nan, pos_over_neg: 398.39422607421875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 327.8739929199219 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6429, loss_val: nan, pos_over_neg: 481.1584167480469 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.7059, loss_val: nan, pos_over_neg: 293.5767517089844 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.6536, loss_val: nan, pos_over_neg: 382.6634521484375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.6686, loss_val: nan, pos_over_neg: 631.4013061523438 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6906, loss_val: nan, pos_over_neg: 491.3509826660156 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6746, loss_val: nan, pos_over_neg: 459.3404541015625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.656, loss_val: nan, pos_over_neg: 383.1576843261719 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6817, loss_val: nan, pos_over_neg: 284.72882080078125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.6758, loss_val: nan, pos_over_neg: 218.79188537597656 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.6242, loss_val: nan, pos_over_neg: 292.3096923828125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6785, loss_val: nan, pos_over_neg: 205.81912231445312 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6684, loss_val: nan, pos_over_neg: 320.1007995605469 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.6569, loss_val: nan, pos_over_neg: 344.9048156738281 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.6451, loss_val: nan, pos_over_neg: 406.31207275390625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6253, loss_val: nan, pos_over_neg: 493.3597412109375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.7012, loss_val: nan, pos_over_neg: 513.6018676757812 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.6902, loss_val: nan, pos_over_neg: 356.92303466796875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.6857, loss_val: nan, pos_over_neg: 285.6195068359375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.6946, loss_val: nan, pos_over_neg: 346.1510314941406 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 499.9530334472656 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6788, loss_val: nan, pos_over_neg: 348.4046325683594 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6908, loss_val: nan, pos_over_neg: 333.1291809082031 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6668, loss_val: nan, pos_over_neg: 366.1190185546875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.6922, loss_val: nan, pos_over_neg: 254.4850311279297 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.6519, loss_val: nan, pos_over_neg: 280.28533935546875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6476, loss_val: nan, pos_over_neg: 654.2879638671875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.6603, loss_val: nan, pos_over_neg: 343.4151306152344 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.7324, loss_val: nan, pos_over_neg: 240.12686157226562 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.6782, loss_val: nan, pos_over_neg: 295.4644775390625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6622, loss_val: nan, pos_over_neg: 419.5915222167969 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6478, loss_val: nan, pos_over_neg: 365.47857666015625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6749, loss_val: nan, pos_over_neg: 259.3464660644531 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.6552, loss_val: nan, pos_over_neg: 398.927001953125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.6507, loss_val: nan, pos_over_neg: 355.53515625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 269.21295166015625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 274.7341613769531 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.7069, loss_val: nan, pos_over_neg: 225.35333251953125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.6584, loss_val: nan, pos_over_neg: 364.41485595703125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 346.0165100097656 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.6854, loss_val: nan, pos_over_neg: 320.9187927246094 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 618.361083984375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.6639, loss_val: nan, pos_over_neg: 387.46014404296875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6423, loss_val: nan, pos_over_neg: 384.67230224609375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.7122, loss_val: nan, pos_over_neg: 301.5832214355469 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.7139, loss_val: nan, pos_over_neg: 207.26983642578125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.6713, loss_val: nan, pos_over_neg: 270.1305236816406 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.687, loss_val: nan, pos_over_neg: 316.31982421875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.7211, loss_val: nan, pos_over_neg: 240.45741271972656 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.635, loss_val: nan, pos_over_neg: 559.5211181640625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.6706, loss_val: nan, pos_over_neg: 521.8133544921875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 477.9287414550781 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.6489, loss_val: nan, pos_over_neg: 665.1336059570312 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.6799, loss_val: nan, pos_over_neg: 370.70654296875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.6453, loss_val: nan, pos_over_neg: 391.9753112792969 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.6037, loss_val: nan, pos_over_neg: 752.9976806640625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.7001, loss_val: nan, pos_over_neg: 356.84222412109375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.655, loss_val: nan, pos_over_neg: 471.1457214355469 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.7147, loss_val: nan, pos_over_neg: 223.55484008789062 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.692, loss_val: nan, pos_over_neg: 323.32415771484375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.6597, loss_val: nan, pos_over_neg: 262.65020751953125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.6834, loss_val: nan, pos_over_neg: 340.1532287597656 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 488.2503356933594 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.7001, loss_val: nan, pos_over_neg: 427.176513671875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6712, loss_val: nan, pos_over_neg: 485.83587646484375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.6451, loss_val: nan, pos_over_neg: 686.5121459960938 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.6587, loss_val: nan, pos_over_neg: 510.9417724609375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 314.9607238769531 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 317.9046630859375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6928, loss_val: nan, pos_over_neg: 338.04742431640625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.6677, loss_val: nan, pos_over_neg: 407.4352722167969 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.715, loss_val: nan, pos_over_neg: 293.3789978027344 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.7018, loss_val: nan, pos_over_neg: 195.5244140625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.656, loss_val: nan, pos_over_neg: 282.70904541015625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.65, loss_val: nan, pos_over_neg: 554.7055053710938 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 332.2987365722656 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.7055, loss_val: nan, pos_over_neg: 363.53070068359375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.7102, loss_val: nan, pos_over_neg: 306.60186767578125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 465.99822998046875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6542, loss_val: nan, pos_over_neg: 475.2701721191406 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.6469, loss_val: nan, pos_over_neg: 512.4816284179688 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 318.0536804199219 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.6833, loss_val: nan, pos_over_neg: 357.42413330078125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.6561, loss_val: nan, pos_over_neg: 435.53985595703125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 254.63272094726562 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.6616, loss_val: nan, pos_over_neg: 297.75555419921875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 341.3850402832031 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.685, loss_val: nan, pos_over_neg: 301.39923095703125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.6942, loss_val: nan, pos_over_neg: 420.4002990722656 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6752, loss_val: nan, pos_over_neg: 426.2332763671875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.6533, loss_val: nan, pos_over_neg: 387.2776184082031 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.6419, loss_val: nan, pos_over_neg: 514.3820190429688 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.6243, loss_val: nan, pos_over_neg: 437.30279541015625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6591, loss_val: nan, pos_over_neg: 592.8833618164062 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.7057, loss_val: nan, pos_over_neg: 298.8460693359375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.6421, loss_val: nan, pos_over_neg: 508.68475341796875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.6618, loss_val: nan, pos_over_neg: 292.30718994140625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.6927, loss_val: nan, pos_over_neg: 282.2952880859375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.6606, loss_val: nan, pos_over_neg: 395.3485107421875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6381, loss_val: nan, pos_over_neg: 520.8552856445312 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.6944, loss_val: nan, pos_over_neg: 368.96856689453125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 417.8780517578125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6629, loss_val: nan, pos_over_neg: 437.61737060546875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6519, loss_val: nan, pos_over_neg: 639.01171875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.72, loss_val: nan, pos_over_neg: 516.6251831054688 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6667, loss_val: nan, pos_over_neg: 370.555419921875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.6995, loss_val: nan, pos_over_neg: 243.04754638671875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6709, loss_val: nan, pos_over_neg: 243.55357360839844 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 304.62359619140625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.7284, loss_val: nan, pos_over_neg: 239.50802612304688 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.674, loss_val: nan, pos_over_neg: 266.9615173339844 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6739, loss_val: nan, pos_over_neg: 271.4338684082031 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.6642, loss_val: nan, pos_over_neg: 297.0318298339844 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.6695, loss_val: nan, pos_over_neg: 344.79412841796875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 698.3672485351562 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.6827, loss_val: nan, pos_over_neg: 294.9825134277344 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.6975, loss_val: nan, pos_over_neg: 306.5369567871094 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6511, loss_val: nan, pos_over_neg: 589.9364624023438 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 384.1954650878906 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.6563, loss_val: nan, pos_over_neg: 466.85888671875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 321.9590148925781 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.6295, loss_val: nan, pos_over_neg: 326.0504455566406 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6921, loss_val: nan, pos_over_neg: 335.19915771484375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.722, loss_val: nan, pos_over_neg: 269.4990234375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.6912, loss_val: nan, pos_over_neg: 379.625244140625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6776, loss_val: nan, pos_over_neg: 298.9317626953125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.658, loss_val: nan, pos_over_neg: 582.6110229492188 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.6541, loss_val: nan, pos_over_neg: 528.9270629882812 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.6462, loss_val: nan, pos_over_neg: 384.90179443359375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.6376, loss_val: nan, pos_over_neg: 510.8805236816406 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.6602, loss_val: nan, pos_over_neg: 374.0227355957031 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.6311, loss_val: nan, pos_over_neg: 336.57220458984375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6384, loss_val: nan, pos_over_neg: 406.7340087890625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.654, loss_val: nan, pos_over_neg: 344.4078674316406 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.6406, loss_val: nan, pos_over_neg: 318.0764465332031 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 623.83203125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.6659, loss_val: nan, pos_over_neg: 373.1850280761719 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.6674, loss_val: nan, pos_over_neg: 375.4076843261719 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.6628, loss_val: nan, pos_over_neg: 430.4196472167969 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6816, loss_val: nan, pos_over_neg: 403.04913330078125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.6778, loss_val: nan, pos_over_neg: 330.86688232421875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.6277, loss_val: nan, pos_over_neg: 442.13800048828125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.6577, loss_val: nan, pos_over_neg: 490.7209167480469 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6835, loss_val: nan, pos_over_neg: 300.54644775390625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.6908, loss_val: nan, pos_over_neg: 230.9498748779297 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.7087, loss_val: nan, pos_over_neg: 230.8838348388672 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.6398, loss_val: nan, pos_over_neg: 400.23431396484375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.679, loss_val: nan, pos_over_neg: 321.3929138183594 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.6855, loss_val: nan, pos_over_neg: 320.92266845703125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6226, loss_val: nan, pos_over_neg: 360.24969482421875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.6597, loss_val: nan, pos_over_neg: 324.7289123535156 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.6826, loss_val: nan, pos_over_neg: 369.9814453125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.6476, loss_val: nan, pos_over_neg: 724.3390502929688 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.6993, loss_val: nan, pos_over_neg: 391.52032470703125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.6727, loss_val: nan, pos_over_neg: 377.4947509765625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.669, loss_val: nan, pos_over_neg: 316.13775634765625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 373.1412048339844 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6725, loss_val: nan, pos_over_neg: 367.76251220703125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.6567, loss_val: nan, pos_over_neg: 298.68280029296875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.6595, loss_val: nan, pos_over_neg: 348.168212890625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.7276, loss_val: nan, pos_over_neg: 294.0390625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 265.7397766113281 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6659, loss_val: nan, pos_over_neg: 506.5634765625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.6776, loss_val: nan, pos_over_neg: 469.9872741699219 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.676, loss_val: nan, pos_over_neg: 370.066650390625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.6675, loss_val: nan, pos_over_neg: 432.36871337890625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 857.4050903320312 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.6469, loss_val: nan, pos_over_neg: 541.4467163085938 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.6347, loss_val: nan, pos_over_neg: 496.6632385253906 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 410.0104675292969 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 341.76580810546875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.733, loss_val: nan, pos_over_neg: 320.6772155761719 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.6782, loss_val: nan, pos_over_neg: 342.06689453125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6662, loss_val: nan, pos_over_neg: 380.229736328125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6459, loss_val: nan, pos_over_neg: 742.8330688476562 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.6205, loss_val: nan, pos_over_neg: 467.4993896484375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.671, loss_val: nan, pos_over_neg: 381.483642578125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 732.028076171875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.6654, loss_val: nan, pos_over_neg: 281.652099609375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.658, loss_val: nan, pos_over_neg: 409.4082336425781 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.629, loss_val: nan, pos_over_neg: 406.71673583984375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6563, loss_val: nan, pos_over_neg: 352.3049011230469 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 289.1426696777344 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.6414, loss_val: nan, pos_over_neg: 485.69207763671875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6264, loss_val: nan, pos_over_neg: 409.192626953125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6763, loss_val: nan, pos_over_neg: 389.2955627441406 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.6391, loss_val: nan, pos_over_neg: 403.2835693359375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6565, loss_val: nan, pos_over_neg: 621.1464233398438 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 525.9601440429688 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.6868, loss_val: nan, pos_over_neg: 355.6223449707031 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.7018, loss_val: nan, pos_over_neg: 341.4873962402344 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.6374, loss_val: nan, pos_over_neg: 350.2093505859375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.6624, loss_val: nan, pos_over_neg: 415.59088134765625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.642, loss_val: nan, pos_over_neg: 322.6611633300781 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.6456, loss_val: nan, pos_over_neg: 421.47064208984375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.6867, loss_val: nan, pos_over_neg: 265.3342590332031 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.6891, loss_val: nan, pos_over_neg: 365.1591491699219 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.6527, loss_val: nan, pos_over_neg: 440.49859619140625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.7181, loss_val: nan, pos_over_neg: 368.3418273925781 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 440.4551086425781 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.6375, loss_val: nan, pos_over_neg: 460.1990661621094 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.737, loss_val: nan, pos_over_neg: 264.1419372558594 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.7146, loss_val: nan, pos_over_neg: 329.8328552246094 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.663, loss_val: nan, pos_over_neg: 215.74786376953125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 359.0159606933594 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6716, loss_val: nan, pos_over_neg: 278.2807922363281 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 491.6124267578125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.6798, loss_val: nan, pos_over_neg: 282.0811462402344 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6202, loss_val: nan, pos_over_neg: 321.184326171875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.6956, loss_val: nan, pos_over_neg: 305.6642150878906 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.7048, loss_val: nan, pos_over_neg: 252.11688232421875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6808, loss_val: nan, pos_over_neg: 264.09942626953125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.6197, loss_val: nan, pos_over_neg: 360.4812316894531 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 431.90960693359375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.6516, loss_val: nan, pos_over_neg: 300.43328857421875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.6506, loss_val: nan, pos_over_neg: 392.9564514160156 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.6599, loss_val: nan, pos_over_neg: 326.4112243652344 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.6296, loss_val: nan, pos_over_neg: 321.37030029296875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6608, loss_val: nan, pos_over_neg: 306.8528747558594 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6723, loss_val: nan, pos_over_neg: 266.5206298828125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6588, loss_val: nan, pos_over_neg: 286.51458740234375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.6798, loss_val: nan, pos_over_neg: 207.77610778808594 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.6848, loss_val: nan, pos_over_neg: 301.3673095703125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 309.5440368652344 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.6864, loss_val: nan, pos_over_neg: 300.7276611328125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.702, loss_val: nan, pos_over_neg: 291.93865966796875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.6763, loss_val: nan, pos_over_neg: 437.6894226074219 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6744, loss_val: nan, pos_over_neg: 564.8406982421875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6229, loss_val: nan, pos_over_neg: 518.38671875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6835, loss_val: nan, pos_over_neg: 357.9139404296875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.647, loss_val: nan, pos_over_neg: 513.7783813476562 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6605, loss_val: nan, pos_over_neg: 486.908447265625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.6859, loss_val: nan, pos_over_neg: 287.46435546875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6754, loss_val: nan, pos_over_neg: 286.6837463378906 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.6586, loss_val: nan, pos_over_neg: 262.83233642578125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6454, loss_val: nan, pos_over_neg: 394.1612854003906 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.6716, loss_val: nan, pos_over_neg: 520.91796875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.6535, loss_val: nan, pos_over_neg: 407.7499694824219 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6898, loss_val: nan, pos_over_neg: 376.6019287109375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6232, loss_val: nan, pos_over_neg: 414.8297424316406 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.6538, loss_val: nan, pos_over_neg: 370.02777099609375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6851, loss_val: nan, pos_over_neg: 416.57037353515625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.6275, loss_val: nan, pos_over_neg: 710.2823486328125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 553.751708984375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.643, loss_val: nan, pos_over_neg: 506.7900390625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.6715, loss_val: nan, pos_over_neg: 258.95001220703125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6472, loss_val: nan, pos_over_neg: 393.7390441894531 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.6335, loss_val: nan, pos_over_neg: 566.2039184570312 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.6651, loss_val: nan, pos_over_neg: 319.07403564453125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 331.7689208984375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.6676, loss_val: nan, pos_over_neg: 348.472900390625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6448, loss_val: nan, pos_over_neg: 577.888427734375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.7027, loss_val: nan, pos_over_neg: 384.4206237792969 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.6533, loss_val: nan, pos_over_neg: 400.08953857421875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6988, loss_val: nan, pos_over_neg: 254.01022338867188 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.6537, loss_val: nan, pos_over_neg: 423.195068359375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 455.701904296875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.7232, loss_val: nan, pos_over_neg: 241.9715576171875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.6566, loss_val: nan, pos_over_neg: 448.38629150390625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6457, loss_val: nan, pos_over_neg: 382.85675048828125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 387.6292724609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.6869, loss_val: nan, pos_over_neg: 561.9312133789062 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 454.232666015625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6959, loss_val: nan, pos_over_neg: 352.9021301269531 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6738, loss_val: nan, pos_over_neg: 450.8518981933594 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.6718, loss_val: nan, pos_over_neg: 267.5989074707031 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.6827, loss_val: nan, pos_over_neg: 451.9443054199219 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.6519, loss_val: nan, pos_over_neg: 468.3576354980469 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.6752, loss_val: nan, pos_over_neg: 339.5231628417969 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.6809, loss_val: nan, pos_over_neg: 321.9518737792969 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.66, loss_val: nan, pos_over_neg: 492.2480773925781 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.6989, loss_val: nan, pos_over_neg: 404.00543212890625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.6803, loss_val: nan, pos_over_neg: 353.3267822265625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.6518, loss_val: nan, pos_over_neg: 618.957763671875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.6354, loss_val: nan, pos_over_neg: 647.5977172851562 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 408.60394287109375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.6624, loss_val: nan, pos_over_neg: 507.1126403808594 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6324, loss_val: nan, pos_over_neg: 658.6882934570312 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.647, loss_val: nan, pos_over_neg: 832.6422119140625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.6338, loss_val: nan, pos_over_neg: 991.7728881835938 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6911, loss_val: nan, pos_over_neg: 518.21826171875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.6899, loss_val: nan, pos_over_neg: 453.4832763671875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.6637, loss_val: nan, pos_over_neg: 402.555908203125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6456, loss_val: nan, pos_over_neg: 484.33056640625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.6911, loss_val: nan, pos_over_neg: 377.78363037109375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 308.93035888671875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6881, loss_val: nan, pos_over_neg: 369.5630798339844 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.7101, loss_val: nan, pos_over_neg: 318.0633850097656 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.6733, loss_val: nan, pos_over_neg: 451.5339660644531 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6222, loss_val: nan, pos_over_neg: 601.6724853515625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.6205, loss_val: nan, pos_over_neg: 569.4473876953125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.673, loss_val: nan, pos_over_neg: 596.5997924804688 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.6313, loss_val: nan, pos_over_neg: 434.15777587890625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6819, loss_val: nan, pos_over_neg: 312.7605285644531 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.6605, loss_val: nan, pos_over_neg: 415.80194091796875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 371.3703918457031 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.6508, loss_val: nan, pos_over_neg: 409.4776306152344 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.6932, loss_val: nan, pos_over_neg: 326.53826904296875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6776, loss_val: nan, pos_over_neg: 317.67132568359375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.6661, loss_val: nan, pos_over_neg: 328.1199645996094 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.6968, loss_val: nan, pos_over_neg: 365.35943603515625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.6728, loss_val: nan, pos_over_neg: 320.5845642089844 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.656, loss_val: nan, pos_over_neg: 343.76531982421875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.6555, loss_val: nan, pos_over_neg: 358.75152587890625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 534.1221313476562 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.6817, loss_val: nan, pos_over_neg: 645.2046508789062 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.6773, loss_val: nan, pos_over_neg: 402.3000793457031 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.6898, loss_val: nan, pos_over_neg: 310.6437072753906 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6659, loss_val: nan, pos_over_neg: 369.6474304199219 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6775, loss_val: nan, pos_over_neg: 345.358642578125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.683, loss_val: nan, pos_over_neg: 326.9714050292969 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.7189, loss_val: nan, pos_over_neg: 235.001708984375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.6536, loss_val: nan, pos_over_neg: 499.1821594238281 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.6605, loss_val: nan, pos_over_neg: 353.9535827636719 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.6704, loss_val: nan, pos_over_neg: 353.517333984375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.6942, loss_val: nan, pos_over_neg: 336.2747802734375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 421.72186279296875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.6115, loss_val: nan, pos_over_neg: 559.5675048828125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.6603, loss_val: nan, pos_over_neg: 399.76507568359375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.6554, loss_val: nan, pos_over_neg: 325.1199951171875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6355, loss_val: nan, pos_over_neg: 631.88525390625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6782, loss_val: nan, pos_over_neg: 333.5953674316406 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.6858, loss_val: nan, pos_over_neg: 336.6158447265625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.6729, loss_val: nan, pos_over_neg: 333.380615234375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 289.56842041015625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 380.54052734375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 409.05169677734375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.643, loss_val: nan, pos_over_neg: 513.9871826171875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.6327, loss_val: nan, pos_over_neg: 399.1628112792969 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.6683, loss_val: nan, pos_over_neg: 380.7005920410156 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.6773, loss_val: nan, pos_over_neg: 405.1549987792969 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6535, loss_val: nan, pos_over_neg: 429.93975830078125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.663, loss_val: nan, pos_over_neg: 479.9574890136719 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.6684, loss_val: nan, pos_over_neg: 549.3028564453125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.6781, loss_val: nan, pos_over_neg: 364.36090087890625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 417.56195068359375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6903, loss_val: nan, pos_over_neg: 318.1622619628906 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.6643, loss_val: nan, pos_over_neg: 243.06842041015625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 241.17013549804688 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.6633, loss_val: nan, pos_over_neg: 275.1292419433594 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 353.2449645996094 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.7034, loss_val: nan, pos_over_neg: 380.5684814453125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6579, loss_val: nan, pos_over_neg: 643.2234497070312 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.6457, loss_val: nan, pos_over_neg: 764.09716796875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.6986, loss_val: nan, pos_over_neg: 362.26055908203125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6335, loss_val: nan, pos_over_neg: 584.5313720703125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6799, loss_val: nan, pos_over_neg: 548.9488525390625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.6584, loss_val: nan, pos_over_neg: 374.2386779785156 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6772, loss_val: nan, pos_over_neg: 335.83941650390625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.6666, loss_val: nan, pos_over_neg: 332.3955993652344 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.678, loss_val: nan, pos_over_neg: 215.71072387695312 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.6904, loss_val: nan, pos_over_neg: 266.1981506347656 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.6723, loss_val: nan, pos_over_neg: 332.3800048828125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.6359, loss_val: nan, pos_over_neg: 704.0358276367188 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 428.6829528808594 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.6515, loss_val: nan, pos_over_neg: 554.2770385742188 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6617, loss_val: nan, pos_over_neg: 460.90399169921875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.6891, loss_val: nan, pos_over_neg: 295.36920166015625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.6536, loss_val: nan, pos_over_neg: 341.7249755859375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.6854, loss_val: nan, pos_over_neg: 351.3562927246094 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.6668, loss_val: nan, pos_over_neg: 508.8108215332031 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.6827, loss_val: nan, pos_over_neg: 328.7088623046875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.6684, loss_val: nan, pos_over_neg: 372.77392578125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.6313, loss_val: nan, pos_over_neg: 500.3059997558594 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.6576, loss_val: nan, pos_over_neg: 413.0570983886719 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.7068, loss_val: nan, pos_over_neg: 290.46270751953125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6921, loss_val: nan, pos_over_neg: 310.0763244628906 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6703, loss_val: nan, pos_over_neg: 425.15435791015625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.6226, loss_val: nan, pos_over_neg: 786.3457641601562 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6682, loss_val: nan, pos_over_neg: 512.7297973632812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6665, loss_val: nan, pos_over_neg: 343.0633239746094 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6482, loss_val: nan, pos_over_neg: 894.9038696289062 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6628, loss_val: nan, pos_over_neg: 345.9139404296875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.6203, loss_val: nan, pos_over_neg: 409.5403137207031 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 319.8171081542969 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6679, loss_val: nan, pos_over_neg: 335.6252746582031 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 468.9632568359375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 341.9460754394531 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.7033, loss_val: nan, pos_over_neg: 349.87353515625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 321.9896240234375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.6832, loss_val: nan, pos_over_neg: 464.1175842285156 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.7138, loss_val: nan, pos_over_neg: 299.5397033691406 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6882, loss_val: nan, pos_over_neg: 395.2701721191406 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6757, loss_val: nan, pos_over_neg: 280.7762451171875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 348.04876708984375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 360.2833251953125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.7296, loss_val: nan, pos_over_neg: 284.6336364746094 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 412.03375244140625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.651, loss_val: nan, pos_over_neg: 320.7052001953125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.6837, loss_val: nan, pos_over_neg: 358.95654296875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.6817, loss_val: nan, pos_over_neg: 535.654296875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.7104, loss_val: nan, pos_over_neg: 561.5469360351562 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.6834, loss_val: nan, pos_over_neg: 385.4906005859375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6762, loss_val: nan, pos_over_neg: 334.33056640625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6337, loss_val: nan, pos_over_neg: 374.12030029296875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.6775, loss_val: nan, pos_over_neg: 321.73358154296875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.6731, loss_val: nan, pos_over_neg: 288.4631042480469 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.675, loss_val: nan, pos_over_neg: 405.4606628417969 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.641, loss_val: nan, pos_over_neg: 338.22418212890625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.6562, loss_val: nan, pos_over_neg: 422.37762451171875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6875, loss_val: nan, pos_over_neg: 387.6615295410156 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.6605, loss_val: nan, pos_over_neg: 421.42657470703125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 573.93505859375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.7269, loss_val: nan, pos_over_neg: 222.83148193359375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6612, loss_val: nan, pos_over_neg: 252.25738525390625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6709, loss_val: nan, pos_over_neg: 280.3609313964844 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [2:00:43<75124:07:56, 901.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.6347, loss_val: nan, pos_over_neg: 440.7281494140625 lr: 0.00031623\n",
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 3.672, loss_val: nan, pos_over_neg: 385.0672302246094 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.656, loss_val: nan, pos_over_neg: 315.0892028808594 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.6434, loss_val: nan, pos_over_neg: 409.05841064453125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.6573, loss_val: nan, pos_over_neg: 341.3085632324219 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 323.9191589355469 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.6865, loss_val: nan, pos_over_neg: 266.92822265625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.6724, loss_val: nan, pos_over_neg: 280.4006652832031 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.6953, loss_val: nan, pos_over_neg: 355.54571533203125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6987, loss_val: nan, pos_over_neg: 279.92413330078125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.633, loss_val: nan, pos_over_neg: 400.6550598144531 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6456, loss_val: nan, pos_over_neg: 425.3658142089844 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 545.9682006835938 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.6942, loss_val: nan, pos_over_neg: 428.8501892089844 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 242.08184814453125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6772, loss_val: nan, pos_over_neg: 304.05609130859375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.6685, loss_val: nan, pos_over_neg: 277.81610107421875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.6644, loss_val: nan, pos_over_neg: 298.7325744628906 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.6431, loss_val: nan, pos_over_neg: 299.18231201171875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.6819, loss_val: nan, pos_over_neg: 355.7049865722656 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.5971, loss_val: nan, pos_over_neg: 441.3858947753906 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6754, loss_val: nan, pos_over_neg: 335.3531799316406 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.6459, loss_val: nan, pos_over_neg: 513.847900390625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.628, loss_val: nan, pos_over_neg: 1007.9420166015625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.6845, loss_val: nan, pos_over_neg: 377.0184326171875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 374.8944091796875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.6594, loss_val: nan, pos_over_neg: 252.1581573486328 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6749, loss_val: nan, pos_over_neg: 307.2151794433594 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.6532, loss_val: nan, pos_over_neg: 251.68299865722656 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6971, loss_val: nan, pos_over_neg: 324.1430969238281 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.6478, loss_val: nan, pos_over_neg: 418.63653564453125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 598.0901489257812 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.6599, loss_val: nan, pos_over_neg: 591.8804931640625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.6841, loss_val: nan, pos_over_neg: 345.216796875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6657, loss_val: nan, pos_over_neg: 720.375732421875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.6464, loss_val: nan, pos_over_neg: 399.62060546875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 1103.138427734375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 673.299560546875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.6795, loss_val: nan, pos_over_neg: 330.546630859375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.7081, loss_val: nan, pos_over_neg: 307.0513000488281 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.731, loss_val: nan, pos_over_neg: 253.94076538085938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 412.60699462890625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.6434, loss_val: nan, pos_over_neg: 318.1790771484375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.6733, loss_val: nan, pos_over_neg: 326.00506591796875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 387.5750732421875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.6375, loss_val: nan, pos_over_neg: 354.5874328613281 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 765.8632202148438 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.6428, loss_val: nan, pos_over_neg: 367.35272216796875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6806, loss_val: nan, pos_over_neg: 391.80926513671875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.6162, loss_val: nan, pos_over_neg: 687.1696166992188 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.7015, loss_val: nan, pos_over_neg: 507.8450927734375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.6439, loss_val: nan, pos_over_neg: 495.6889953613281 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.6693, loss_val: nan, pos_over_neg: 700.5608520507812 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.6345, loss_val: nan, pos_over_neg: 506.7965087890625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.6621, loss_val: nan, pos_over_neg: 406.6797180175781 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6482, loss_val: nan, pos_over_neg: 270.8578796386719 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.7162, loss_val: nan, pos_over_neg: 247.80633544921875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.633, loss_val: nan, pos_over_neg: 541.8820190429688 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 404.7507019042969 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 626.01708984375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.639, loss_val: nan, pos_over_neg: 700.6761474609375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.6667, loss_val: nan, pos_over_neg: 419.8052673339844 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 534.2977905273438 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 432.685546875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6403, loss_val: nan, pos_over_neg: 604.9305419921875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.6839, loss_val: nan, pos_over_neg: 530.297119140625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.6762, loss_val: nan, pos_over_neg: 412.6319274902344 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.6712, loss_val: nan, pos_over_neg: 286.798828125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6527, loss_val: nan, pos_over_neg: 395.10479736328125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.6404, loss_val: nan, pos_over_neg: 419.89984130859375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.6297, loss_val: nan, pos_over_neg: 390.2235107421875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6379, loss_val: nan, pos_over_neg: 320.71240234375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.6273, loss_val: nan, pos_over_neg: 682.6065063476562 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.6718, loss_val: nan, pos_over_neg: 318.46685791015625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.6818, loss_val: nan, pos_over_neg: 414.8762512207031 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6772, loss_val: nan, pos_over_neg: 496.1176452636719 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.6761, loss_val: nan, pos_over_neg: 469.4169616699219 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.6865, loss_val: nan, pos_over_neg: 288.9779052734375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.6578, loss_val: nan, pos_over_neg: 353.2785339355469 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.6782, loss_val: nan, pos_over_neg: 327.8230285644531 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6563, loss_val: nan, pos_over_neg: 363.366943359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.667, loss_val: nan, pos_over_neg: 287.4648132324219 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 274.1607971191406 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.6638, loss_val: nan, pos_over_neg: 267.02264404296875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.6403, loss_val: nan, pos_over_neg: 307.0514831542969 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.6597, loss_val: nan, pos_over_neg: 656.2926025390625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.7179, loss_val: nan, pos_over_neg: 379.1947021484375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.651, loss_val: nan, pos_over_neg: 390.7485046386719 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 352.2391052246094 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.6473, loss_val: nan, pos_over_neg: 592.6251831054688 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6361, loss_val: nan, pos_over_neg: 502.6114196777344 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.6311, loss_val: nan, pos_over_neg: 439.6148681640625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.7128, loss_val: nan, pos_over_neg: 240.107666015625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6838, loss_val: nan, pos_over_neg: 225.8218231201172 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.6665, loss_val: nan, pos_over_neg: 371.7372131347656 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6549, loss_val: nan, pos_over_neg: 435.2203369140625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.6964, loss_val: nan, pos_over_neg: 259.43914794921875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.6618, loss_val: nan, pos_over_neg: 298.40380859375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.6523, loss_val: nan, pos_over_neg: 411.2962646484375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.6746, loss_val: nan, pos_over_neg: 522.505859375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.6839, loss_val: nan, pos_over_neg: 431.53045654296875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 533.8847045898438 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 376.901123046875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.6784, loss_val: nan, pos_over_neg: 296.8974609375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.6446, loss_val: nan, pos_over_neg: 303.9369812011719 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.7007, loss_val: nan, pos_over_neg: 258.24853515625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.6773, loss_val: nan, pos_over_neg: 250.2002716064453 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.6784, loss_val: nan, pos_over_neg: 282.2126770019531 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 569.030029296875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6642, loss_val: nan, pos_over_neg: 596.4247436523438 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6539, loss_val: nan, pos_over_neg: 398.16827392578125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6457, loss_val: nan, pos_over_neg: 368.6442565917969 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6564, loss_val: nan, pos_over_neg: 432.2467956542969 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.6613, loss_val: nan, pos_over_neg: 346.5358581542969 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.6572, loss_val: nan, pos_over_neg: 387.8109436035156 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 297.7205505371094 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 475.36419677734375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 361.36004638671875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6343, loss_val: nan, pos_over_neg: 389.4831848144531 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.6704, loss_val: nan, pos_over_neg: 362.6265563964844 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.64, loss_val: nan, pos_over_neg: 449.8388977050781 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 510.1036071777344 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 384.8521728515625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6315, loss_val: nan, pos_over_neg: 515.3641967773438 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6696, loss_val: nan, pos_over_neg: 334.99127197265625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.6648, loss_val: nan, pos_over_neg: 506.38287353515625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6241, loss_val: nan, pos_over_neg: 453.4615783691406 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.6264, loss_val: nan, pos_over_neg: 417.058349609375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.6131, loss_val: nan, pos_over_neg: 440.38677978515625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 507.9557800292969 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.6631, loss_val: nan, pos_over_neg: 433.83367919921875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.6814, loss_val: nan, pos_over_neg: 537.28076171875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.6796, loss_val: nan, pos_over_neg: 523.1685180664062 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.668, loss_val: nan, pos_over_neg: 399.47259521484375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.658, loss_val: nan, pos_over_neg: 440.1961364746094 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6715, loss_val: nan, pos_over_neg: 357.66046142578125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6296, loss_val: nan, pos_over_neg: 466.3369140625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.682, loss_val: nan, pos_over_neg: 384.4563293457031 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.6852, loss_val: nan, pos_over_neg: 434.8343505859375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.6407, loss_val: nan, pos_over_neg: 445.895751953125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.6672, loss_val: nan, pos_over_neg: 315.1209716796875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 440.69781494140625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.7136, loss_val: nan, pos_over_neg: 436.9305419921875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.634, loss_val: nan, pos_over_neg: 697.1961059570312 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.6354, loss_val: nan, pos_over_neg: 471.69500732421875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 509.5467529296875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.6597, loss_val: nan, pos_over_neg: 412.85833740234375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.6152, loss_val: nan, pos_over_neg: 808.344970703125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.6995, loss_val: nan, pos_over_neg: 347.9198303222656 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 365.0114440917969 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.6726, loss_val: nan, pos_over_neg: 340.421875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.6577, loss_val: nan, pos_over_neg: 420.6259765625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.6653, loss_val: nan, pos_over_neg: 405.5477294921875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6648, loss_val: nan, pos_over_neg: 658.631591796875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6486, loss_val: nan, pos_over_neg: 601.3928833007812 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.6502, loss_val: nan, pos_over_neg: 392.6919250488281 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6673, loss_val: nan, pos_over_neg: 360.5977478027344 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6556, loss_val: nan, pos_over_neg: 606.5318603515625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.6874, loss_val: nan, pos_over_neg: 338.30511474609375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.6901, loss_val: nan, pos_over_neg: 244.22088623046875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6612, loss_val: nan, pos_over_neg: 231.79852294921875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.6494, loss_val: nan, pos_over_neg: 496.8236389160156 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.6876, loss_val: nan, pos_over_neg: 346.23663330078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.6573, loss_val: nan, pos_over_neg: 266.95379638671875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.6415, loss_val: nan, pos_over_neg: 279.01251220703125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6118, loss_val: nan, pos_over_neg: 421.90667724609375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.6131, loss_val: nan, pos_over_neg: 584.2542724609375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 423.8519592285156 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.7058, loss_val: nan, pos_over_neg: 260.82440185546875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.6411, loss_val: nan, pos_over_neg: 349.26953125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.6679, loss_val: nan, pos_over_neg: 318.9931945800781 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.6574, loss_val: nan, pos_over_neg: 385.2632751464844 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 348.0287170410156 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.6479, loss_val: nan, pos_over_neg: 241.84085083007812 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.6848, loss_val: nan, pos_over_neg: 197.1895294189453 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.6374, loss_val: nan, pos_over_neg: 266.511474609375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.6599, loss_val: nan, pos_over_neg: 326.38677978515625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6607, loss_val: nan, pos_over_neg: 331.1677551269531 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.6602, loss_val: nan, pos_over_neg: 477.9006042480469 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6957, loss_val: nan, pos_over_neg: 336.9657897949219 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6514, loss_val: nan, pos_over_neg: 338.2417297363281 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 648.566650390625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 350.50006103515625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.634, loss_val: nan, pos_over_neg: 542.5846557617188 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.625, loss_val: nan, pos_over_neg: 369.3379211425781 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6722, loss_val: nan, pos_over_neg: 387.13507080078125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.6278, loss_val: nan, pos_over_neg: 570.9501342773438 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.6065, loss_val: nan, pos_over_neg: 541.7201538085938 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.6888, loss_val: nan, pos_over_neg: 270.2301025390625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.6511, loss_val: nan, pos_over_neg: 326.6685485839844 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 1205.948974609375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.6897, loss_val: nan, pos_over_neg: 374.1159362792969 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.6412, loss_val: nan, pos_over_neg: 406.06658935546875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.6687, loss_val: nan, pos_over_neg: 483.9266357421875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.6077, loss_val: nan, pos_over_neg: 619.3203125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 387.2840576171875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.6437, loss_val: nan, pos_over_neg: 403.1642761230469 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.618, loss_val: nan, pos_over_neg: 444.5074462890625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.6626, loss_val: nan, pos_over_neg: 242.37086486816406 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.6108, loss_val: nan, pos_over_neg: 395.24969482421875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6669, loss_val: nan, pos_over_neg: 270.0972900390625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6622, loss_val: nan, pos_over_neg: 388.3899230957031 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.6402, loss_val: nan, pos_over_neg: 281.2242431640625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6692, loss_val: nan, pos_over_neg: 362.7415466308594 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.6625, loss_val: nan, pos_over_neg: 372.462646484375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 544.6609497070312 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.6956, loss_val: nan, pos_over_neg: 262.93328857421875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.6981, loss_val: nan, pos_over_neg: 221.5636444091797 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.6482, loss_val: nan, pos_over_neg: 372.3191833496094 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.6716, loss_val: nan, pos_over_neg: 377.94635009765625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.6758, loss_val: nan, pos_over_neg: 398.9732666015625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6314, loss_val: nan, pos_over_neg: 421.73248291015625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6477, loss_val: nan, pos_over_neg: 324.0772705078125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.6493, loss_val: nan, pos_over_neg: 323.99383544921875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.6534, loss_val: nan, pos_over_neg: 321.97955322265625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.6224, loss_val: nan, pos_over_neg: 468.2146301269531 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 350.4339904785156 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.6334, loss_val: nan, pos_over_neg: 553.6255493164062 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6629, loss_val: nan, pos_over_neg: 429.79095458984375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.6317, loss_val: nan, pos_over_neg: 455.2183532714844 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.6441, loss_val: nan, pos_over_neg: 241.6189422607422 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 350.3173522949219 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.6745, loss_val: nan, pos_over_neg: 241.4576416015625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.6695, loss_val: nan, pos_over_neg: 304.1137390136719 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6843, loss_val: nan, pos_over_neg: 276.4601135253906 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.6616, loss_val: nan, pos_over_neg: 340.2651672363281 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 408.4819030761719 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6588, loss_val: nan, pos_over_neg: 365.0385437011719 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6619, loss_val: nan, pos_over_neg: 528.0346069335938 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 565.8819580078125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 357.31005859375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 353.3890686035156 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.6262, loss_val: nan, pos_over_neg: 555.1268310546875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 318.29345703125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.7567, loss_val: nan, pos_over_neg: 197.68975830078125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 270.0103759765625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 480.5087585449219 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.6493, loss_val: nan, pos_over_neg: 606.6812133789062 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.6542, loss_val: nan, pos_over_neg: 390.7677001953125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.6902, loss_val: nan, pos_over_neg: 332.9606628417969 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 373.4038391113281 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.6701, loss_val: nan, pos_over_neg: 289.7950134277344 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.6737, loss_val: nan, pos_over_neg: 247.62107849121094 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.6639, loss_val: nan, pos_over_neg: 374.5674743652344 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 354.5975036621094 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 338.77294921875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6453, loss_val: nan, pos_over_neg: 325.1557922363281 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.638, loss_val: nan, pos_over_neg: 357.21807861328125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 451.8268127441406 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 347.21417236328125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.6647, loss_val: nan, pos_over_neg: 327.2812194824219 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6448, loss_val: nan, pos_over_neg: 630.1281127929688 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.6726, loss_val: nan, pos_over_neg: 366.4385681152344 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.7132, loss_val: nan, pos_over_neg: 242.40406799316406 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.6748, loss_val: nan, pos_over_neg: 338.9576721191406 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6108, loss_val: nan, pos_over_neg: 381.2647399902344 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.7103, loss_val: nan, pos_over_neg: 214.32452392578125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.6432, loss_val: nan, pos_over_neg: 345.3072204589844 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6213, loss_val: nan, pos_over_neg: 384.9076843261719 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 503.2916564941406 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 547.6995239257812 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.6488, loss_val: nan, pos_over_neg: 300.2483215332031 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6639, loss_val: nan, pos_over_neg: 362.7248840332031 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6839, loss_val: nan, pos_over_neg: 310.09063720703125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.6301, loss_val: nan, pos_over_neg: 467.908203125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 290.302490234375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.6399, loss_val: nan, pos_over_neg: 272.7040710449219 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6318, loss_val: nan, pos_over_neg: 413.0862121582031 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.6859, loss_val: nan, pos_over_neg: 244.46798706054688 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.6559, loss_val: nan, pos_over_neg: 280.0027770996094 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.6508, loss_val: nan, pos_over_neg: 268.7569274902344 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 445.8227233886719 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.6467, loss_val: nan, pos_over_neg: 819.7099609375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.7083, loss_val: nan, pos_over_neg: 383.8887939453125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 432.9227600097656 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6198, loss_val: nan, pos_over_neg: 748.144287109375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.6367, loss_val: nan, pos_over_neg: 429.91839599609375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6423, loss_val: nan, pos_over_neg: 296.8389587402344 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.7046, loss_val: nan, pos_over_neg: 249.0454559326172 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.6768, loss_val: nan, pos_over_neg: 213.42684936523438 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6507, loss_val: nan, pos_over_neg: 256.9405822753906 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6296, loss_val: nan, pos_over_neg: 527.1946411132812 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 466.6715393066406 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6584, loss_val: nan, pos_over_neg: 352.380126953125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 448.2545471191406 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 489.4650573730469 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6482, loss_val: nan, pos_over_neg: 462.44744873046875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.6245, loss_val: nan, pos_over_neg: 427.9807434082031 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.629, loss_val: nan, pos_over_neg: 437.07440185546875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.6372, loss_val: nan, pos_over_neg: 610.79931640625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6879, loss_val: nan, pos_over_neg: 244.88900756835938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.6199, loss_val: nan, pos_over_neg: 282.34722900390625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 336.40740966796875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.6511, loss_val: nan, pos_over_neg: 418.0456848144531 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6459, loss_val: nan, pos_over_neg: 292.9356994628906 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6854, loss_val: nan, pos_over_neg: 259.22711181640625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.6609, loss_val: nan, pos_over_neg: 299.5221252441406 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 525.933837890625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 456.5273742675781 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.6832, loss_val: nan, pos_over_neg: 285.38702392578125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6793, loss_val: nan, pos_over_neg: 392.68743896484375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6927, loss_val: nan, pos_over_neg: 239.0904083251953 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 578.334716796875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.6282, loss_val: nan, pos_over_neg: 458.2179870605469 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6352, loss_val: nan, pos_over_neg: 461.0437316894531 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.6579, loss_val: nan, pos_over_neg: 284.94140625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 401.31280517578125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.6634, loss_val: nan, pos_over_neg: 391.5332946777344 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.6957, loss_val: nan, pos_over_neg: 293.79168701171875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.6475, loss_val: nan, pos_over_neg: 416.7858581542969 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 353.7140808105469 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6811, loss_val: nan, pos_over_neg: 342.8905029296875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 575.7045288085938 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 435.0204772949219 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.6555, loss_val: nan, pos_over_neg: 438.8993835449219 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 647.1187133789062 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 397.8101806640625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 455.5085754394531 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.6763, loss_val: nan, pos_over_neg: 454.6411437988281 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6566, loss_val: nan, pos_over_neg: 504.4491271972656 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6741, loss_val: nan, pos_over_neg: 697.6906127929688 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6713, loss_val: nan, pos_over_neg: 377.1649475097656 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.6519, loss_val: nan, pos_over_neg: 379.0301513671875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.6786, loss_val: nan, pos_over_neg: 259.10845947265625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.6328, loss_val: nan, pos_over_neg: 444.801025390625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.6331, loss_val: nan, pos_over_neg: 393.4024353027344 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.6727, loss_val: nan, pos_over_neg: 293.000244140625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 463.7999572753906 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6403, loss_val: nan, pos_over_neg: 556.1148071289062 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.6769, loss_val: nan, pos_over_neg: 375.6598815917969 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.6527, loss_val: nan, pos_over_neg: 376.75970458984375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.6486, loss_val: nan, pos_over_neg: 394.25372314453125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6322, loss_val: nan, pos_over_neg: 378.5039367675781 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 594.6801147460938 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.6727, loss_val: nan, pos_over_neg: 464.0901794433594 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.636, loss_val: nan, pos_over_neg: 424.68023681640625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6628, loss_val: nan, pos_over_neg: 261.58233642578125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.6639, loss_val: nan, pos_over_neg: 316.0851745605469 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 282.3852233886719 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 331.44873046875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 474.4385070800781 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.63, loss_val: nan, pos_over_neg: 668.9811401367188 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.6272, loss_val: nan, pos_over_neg: 405.5342712402344 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 313.0742492675781 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.6411, loss_val: nan, pos_over_neg: 327.8620300292969 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 420.418701171875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.6192, loss_val: nan, pos_over_neg: 542.312255859375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6431, loss_val: nan, pos_over_neg: 458.5660705566406 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.6537, loss_val: nan, pos_over_neg: 416.9043884277344 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.653, loss_val: nan, pos_over_neg: 497.2388000488281 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.6638, loss_val: nan, pos_over_neg: 380.8438720703125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.6939, loss_val: nan, pos_over_neg: 440.77655029296875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.6322, loss_val: nan, pos_over_neg: 647.974365234375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6836, loss_val: nan, pos_over_neg: 298.2257995605469 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.6698, loss_val: nan, pos_over_neg: 330.9167175292969 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 658.4307250976562 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6431, loss_val: nan, pos_over_neg: 407.541259765625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.655, loss_val: nan, pos_over_neg: 488.684326171875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 338.5810852050781 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.6652, loss_val: nan, pos_over_neg: 401.77630615234375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.6447, loss_val: nan, pos_over_neg: 345.3629150390625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.6612, loss_val: nan, pos_over_neg: 367.673583984375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.6301, loss_val: nan, pos_over_neg: 423.47412109375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6344, loss_val: nan, pos_over_neg: 327.38909912109375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.7001, loss_val: nan, pos_over_neg: 227.46788024902344 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6388, loss_val: nan, pos_over_neg: 452.3745422363281 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.6498, loss_val: nan, pos_over_neg: 632.6165771484375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.6693, loss_val: nan, pos_over_neg: 452.49029541015625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6442, loss_val: nan, pos_over_neg: 416.0025939941406 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 403.1199035644531 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.6524, loss_val: nan, pos_over_neg: 367.57781982421875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.6851, loss_val: nan, pos_over_neg: 328.7995300292969 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.7195, loss_val: nan, pos_over_neg: 207.47731018066406 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.693, loss_val: nan, pos_over_neg: 301.6632995605469 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.6329, loss_val: nan, pos_over_neg: 487.4084167480469 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 483.8958740234375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.7218, loss_val: nan, pos_over_neg: 205.7020263671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.6554, loss_val: nan, pos_over_neg: 313.6748352050781 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6854, loss_val: nan, pos_over_neg: 351.75091552734375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.6485, loss_val: nan, pos_over_neg: 481.79937744140625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.61, loss_val: nan, pos_over_neg: 449.9844970703125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 481.2179260253906 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6389, loss_val: nan, pos_over_neg: 423.3062438964844 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6255, loss_val: nan, pos_over_neg: 665.4940185546875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.6476, loss_val: nan, pos_over_neg: 565.2075805664062 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.6477, loss_val: nan, pos_over_neg: 359.81427001953125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.6398, loss_val: nan, pos_over_neg: 370.7340087890625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.6457, loss_val: nan, pos_over_neg: 382.7571716308594 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 346.59027099609375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 319.47918701171875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.6871, loss_val: nan, pos_over_neg: 183.5677947998047 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 511.38250732421875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6112, loss_val: nan, pos_over_neg: 355.6069030761719 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.6466, loss_val: nan, pos_over_neg: 354.40899658203125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6338, loss_val: nan, pos_over_neg: 574.1920776367188 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.7003, loss_val: nan, pos_over_neg: 398.4164123535156 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6326, loss_val: nan, pos_over_neg: 360.69171142578125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6484, loss_val: nan, pos_over_neg: 432.1786193847656 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.6278, loss_val: nan, pos_over_neg: 429.94281005859375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.6234, loss_val: nan, pos_over_neg: 406.77288818359375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6089, loss_val: nan, pos_over_neg: 430.46026611328125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.6752, loss_val: nan, pos_over_neg: 320.90277099609375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 509.3097839355469 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.6326, loss_val: nan, pos_over_neg: 530.7992553710938 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.6476, loss_val: nan, pos_over_neg: 415.8706359863281 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.6854, loss_val: nan, pos_over_neg: 398.40716552734375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6585, loss_val: nan, pos_over_neg: 490.6357727050781 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 606.5036010742188 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.6261, loss_val: nan, pos_over_neg: 496.7488098144531 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6732, loss_val: nan, pos_over_neg: 465.79541015625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 414.6575012207031 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6438, loss_val: nan, pos_over_neg: 432.0270080566406 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 506.9523620605469 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.6589, loss_val: nan, pos_over_neg: 501.39556884765625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 1058.461669921875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 520.0750732421875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.6749, loss_val: nan, pos_over_neg: 372.1300354003906 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.6117, loss_val: nan, pos_over_neg: 487.1084899902344 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.6529, loss_val: nan, pos_over_neg: 442.3795166015625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 569.9398193359375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.6337, loss_val: nan, pos_over_neg: 448.7911682128906 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6545, loss_val: nan, pos_over_neg: 490.1974182128906 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.6652, loss_val: nan, pos_over_neg: 582.5550537109375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.6791, loss_val: nan, pos_over_neg: 304.05743408203125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 540.3792114257812 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 715.89404296875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.6317, loss_val: nan, pos_over_neg: 521.3341064453125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 299.73370361328125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6764, loss_val: nan, pos_over_neg: 334.19720458984375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.708, loss_val: nan, pos_over_neg: 319.3696594238281 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.6411, loss_val: nan, pos_over_neg: 353.2677001953125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.6413, loss_val: nan, pos_over_neg: 761.9114990234375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6306, loss_val: nan, pos_over_neg: 423.37969970703125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 386.7850341796875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.6663, loss_val: nan, pos_over_neg: 348.60321044921875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.6445, loss_val: nan, pos_over_neg: 372.33154296875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.5918, loss_val: nan, pos_over_neg: 455.4629211425781 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.6463, loss_val: nan, pos_over_neg: 532.5263671875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6412, loss_val: nan, pos_over_neg: 508.08941650390625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.6335, loss_val: nan, pos_over_neg: 511.1834716796875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.6975, loss_val: nan, pos_over_neg: 336.18487548828125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 799.06982421875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.672, loss_val: nan, pos_over_neg: 461.56402587890625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 438.9635925292969 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.6297, loss_val: nan, pos_over_neg: 418.0903015136719 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6733, loss_val: nan, pos_over_neg: 260.01702880859375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6443, loss_val: nan, pos_over_neg: 287.78875732421875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.643, loss_val: nan, pos_over_neg: 339.41741943359375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.6736, loss_val: nan, pos_over_neg: 318.690673828125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 628.6578979492188 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.6412, loss_val: nan, pos_over_neg: 365.94134521484375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6794, loss_val: nan, pos_over_neg: 441.2962341308594 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 342.09478759765625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.691, loss_val: nan, pos_over_neg: 372.5657958984375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.6647, loss_val: nan, pos_over_neg: 418.241943359375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 483.38592529296875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 623.7466430664062 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.6551, loss_val: nan, pos_over_neg: 512.0941162109375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.6787, loss_val: nan, pos_over_neg: 341.2408752441406 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.6508, loss_val: nan, pos_over_neg: 340.6900329589844 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 266.11602783203125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.6589, loss_val: nan, pos_over_neg: 274.9892883300781 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6755, loss_val: nan, pos_over_neg: 293.4468078613281 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6123, loss_val: nan, pos_over_neg: 549.935546875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.6682, loss_val: nan, pos_over_neg: 371.9187927246094 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.6493, loss_val: nan, pos_over_neg: 521.6856079101562 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 453.44598388671875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 437.64361572265625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.6613, loss_val: nan, pos_over_neg: 561.9909057617188 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.61, loss_val: nan, pos_over_neg: 953.74169921875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6523, loss_val: nan, pos_over_neg: 315.63629150390625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 232.11842346191406 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 343.0138854980469 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6712, loss_val: nan, pos_over_neg: 260.6299133300781 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6622, loss_val: nan, pos_over_neg: 290.517822265625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 410.49896240234375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6662, loss_val: nan, pos_over_neg: 489.8173522949219 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6585, loss_val: nan, pos_over_neg: 370.5215148925781 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 388.53955078125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.6341, loss_val: nan, pos_over_neg: 844.2962646484375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 481.90728759765625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.6178, loss_val: nan, pos_over_neg: 666.7906494140625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.6961, loss_val: nan, pos_over_neg: 306.3628845214844 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.6604, loss_val: nan, pos_over_neg: 298.11700439453125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.6838, loss_val: nan, pos_over_neg: 251.02537536621094 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.6529, loss_val: nan, pos_over_neg: 232.0339813232422 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 288.18994140625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.6337, loss_val: nan, pos_over_neg: 340.7868957519531 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 326.66387939453125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.6916, loss_val: nan, pos_over_neg: 272.3534240722656 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.6691, loss_val: nan, pos_over_neg: 348.7223815917969 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.6586, loss_val: nan, pos_over_neg: 460.5225830078125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 715.35791015625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 672.64794921875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6823, loss_val: nan, pos_over_neg: 469.5088806152344 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.6131, loss_val: nan, pos_over_neg: 420.06646728515625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.5858, loss_val: nan, pos_over_neg: 496.486572265625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 496.0420837402344 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.6243, loss_val: nan, pos_over_neg: 360.6083984375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.6852, loss_val: nan, pos_over_neg: 249.1669464111328 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6846, loss_val: nan, pos_over_neg: 277.4366149902344 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.6696, loss_val: nan, pos_over_neg: 366.92279052734375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6369, loss_val: nan, pos_over_neg: 367.84991455078125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.6649, loss_val: nan, pos_over_neg: 369.2586669921875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.5805, loss_val: nan, pos_over_neg: 673.0934448242188 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.643, loss_val: nan, pos_over_neg: 547.3721313476562 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.6445, loss_val: nan, pos_over_neg: 419.2183837890625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 636.1001586914062 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6802, loss_val: nan, pos_over_neg: 364.83013916015625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.636, loss_val: nan, pos_over_neg: 292.88897705078125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.6562, loss_val: nan, pos_over_neg: 381.99761962890625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.6751, loss_val: nan, pos_over_neg: 212.68228149414062 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6549, loss_val: nan, pos_over_neg: 264.9371643066406 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 504.10565185546875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.6545, loss_val: nan, pos_over_neg: 397.84197998046875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.6578, loss_val: nan, pos_over_neg: 391.48297119140625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6615, loss_val: nan, pos_over_neg: 284.6888732910156 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 575.5220336914062 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6156, loss_val: nan, pos_over_neg: 429.9876403808594 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 277.8757019042969 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 534.6947631835938 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.634, loss_val: nan, pos_over_neg: 345.69110107421875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 315.4163513183594 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.6559, loss_val: nan, pos_over_neg: 228.54246520996094 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6557, loss_val: nan, pos_over_neg: 419.85894775390625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 364.64544677734375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 375.94866943359375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6592, loss_val: nan, pos_over_neg: 385.5572204589844 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6887, loss_val: nan, pos_over_neg: 270.5244140625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.6933, loss_val: nan, pos_over_neg: 334.32061767578125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6984, loss_val: nan, pos_over_neg: 344.13934326171875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.6761, loss_val: nan, pos_over_neg: 402.56768798828125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6566, loss_val: nan, pos_over_neg: 374.9699401855469 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.6423, loss_val: nan, pos_over_neg: 506.09375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.7039, loss_val: nan, pos_over_neg: 274.6843566894531 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6432, loss_val: nan, pos_over_neg: 338.0992736816406 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.677, loss_val: nan, pos_over_neg: 334.3331604003906 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.6375, loss_val: nan, pos_over_neg: 315.9920959472656 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.6508, loss_val: nan, pos_over_neg: 285.1445007324219 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.6788, loss_val: nan, pos_over_neg: 308.8619079589844 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6613, loss_val: nan, pos_over_neg: 307.9971618652344 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.678, loss_val: nan, pos_over_neg: 399.22625732421875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.6885, loss_val: nan, pos_over_neg: 259.46142578125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6553, loss_val: nan, pos_over_neg: 375.9271545410156 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.6624, loss_val: nan, pos_over_neg: 361.6841735839844 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.6807, loss_val: nan, pos_over_neg: 298.7609558105469 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 360.6974182128906 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 309.7916564941406 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6581, loss_val: nan, pos_over_neg: 354.6300048828125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 440.530517578125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.6329, loss_val: nan, pos_over_neg: 415.3890686035156 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6269, loss_val: nan, pos_over_neg: 442.5060729980469 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 247.88748168945312 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6896, loss_val: nan, pos_over_neg: 290.7362365722656 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.6101, loss_val: nan, pos_over_neg: 330.20184326171875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 422.96002197265625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 406.5028381347656 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.6128, loss_val: nan, pos_over_neg: 423.9976501464844 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 254.5359344482422 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.6889, loss_val: nan, pos_over_neg: 229.15492248535156 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.6527, loss_val: nan, pos_over_neg: 338.9270935058594 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.6705, loss_val: nan, pos_over_neg: 288.1567687988281 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 240.6489715576172 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.6369, loss_val: nan, pos_over_neg: 373.7188415527344 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.6665, loss_val: nan, pos_over_neg: 315.73577880859375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.6403, loss_val: nan, pos_over_neg: 473.3626403808594 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6376, loss_val: nan, pos_over_neg: 441.2801818847656 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.6652, loss_val: nan, pos_over_neg: 354.2283630371094 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.6483, loss_val: nan, pos_over_neg: 408.3945007324219 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6579, loss_val: nan, pos_over_neg: 361.24090576171875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 503.1236572265625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 383.46417236328125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6506, loss_val: nan, pos_over_neg: 451.8619079589844 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.6408, loss_val: nan, pos_over_neg: 319.8655700683594 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 329.80218505859375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6852, loss_val: nan, pos_over_neg: 315.2092590332031 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.6198, loss_val: nan, pos_over_neg: 479.0484313964844 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.6875, loss_val: nan, pos_over_neg: 340.0733337402344 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 295.4646301269531 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.631, loss_val: nan, pos_over_neg: 401.74713134765625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.6597, loss_val: nan, pos_over_neg: 383.19964599609375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.5836, loss_val: nan, pos_over_neg: 693.5641479492188 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6082, loss_val: nan, pos_over_neg: 625.5496215820312 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.6271, loss_val: nan, pos_over_neg: 380.87109375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.6807, loss_val: nan, pos_over_neg: 251.02171325683594 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.6801, loss_val: nan, pos_over_neg: 445.3000183105469 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.6483, loss_val: nan, pos_over_neg: 376.4252014160156 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6667, loss_val: nan, pos_over_neg: 334.62322998046875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.6612, loss_val: nan, pos_over_neg: 413.57855224609375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.6321, loss_val: nan, pos_over_neg: 363.3445129394531 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.648, loss_val: nan, pos_over_neg: 316.33978271484375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.6426, loss_val: nan, pos_over_neg: 396.7277526855469 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.5912, loss_val: nan, pos_over_neg: 505.5816345214844 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 271.952880859375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.6968, loss_val: nan, pos_over_neg: 292.084716796875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.6731, loss_val: nan, pos_over_neg: 296.4303283691406 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.6723, loss_val: nan, pos_over_neg: 383.1914367675781 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6453, loss_val: nan, pos_over_neg: 419.5746765136719 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6648, loss_val: nan, pos_over_neg: 657.2457275390625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 605.5844116210938 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.6195, loss_val: nan, pos_over_neg: 491.3686218261719 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.6816, loss_val: nan, pos_over_neg: 304.66802978515625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.6506, loss_val: nan, pos_over_neg: 261.46600341796875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.6638, loss_val: nan, pos_over_neg: 222.2692413330078 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 220.7952880859375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.6448, loss_val: nan, pos_over_neg: 239.37945556640625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.6551, loss_val: nan, pos_over_neg: 299.2407531738281 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.6677, loss_val: nan, pos_over_neg: 250.71713256835938 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.6755, loss_val: nan, pos_over_neg: 271.4141845703125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 449.045166015625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 340.7118225097656 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.5926, loss_val: nan, pos_over_neg: 571.3577880859375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.6572, loss_val: nan, pos_over_neg: 567.0577392578125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6668, loss_val: nan, pos_over_neg: 361.5217590332031 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.6616, loss_val: nan, pos_over_neg: 375.6087951660156 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.6498, loss_val: nan, pos_over_neg: 330.2063903808594 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.6256, loss_val: nan, pos_over_neg: 538.7109985351562 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.6617, loss_val: nan, pos_over_neg: 337.81524658203125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.6792, loss_val: nan, pos_over_neg: 253.86875915527344 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.6832, loss_val: nan, pos_over_neg: 351.9191589355469 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6291, loss_val: nan, pos_over_neg: 337.6476745605469 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 606.4238891601562 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.6714, loss_val: nan, pos_over_neg: 289.4356689453125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.6544, loss_val: nan, pos_over_neg: 288.5577087402344 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 311.7420654296875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6931, loss_val: nan, pos_over_neg: 339.1260986328125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.6411, loss_val: nan, pos_over_neg: 445.90179443359375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 515.1053466796875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.6616, loss_val: nan, pos_over_neg: 505.5008239746094 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6492, loss_val: nan, pos_over_neg: 446.7274475097656 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.6721, loss_val: nan, pos_over_neg: 403.4342346191406 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6818, loss_val: nan, pos_over_neg: 337.9139404296875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 615.608154296875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.6624, loss_val: nan, pos_over_neg: 419.3496398925781 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6173, loss_val: nan, pos_over_neg: 451.43768310546875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6186, loss_val: nan, pos_over_neg: 352.8441467285156 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.6645, loss_val: nan, pos_over_neg: 295.4499816894531 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6638, loss_val: nan, pos_over_neg: 311.6161804199219 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 404.88128662109375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6338, loss_val: nan, pos_over_neg: 426.1877746582031 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.6514, loss_val: nan, pos_over_neg: 771.35400390625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.6626, loss_val: nan, pos_over_neg: 423.0819091796875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.6318, loss_val: nan, pos_over_neg: 551.9066162109375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.629, loss_val: nan, pos_over_neg: 373.6214904785156 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.6514, loss_val: nan, pos_over_neg: 529.2794189453125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6317, loss_val: nan, pos_over_neg: 390.6125183105469 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.6877, loss_val: nan, pos_over_neg: 593.551025390625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.6311, loss_val: nan, pos_over_neg: 389.6457214355469 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.6947, loss_val: nan, pos_over_neg: 407.3638000488281 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.6545, loss_val: nan, pos_over_neg: 350.2727355957031 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 384.76104736328125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.6504, loss_val: nan, pos_over_neg: 327.8922119140625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.6053, loss_val: nan, pos_over_neg: 527.938720703125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 414.4748840332031 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 294.7003479003906 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 337.89288330078125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6815, loss_val: nan, pos_over_neg: 347.3505554199219 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.6148, loss_val: nan, pos_over_neg: 358.57196044921875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6548, loss_val: nan, pos_over_neg: 333.55841064453125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6355, loss_val: nan, pos_over_neg: 296.9881286621094 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6708, loss_val: nan, pos_over_neg: 342.4604797363281 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6442, loss_val: nan, pos_over_neg: 534.8339233398438 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.648, loss_val: nan, pos_over_neg: 544.0737915039062 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.6686, loss_val: nan, pos_over_neg: 417.9751892089844 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6454, loss_val: nan, pos_over_neg: 417.2490234375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.6206, loss_val: nan, pos_over_neg: 311.52972412109375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.6915, loss_val: nan, pos_over_neg: 339.093505859375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 400.9290466308594 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.6422, loss_val: nan, pos_over_neg: 448.382080078125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.6684, loss_val: nan, pos_over_neg: 297.40191650390625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.6563, loss_val: nan, pos_over_neg: 425.0058288574219 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 557.9315185546875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 417.1418762207031 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.6623, loss_val: nan, pos_over_neg: 330.5983581542969 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 384.9200134277344 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.6507, loss_val: nan, pos_over_neg: 761.7713623046875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.6779, loss_val: nan, pos_over_neg: 364.63555908203125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.627, loss_val: nan, pos_over_neg: 291.64300537109375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.6373, loss_val: nan, pos_over_neg: 279.0660400390625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 296.3032531738281 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.6127, loss_val: nan, pos_over_neg: 475.0931396484375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.6894, loss_val: nan, pos_over_neg: 287.4500427246094 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 423.5516357421875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6348, loss_val: nan, pos_over_neg: 450.3358154296875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.6568, loss_val: nan, pos_over_neg: 373.85528564453125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.6657, loss_val: nan, pos_over_neg: 492.36236572265625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.6681, loss_val: nan, pos_over_neg: 406.54998779296875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.7032, loss_val: nan, pos_over_neg: 284.7174072265625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 427.17645263671875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 354.4559020996094 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.7229, loss_val: nan, pos_over_neg: 376.10980224609375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 574.2129516601562 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 466.3714599609375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 339.4920349121094 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.68, loss_val: nan, pos_over_neg: 282.3290100097656 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/300000 [2:15:51<75302:21:01, 903.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.6346, loss_val: nan, pos_over_neg: 307.8758544921875 lr: 0.00031623\n",
      "epoch: 9\n",
      "Iter: 0/695, loss_train: 3.667, loss_val: nan, pos_over_neg: 384.30389404296875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.6758, loss_val: nan, pos_over_neg: 452.3017578125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.685, loss_val: nan, pos_over_neg: 289.26068115234375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.5972, loss_val: nan, pos_over_neg: 776.3763427734375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 355.4818115234375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 387.5028991699219 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.6989, loss_val: nan, pos_over_neg: 264.13238525390625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 315.4355163574219 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 379.8302917480469 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6881, loss_val: nan, pos_over_neg: 271.2949523925781 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6605, loss_val: nan, pos_over_neg: 266.9708251953125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.6121, loss_val: nan, pos_over_neg: 414.6895751953125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.605, loss_val: nan, pos_over_neg: 401.4328918457031 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.6574, loss_val: nan, pos_over_neg: 372.2627868652344 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 303.9439392089844 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 381.587646484375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 665.0254516601562 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.66, loss_val: nan, pos_over_neg: 323.5873107910156 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.6388, loss_val: nan, pos_over_neg: 577.261962890625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.6606, loss_val: nan, pos_over_neg: 286.169921875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6502, loss_val: nan, pos_over_neg: 359.003662109375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 320.6239318847656 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.6811, loss_val: nan, pos_over_neg: 458.7038269042969 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.6835, loss_val: nan, pos_over_neg: 361.5257873535156 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6499, loss_val: nan, pos_over_neg: 319.5437927246094 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 530.9113159179688 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6619, loss_val: nan, pos_over_neg: 369.7342529296875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.6543, loss_val: nan, pos_over_neg: 252.9501190185547 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6685, loss_val: nan, pos_over_neg: 288.70001220703125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.6588, loss_val: nan, pos_over_neg: 519.1876220703125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6654, loss_val: nan, pos_over_neg: 322.5035705566406 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.5865, loss_val: nan, pos_over_neg: 648.4403076171875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 313.4006042480469 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6554, loss_val: nan, pos_over_neg: 481.6491394042969 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 480.58251953125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.6653, loss_val: nan, pos_over_neg: 523.2721557617188 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.686, loss_val: nan, pos_over_neg: 332.1561584472656 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.6549, loss_val: nan, pos_over_neg: 442.215087890625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6447, loss_val: nan, pos_over_neg: 349.7926025390625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.6532, loss_val: nan, pos_over_neg: 377.11492919921875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6412, loss_val: nan, pos_over_neg: 363.4472351074219 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 408.2232666015625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.6553, loss_val: nan, pos_over_neg: 333.7938537597656 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.6554, loss_val: nan, pos_over_neg: 233.2049560546875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.6108, loss_val: nan, pos_over_neg: 530.5741577148438 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 948.0861206054688 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 710.9557495117188 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 512.410400390625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 404.0794982910156 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 628.5301513671875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.6426, loss_val: nan, pos_over_neg: 603.7451171875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 380.846435546875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.6494, loss_val: nan, pos_over_neg: 323.3693542480469 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.6335, loss_val: nan, pos_over_neg: 353.7088317871094 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6203, loss_val: nan, pos_over_neg: 353.9796142578125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.6769, loss_val: nan, pos_over_neg: 346.0005187988281 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.6441, loss_val: nan, pos_over_neg: 459.66119384765625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.6699, loss_val: nan, pos_over_neg: 370.8052673339844 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.6352, loss_val: nan, pos_over_neg: 502.4988708496094 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 409.27679443359375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.6488, loss_val: nan, pos_over_neg: 449.92486572265625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.7014, loss_val: nan, pos_over_neg: 383.00518798828125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.6477, loss_val: nan, pos_over_neg: 364.2162780761719 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6241, loss_val: nan, pos_over_neg: 350.19580078125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 321.7152099609375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.6483, loss_val: nan, pos_over_neg: 360.6770935058594 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.6523, loss_val: nan, pos_over_neg: 293.97967529296875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 329.7204284667969 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.6543, loss_val: nan, pos_over_neg: 1132.5736083984375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.6488, loss_val: nan, pos_over_neg: 746.8878784179688 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6399, loss_val: nan, pos_over_neg: 768.9920043945312 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.6536, loss_val: nan, pos_over_neg: 501.7103576660156 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.6552, loss_val: nan, pos_over_neg: 326.6413269042969 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 432.5895080566406 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6586, loss_val: nan, pos_over_neg: 219.7767791748047 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.6784, loss_val: nan, pos_over_neg: 260.6949462890625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 453.4803466796875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.6609, loss_val: nan, pos_over_neg: 303.0378723144531 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 440.0505676269531 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 405.8272705078125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.6613, loss_val: nan, pos_over_neg: 422.96295166015625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.6395, loss_val: nan, pos_over_neg: 515.639404296875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.6117, loss_val: nan, pos_over_neg: 511.9252624511719 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.6768, loss_val: nan, pos_over_neg: 325.966064453125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.6219, loss_val: nan, pos_over_neg: 551.6055297851562 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 517.5606079101562 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.631, loss_val: nan, pos_over_neg: 318.36474609375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.6262, loss_val: nan, pos_over_neg: 380.5349426269531 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 436.4312744140625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 332.3025817871094 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 317.7884216308594 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6426, loss_val: nan, pos_over_neg: 278.138427734375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6595, loss_val: nan, pos_over_neg: 327.66192626953125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.6439, loss_val: nan, pos_over_neg: 376.4552001953125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6462, loss_val: nan, pos_over_neg: 338.87786865234375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.6977, loss_val: nan, pos_over_neg: 297.1979064941406 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 370.8858947753906 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 657.3757934570312 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.641, loss_val: nan, pos_over_neg: 463.9747009277344 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.6591, loss_val: nan, pos_over_neg: 455.18072509765625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 509.082275390625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 512.2094116210938 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 382.77825927734375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.6277, loss_val: nan, pos_over_neg: 295.6454162597656 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 433.6822204589844 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 541.2279663085938 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.6742, loss_val: nan, pos_over_neg: 272.01678466796875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6541, loss_val: nan, pos_over_neg: 273.4245300292969 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6338, loss_val: nan, pos_over_neg: 372.36041259765625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6442, loss_val: nan, pos_over_neg: 392.1305847167969 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6711, loss_val: nan, pos_over_neg: 483.2123107910156 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6467, loss_val: nan, pos_over_neg: 760.6510620117188 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.654, loss_val: nan, pos_over_neg: 570.9654541015625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.65, loss_val: nan, pos_over_neg: 601.875732421875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.6321, loss_val: nan, pos_over_neg: 753.576171875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.6278, loss_val: nan, pos_over_neg: 525.4585571289062 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 828.2805786132812 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 262.2540283203125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.6192, loss_val: nan, pos_over_neg: 319.15863037109375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.6426, loss_val: nan, pos_over_neg: 289.8537902832031 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 391.7579345703125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 506.5770568847656 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 401.207275390625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 331.9441833496094 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.6545, loss_val: nan, pos_over_neg: 366.30841064453125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 298.1261901855469 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.6835, loss_val: nan, pos_over_neg: 356.39141845703125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.6218, loss_val: nan, pos_over_neg: 437.2247619628906 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 498.8517150878906 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.6462, loss_val: nan, pos_over_neg: 497.33282470703125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 597.4420166015625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 533.81982421875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.6673, loss_val: nan, pos_over_neg: 325.85882568359375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6586, loss_val: nan, pos_over_neg: 335.39764404296875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 434.0704040527344 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 478.6651916503906 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.6793, loss_val: nan, pos_over_neg: 307.96142578125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.6581, loss_val: nan, pos_over_neg: 431.9330749511719 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.6016, loss_val: nan, pos_over_neg: 792.6436767578125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.6585, loss_val: nan, pos_over_neg: 444.5909118652344 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6723, loss_val: nan, pos_over_neg: 538.9906616210938 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.654, loss_val: nan, pos_over_neg: 384.6695251464844 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6656, loss_val: nan, pos_over_neg: 310.69281005859375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 528.2800903320312 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.6326, loss_val: nan, pos_over_neg: 376.9586486816406 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.6583, loss_val: nan, pos_over_neg: 379.5480651855469 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.6289, loss_val: nan, pos_over_neg: 380.4951477050781 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.6843, loss_val: nan, pos_over_neg: 234.72625732421875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 334.77642822265625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.6184, loss_val: nan, pos_over_neg: 381.296630859375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 444.5782470703125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 551.9907836914062 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 509.49407958984375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 371.388916015625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.6443, loss_val: nan, pos_over_neg: 457.02056884765625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 494.089111328125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6289, loss_val: nan, pos_over_neg: 311.9211120605469 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.6458, loss_val: nan, pos_over_neg: 262.06597900390625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.68, loss_val: nan, pos_over_neg: 219.40087890625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 414.38665771484375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.691, loss_val: nan, pos_over_neg: 267.0484924316406 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 383.232421875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 356.4315490722656 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 421.5260314941406 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6324, loss_val: nan, pos_over_neg: 905.7127075195312 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.6494, loss_val: nan, pos_over_neg: 554.0845947265625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 396.9674377441406 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.6259, loss_val: nan, pos_over_neg: 299.06884765625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.6442, loss_val: nan, pos_over_neg: 325.220458984375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 364.946044921875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.6437, loss_val: nan, pos_over_neg: 369.1535949707031 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 375.68597412109375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.6697, loss_val: nan, pos_over_neg: 483.86865234375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.6384, loss_val: nan, pos_over_neg: 440.7304382324219 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.6188, loss_val: nan, pos_over_neg: 581.5260620117188 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.634, loss_val: nan, pos_over_neg: 711.0675659179688 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6706, loss_val: nan, pos_over_neg: 313.9711608886719 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 385.09375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6589, loss_val: nan, pos_over_neg: 344.3475341796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6642, loss_val: nan, pos_over_neg: 408.5388488769531 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 766.653076171875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 732.22412109375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 730.18212890625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.664, loss_val: nan, pos_over_neg: 349.9546203613281 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 687.0689697265625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.6188, loss_val: nan, pos_over_neg: 581.70361328125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.6452, loss_val: nan, pos_over_neg: 675.403564453125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 526.018798828125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 591.1608276367188 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.6992, loss_val: nan, pos_over_neg: 316.11883544921875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 542.2725219726562 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 325.6708068847656 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 730.4647216796875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.6271, loss_val: nan, pos_over_neg: 372.7030334472656 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.6759, loss_val: nan, pos_over_neg: 317.1839599609375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.6945, loss_val: nan, pos_over_neg: 327.03326416015625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 318.0228576660156 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.6632, loss_val: nan, pos_over_neg: 324.9909973144531 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.6391, loss_val: nan, pos_over_neg: 561.010986328125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 513.9512329101562 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6277, loss_val: nan, pos_over_neg: 429.2550354003906 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.657, loss_val: nan, pos_over_neg: 385.0185546875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 384.476806640625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.6443, loss_val: nan, pos_over_neg: 341.5869445800781 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.6693, loss_val: nan, pos_over_neg: 421.140869140625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.6071, loss_val: nan, pos_over_neg: 536.6041259765625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.6624, loss_val: nan, pos_over_neg: 303.1103515625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 343.95281982421875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.6434, loss_val: nan, pos_over_neg: 313.5081481933594 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.6449, loss_val: nan, pos_over_neg: 398.68963623046875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6235, loss_val: nan, pos_over_neg: 505.9233093261719 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6468, loss_val: nan, pos_over_neg: 435.159912109375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.6555, loss_val: nan, pos_over_neg: 369.1468200683594 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 1242.6239013671875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.6623, loss_val: nan, pos_over_neg: 545.8607788085938 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.651, loss_val: nan, pos_over_neg: 817.1063232421875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 432.64471435546875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 256.46148681640625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.6351, loss_val: nan, pos_over_neg: 324.2792663574219 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 252.70486450195312 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.6264, loss_val: nan, pos_over_neg: 518.9603271484375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 470.9663391113281 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.6367, loss_val: nan, pos_over_neg: 398.0757751464844 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 1049.0628662109375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.657, loss_val: nan, pos_over_neg: 434.7976379394531 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 567.1002197265625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 409.818603515625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6707, loss_val: nan, pos_over_neg: 290.6208190917969 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 500.5871276855469 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 669.52734375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.6463, loss_val: nan, pos_over_neg: 284.703125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.6251, loss_val: nan, pos_over_neg: 308.2864685058594 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.633, loss_val: nan, pos_over_neg: 372.5347595214844 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.6541, loss_val: nan, pos_over_neg: 337.6920166015625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6669, loss_val: nan, pos_over_neg: 453.5177001953125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6381, loss_val: nan, pos_over_neg: 409.15081787109375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.5785, loss_val: nan, pos_over_neg: 538.5117797851562 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.6469, loss_val: nan, pos_over_neg: 342.7846984863281 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 379.37762451171875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.6348, loss_val: nan, pos_over_neg: 491.6920166015625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 635.76953125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.6482, loss_val: nan, pos_over_neg: 517.495361328125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.6423, loss_val: nan, pos_over_neg: 342.6105041503906 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.6373, loss_val: nan, pos_over_neg: 375.2571716308594 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.6133, loss_val: nan, pos_over_neg: 329.2203063964844 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6402, loss_val: nan, pos_over_neg: 216.30474853515625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.6745, loss_val: nan, pos_over_neg: 236.06283569335938 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 715.90087890625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 345.77459716796875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.6632, loss_val: nan, pos_over_neg: 303.81231689453125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6563, loss_val: nan, pos_over_neg: 399.3760986328125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.6321, loss_val: nan, pos_over_neg: 640.9869384765625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6334, loss_val: nan, pos_over_neg: 591.62744140625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 646.583251953125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 380.9126281738281 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.644, loss_val: nan, pos_over_neg: 419.9729309082031 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.6611, loss_val: nan, pos_over_neg: 306.13409423828125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 342.5764465332031 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.6082, loss_val: nan, pos_over_neg: 413.4552001953125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.5987, loss_val: nan, pos_over_neg: 418.2567138671875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.6301, loss_val: nan, pos_over_neg: 338.4463806152344 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.672, loss_val: nan, pos_over_neg: 312.57965087890625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.651, loss_val: nan, pos_over_neg: 370.3174133300781 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 379.7577209472656 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.7112, loss_val: nan, pos_over_neg: 221.88076782226562 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 329.32757568359375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6531, loss_val: nan, pos_over_neg: 267.8593444824219 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 554.90625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.5984, loss_val: nan, pos_over_neg: 587.62158203125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.6054, loss_val: nan, pos_over_neg: 390.8222351074219 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.6459, loss_val: nan, pos_over_neg: 347.8160400390625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.7165, loss_val: nan, pos_over_neg: 166.8049774169922 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.659, loss_val: nan, pos_over_neg: 193.92710876464844 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 409.873291015625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6346, loss_val: nan, pos_over_neg: 298.5594787597656 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 328.34344482421875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6607, loss_val: nan, pos_over_neg: 293.637939453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 334.5218505859375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.6272, loss_val: nan, pos_over_neg: 439.0705261230469 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6498, loss_val: nan, pos_over_neg: 400.9380187988281 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 508.5074462890625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.6498, loss_val: nan, pos_over_neg: 460.6846008300781 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6543, loss_val: nan, pos_over_neg: 278.74163818359375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6586, loss_val: nan, pos_over_neg: 273.7070617675781 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.6511, loss_val: nan, pos_over_neg: 268.66656494140625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.602, loss_val: nan, pos_over_neg: 280.7738037109375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 268.8117980957031 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.6529, loss_val: nan, pos_over_neg: 357.41522216796875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.6604, loss_val: nan, pos_over_neg: 286.4878845214844 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 266.642333984375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.6668, loss_val: nan, pos_over_neg: 368.1212463378906 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.6446, loss_val: nan, pos_over_neg: 468.8587341308594 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.6533, loss_val: nan, pos_over_neg: 554.0299072265625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 423.1470031738281 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 423.2449645996094 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.6424, loss_val: nan, pos_over_neg: 318.3111572265625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6717, loss_val: nan, pos_over_neg: 471.902099609375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 382.89935302734375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 222.71180725097656 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6946, loss_val: nan, pos_over_neg: 291.53564453125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 523.7534790039062 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 294.41351318359375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.5941, loss_val: nan, pos_over_neg: 584.8739624023438 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6386, loss_val: nan, pos_over_neg: 565.3381958007812 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.6498, loss_val: nan, pos_over_neg: 392.3482971191406 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.6215, loss_val: nan, pos_over_neg: 434.76751708984375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.6833, loss_val: nan, pos_over_neg: 377.0244445800781 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 653.3797607421875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.6232, loss_val: nan, pos_over_neg: 537.1731567382812 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6406, loss_val: nan, pos_over_neg: 394.3345947265625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6352, loss_val: nan, pos_over_neg: 351.1881408691406 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 387.22332763671875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 382.89569091796875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.6243, loss_val: nan, pos_over_neg: 490.88055419921875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6822, loss_val: nan, pos_over_neg: 313.11517333984375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.6472, loss_val: nan, pos_over_neg: 471.7105407714844 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 534.3171997070312 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 326.0933837890625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6208, loss_val: nan, pos_over_neg: 522.0653686523438 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 456.57879638671875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6382, loss_val: nan, pos_over_neg: 407.79119873046875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 624.3025512695312 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.6528, loss_val: nan, pos_over_neg: 485.9354248046875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.5786, loss_val: nan, pos_over_neg: 662.7201538085938 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 334.6430969238281 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 993.9559936523438 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.6525, loss_val: nan, pos_over_neg: 358.2568054199219 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6422, loss_val: nan, pos_over_neg: 310.1929626464844 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.6483, loss_val: nan, pos_over_neg: 233.47515869140625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 435.6356506347656 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.6835, loss_val: nan, pos_over_neg: 327.6484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 237.5840606689453 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.6638, loss_val: nan, pos_over_neg: 350.09759521484375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.7066, loss_val: nan, pos_over_neg: 304.2435607910156 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.6471, loss_val: nan, pos_over_neg: 421.5664367675781 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 287.07843017578125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.6506, loss_val: nan, pos_over_neg: 416.5842590332031 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.6408, loss_val: nan, pos_over_neg: 482.8276062011719 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.6438, loss_val: nan, pos_over_neg: 513.3736572265625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6127, loss_val: nan, pos_over_neg: 328.3834533691406 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 380.1734924316406 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.6159, loss_val: nan, pos_over_neg: 457.18182373046875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.6686, loss_val: nan, pos_over_neg: 220.4926300048828 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.658, loss_val: nan, pos_over_neg: 419.16107177734375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.6399, loss_val: nan, pos_over_neg: 478.6652526855469 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.6677, loss_val: nan, pos_over_neg: 385.0943298339844 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6188, loss_val: nan, pos_over_neg: 508.0325927734375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.6359, loss_val: nan, pos_over_neg: 400.32574462890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.6593, loss_val: nan, pos_over_neg: 379.12615966796875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.6855, loss_val: nan, pos_over_neg: 416.82958984375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 378.7253112792969 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.6773, loss_val: nan, pos_over_neg: 312.20330810546875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 621.5927124023438 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 672.5140991210938 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 516.1287231445312 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6705, loss_val: nan, pos_over_neg: 359.3746337890625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 301.1778869628906 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 491.8805236816406 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.6299, loss_val: nan, pos_over_neg: 425.2288818359375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.6243, loss_val: nan, pos_over_neg: 876.6455078125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.7118, loss_val: nan, pos_over_neg: 370.7446594238281 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 445.24505615234375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6529, loss_val: nan, pos_over_neg: 307.1153869628906 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.6731, loss_val: nan, pos_over_neg: 445.4801940917969 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6075, loss_val: nan, pos_over_neg: 727.121337890625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 665.6039428710938 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.6501, loss_val: nan, pos_over_neg: 383.2342224121094 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6785, loss_val: nan, pos_over_neg: 286.9858093261719 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.6815, loss_val: nan, pos_over_neg: 259.9785461425781 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.6543, loss_val: nan, pos_over_neg: 303.9095458984375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.6399, loss_val: nan, pos_over_neg: 457.462158203125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 448.8785705566406 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 425.7839660644531 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.6422, loss_val: nan, pos_over_neg: 616.7328491210938 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.5995, loss_val: nan, pos_over_neg: 729.9476928710938 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.6192, loss_val: nan, pos_over_neg: 423.4493103027344 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.6306, loss_val: nan, pos_over_neg: 318.2249755859375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6442, loss_val: nan, pos_over_neg: 396.8244323730469 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 337.207763671875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 449.2337951660156 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.6355, loss_val: nan, pos_over_neg: 635.5266723632812 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6325, loss_val: nan, pos_over_neg: 394.7648010253906 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6381, loss_val: nan, pos_over_neg: 271.46051025390625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 366.22698974609375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.5946, loss_val: nan, pos_over_neg: 608.721923828125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.647, loss_val: nan, pos_over_neg: 362.7651672363281 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 515.8181762695312 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6325, loss_val: nan, pos_over_neg: 455.78106689453125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.6606, loss_val: nan, pos_over_neg: 384.537109375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.653, loss_val: nan, pos_over_neg: 310.8546142578125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 281.9698486328125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6405, loss_val: nan, pos_over_neg: 398.8207702636719 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 316.69647216796875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6191, loss_val: nan, pos_over_neg: 312.7782897949219 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.7043, loss_val: nan, pos_over_neg: 352.7981262207031 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 553.7392578125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6009, loss_val: nan, pos_over_neg: 486.55987548828125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.6293, loss_val: nan, pos_over_neg: 256.82354736328125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.6499, loss_val: nan, pos_over_neg: 283.9764404296875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 589.3645629882812 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 330.7179260253906 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.7075, loss_val: nan, pos_over_neg: 223.9796142578125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.6746, loss_val: nan, pos_over_neg: 230.10635375976562 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 717.879638671875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.6656, loss_val: nan, pos_over_neg: 318.092041015625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6314, loss_val: nan, pos_over_neg: 419.4344787597656 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.6268, loss_val: nan, pos_over_neg: 392.7225646972656 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 367.0867004394531 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6562, loss_val: nan, pos_over_neg: 414.9121398925781 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 351.0418701171875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 350.07257080078125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.657, loss_val: nan, pos_over_neg: 337.4151306152344 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.6296, loss_val: nan, pos_over_neg: 439.35943603515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6787, loss_val: nan, pos_over_neg: 326.05914306640625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.6389, loss_val: nan, pos_over_neg: 388.1241455078125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.6303, loss_val: nan, pos_over_neg: 526.7711181640625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 416.7231750488281 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 493.22320556640625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.6222, loss_val: nan, pos_over_neg: 490.677978515625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.609, loss_val: nan, pos_over_neg: 509.1644592285156 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6295, loss_val: nan, pos_over_neg: 462.593505859375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.6235, loss_val: nan, pos_over_neg: 617.954345703125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.5791, loss_val: nan, pos_over_neg: 385.2849426269531 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.6784, loss_val: nan, pos_over_neg: 271.5845031738281 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.6341, loss_val: nan, pos_over_neg: 280.72906494140625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.6491, loss_val: nan, pos_over_neg: 347.0321044921875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 338.3807067871094 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6914, loss_val: nan, pos_over_neg: 228.7845001220703 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.6324, loss_val: nan, pos_over_neg: 772.488037109375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.6296, loss_val: nan, pos_over_neg: 467.51513671875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.6424, loss_val: nan, pos_over_neg: 346.7181091308594 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6687, loss_val: nan, pos_over_neg: 344.77362060546875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 526.40576171875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 810.2349853515625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.5859, loss_val: nan, pos_over_neg: 529.7048950195312 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 257.2227478027344 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.692, loss_val: nan, pos_over_neg: 201.05142211914062 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6429, loss_val: nan, pos_over_neg: 244.17015075683594 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 289.38018798828125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 308.9826965332031 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.6428, loss_val: nan, pos_over_neg: 266.74102783203125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.6592, loss_val: nan, pos_over_neg: 344.09454345703125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.6048, loss_val: nan, pos_over_neg: 393.76373291015625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 333.16705322265625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 372.5349426269531 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6116, loss_val: nan, pos_over_neg: 398.8907165527344 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.6395, loss_val: nan, pos_over_neg: 368.3716125488281 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 626.086669921875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 426.9198303222656 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 382.64398193359375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6488, loss_val: nan, pos_over_neg: 335.81134033203125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 356.30694580078125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.6472, loss_val: nan, pos_over_neg: 312.4010925292969 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.6805, loss_val: nan, pos_over_neg: 330.35675048828125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.6438, loss_val: nan, pos_over_neg: 337.6920471191406 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.6592, loss_val: nan, pos_over_neg: 372.67388916015625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 424.5803527832031 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 652.5960083007812 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 455.3777770996094 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.6742, loss_val: nan, pos_over_neg: 469.488525390625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 790.349853515625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6633, loss_val: nan, pos_over_neg: 389.47198486328125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6468, loss_val: nan, pos_over_neg: 359.4975280761719 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.6211, loss_val: nan, pos_over_neg: 353.51373291015625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 408.5909423828125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 413.343505859375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.6524, loss_val: nan, pos_over_neg: 341.68267822265625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.6637, loss_val: nan, pos_over_neg: 314.44293212890625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.6388, loss_val: nan, pos_over_neg: 420.9566650390625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6383, loss_val: nan, pos_over_neg: 297.7166442871094 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.6478, loss_val: nan, pos_over_neg: 392.8726806640625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 369.50799560546875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 404.7325744628906 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6548, loss_val: nan, pos_over_neg: 594.3009033203125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.6581, loss_val: nan, pos_over_neg: 427.71636962890625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6336, loss_val: nan, pos_over_neg: 319.03985595703125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 498.7091979980469 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 361.2925109863281 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.6537, loss_val: nan, pos_over_neg: 280.3671875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.6587, loss_val: nan, pos_over_neg: 234.11570739746094 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.6432, loss_val: nan, pos_over_neg: 293.3314208984375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 470.92230224609375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.6157, loss_val: nan, pos_over_neg: 551.5178833007812 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 592.0362548828125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.6084, loss_val: nan, pos_over_neg: 613.6901245117188 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.6565, loss_val: nan, pos_over_neg: 390.3750915527344 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.6386, loss_val: nan, pos_over_neg: 546.9769897460938 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.699, loss_val: nan, pos_over_neg: 532.9591674804688 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.6606, loss_val: nan, pos_over_neg: 455.3982849121094 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 251.40780639648438 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.6314, loss_val: nan, pos_over_neg: 296.5345458984375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.6191, loss_val: nan, pos_over_neg: 336.1401062011719 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 231.954345703125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6665, loss_val: nan, pos_over_neg: 248.28758239746094 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.6261, loss_val: nan, pos_over_neg: 281.4193420410156 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.6328, loss_val: nan, pos_over_neg: 641.9118041992188 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6865, loss_val: nan, pos_over_neg: 345.5967102050781 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 484.71929931640625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 386.55938720703125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6202, loss_val: nan, pos_over_neg: 511.5388488769531 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 756.65869140625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6552, loss_val: nan, pos_over_neg: 315.51611328125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 239.02972412109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.6925, loss_val: nan, pos_over_neg: 252.109375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 237.77146911621094 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.6441, loss_val: nan, pos_over_neg: 291.7682800292969 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6275, loss_val: nan, pos_over_neg: 449.9060363769531 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6789, loss_val: nan, pos_over_neg: 314.3482666015625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.667, loss_val: nan, pos_over_neg: 556.9803466796875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.6874, loss_val: nan, pos_over_neg: 300.5652160644531 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.6215, loss_val: nan, pos_over_neg: 524.1873779296875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6376, loss_val: nan, pos_over_neg: 439.84967041015625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.6125, loss_val: nan, pos_over_neg: 380.5867919921875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.6443, loss_val: nan, pos_over_neg: 384.62274169921875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 311.9379577636719 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 312.55426025390625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6697, loss_val: nan, pos_over_neg: 279.29498291015625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 309.69580078125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.6203, loss_val: nan, pos_over_neg: 287.212646484375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6123, loss_val: nan, pos_over_neg: 555.6499633789062 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.6477, loss_val: nan, pos_over_neg: 286.70806884765625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6839, loss_val: nan, pos_over_neg: 320.5598449707031 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 372.7815856933594 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6477, loss_val: nan, pos_over_neg: 375.22235107421875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 399.3279113769531 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.6219, loss_val: nan, pos_over_neg: 409.8348693847656 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6351, loss_val: nan, pos_over_neg: 422.5888366699219 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 351.8426208496094 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.6446, loss_val: nan, pos_over_neg: 247.94578552246094 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6435, loss_val: nan, pos_over_neg: 287.1582946777344 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.5943, loss_val: nan, pos_over_neg: 518.7752685546875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6406, loss_val: nan, pos_over_neg: 329.2419738769531 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.6298, loss_val: nan, pos_over_neg: 292.8636474609375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 372.5721130371094 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6163, loss_val: nan, pos_over_neg: 536.53369140625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 400.4082336425781 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 506.54632568359375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 435.8080749511719 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.6167, loss_val: nan, pos_over_neg: 342.15301513671875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.573, loss_val: nan, pos_over_neg: 550.9247436523438 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.6208, loss_val: nan, pos_over_neg: 268.74700927734375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.6489, loss_val: nan, pos_over_neg: 383.9497375488281 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 431.8974609375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.6549, loss_val: nan, pos_over_neg: 265.3291015625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 482.92474365234375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.638, loss_val: nan, pos_over_neg: 426.1805419921875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.6569, loss_val: nan, pos_over_neg: 378.2354736328125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6554, loss_val: nan, pos_over_neg: 486.9266357421875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6183, loss_val: nan, pos_over_neg: 469.8851318359375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 414.7524108886719 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6321, loss_val: nan, pos_over_neg: 616.4925537109375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6395, loss_val: nan, pos_over_neg: 648.0900268554688 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6402, loss_val: nan, pos_over_neg: 633.2930908203125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.6182, loss_val: nan, pos_over_neg: 600.2562255859375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.6234, loss_val: nan, pos_over_neg: 626.7177734375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.6065, loss_val: nan, pos_over_neg: 452.7867126464844 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 580.3861694335938 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.5948, loss_val: nan, pos_over_neg: 346.024169921875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 452.97998046875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.5905, loss_val: nan, pos_over_neg: 677.63623046875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 253.94796752929688 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.6264, loss_val: nan, pos_over_neg: 385.3457946777344 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.679, loss_val: nan, pos_over_neg: 241.70672607421875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.6719, loss_val: nan, pos_over_neg: 344.26190185546875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 339.58221435546875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6575, loss_val: nan, pos_over_neg: 415.8290100097656 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 638.65087890625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.6976, loss_val: nan, pos_over_neg: 461.76434326171875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 484.9439392089844 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.6251, loss_val: nan, pos_over_neg: 556.6949462890625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.7139, loss_val: nan, pos_over_neg: 258.09405517578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6167, loss_val: nan, pos_over_neg: 400.8737487792969 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 345.91107177734375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.638, loss_val: nan, pos_over_neg: 338.2591247558594 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 361.3371276855469 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.6726, loss_val: nan, pos_over_neg: 359.4927062988281 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.6184, loss_val: nan, pos_over_neg: 384.0796203613281 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 256.53228759765625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.6112, loss_val: nan, pos_over_neg: 693.3819580078125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.6262, loss_val: nan, pos_over_neg: 376.2998962402344 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.6224, loss_val: nan, pos_over_neg: 575.9407958984375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6448, loss_val: nan, pos_over_neg: 592.1387939453125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 618.7028198242188 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.6303, loss_val: nan, pos_over_neg: 551.677001953125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.5789, loss_val: nan, pos_over_neg: 682.680908203125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.6478, loss_val: nan, pos_over_neg: 442.0200500488281 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6711, loss_val: nan, pos_over_neg: 357.0263671875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.6372, loss_val: nan, pos_over_neg: 371.9429931640625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.6361, loss_val: nan, pos_over_neg: 334.540283203125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 535.9244995117188 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 496.548095703125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.6338, loss_val: nan, pos_over_neg: 488.2276916503906 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.6523, loss_val: nan, pos_over_neg: 344.2794494628906 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.5976, loss_val: nan, pos_over_neg: 739.5156860351562 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.6599, loss_val: nan, pos_over_neg: 668.8059692382812 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 379.0141296386719 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 402.056884765625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6093, loss_val: nan, pos_over_neg: 523.5072021484375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.6375, loss_val: nan, pos_over_neg: 328.364990234375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.6438, loss_val: nan, pos_over_neg: 359.53546142578125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 389.5110778808594 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 572.8187255859375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 669.6930541992188 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 574.796142578125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.6613, loss_val: nan, pos_over_neg: 325.0995178222656 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 376.26397705078125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.676, loss_val: nan, pos_over_neg: 243.78350830078125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 382.2156982421875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6381, loss_val: nan, pos_over_neg: 311.784423828125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6404, loss_val: nan, pos_over_neg: 354.51812744140625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.6439, loss_val: nan, pos_over_neg: 391.7001037597656 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.6531, loss_val: nan, pos_over_neg: 502.9361877441406 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6307, loss_val: nan, pos_over_neg: 534.6968383789062 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 1208.0333251953125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.6834, loss_val: nan, pos_over_neg: 345.92755126953125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 428.79132080078125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 561.8187255859375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.6224, loss_val: nan, pos_over_neg: 297.4950256347656 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.6672, loss_val: nan, pos_over_neg: 243.10543823242188 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6805, loss_val: nan, pos_over_neg: 283.4477233886719 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.5958, loss_val: nan, pos_over_neg: 661.92626953125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.625, loss_val: nan, pos_over_neg: 469.99432373046875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 406.30023193359375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.6705, loss_val: nan, pos_over_neg: 563.0155029296875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 614.5829467773438 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.6508, loss_val: nan, pos_over_neg: 557.6057739257812 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.6758, loss_val: nan, pos_over_neg: 330.8212890625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 395.3005676269531 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 438.7875061035156 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 524.375732421875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6373, loss_val: nan, pos_over_neg: 404.6965637207031 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.6454, loss_val: nan, pos_over_neg: 254.80349731445312 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.6849, loss_val: nan, pos_over_neg: 303.8841247558594 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6361, loss_val: nan, pos_over_neg: 375.42974853515625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6492, loss_val: nan, pos_over_neg: 348.64990234375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.6469, loss_val: nan, pos_over_neg: 411.338623046875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6553, loss_val: nan, pos_over_neg: 387.13818359375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 436.27593994140625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6677, loss_val: nan, pos_over_neg: 532.006591796875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.6421, loss_val: nan, pos_over_neg: 290.15283203125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 351.3595275878906 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.5833, loss_val: nan, pos_over_neg: 438.170654296875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 428.69830322265625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 317.734130859375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 419.0626525878906 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 466.8641052246094 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 405.4144287109375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.629, loss_val: nan, pos_over_neg: 306.1114501953125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 248.06822204589844 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 361.83612060546875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.6051, loss_val: nan, pos_over_neg: 418.54656982421875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.6198, loss_val: nan, pos_over_neg: 422.296630859375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 280.2835998535156 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.627, loss_val: nan, pos_over_neg: 404.4073486328125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6503, loss_val: nan, pos_over_neg: 333.3215026855469 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6721, loss_val: nan, pos_over_neg: 460.5337219238281 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.644, loss_val: nan, pos_over_neg: 320.732666015625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 375.0292663574219 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6358, loss_val: nan, pos_over_neg: 325.7743835449219 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6564, loss_val: nan, pos_over_neg: 341.7266540527344 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.619, loss_val: nan, pos_over_neg: 429.404052734375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 419.22515869140625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 293.3246154785156 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6831, loss_val: nan, pos_over_neg: 429.803466796875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 294.66571044921875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 364.7695617675781 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.6195, loss_val: nan, pos_over_neg: 536.1710205078125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.5744, loss_val: nan, pos_over_neg: 794.7238159179688 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.6348, loss_val: nan, pos_over_neg: 433.6927490234375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.6606, loss_val: nan, pos_over_neg: 293.93560791015625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6634, loss_val: nan, pos_over_neg: 226.3399200439453 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 467.7091369628906 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.6423, loss_val: nan, pos_over_neg: 332.50714111328125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.642, loss_val: nan, pos_over_neg: 425.8868408203125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 461.6265869140625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 473.3090515136719 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6322, loss_val: nan, pos_over_neg: 337.5414123535156 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 473.3698425292969 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 453.90374755859375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.635, loss_val: nan, pos_over_neg: 308.10272216796875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 456.4367370605469 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 508.00506591796875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6562, loss_val: nan, pos_over_neg: 389.4823303222656 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 713.781494140625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 493.67474365234375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 419.0374450683594 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 440.4635314941406 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.6679, loss_val: nan, pos_over_neg: 331.847900390625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 516.9821166992188 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.6322, loss_val: nan, pos_over_neg: 299.61163330078125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 459.8595275878906 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6075, loss_val: nan, pos_over_neg: 329.4732360839844 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6453, loss_val: nan, pos_over_neg: 411.9139709472656 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6449, loss_val: nan, pos_over_neg: 423.8961486816406 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/300000 [2:31:00<75422:19:28, 905.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.6352, loss_val: nan, pos_over_neg: 379.4442443847656 lr: 0.00031623\n",
      "epoch: 10\n",
      "Iter: 0/695, loss_train: 3.6161, loss_val: nan, pos_over_neg: 394.3394775390625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.6422, loss_val: nan, pos_over_neg: 544.0185546875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.6526, loss_val: nan, pos_over_neg: 398.9220275878906 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 602.7649536132812 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6347, loss_val: nan, pos_over_neg: 438.0828857421875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.6007, loss_val: nan, pos_over_neg: 579.3838500976562 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 405.13897705078125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 550.7417602539062 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6351, loss_val: nan, pos_over_neg: 371.0616760253906 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6804, loss_val: nan, pos_over_neg: 374.0187683105469 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6703, loss_val: nan, pos_over_neg: 290.2640380859375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.6317, loss_val: nan, pos_over_neg: 509.2417297363281 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.605, loss_val: nan, pos_over_neg: 361.15692138671875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.6124, loss_val: nan, pos_over_neg: 458.3446960449219 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6374, loss_val: nan, pos_over_neg: 396.1221618652344 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.6319, loss_val: nan, pos_over_neg: 458.62213134765625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 379.1021423339844 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.6224, loss_val: nan, pos_over_neg: 559.2547607421875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 466.29461669921875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 388.36090087890625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 443.6742248535156 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.6817, loss_val: nan, pos_over_neg: 327.5626525878906 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.6295, loss_val: nan, pos_over_neg: 357.7745666503906 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.6757, loss_val: nan, pos_over_neg: 275.7009582519531 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 461.0135192871094 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.6789, loss_val: nan, pos_over_neg: 330.86004638671875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6289, loss_val: nan, pos_over_neg: 834.0045166015625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 496.6179504394531 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6685, loss_val: nan, pos_over_neg: 365.42791748046875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.6264, loss_val: nan, pos_over_neg: 357.5384521484375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6794, loss_val: nan, pos_over_neg: 475.2346496582031 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 622.0568237304688 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 426.41949462890625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6627, loss_val: nan, pos_over_neg: 458.07806396484375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.6604, loss_val: nan, pos_over_neg: 428.75677490234375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 316.10516357421875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6381, loss_val: nan, pos_over_neg: 340.291015625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.659, loss_val: nan, pos_over_neg: 327.525390625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6472, loss_val: nan, pos_over_neg: 559.2863159179688 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.6524, loss_val: nan, pos_over_neg: 356.801513671875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6295, loss_val: nan, pos_over_neg: 586.7252807617188 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 607.3927612304688 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 619.4337768554688 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.6558, loss_val: nan, pos_over_neg: 335.48089599609375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.6225, loss_val: nan, pos_over_neg: 301.40936279296875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6484, loss_val: nan, pos_over_neg: 224.87979125976562 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.6156, loss_val: nan, pos_over_neg: 250.65518188476562 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6285, loss_val: nan, pos_over_neg: 292.31298828125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 434.1797790527344 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.6428, loss_val: nan, pos_over_neg: 415.32550048828125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.6561, loss_val: nan, pos_over_neg: 625.4187622070312 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.6377, loss_val: nan, pos_over_neg: 827.0099487304688 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.6107, loss_val: nan, pos_over_neg: 409.30352783203125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.6388, loss_val: nan, pos_over_neg: 417.7189636230469 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 454.1585998535156 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.638, loss_val: nan, pos_over_neg: 480.0065612792969 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.6182, loss_val: nan, pos_over_neg: 570.1990966796875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.6398, loss_val: nan, pos_over_neg: 291.8591613769531 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.657, loss_val: nan, pos_over_neg: 286.1929626464844 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 388.6080322265625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.6446, loss_val: nan, pos_over_neg: 492.464111328125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.6271, loss_val: nan, pos_over_neg: 392.8722229003906 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 359.5527648925781 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6671, loss_val: nan, pos_over_neg: 389.4424133300781 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 386.7543029785156 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.5713, loss_val: nan, pos_over_neg: 908.4556884765625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.5917, loss_val: nan, pos_over_neg: 583.385009765625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 847.3124389648438 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.672, loss_val: nan, pos_over_neg: 439.583251953125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.6267, loss_val: nan, pos_over_neg: 631.9340209960938 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6224, loss_val: nan, pos_over_neg: 720.8240966796875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 1044.9320068359375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.6201, loss_val: nan, pos_over_neg: 502.3603820800781 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 492.5279541015625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6133, loss_val: nan, pos_over_neg: 385.05670166015625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.618, loss_val: nan, pos_over_neg: 450.12725830078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.6481, loss_val: nan, pos_over_neg: 384.4459228515625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.5714, loss_val: nan, pos_over_neg: 805.1343994140625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.6345, loss_val: nan, pos_over_neg: 329.602294921875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6156, loss_val: nan, pos_over_neg: 380.3621826171875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 489.16424560546875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.6303, loss_val: nan, pos_over_neg: 626.33251953125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.6163, loss_val: nan, pos_over_neg: 437.6621398925781 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.6462, loss_val: nan, pos_over_neg: 473.8533020019531 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.6359, loss_val: nan, pos_over_neg: 771.5997314453125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 449.7274475097656 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.6395, loss_val: nan, pos_over_neg: 303.71466064453125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.6186, loss_val: nan, pos_over_neg: 441.7571105957031 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.6479, loss_val: nan, pos_over_neg: 352.0898742675781 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 451.1886291503906 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.6565, loss_val: nan, pos_over_neg: 312.6030578613281 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 508.0015869140625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6367, loss_val: nan, pos_over_neg: 445.3508605957031 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 432.92578125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.5989, loss_val: nan, pos_over_neg: 366.6697998046875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.6686, loss_val: nan, pos_over_neg: 341.58636474609375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.6608, loss_val: nan, pos_over_neg: 250.0391387939453 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.6411, loss_val: nan, pos_over_neg: 378.3877258300781 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 351.81451416015625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.6833, loss_val: nan, pos_over_neg: 296.2898864746094 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6076, loss_val: nan, pos_over_neg: 339.0053405761719 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6245, loss_val: nan, pos_over_neg: 412.57647705078125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.6802, loss_val: nan, pos_over_neg: 234.84869384765625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.6528, loss_val: nan, pos_over_neg: 300.4534912109375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 576.542724609375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 383.6200866699219 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 516.389404296875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6571, loss_val: nan, pos_over_neg: 658.1832885742188 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6438, loss_val: nan, pos_over_neg: 488.72607421875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 550.0518798828125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 352.4961242675781 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 445.44000244140625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 538.1732788085938 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.6584, loss_val: nan, pos_over_neg: 371.77655029296875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.6405, loss_val: nan, pos_over_neg: 325.3931579589844 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.6117, loss_val: nan, pos_over_neg: 474.54095458984375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6251, loss_val: nan, pos_over_neg: 333.2710876464844 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 441.29443359375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.606, loss_val: nan, pos_over_neg: 496.1435546875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.655, loss_val: nan, pos_over_neg: 383.46978759765625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6251, loss_val: nan, pos_over_neg: 417.0892333984375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 370.04681396484375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 427.2734375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 517.0805053710938 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 423.2590637207031 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6127, loss_val: nan, pos_over_neg: 325.1791687011719 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 403.7543029785156 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 397.6421813964844 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.63, loss_val: nan, pos_over_neg: 363.11712646484375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.6972, loss_val: nan, pos_over_neg: 304.9053039550781 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.6383, loss_val: nan, pos_over_neg: 418.60565185546875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.6336, loss_val: nan, pos_over_neg: 353.6609191894531 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.648, loss_val: nan, pos_over_neg: 283.55316162109375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6168, loss_val: nan, pos_over_neg: 307.9256591796875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6256, loss_val: nan, pos_over_neg: 342.31439208984375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.613, loss_val: nan, pos_over_neg: 281.93310546875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.6538, loss_val: nan, pos_over_neg: 284.61712646484375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.6241, loss_val: nan, pos_over_neg: 380.01934814453125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.6526, loss_val: nan, pos_over_neg: 484.4205627441406 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.5926, loss_val: nan, pos_over_neg: 385.7510681152344 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 320.1190490722656 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 697.3907470703125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 366.8327331542969 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.6703, loss_val: nan, pos_over_neg: 300.9538879394531 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 314.846435546875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.64, loss_val: nan, pos_over_neg: 371.48931884765625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.6584, loss_val: nan, pos_over_neg: 315.1572265625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.6352, loss_val: nan, pos_over_neg: 369.9039001464844 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.6753, loss_val: nan, pos_over_neg: 336.86865234375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.681, loss_val: nan, pos_over_neg: 368.55902099609375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.6071, loss_val: nan, pos_over_neg: 398.56964111328125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.5903, loss_val: nan, pos_over_neg: 612.1698608398438 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6553, loss_val: nan, pos_over_neg: 460.95098876953125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6148, loss_val: nan, pos_over_neg: 491.7865295410156 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 900.3242797851562 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6406, loss_val: nan, pos_over_neg: 477.835205078125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6676, loss_val: nan, pos_over_neg: 391.1042175292969 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.6831, loss_val: nan, pos_over_neg: 244.68191528320312 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.6411, loss_val: nan, pos_over_neg: 304.5118713378906 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6548, loss_val: nan, pos_over_neg: 328.88623046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.6489, loss_val: nan, pos_over_neg: 279.66156005859375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.6121, loss_val: nan, pos_over_neg: 507.6235656738281 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.6243, loss_val: nan, pos_over_neg: 467.6341857910156 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 468.2564392089844 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6332, loss_val: nan, pos_over_neg: 595.4492797851562 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.5931, loss_val: nan, pos_over_neg: 351.96038818359375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.6093, loss_val: nan, pos_over_neg: 300.04791259765625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.6493, loss_val: nan, pos_over_neg: 340.3614807128906 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.5881, loss_val: nan, pos_over_neg: 692.8115844726562 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 471.43524169921875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 407.6997985839844 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6533, loss_val: nan, pos_over_neg: 363.3662109375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.6354, loss_val: nan, pos_over_neg: 611.40771484375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.6598, loss_val: nan, pos_over_neg: 392.8922424316406 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.7044, loss_val: nan, pos_over_neg: 282.8039855957031 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.6578, loss_val: nan, pos_over_neg: 372.8058166503906 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 478.7623596191406 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.5944, loss_val: nan, pos_over_neg: 518.8099365234375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6582, loss_val: nan, pos_over_neg: 275.4678649902344 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6475, loss_val: nan, pos_over_neg: 457.39044189453125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6004, loss_val: nan, pos_over_neg: 477.19403076171875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 319.1083679199219 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.6694, loss_val: nan, pos_over_neg: 453.6473083496094 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.638, loss_val: nan, pos_over_neg: 433.6959533691406 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 648.5618896484375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 330.6978759765625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.5879, loss_val: nan, pos_over_neg: 550.76611328125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.6091, loss_val: nan, pos_over_neg: 1035.3394775390625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 882.0657958984375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.6354, loss_val: nan, pos_over_neg: 393.5260009765625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 247.71278381347656 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 502.9961242675781 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.6533, loss_val: nan, pos_over_neg: 564.126953125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 568.8225708007812 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 457.4176025390625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.665, loss_val: nan, pos_over_neg: 377.27227783203125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.6286, loss_val: nan, pos_over_neg: 574.1703491210938 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 808.789306640625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 661.1363525390625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6023, loss_val: nan, pos_over_neg: 362.7227478027344 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.5736, loss_val: nan, pos_over_neg: 601.1915893554688 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.6677, loss_val: nan, pos_over_neg: 236.00852966308594 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 427.3326721191406 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.6241, loss_val: nan, pos_over_neg: 726.7398681640625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.6361, loss_val: nan, pos_over_neg: 492.26995849609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.6576, loss_val: nan, pos_over_neg: 332.0123596191406 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.6615, loss_val: nan, pos_over_neg: 448.0297546386719 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 396.947021484375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.6057, loss_val: nan, pos_over_neg: 780.3631591796875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 632.8438720703125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6174, loss_val: nan, pos_over_neg: 481.079345703125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6418, loss_val: nan, pos_over_neg: 575.6516723632812 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.5796, loss_val: nan, pos_over_neg: 514.5435180664062 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 316.4866943359375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.6552, loss_val: nan, pos_over_neg: 256.4183044433594 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.6235, loss_val: nan, pos_over_neg: 521.1805419921875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.6163, loss_val: nan, pos_over_neg: 349.6490478515625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6358, loss_val: nan, pos_over_neg: 324.8075256347656 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.6197, loss_val: nan, pos_over_neg: 357.7914123535156 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.643, loss_val: nan, pos_over_neg: 468.6143798828125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.692, loss_val: nan, pos_over_neg: 271.9728088378906 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 450.85919189453125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.629, loss_val: nan, pos_over_neg: 332.57733154296875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6826, loss_val: nan, pos_over_neg: 270.3084411621094 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 520.3299560546875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 356.5215759277344 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6783, loss_val: nan, pos_over_neg: 436.3915100097656 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 357.58050537109375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 330.8339538574219 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6639, loss_val: nan, pos_over_neg: 261.4259948730469 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 370.3552551269531 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.619, loss_val: nan, pos_over_neg: 457.2708740234375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 380.6474304199219 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.6607, loss_val: nan, pos_over_neg: 326.6305847167969 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6637, loss_val: nan, pos_over_neg: 431.7573547363281 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6645, loss_val: nan, pos_over_neg: 456.7255859375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.6703, loss_val: nan, pos_over_neg: 356.02178955078125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 530.4976806640625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.619, loss_val: nan, pos_over_neg: 390.890625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 504.9815979003906 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.6488, loss_val: nan, pos_over_neg: 304.3840637207031 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.6834, loss_val: nan, pos_over_neg: 210.94345092773438 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 534.979736328125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 398.2149658203125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 416.5487060546875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 370.6156311035156 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.6447, loss_val: nan, pos_over_neg: 392.47686767578125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6453, loss_val: nan, pos_over_neg: 532.60791015625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6219, loss_val: nan, pos_over_neg: 502.2403259277344 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 477.98468017578125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6369, loss_val: nan, pos_over_neg: 345.6196594238281 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 325.3306884765625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6269, loss_val: nan, pos_over_neg: 294.39569091796875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 489.6686706542969 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6107, loss_val: nan, pos_over_neg: 404.97186279296875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 336.02984619140625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.6606, loss_val: nan, pos_over_neg: 264.64013671875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 493.2714538574219 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 622.454833984375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 495.185546875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.5972, loss_val: nan, pos_over_neg: 455.3099060058594 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6552, loss_val: nan, pos_over_neg: 277.2411804199219 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6382, loss_val: nan, pos_over_neg: 297.4364318847656 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.6569, loss_val: nan, pos_over_neg: 277.41485595703125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 425.33978271484375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.6129, loss_val: nan, pos_over_neg: 330.9671936035156 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 511.8553771972656 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.6535, loss_val: nan, pos_over_neg: 325.00750732421875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.659, loss_val: nan, pos_over_neg: 477.6094055175781 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 508.8175048828125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.6275, loss_val: nan, pos_over_neg: 429.7145080566406 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 776.2122192382812 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.6501, loss_val: nan, pos_over_neg: 334.34515380859375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 478.0565185546875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 382.62835693359375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 508.9625549316406 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6504, loss_val: nan, pos_over_neg: 270.1813659667969 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.6477, loss_val: nan, pos_over_neg: 318.3974304199219 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5972, loss_val: nan, pos_over_neg: 341.939697265625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6382, loss_val: nan, pos_over_neg: 518.5997924804688 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6517, loss_val: nan, pos_over_neg: 335.95574951171875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.5681, loss_val: nan, pos_over_neg: 462.0813903808594 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 629.1759033203125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 455.413818359375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 656.2345581054688 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6224, loss_val: nan, pos_over_neg: 407.6382751464844 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.6665, loss_val: nan, pos_over_neg: 274.9363708496094 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.6306, loss_val: nan, pos_over_neg: 282.03778076171875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.6282, loss_val: nan, pos_over_neg: 339.5101013183594 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6677, loss_val: nan, pos_over_neg: 294.8684387207031 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 362.2875061035156 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 449.2434997558594 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 605.0860595703125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6372, loss_val: nan, pos_over_neg: 474.11749267578125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6225, loss_val: nan, pos_over_neg: 431.5115661621094 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 667.1910400390625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6406, loss_val: nan, pos_over_neg: 328.8730163574219 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.6413, loss_val: nan, pos_over_neg: 242.19752502441406 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.6204, loss_val: nan, pos_over_neg: 432.9100646972656 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.61, loss_val: nan, pos_over_neg: 428.27716064453125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6318, loss_val: nan, pos_over_neg: 343.8044128417969 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.612, loss_val: nan, pos_over_neg: 483.1535949707031 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.6273, loss_val: nan, pos_over_neg: 495.4921569824219 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 518.0388793945312 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 1079.28076171875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.5795, loss_val: nan, pos_over_neg: 649.6580200195312 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.6298, loss_val: nan, pos_over_neg: 722.2401733398438 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.5931, loss_val: nan, pos_over_neg: 572.9849243164062 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 347.1817626953125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 496.2333068847656 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.663, loss_val: nan, pos_over_neg: 232.0161590576172 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6624, loss_val: nan, pos_over_neg: 254.3539276123047 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.6451, loss_val: nan, pos_over_neg: 236.34341430664062 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.6739, loss_val: nan, pos_over_neg: 329.6282958984375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 451.3577880859375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 682.5830688476562 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6416, loss_val: nan, pos_over_neg: 658.4031372070312 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 503.77679443359375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6328, loss_val: nan, pos_over_neg: 513.7279663085938 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 481.47698974609375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6454, loss_val: nan, pos_over_neg: 376.45220947265625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 396.5068664550781 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.6611, loss_val: nan, pos_over_neg: 257.797607421875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.6233, loss_val: nan, pos_over_neg: 269.2038879394531 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.5958, loss_val: nan, pos_over_neg: 391.83160400390625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.5951, loss_val: nan, pos_over_neg: 407.4014892578125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 234.0482177734375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6133, loss_val: nan, pos_over_neg: 484.1816101074219 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.6028, loss_val: nan, pos_over_neg: 679.2933959960938 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.6669, loss_val: nan, pos_over_neg: 341.447265625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.6684, loss_val: nan, pos_over_neg: 328.72869873046875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6628, loss_val: nan, pos_over_neg: 455.9583435058594 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 465.9096984863281 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.6334, loss_val: nan, pos_over_neg: 423.50927734375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.6026, loss_val: nan, pos_over_neg: 446.61029052734375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6211, loss_val: nan, pos_over_neg: 457.0479736328125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 437.7900390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.6579, loss_val: nan, pos_over_neg: 208.3425750732422 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.5875, loss_val: nan, pos_over_neg: 487.5622253417969 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6864, loss_val: nan, pos_over_neg: 353.7871398925781 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 252.68663024902344 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.6463, loss_val: nan, pos_over_neg: 497.0876159667969 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 522.8485717773438 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.6704, loss_val: nan, pos_over_neg: 267.26953125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.6494, loss_val: nan, pos_over_neg: 334.552490234375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 400.5359191894531 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6267, loss_val: nan, pos_over_neg: 386.6592102050781 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 503.00616455078125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.6196, loss_val: nan, pos_over_neg: 354.9365234375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 626.4033203125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.612, loss_val: nan, pos_over_neg: 635.150634765625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.65, loss_val: nan, pos_over_neg: 336.4869689941406 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6096, loss_val: nan, pos_over_neg: 465.2049255371094 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.5825, loss_val: nan, pos_over_neg: 557.4806518554688 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 356.9885559082031 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6211, loss_val: nan, pos_over_neg: 397.3231506347656 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6338, loss_val: nan, pos_over_neg: 289.0537414550781 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6315, loss_val: nan, pos_over_neg: 441.162109375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 692.9810180664062 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.6328, loss_val: nan, pos_over_neg: 347.75360107421875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.6528, loss_val: nan, pos_over_neg: 338.0546875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.6172, loss_val: nan, pos_over_neg: 756.066162109375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6242, loss_val: nan, pos_over_neg: 395.2113952636719 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.6123, loss_val: nan, pos_over_neg: 847.4677124023438 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6424, loss_val: nan, pos_over_neg: 457.3326110839844 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 405.2363586425781 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.6381, loss_val: nan, pos_over_neg: 389.5283508300781 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6125, loss_val: nan, pos_over_neg: 313.9712829589844 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.644, loss_val: nan, pos_over_neg: 314.27984619140625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.628, loss_val: nan, pos_over_neg: 366.4375915527344 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 546.5538940429688 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.6296, loss_val: nan, pos_over_neg: 349.0037536621094 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.6165, loss_val: nan, pos_over_neg: 369.74114990234375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 417.6861877441406 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6523, loss_val: nan, pos_over_neg: 458.9298095703125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.6226, loss_val: nan, pos_over_neg: 423.2975158691406 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 392.03582763671875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6415, loss_val: nan, pos_over_neg: 364.8470153808594 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 319.9512023925781 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.6577, loss_val: nan, pos_over_neg: 356.18707275390625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 361.0321960449219 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 347.4685974121094 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6319, loss_val: nan, pos_over_neg: 537.9043579101562 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.6337, loss_val: nan, pos_over_neg: 431.2914123535156 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 303.841064453125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.6389, loss_val: nan, pos_over_neg: 527.2019653320312 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.6427, loss_val: nan, pos_over_neg: 376.2926025390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6448, loss_val: nan, pos_over_neg: 381.7563781738281 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 478.0693054199219 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 497.7275390625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6163, loss_val: nan, pos_over_neg: 399.3882141113281 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 502.5700378417969 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 341.3163146972656 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6344, loss_val: nan, pos_over_neg: 273.91778564453125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 395.5928039550781 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 335.6182861328125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 378.4150695800781 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.6467, loss_val: nan, pos_over_neg: 307.16021728515625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 399.7400207519531 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 427.55572509765625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 325.5654296875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 376.2046813964844 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.602, loss_val: nan, pos_over_neg: 703.616943359375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.616, loss_val: nan, pos_over_neg: 512.941162109375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.6108, loss_val: nan, pos_over_neg: 524.2594604492188 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 278.7928466796875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.5958, loss_val: nan, pos_over_neg: 350.90966796875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 460.94384765625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6484, loss_val: nan, pos_over_neg: 304.5453796386719 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.631, loss_val: nan, pos_over_neg: 389.21417236328125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6555, loss_val: nan, pos_over_neg: 239.3456268310547 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 464.1610412597656 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 674.9600830078125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6066, loss_val: nan, pos_over_neg: 585.5845947265625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 444.0315856933594 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 476.1929931640625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.6482, loss_val: nan, pos_over_neg: 426.1624755859375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 633.1265258789062 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 294.2459411621094 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 362.00457763671875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 410.09027099609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.6459, loss_val: nan, pos_over_neg: 313.3400573730469 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 270.7104187011719 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.6329, loss_val: nan, pos_over_neg: 281.3223876953125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.6462, loss_val: nan, pos_over_neg: 365.17376708984375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.6179, loss_val: nan, pos_over_neg: 366.87066650390625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.6361, loss_val: nan, pos_over_neg: 399.02252197265625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6457, loss_val: nan, pos_over_neg: 473.06390380859375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.6197, loss_val: nan, pos_over_neg: 399.8276062011719 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 473.1526794433594 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.6565, loss_val: nan, pos_over_neg: 422.0188903808594 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 420.1356506347656 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 307.992431640625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 368.6600646972656 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.6485, loss_val: nan, pos_over_neg: 280.4000244140625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.6649, loss_val: nan, pos_over_neg: 291.8916015625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.5896, loss_val: nan, pos_over_neg: 314.012939453125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 338.48028564453125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.61, loss_val: nan, pos_over_neg: 320.7639465332031 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 287.4058837890625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 553.2118530273438 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.6573, loss_val: nan, pos_over_neg: 311.5973205566406 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.6127, loss_val: nan, pos_over_neg: 495.4242858886719 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 431.91259765625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 490.4425048828125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6364, loss_val: nan, pos_over_neg: 476.85882568359375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 551.7211303710938 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.6621, loss_val: nan, pos_over_neg: 284.3989562988281 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.6446, loss_val: nan, pos_over_neg: 264.316162109375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.6434, loss_val: nan, pos_over_neg: 303.0745544433594 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6272, loss_val: nan, pos_over_neg: 226.45223999023438 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.6253, loss_val: nan, pos_over_neg: 310.7078857421875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 365.17669677734375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.6264, loss_val: nan, pos_over_neg: 295.27496337890625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.6091, loss_val: nan, pos_over_neg: 445.61444091796875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.6479, loss_val: nan, pos_over_neg: 329.7841491699219 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.5873, loss_val: nan, pos_over_neg: 538.2682495117188 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.661, loss_val: nan, pos_over_neg: 346.0127868652344 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.6116, loss_val: nan, pos_over_neg: 446.1188659667969 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.6338, loss_val: nan, pos_over_neg: 349.5477600097656 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.5713, loss_val: nan, pos_over_neg: 590.25390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6577, loss_val: nan, pos_over_neg: 260.1839904785156 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 484.9570007324219 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.6085, loss_val: nan, pos_over_neg: 359.19586181640625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.6415, loss_val: nan, pos_over_neg: 256.0934143066406 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 365.6155700683594 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 332.08648681640625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.6079, loss_val: nan, pos_over_neg: 332.7614440917969 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 366.15704345703125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 660.8959350585938 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.634, loss_val: nan, pos_over_neg: 413.1310119628906 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.6587, loss_val: nan, pos_over_neg: 275.0027770996094 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6688, loss_val: nan, pos_over_neg: 282.5294189453125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6275, loss_val: nan, pos_over_neg: 460.7422790527344 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.633, loss_val: nan, pos_over_neg: 293.6009521484375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6722, loss_val: nan, pos_over_neg: 242.14959716796875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6533, loss_val: nan, pos_over_neg: 305.05572509765625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.575, loss_val: nan, pos_over_neg: 706.9375610351562 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.5988, loss_val: nan, pos_over_neg: 528.414306640625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.5977, loss_val: nan, pos_over_neg: 715.0208740234375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.6541, loss_val: nan, pos_over_neg: 703.1876831054688 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 462.14727783203125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 574.9161987304688 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.6161, loss_val: nan, pos_over_neg: 289.31634521484375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.6183, loss_val: nan, pos_over_neg: 335.0904541015625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.6411, loss_val: nan, pos_over_neg: 231.81134033203125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.6177, loss_val: nan, pos_over_neg: 345.7982177734375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 283.0528564453125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.6096, loss_val: nan, pos_over_neg: 373.8919677734375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 512.39990234375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.6408, loss_val: nan, pos_over_neg: 569.0714111328125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.6421, loss_val: nan, pos_over_neg: 594.664306640625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6293, loss_val: nan, pos_over_neg: 327.5639953613281 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 585.7814331054688 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 696.9483642578125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.6389, loss_val: nan, pos_over_neg: 553.6627807617188 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6142, loss_val: nan, pos_over_neg: 433.1866760253906 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 505.9364318847656 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.5796, loss_val: nan, pos_over_neg: 618.5814819335938 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6226, loss_val: nan, pos_over_neg: 373.40142822265625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.6612, loss_val: nan, pos_over_neg: 253.8006591796875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6578, loss_val: nan, pos_over_neg: 294.90692138671875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 472.52850341796875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.5977, loss_val: nan, pos_over_neg: 368.174072265625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 472.136474609375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 384.3312683105469 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6282, loss_val: nan, pos_over_neg: 452.5760498046875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 469.156982421875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6343, loss_val: nan, pos_over_neg: 485.452392578125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.6777, loss_val: nan, pos_over_neg: 306.47906494140625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 401.7921142578125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6201, loss_val: nan, pos_over_neg: 402.67938232421875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.6272, loss_val: nan, pos_over_neg: 318.72467041015625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 328.2775573730469 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 332.88177490234375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 460.7452087402344 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6191, loss_val: nan, pos_over_neg: 357.31976318359375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6414, loss_val: nan, pos_over_neg: 347.614501953125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 533.641357421875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 408.33929443359375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 418.5001220703125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6371, loss_val: nan, pos_over_neg: 333.2891845703125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 554.9360961914062 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 531.3099975585938 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.676, loss_val: nan, pos_over_neg: 230.3404083251953 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.589, loss_val: nan, pos_over_neg: 392.1595764160156 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 381.93804931640625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6449, loss_val: nan, pos_over_neg: 357.2242126464844 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.6746, loss_val: nan, pos_over_neg: 272.6781005859375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6051, loss_val: nan, pos_over_neg: 389.000244140625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.6512, loss_val: nan, pos_over_neg: 271.3194580078125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6128, loss_val: nan, pos_over_neg: 406.1803283691406 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 452.11138916015625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.578, loss_val: nan, pos_over_neg: 604.5736694335938 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6406, loss_val: nan, pos_over_neg: 481.12457275390625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.6437, loss_val: nan, pos_over_neg: 369.4693603515625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 494.95428466796875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.6332, loss_val: nan, pos_over_neg: 368.9172668457031 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.616, loss_val: nan, pos_over_neg: 404.5861511230469 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6376, loss_val: nan, pos_over_neg: 364.002685546875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 266.1360778808594 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.631, loss_val: nan, pos_over_neg: 314.77008056640625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6174, loss_val: nan, pos_over_neg: 286.6221923828125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.6222, loss_val: nan, pos_over_neg: 670.9038696289062 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.6298, loss_val: nan, pos_over_neg: 395.05206298828125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 325.1177062988281 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.5865, loss_val: nan, pos_over_neg: 562.0990600585938 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6255, loss_val: nan, pos_over_neg: 372.2131042480469 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 316.53369140625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.6218, loss_val: nan, pos_over_neg: 368.21063232421875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 390.6315002441406 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6249, loss_val: nan, pos_over_neg: 417.8595275878906 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 703.1989135742188 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.6049, loss_val: nan, pos_over_neg: 434.78692626953125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 275.54949951171875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.6346, loss_val: nan, pos_over_neg: 250.55909729003906 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.6249, loss_val: nan, pos_over_neg: 321.9184875488281 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.6781, loss_val: nan, pos_over_neg: 352.4934997558594 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.6348, loss_val: nan, pos_over_neg: 336.4700622558594 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.5826, loss_val: nan, pos_over_neg: 636.2894287109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.6172, loss_val: nan, pos_over_neg: 546.1076049804688 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.628, loss_val: nan, pos_over_neg: 464.12969970703125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 691.966796875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 441.5411376953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 413.0226745605469 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6299, loss_val: nan, pos_over_neg: 429.7962951660156 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 339.3707275390625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.6107, loss_val: nan, pos_over_neg: 527.011474609375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6107, loss_val: nan, pos_over_neg: 528.5256958007812 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 481.3969421386719 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.6293, loss_val: nan, pos_over_neg: 350.10693359375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6172, loss_val: nan, pos_over_neg: 511.7103271484375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 929.361083984375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.6756, loss_val: nan, pos_over_neg: 347.4827880859375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6639, loss_val: nan, pos_over_neg: 257.2463073730469 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 396.42193603515625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.592, loss_val: nan, pos_over_neg: 458.3487243652344 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6096, loss_val: nan, pos_over_neg: 451.170654296875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.6493, loss_val: nan, pos_over_neg: 290.6208190917969 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 268.0955505371094 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.6203, loss_val: nan, pos_over_neg: 290.3575439453125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 434.1471862792969 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.6471, loss_val: nan, pos_over_neg: 416.8944091796875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 525.7019653320312 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.6544, loss_val: nan, pos_over_neg: 339.7501220703125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 438.33984375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6204, loss_val: nan, pos_over_neg: 463.2706604003906 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 560.8510131835938 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 1672.9603271484375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.6783, loss_val: nan, pos_over_neg: 278.3394775390625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.6485, loss_val: nan, pos_over_neg: 276.16754150390625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 385.0888977050781 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.6709, loss_val: nan, pos_over_neg: 238.35348510742188 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.6271, loss_val: nan, pos_over_neg: 289.7542419433594 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.6463, loss_val: nan, pos_over_neg: 282.92706298828125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 506.1907958984375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6682, loss_val: nan, pos_over_neg: 535.0165405273438 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 621.1150512695312 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.6245, loss_val: nan, pos_over_neg: 414.1647033691406 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.6282, loss_val: nan, pos_over_neg: 460.69012451171875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 491.80364990234375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.67, loss_val: nan, pos_over_neg: 456.4375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 413.69281005859375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.6188, loss_val: nan, pos_over_neg: 471.3915100097656 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.6118, loss_val: nan, pos_over_neg: 357.8338317871094 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 631.2128295898438 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.6355, loss_val: nan, pos_over_neg: 413.42626953125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.6575, loss_val: nan, pos_over_neg: 459.0141296386719 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.616, loss_val: nan, pos_over_neg: 389.3721008300781 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.609, loss_val: nan, pos_over_neg: 749.0947265625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.6032, loss_val: nan, pos_over_neg: 574.7789306640625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.6326, loss_val: nan, pos_over_neg: 674.92626953125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6426, loss_val: nan, pos_over_neg: 348.1758117675781 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 788.0985717773438 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.6291, loss_val: nan, pos_over_neg: 437.90765380859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.6121, loss_val: nan, pos_over_neg: 456.39642333984375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 471.8758850097656 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.6442, loss_val: nan, pos_over_neg: 421.8842468261719 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 583.2064819335938 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 367.9217529296875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.6665, loss_val: nan, pos_over_neg: 280.69366455078125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.6735, loss_val: nan, pos_over_neg: 221.8337860107422 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 287.22515869140625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.587, loss_val: nan, pos_over_neg: 524.5068359375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6385, loss_val: nan, pos_over_neg: 397.3207092285156 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.6049, loss_val: nan, pos_over_neg: 463.48822021484375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.6322, loss_val: nan, pos_over_neg: 486.4924621582031 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.6551, loss_val: nan, pos_over_neg: 268.1096496582031 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6385, loss_val: nan, pos_over_neg: 388.934326171875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.6133, loss_val: nan, pos_over_neg: 370.43463134765625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6181, loss_val: nan, pos_over_neg: 303.20458984375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.6543, loss_val: nan, pos_over_neg: 251.6327667236328 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 382.2634582519531 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.5835, loss_val: nan, pos_over_neg: 644.1526489257812 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 403.7646484375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.6407, loss_val: nan, pos_over_neg: 344.4919738769531 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6156, loss_val: nan, pos_over_neg: 396.9940490722656 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.6593, loss_val: nan, pos_over_neg: 305.43328857421875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 684.85693359375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 611.6549072265625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 587.6878051757812 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 360.6261901855469 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.5724, loss_val: nan, pos_over_neg: 735.7464599609375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.6675, loss_val: nan, pos_over_neg: 175.7740478515625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.63, loss_val: nan, pos_over_neg: 268.23114013671875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.5721, loss_val: nan, pos_over_neg: 538.8340454101562 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.6301, loss_val: nan, pos_over_neg: 409.8913269042969 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.605, loss_val: nan, pos_over_neg: 321.5182189941406 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.5714, loss_val: nan, pos_over_neg: 601.0518188476562 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 385.9545593261719 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.6275, loss_val: nan, pos_over_neg: 392.396240234375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.662, loss_val: nan, pos_over_neg: 406.06988525390625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.6697, loss_val: nan, pos_over_neg: 289.11456298828125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.5947, loss_val: nan, pos_over_neg: 498.1302490234375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6198, loss_val: nan, pos_over_neg: 461.61822509765625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 583.7978515625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.6298, loss_val: nan, pos_over_neg: 312.6526184082031 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6646, loss_val: nan, pos_over_neg: 284.49224853515625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 472.12237548828125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 348.4529724121094 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 304.7929382324219 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.6289, loss_val: nan, pos_over_neg: 342.2262878417969 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 349.7190246582031 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 359.1717529296875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.613, loss_val: nan, pos_over_neg: 606.2433471679688 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.5901, loss_val: nan, pos_over_neg: 464.6885681152344 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 346.6685485839844 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.6364, loss_val: nan, pos_over_neg: 327.2276916503906 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.627, loss_val: nan, pos_over_neg: 506.8390197753906 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.6178, loss_val: nan, pos_over_neg: 378.44586181640625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 483.3709411621094 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 322.497314453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 639.3399658203125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.6323, loss_val: nan, pos_over_neg: 348.1750183105469 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.5764, loss_val: nan, pos_over_neg: 493.3028259277344 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 452.61328125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 351.3745422363281 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 506.23931884765625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.6065, loss_val: nan, pos_over_neg: 418.7507019042969 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 294.733154296875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.639, loss_val: nan, pos_over_neg: 349.97308349609375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6573, loss_val: nan, pos_over_neg: 347.3200988769531 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 507.0740966796875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.6299, loss_val: nan, pos_over_neg: 537.3367919921875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.6467, loss_val: nan, pos_over_neg: 845.7100830078125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.6282, loss_val: nan, pos_over_neg: 421.64312744140625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 589.763916015625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.6505, loss_val: nan, pos_over_neg: 250.5803680419922 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6315, loss_val: nan, pos_over_neg: 311.5141906738281 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.6602, loss_val: nan, pos_over_neg: 335.06573486328125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.6336, loss_val: nan, pos_over_neg: 351.19183349609375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6313, loss_val: nan, pos_over_neg: 302.64007568359375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6656, loss_val: nan, pos_over_neg: 364.4021301269531 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6116, loss_val: nan, pos_over_neg: 452.0185241699219 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/300000 [2:46:12<75598:39:15, 907.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.641, loss_val: nan, pos_over_neg: 464.3194274902344 lr: 0.00031623\n",
      "epoch: 11\n",
      "Iter: 0/695, loss_train: 3.653, loss_val: nan, pos_over_neg: 315.9017639160156 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 615.8638916015625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 432.37353515625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.6089, loss_val: nan, pos_over_neg: 394.6722106933594 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 366.3486022949219 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.5923, loss_val: nan, pos_over_neg: 450.6864929199219 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 406.7554626464844 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.6431, loss_val: nan, pos_over_neg: 491.5956115722656 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 413.7459716796875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 397.35565185546875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6631, loss_val: nan, pos_over_neg: 319.6192321777344 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 373.2179260253906 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 507.5086975097656 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.6225, loss_val: nan, pos_over_neg: 596.3215942382812 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 438.00738525390625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.6152, loss_val: nan, pos_over_neg: 458.7691345214844 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.66, loss_val: nan, pos_over_neg: 276.89141845703125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.6478, loss_val: nan, pos_over_neg: 324.54949951171875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.61, loss_val: nan, pos_over_neg: 442.6697998046875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 471.0374450683594 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 308.25189208984375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 431.0335388183594 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.6345, loss_val: nan, pos_over_neg: 429.65850830078125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 468.0513000488281 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 310.85614013671875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 518.3623046875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 393.5338439941406 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 425.1807556152344 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 749.1045532226562 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.677, loss_val: nan, pos_over_neg: 342.0096740722656 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6399, loss_val: nan, pos_over_neg: 351.2919921875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.6121, loss_val: nan, pos_over_neg: 312.8758850097656 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 391.1129455566406 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 455.5637512207031 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.5809, loss_val: nan, pos_over_neg: 465.0533752441406 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.5571, loss_val: nan, pos_over_neg: 461.5554504394531 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6332, loss_val: nan, pos_over_neg: 328.44512939453125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 375.9522399902344 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 417.77398681640625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 557.0437622070312 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6317, loss_val: nan, pos_over_neg: 260.3302307128906 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.5713, loss_val: nan, pos_over_neg: 358.7158203125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.6561, loss_val: nan, pos_over_neg: 292.1582946777344 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.6674, loss_val: nan, pos_over_neg: 413.8344421386719 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.6207, loss_val: nan, pos_over_neg: 419.45599365234375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6245, loss_val: nan, pos_over_neg: 460.0863952636719 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.6074, loss_val: nan, pos_over_neg: 592.8899536132812 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 449.8478088378906 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.6196, loss_val: nan, pos_over_neg: 344.0677490234375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 368.6990661621094 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.6539, loss_val: nan, pos_over_neg: 337.57623291015625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.6139, loss_val: nan, pos_over_neg: 540.0941772460938 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.6273, loss_val: nan, pos_over_neg: 372.83782958984375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 320.6297912597656 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6165, loss_val: nan, pos_over_neg: 400.45721435546875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 684.1182250976562 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 593.2327880859375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.6139, loss_val: nan, pos_over_neg: 431.2439270019531 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 353.91912841796875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 233.8463134765625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 412.15289306640625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.6348, loss_val: nan, pos_over_neg: 428.3888854980469 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.592, loss_val: nan, pos_over_neg: 598.32177734375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 536.0634765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 383.3157653808594 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 501.77374267578125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.6434, loss_val: nan, pos_over_neg: 339.95916748046875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6456, loss_val: nan, pos_over_neg: 313.0758056640625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 414.7695007324219 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 392.5078430175781 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6186, loss_val: nan, pos_over_neg: 565.1792602539062 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 338.585205078125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.647, loss_val: nan, pos_over_neg: 471.2223205566406 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 451.1413269042969 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 266.6226806640625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.5971, loss_val: nan, pos_over_neg: 332.3766784667969 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.6678, loss_val: nan, pos_over_neg: 241.87095642089844 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 375.95263671875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.604, loss_val: nan, pos_over_neg: 406.9922180175781 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 399.5852355957031 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 445.1415100097656 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.6331, loss_val: nan, pos_over_neg: 437.3438415527344 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.6913, loss_val: nan, pos_over_neg: 309.4940490722656 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 411.035888671875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.6163, loss_val: nan, pos_over_neg: 471.00225830078125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6519, loss_val: nan, pos_over_neg: 437.50494384765625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.6364, loss_val: nan, pos_over_neg: 399.7456359863281 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.6085, loss_val: nan, pos_over_neg: 471.7137756347656 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.5813, loss_val: nan, pos_over_neg: 449.4811706542969 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.5827, loss_val: nan, pos_over_neg: 403.74072265625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.5971, loss_val: nan, pos_over_neg: 523.4716796875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 444.7235107421875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 307.4024658203125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.6256, loss_val: nan, pos_over_neg: 341.395751953125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6343, loss_val: nan, pos_over_neg: 276.7563781738281 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.6313, loss_val: nan, pos_over_neg: 244.39866638183594 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.643, loss_val: nan, pos_over_neg: 450.2790832519531 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.6482, loss_val: nan, pos_over_neg: 438.2541198730469 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.6118, loss_val: nan, pos_over_neg: 679.4619140625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.6007, loss_val: nan, pos_over_neg: 481.1590881347656 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 445.0640563964844 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6532, loss_val: nan, pos_over_neg: 722.9580078125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.6463, loss_val: nan, pos_over_neg: 423.396728515625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.6323, loss_val: nan, pos_over_neg: 363.1026916503906 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 368.11480712890625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.6443, loss_val: nan, pos_over_neg: 307.8530578613281 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.6346, loss_val: nan, pos_over_neg: 399.0863342285156 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6435, loss_val: nan, pos_over_neg: 399.59820556640625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6261, loss_val: nan, pos_over_neg: 374.3058776855469 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.5993, loss_val: nan, pos_over_neg: 566.33447265625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6384, loss_val: nan, pos_over_neg: 453.1216735839844 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 460.3155212402344 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 342.3604736328125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.6419, loss_val: nan, pos_over_neg: 458.36669921875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.5826, loss_val: nan, pos_over_neg: 422.5371398925781 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.6043, loss_val: nan, pos_over_neg: 354.59710693359375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 495.0135803222656 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 500.0246887207031 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 645.5543212890625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.6602, loss_val: nan, pos_over_neg: 335.78216552734375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6306, loss_val: nan, pos_over_neg: 339.87799072265625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 311.8636779785156 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6336, loss_val: nan, pos_over_neg: 294.4083251953125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6497, loss_val: nan, pos_over_neg: 321.507568359375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 503.2510681152344 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 353.39251708984375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.6289, loss_val: nan, pos_over_neg: 384.67852783203125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 527.4683227539062 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 566.8131713867188 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.6218, loss_val: nan, pos_over_neg: 480.16943359375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.6555, loss_val: nan, pos_over_neg: 391.9358215332031 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.617, loss_val: nan, pos_over_neg: 447.4820251464844 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.6311, loss_val: nan, pos_over_neg: 495.52117919921875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 672.4586791992188 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6204, loss_val: nan, pos_over_neg: 553.8030395507812 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 322.4643249511719 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 292.1299133300781 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 582.1356811523438 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 735.43408203125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.5791, loss_val: nan, pos_over_neg: 1023.4165649414062 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 790.7259521484375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 517.9826049804688 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 501.0635681152344 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.6113, loss_val: nan, pos_over_neg: 715.4049072265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.6331, loss_val: nan, pos_over_neg: 349.75146484375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.5915, loss_val: nan, pos_over_neg: 426.3246765136719 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 424.3600158691406 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.6599, loss_val: nan, pos_over_neg: 213.85093688964844 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 355.5853271484375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 535.9624633789062 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 573.475341796875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 456.6789245605469 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 331.80517578125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 365.4049987792969 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.6379, loss_val: nan, pos_over_neg: 439.8307800292969 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6211, loss_val: nan, pos_over_neg: 291.7917175292969 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 345.5900573730469 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.6002, loss_val: nan, pos_over_neg: 408.86309814453125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.6311, loss_val: nan, pos_over_neg: 485.791259765625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.5744, loss_val: nan, pos_over_neg: 710.321533203125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 525.5369262695312 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 487.9289245605469 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 548.2124633789062 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.6113, loss_val: nan, pos_over_neg: 411.0894775390625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6277, loss_val: nan, pos_over_neg: 375.3697204589844 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 320.07940673828125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.6218, loss_val: nan, pos_over_neg: 253.6355743408203 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.6369, loss_val: nan, pos_over_neg: 248.28614807128906 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 393.76336669921875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 461.1632080078125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.5773, loss_val: nan, pos_over_neg: 546.4588012695312 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6293, loss_val: nan, pos_over_neg: 443.32904052734375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.6304, loss_val: nan, pos_over_neg: 429.7939453125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 683.0361328125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 502.5862121582031 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 442.200439453125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 586.66455078125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.6048, loss_val: nan, pos_over_neg: 367.7908630371094 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6318, loss_val: nan, pos_over_neg: 288.96282958984375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 515.6618041992188 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6181, loss_val: nan, pos_over_neg: 322.3244323730469 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.6172, loss_val: nan, pos_over_neg: 317.89453125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.6157, loss_val: nan, pos_over_neg: 330.67840576171875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.683, loss_val: nan, pos_over_neg: 430.2460021972656 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6374, loss_val: nan, pos_over_neg: 319.4306945800781 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.6235, loss_val: nan, pos_over_neg: 399.73651123046875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.5995, loss_val: nan, pos_over_neg: 623.7296142578125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 507.9557189941406 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 496.4967346191406 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.5976, loss_val: nan, pos_over_neg: 736.268798828125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.5834, loss_val: nan, pos_over_neg: 673.247802734375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.5732, loss_val: nan, pos_over_neg: 1067.7493896484375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 446.1670837402344 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.6359, loss_val: nan, pos_over_neg: 338.9834899902344 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 453.3304748535156 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 710.130615234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 509.5486145019531 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.5704, loss_val: nan, pos_over_neg: 556.4529418945312 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 425.4496765136719 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6588, loss_val: nan, pos_over_neg: 366.3287048339844 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 365.1723327636719 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 488.8938293457031 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 554.5030517578125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.607, loss_val: nan, pos_over_neg: 370.7328796386719 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.6885, loss_val: nan, pos_over_neg: 232.37399291992188 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 293.5331115722656 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.6669, loss_val: nan, pos_over_neg: 317.23193359375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 529.1528930664062 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 307.9436950683594 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 614.1273803710938 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6467, loss_val: nan, pos_over_neg: 365.2524108886719 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 316.0389404296875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 354.247314453125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.6213, loss_val: nan, pos_over_neg: 277.17767333984375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 264.2569580078125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 292.9102783203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.5957, loss_val: nan, pos_over_neg: 383.76104736328125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 267.22222900390625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.6057, loss_val: nan, pos_over_neg: 392.96356201171875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 669.6134643554688 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.6458, loss_val: nan, pos_over_neg: 297.0715026855469 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.6105, loss_val: nan, pos_over_neg: 465.60638427734375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.6314, loss_val: nan, pos_over_neg: 322.31744384765625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6232, loss_val: nan, pos_over_neg: 310.2970886230469 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 345.475341796875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6499, loss_val: nan, pos_over_neg: 300.9217529296875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6249, loss_val: nan, pos_over_neg: 337.0943908691406 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6268, loss_val: nan, pos_over_neg: 308.13787841796875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6211, loss_val: nan, pos_over_neg: 449.5418701171875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6037, loss_val: nan, pos_over_neg: 689.8743896484375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.628, loss_val: nan, pos_over_neg: 557.95458984375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.6128, loss_val: nan, pos_over_neg: 246.67434692382812 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 408.0528564453125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 430.4317932128906 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6018, loss_val: nan, pos_over_neg: 297.55914306640625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 304.9663391113281 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 304.6967468261719 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.6079, loss_val: nan, pos_over_neg: 512.0664672851562 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 562.6188354492188 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.6183, loss_val: nan, pos_over_neg: 391.9476013183594 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 400.85888671875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 257.31878662109375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.565, loss_val: nan, pos_over_neg: 549.2692260742188 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 345.2005920410156 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 303.5682373046875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6117, loss_val: nan, pos_over_neg: 280.10516357421875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 493.4971618652344 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 591.9718627929688 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6224, loss_val: nan, pos_over_neg: 521.3890991210938 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 730.9656372070312 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 846.169921875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 393.7255859375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6182, loss_val: nan, pos_over_neg: 384.9687194824219 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.674, loss_val: nan, pos_over_neg: 416.8892822265625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 444.6352844238281 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6468, loss_val: nan, pos_over_neg: 333.51043701171875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 633.0747680664062 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6548, loss_val: nan, pos_over_neg: 416.3908996582031 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.5912, loss_val: nan, pos_over_neg: 590.71630859375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 693.3023681640625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.6373, loss_val: nan, pos_over_neg: 360.62677001953125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 565.1542358398438 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 436.0279541015625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.6419, loss_val: nan, pos_over_neg: 329.44476318359375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 588.6670532226562 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 886.9986572265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6385, loss_val: nan, pos_over_neg: 740.1254272460938 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 918.19189453125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 594.3245849609375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.6182, loss_val: nan, pos_over_neg: 556.9248046875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 694.7166137695312 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 306.7476501464844 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 306.60382080078125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 444.8436279296875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.5943, loss_val: nan, pos_over_neg: 358.6087646484375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.602, loss_val: nan, pos_over_neg: 460.1211853027344 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 506.8901672363281 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.653, loss_val: nan, pos_over_neg: 447.72216796875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5678, loss_val: nan, pos_over_neg: 960.518798828125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 282.7702941894531 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 408.8419494628906 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 674.0970458984375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 497.3874206542969 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6329, loss_val: nan, pos_over_neg: 349.89056396484375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 540.9714965820312 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 424.10870361328125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 546.8126831054688 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.6311, loss_val: nan, pos_over_neg: 557.6939697265625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 614.459716796875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 409.87298583984375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 485.2121276855469 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.5926, loss_val: nan, pos_over_neg: 496.8170166015625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.6322, loss_val: nan, pos_over_neg: 488.3448791503906 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6065, loss_val: nan, pos_over_neg: 656.2584228515625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6567, loss_val: nan, pos_over_neg: 359.00433349609375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.6271, loss_val: nan, pos_over_neg: 318.19134521484375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 380.3295593261719 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.6609, loss_val: nan, pos_over_neg: 404.03717041015625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 386.7877197265625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6655, loss_val: nan, pos_over_neg: 375.2391662597656 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6403, loss_val: nan, pos_over_neg: 471.6011657714844 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.6435, loss_val: nan, pos_over_neg: 418.5887451171875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 369.3011779785156 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6303, loss_val: nan, pos_over_neg: 271.4647216796875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 381.4833068847656 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 506.17803955078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 537.5660400390625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 352.17041015625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.5958, loss_val: nan, pos_over_neg: 439.0196838378906 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6611, loss_val: nan, pos_over_neg: 480.8904113769531 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 348.8418273925781 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 368.6564025878906 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.6016, loss_val: nan, pos_over_neg: 622.4178466796875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 476.97772216796875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6647, loss_val: nan, pos_over_neg: 301.4207763671875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.609, loss_val: nan, pos_over_neg: 569.5067138671875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 1017.0245971679688 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.6133, loss_val: nan, pos_over_neg: 579.793701171875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 564.181396484375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6079, loss_val: nan, pos_over_neg: 748.0810546875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 1012.2894897460938 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 563.2405395507812 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.6065, loss_val: nan, pos_over_neg: 465.9669494628906 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.6299, loss_val: nan, pos_over_neg: 366.1002197265625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 519.7542114257812 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.625, loss_val: nan, pos_over_neg: 365.74700927734375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.6195, loss_val: nan, pos_over_neg: 378.31781005859375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 406.3397216796875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 439.2488098144531 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 638.39306640625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.6399, loss_val: nan, pos_over_neg: 518.5809326171875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 499.8174743652344 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 295.309326171875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 358.6638488769531 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.6374, loss_val: nan, pos_over_neg: 276.7218933105469 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6616, loss_val: nan, pos_over_neg: 227.87916564941406 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 337.47723388671875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 307.1765441894531 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.6379, loss_val: nan, pos_over_neg: 320.751953125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 399.3354187011719 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 470.74151611328125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.5918, loss_val: nan, pos_over_neg: 487.066162109375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 485.08038330078125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.5972, loss_val: nan, pos_over_neg: 368.981689453125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.6437, loss_val: nan, pos_over_neg: 254.51702880859375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.6385, loss_val: nan, pos_over_neg: 295.39886474609375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.5789, loss_val: nan, pos_over_neg: 439.8914489746094 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 463.7453918457031 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 433.66827392578125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.6274, loss_val: nan, pos_over_neg: 329.1043701171875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 324.8766174316406 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.6105, loss_val: nan, pos_over_neg: 416.8882141113281 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.5698, loss_val: nan, pos_over_neg: 445.0790100097656 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 367.57061767578125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 266.8623046875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 337.2806091308594 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 243.08786010742188 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6672, loss_val: nan, pos_over_neg: 204.03004455566406 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 409.3111877441406 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 282.88818359375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.6241, loss_val: nan, pos_over_neg: 545.6336059570312 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.5859, loss_val: nan, pos_over_neg: 672.0045166015625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 445.43499755859375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.5927, loss_val: nan, pos_over_neg: 593.6273803710938 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 483.1707458496094 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 390.152587890625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.6113, loss_val: nan, pos_over_neg: 459.5670166015625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 425.0577392578125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 557.51953125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 281.53082275390625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.5715, loss_val: nan, pos_over_neg: 374.3880310058594 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.6467, loss_val: nan, pos_over_neg: 330.1558532714844 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.6489, loss_val: nan, pos_over_neg: 360.73095703125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.6574, loss_val: nan, pos_over_neg: 275.57281494140625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.664, loss_val: nan, pos_over_neg: 307.9817810058594 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.6614, loss_val: nan, pos_over_neg: 254.17630004882812 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.6344, loss_val: nan, pos_over_neg: 394.38433837890625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 308.54449462890625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 598.79833984375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.6578, loss_val: nan, pos_over_neg: 515.0975952148438 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 494.6867370605469 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6664, loss_val: nan, pos_over_neg: 292.7379150390625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6462, loss_val: nan, pos_over_neg: 309.4165344238281 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.6734, loss_val: nan, pos_over_neg: 428.9807434082031 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.589, loss_val: nan, pos_over_neg: 437.6726989746094 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 701.24853515625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.606, loss_val: nan, pos_over_neg: 459.84149169921875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 496.6951599121094 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.6325, loss_val: nan, pos_over_neg: 359.9787292480469 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.6297, loss_val: nan, pos_over_neg: 423.7498779296875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 295.4510192871094 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 668.826416015625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.5977, loss_val: nan, pos_over_neg: 441.3970642089844 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 446.3430480957031 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 351.68890380859375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 334.4503173828125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 365.9534912109375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.6335, loss_val: nan, pos_over_neg: 378.3462219238281 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.6307, loss_val: nan, pos_over_neg: 429.779052734375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6091, loss_val: nan, pos_over_neg: 683.5740966796875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 402.0290222167969 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.589, loss_val: nan, pos_over_neg: 567.8697509765625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.6135, loss_val: nan, pos_over_neg: 480.610107421875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 293.9212646484375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.6308, loss_val: nan, pos_over_neg: 298.9654235839844 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6628, loss_val: nan, pos_over_neg: 202.17747497558594 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.666, loss_val: nan, pos_over_neg: 269.9130554199219 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 360.1109313964844 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6139, loss_val: nan, pos_over_neg: 454.8730773925781 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 413.3887939453125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 502.90045166015625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 575.8892211914062 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.6275, loss_val: nan, pos_over_neg: 571.1651000976562 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 711.6907958984375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 405.2442321777344 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.6108, loss_val: nan, pos_over_neg: 481.8079833984375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.616, loss_val: nan, pos_over_neg: 444.2337951660156 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.5953, loss_val: nan, pos_over_neg: 789.3184814453125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.6207, loss_val: nan, pos_over_neg: 398.6684875488281 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.6062, loss_val: nan, pos_over_neg: 461.91082763671875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6546, loss_val: nan, pos_over_neg: 350.20355224609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 488.68084716796875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 624.3155517578125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.5839, loss_val: nan, pos_over_neg: 678.3929443359375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.5836, loss_val: nan, pos_over_neg: 388.4891357421875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.5976, loss_val: nan, pos_over_neg: 536.5250854492188 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.6371, loss_val: nan, pos_over_neg: 339.8583068847656 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 405.71209716796875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 498.8771667480469 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.6269, loss_val: nan, pos_over_neg: 267.1451416015625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.6584, loss_val: nan, pos_over_neg: 231.21298217773438 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6457, loss_val: nan, pos_over_neg: 264.8912353515625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 462.1839599609375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.5967, loss_val: nan, pos_over_neg: 392.2713623046875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.6471, loss_val: nan, pos_over_neg: 326.5224914550781 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.5963, loss_val: nan, pos_over_neg: 742.407470703125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.6055, loss_val: nan, pos_over_neg: 498.8068542480469 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6371, loss_val: nan, pos_over_neg: 324.1821594238281 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 433.7004699707031 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.6443, loss_val: nan, pos_over_neg: 302.0788269042969 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.6115, loss_val: nan, pos_over_neg: 487.8501892089844 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.6575, loss_val: nan, pos_over_neg: 279.7507019042969 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.627, loss_val: nan, pos_over_neg: 280.8620910644531 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.6496, loss_val: nan, pos_over_neg: 301.4004821777344 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 330.88604736328125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6615, loss_val: nan, pos_over_neg: 276.4309997558594 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.6307, loss_val: nan, pos_over_neg: 433.3930358886719 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.6239, loss_val: nan, pos_over_neg: 336.7575988769531 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 602.9436645507812 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 598.83984375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6313, loss_val: nan, pos_over_neg: 854.0079956054688 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 486.6534729003906 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.6552, loss_val: nan, pos_over_neg: 495.45281982421875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.5542, loss_val: nan, pos_over_neg: 730.8970336914062 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.6192, loss_val: nan, pos_over_neg: 610.7742919921875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 338.14508056640625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.6441, loss_val: nan, pos_over_neg: 379.521728515625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.6539, loss_val: nan, pos_over_neg: 212.72607421875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.6028, loss_val: nan, pos_over_neg: 473.7371826171875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 302.1412048339844 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 367.450439453125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 311.3145446777344 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 518.023193359375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 564.020751953125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.657, loss_val: nan, pos_over_neg: 656.1617431640625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 639.5283813476562 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.5987, loss_val: nan, pos_over_neg: 615.0560913085938 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 488.95074462890625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 414.34771728515625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6376, loss_val: nan, pos_over_neg: 362.28521728515625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.6105, loss_val: nan, pos_over_neg: 282.26263427734375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 407.96783447265625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.5812, loss_val: nan, pos_over_neg: 405.78802490234375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6331, loss_val: nan, pos_over_neg: 263.0951843261719 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.5884, loss_val: nan, pos_over_neg: 478.09478759765625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 386.26678466796875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6159, loss_val: nan, pos_over_neg: 479.9648742675781 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.6715, loss_val: nan, pos_over_neg: 351.5357971191406 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.642, loss_val: nan, pos_over_neg: 332.0401611328125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 447.9234619140625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.6233, loss_val: nan, pos_over_neg: 371.9960632324219 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 256.6058044433594 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.6529, loss_val: nan, pos_over_neg: 279.0209045410156 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 345.3936462402344 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.6326, loss_val: nan, pos_over_neg: 257.44091796875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 461.8378601074219 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.6259, loss_val: nan, pos_over_neg: 386.12786865234375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 357.8000183105469 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.592, loss_val: nan, pos_over_neg: 423.8598937988281 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.6493, loss_val: nan, pos_over_neg: 433.5740966796875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 270.9922180175781 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.6207, loss_val: nan, pos_over_neg: 228.70431518554688 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 401.9703369140625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.5957, loss_val: nan, pos_over_neg: 664.497314453125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.648, loss_val: nan, pos_over_neg: 472.6831970214844 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.6348, loss_val: nan, pos_over_neg: 469.11553955078125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 620.2417602539062 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 794.7843017578125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.6645, loss_val: nan, pos_over_neg: 340.4964599609375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6253, loss_val: nan, pos_over_neg: 437.00311279296875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.6672, loss_val: nan, pos_over_neg: 308.7569580078125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 347.6150207519531 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.6444, loss_val: nan, pos_over_neg: 290.4231262207031 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 581.7059936523438 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 565.3440551757812 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.6124, loss_val: nan, pos_over_neg: 375.1373291015625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6088, loss_val: nan, pos_over_neg: 499.8208312988281 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 461.6031494140625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6023, loss_val: nan, pos_over_neg: 1079.13134765625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.6407, loss_val: nan, pos_over_neg: 332.9349670410156 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.6514, loss_val: nan, pos_over_neg: 279.6405944824219 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6314, loss_val: nan, pos_over_neg: 339.131591796875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 587.2974853515625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 529.4764404296875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 485.38153076171875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 423.6538391113281 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6127, loss_val: nan, pos_over_neg: 469.10296630859375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6601, loss_val: nan, pos_over_neg: 338.9642333984375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 544.1561889648438 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6371, loss_val: nan, pos_over_neg: 417.79241943359375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 430.0871887207031 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 342.7438659667969 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 423.3068542480469 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6005, loss_val: nan, pos_over_neg: 381.51739501953125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 647.9148559570312 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 482.1283264160156 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6355, loss_val: nan, pos_over_neg: 423.5224609375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6299, loss_val: nan, pos_over_neg: 351.71728515625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.6297, loss_val: nan, pos_over_neg: 396.7474060058594 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6139, loss_val: nan, pos_over_neg: 658.37939453125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 466.0289001464844 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6489, loss_val: nan, pos_over_neg: 352.8653259277344 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 419.6853332519531 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 482.8887023925781 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 488.6952819824219 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 267.5688171386719 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.6277, loss_val: nan, pos_over_neg: 417.6556701660156 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 343.72332763671875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 445.93353271484375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 313.90234375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 390.48736572265625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 398.51318359375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6148, loss_val: nan, pos_over_neg: 564.3394165039062 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.578, loss_val: nan, pos_over_neg: 802.4336547851562 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.5648, loss_val: nan, pos_over_neg: 821.0731201171875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 462.6893005371094 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.6022, loss_val: nan, pos_over_neg: 425.59442138671875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.5891, loss_val: nan, pos_over_neg: 313.8451232910156 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 338.9653625488281 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 302.5317687988281 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6026, loss_val: nan, pos_over_neg: 356.45892333984375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6048, loss_val: nan, pos_over_neg: 319.4488830566406 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6197, loss_val: nan, pos_over_neg: 307.52740478515625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.5694, loss_val: nan, pos_over_neg: 792.9252319335938 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 394.6576843261719 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 395.5550231933594 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 435.6934509277344 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.5594, loss_val: nan, pos_over_neg: 562.1790771484375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.6074, loss_val: nan, pos_over_neg: 487.89862060546875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.6197, loss_val: nan, pos_over_neg: 232.49539184570312 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 386.5113830566406 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 503.4421691894531 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.6075, loss_val: nan, pos_over_neg: 483.70709228515625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 483.5832214355469 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.6016, loss_val: nan, pos_over_neg: 511.6595764160156 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 558.6594848632812 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 679.3585205078125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.6373, loss_val: nan, pos_over_neg: 576.9923095703125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 464.4078674316406 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.6242, loss_val: nan, pos_over_neg: 245.6849365234375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 228.82366943359375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 317.7567138671875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 494.257568359375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 493.75927734375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 435.0372009277344 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 783.1882934570312 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 581.1116333007812 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 343.4369812011719 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 581.5889892578125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 373.7411193847656 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 302.84014892578125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6128, loss_val: nan, pos_over_neg: 464.6216735839844 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.617, loss_val: nan, pos_over_neg: 287.0197448730469 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.5886, loss_val: nan, pos_over_neg: 380.27557373046875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.6062, loss_val: nan, pos_over_neg: 384.8688049316406 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 431.30511474609375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 269.54205322265625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 656.1141357421875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 282.53167724609375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.5941, loss_val: nan, pos_over_neg: 529.728271484375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.6259, loss_val: nan, pos_over_neg: 383.77606201171875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.5985, loss_val: nan, pos_over_neg: 417.00250244140625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.6301, loss_val: nan, pos_over_neg: 340.9083251953125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 238.28488159179688 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.6267, loss_val: nan, pos_over_neg: 320.3466491699219 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.5891, loss_val: nan, pos_over_neg: 339.9288330078125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6125, loss_val: nan, pos_over_neg: 240.9375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6054, loss_val: nan, pos_over_neg: 371.94580078125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.5643, loss_val: nan, pos_over_neg: 395.18609619140625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 415.11346435546875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 455.4497985839844 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 371.8319396972656 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 380.1396484375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 348.0263671875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.6101, loss_val: nan, pos_over_neg: 457.08831787109375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.6172, loss_val: nan, pos_over_neg: 522.9609375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 444.7653503417969 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 639.4173583984375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.5812, loss_val: nan, pos_over_neg: 444.4468688964844 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6005, loss_val: nan, pos_over_neg: 452.8852844238281 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.654, loss_val: nan, pos_over_neg: 316.6419372558594 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 309.9107971191406 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 352.8516845703125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 794.7052001953125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 421.834716796875 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 920.1515502929688 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 577.1143798828125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 833.0888061523438 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 519.6082763671875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6528, loss_val: nan, pos_over_neg: 236.80824279785156 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 558.0698852539062 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.6285, loss_val: nan, pos_over_neg: 603.0325927734375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 571.2387084960938 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 579.9956665039062 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.5944, loss_val: nan, pos_over_neg: 612.8442993164062 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.5858, loss_val: nan, pos_over_neg: 642.5283203125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 468.5495910644531 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.6418, loss_val: nan, pos_over_neg: 341.0799255371094 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 403.0179138183594 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.6527, loss_val: nan, pos_over_neg: 450.8042297363281 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 328.47601318359375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 355.54888916015625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.5808, loss_val: nan, pos_over_neg: 604.933837890625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6115, loss_val: nan, pos_over_neg: 535.9976806640625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6123, loss_val: nan, pos_over_neg: 325.35015869140625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 600.8782958984375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 567.1448364257812 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 602.8488159179688 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6023, loss_val: nan, pos_over_neg: 656.2738037109375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.587, loss_val: nan, pos_over_neg: 559.9046630859375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.6116, loss_val: nan, pos_over_neg: 454.32208251953125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.6269, loss_val: nan, pos_over_neg: 301.1445007324219 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.6948, loss_val: nan, pos_over_neg: 252.9417724609375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 308.8041687011719 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 403.35699462890625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 347.3113708496094 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 589.2976684570312 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 532.8418579101562 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.6127, loss_val: nan, pos_over_neg: 564.1002807617188 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.6361, loss_val: nan, pos_over_neg: 1028.546142578125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.6386, loss_val: nan, pos_over_neg: 980.213623046875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 629.5427856445312 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.6271, loss_val: nan, pos_over_neg: 685.808349609375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.637, loss_val: nan, pos_over_neg: 528.9882202148438 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.55, loss_val: nan, pos_over_neg: 539.296875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 416.249755859375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 401.0074157714844 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.5858, loss_val: nan, pos_over_neg: 454.8382568359375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 357.3973083496094 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 495.5408630371094 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6494, loss_val: nan, pos_over_neg: 300.09515380859375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 636.5873413085938 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.656, loss_val: nan, pos_over_neg: 552.8219604492188 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6098, loss_val: nan, pos_over_neg: 497.2845153808594 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.6281, loss_val: nan, pos_over_neg: 384.7298889160156 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 424.203125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.5923, loss_val: nan, pos_over_neg: 728.9927978515625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 969.6098022460938 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.6406, loss_val: nan, pos_over_neg: 331.6134948730469 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 415.3377990722656 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 307.7001647949219 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 269.4847717285156 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.6249, loss_val: nan, pos_over_neg: 329.2317199707031 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.5989, loss_val: nan, pos_over_neg: 714.3544311523438 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.544, loss_val: nan, pos_over_neg: 673.2669067382812 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 396.8448181152344 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6053, loss_val: nan, pos_over_neg: 414.9801025390625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.645, loss_val: nan, pos_over_neg: 818.2838745117188 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.6241, loss_val: nan, pos_over_neg: 496.40045166015625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.6446, loss_val: nan, pos_over_neg: 471.46728515625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 375.3070983886719 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 305.7180480957031 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 368.9438781738281 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 261.61553955078125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 554.759765625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 407.7920227050781 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.6635, loss_val: nan, pos_over_neg: 314.4948425292969 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.618, loss_val: nan, pos_over_neg: 427.5233154296875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 388.3873596191406 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 550.1192016601562 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.638, loss_val: nan, pos_over_neg: 416.3338317871094 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 361.8100891113281 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6462, loss_val: nan, pos_over_neg: 252.02735900878906 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6213, loss_val: nan, pos_over_neg: 494.581298828125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/300000 [3:01:16<75531:13:42, 906.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 991.2781982421875 lr: 0.00031623\n",
      "epoch: 12\n",
      "Iter: 0/695, loss_train: 3.6201, loss_val: nan, pos_over_neg: 460.0372314453125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 564.41455078125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 361.38916015625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.6256, loss_val: nan, pos_over_neg: 433.179931640625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6268, loss_val: nan, pos_over_neg: 580.346923828125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.635, loss_val: nan, pos_over_neg: 322.5214538574219 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.6028, loss_val: nan, pos_over_neg: 457.7332763671875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.6239, loss_val: nan, pos_over_neg: 376.9748229980469 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 360.3763732910156 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.565, loss_val: nan, pos_over_neg: 524.2963256835938 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.5884, loss_val: nan, pos_over_neg: 437.0477600097656 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 382.6034240722656 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.6054, loss_val: nan, pos_over_neg: 464.53961181640625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.5875, loss_val: nan, pos_over_neg: 616.5682373046875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 492.5683288574219 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.619, loss_val: nan, pos_over_neg: 437.4454650878906 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 367.20281982421875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 668.1860961914062 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.6105, loss_val: nan, pos_over_neg: 481.1348876953125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.691, loss_val: nan, pos_over_neg: 287.80548095703125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 367.03094482421875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 451.3962707519531 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 400.36883544921875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 334.0264587402344 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 282.96478271484375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.6195, loss_val: nan, pos_over_neg: 262.3280944824219 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 443.0848693847656 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 483.4118347167969 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6277, loss_val: nan, pos_over_neg: 448.923583984375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.6255, loss_val: nan, pos_over_neg: 353.74560546875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 508.8179626464844 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.5823, loss_val: nan, pos_over_neg: 747.5228271484375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.6358, loss_val: nan, pos_over_neg: 318.56304931640625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 514.60791015625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.6485, loss_val: nan, pos_over_neg: 457.3211364746094 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 329.2322998046875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.5634, loss_val: nan, pos_over_neg: 398.514892578125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 648.5950927734375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6274, loss_val: nan, pos_over_neg: 319.4770202636719 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 257.80364990234375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 476.36376953125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 526.5101928710938 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.6345, loss_val: nan, pos_over_neg: 385.2593688964844 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 434.6264343261719 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.6202, loss_val: nan, pos_over_neg: 452.5758972167969 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 365.694091796875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.6202, loss_val: nan, pos_over_neg: 410.0318603515625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 343.20697021484375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 404.4701843261719 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.5757, loss_val: nan, pos_over_neg: 520.4149169921875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.5907, loss_val: nan, pos_over_neg: 523.4553833007812 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.6037, loss_val: nan, pos_over_neg: 447.2961120605469 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 476.94085693359375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.6274, loss_val: nan, pos_over_neg: 599.6265869140625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6009, loss_val: nan, pos_over_neg: 705.4322509765625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 361.396728515625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 477.8727722167969 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 503.47467041015625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.6004, loss_val: nan, pos_over_neg: 469.5086975097656 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 525.5807495117188 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.5611, loss_val: nan, pos_over_neg: 551.6638793945312 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.6383, loss_val: nan, pos_over_neg: 265.4134826660156 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.6159, loss_val: nan, pos_over_neg: 393.4249572753906 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6274, loss_val: nan, pos_over_neg: 415.326416015625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 418.3456115722656 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 520.3994140625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.6304, loss_val: nan, pos_over_neg: 433.504150390625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 546.8665771484375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 420.5775451660156 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 390.1726379394531 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.5643, loss_val: nan, pos_over_neg: 642.000244140625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 322.91448974609375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 366.3135070800781 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 341.22174072265625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 470.0997619628906 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 393.5920715332031 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 409.4595642089844 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 299.8028259277344 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.555, loss_val: nan, pos_over_neg: 546.1033325195312 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 439.0023193359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 356.2784423828125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.5861, loss_val: nan, pos_over_neg: 336.9895324707031 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 342.1578369140625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 493.8815612792969 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.5841, loss_val: nan, pos_over_neg: 524.8065795898438 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 409.66888427734375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.6473, loss_val: nan, pos_over_neg: 301.83154296875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.578, loss_val: nan, pos_over_neg: 700.945068359375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 587.8128662109375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.5665, loss_val: nan, pos_over_neg: 710.4971923828125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.6243, loss_val: nan, pos_over_neg: 483.5174865722656 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 366.54693603515625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 565.8465576171875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 361.9061584472656 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 330.3837585449219 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 469.1792297363281 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 569.630859375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.5773, loss_val: nan, pos_over_neg: 424.61126708984375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 442.4813537597656 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.6142, loss_val: nan, pos_over_neg: 365.6925354003906 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6235, loss_val: nan, pos_over_neg: 356.1807861328125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 333.79608154296875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 506.394287109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.6418, loss_val: nan, pos_over_neg: 341.5840148925781 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.6353, loss_val: nan, pos_over_neg: 399.8020935058594 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.5855, loss_val: nan, pos_over_neg: 436.7655029296875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.6152, loss_val: nan, pos_over_neg: 479.39007568359375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 419.5386047363281 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 590.685791015625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6229, loss_val: nan, pos_over_neg: 490.7060241699219 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6177, loss_val: nan, pos_over_neg: 339.2135314941406 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 282.66632080078125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 329.9421691894531 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.5951, loss_val: nan, pos_over_neg: 372.2398986816406 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.5538, loss_val: nan, pos_over_neg: 605.509765625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 424.7932434082031 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6331, loss_val: nan, pos_over_neg: 287.7787780761719 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 260.96502685546875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.613, loss_val: nan, pos_over_neg: 437.72161865234375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.6556, loss_val: nan, pos_over_neg: 432.3569641113281 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6576, loss_val: nan, pos_over_neg: 350.64666748046875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.6344, loss_val: nan, pos_over_neg: 353.1855163574219 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.5866, loss_val: nan, pos_over_neg: 583.71044921875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6464, loss_val: nan, pos_over_neg: 407.5576171875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.6364, loss_val: nan, pos_over_neg: 675.1463012695312 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.5829, loss_val: nan, pos_over_neg: 732.2806396484375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 951.2177124023438 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.652, loss_val: nan, pos_over_neg: 539.597412109375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.5744, loss_val: nan, pos_over_neg: 549.0369262695312 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 340.86065673828125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.6469, loss_val: nan, pos_over_neg: 285.79852294921875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 362.2428894042969 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 389.76556396484375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 424.32012939453125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.583, loss_val: nan, pos_over_neg: 457.77972412109375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6178, loss_val: nan, pos_over_neg: 318.0847473144531 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.5752, loss_val: nan, pos_over_neg: 431.8385314941406 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.6459, loss_val: nan, pos_over_neg: 306.4186096191406 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 595.0410766601562 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.6205, loss_val: nan, pos_over_neg: 737.5761108398438 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 641.779541015625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 386.27960205078125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 375.6133728027344 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.6253, loss_val: nan, pos_over_neg: 401.57421875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.6148, loss_val: nan, pos_over_neg: 419.4750061035156 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.6137, loss_val: nan, pos_over_neg: 557.9049072265625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 352.306396484375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 402.8670959472656 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.6324, loss_val: nan, pos_over_neg: 261.2789306640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 336.5537414550781 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 622.8308715820312 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 492.86456298828125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6018, loss_val: nan, pos_over_neg: 368.8167419433594 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 568.1700439453125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 389.4207458496094 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6377, loss_val: nan, pos_over_neg: 350.9315490722656 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6262, loss_val: nan, pos_over_neg: 341.04327392578125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.5918, loss_val: nan, pos_over_neg: 369.0520324707031 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.6233, loss_val: nan, pos_over_neg: 298.9175109863281 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6331, loss_val: nan, pos_over_neg: 274.4853515625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 346.1198425292969 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.6026, loss_val: nan, pos_over_neg: 450.34478759765625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.5905, loss_val: nan, pos_over_neg: 518.392333984375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 361.6617736816406 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6208, loss_val: nan, pos_over_neg: 253.11607360839844 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.5914, loss_val: nan, pos_over_neg: 356.82611083984375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 358.25677490234375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.6053, loss_val: nan, pos_over_neg: 277.69732666015625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.5549, loss_val: nan, pos_over_neg: 546.8749389648438 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 523.5487060546875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.6323, loss_val: nan, pos_over_neg: 316.76153564453125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 498.99481201171875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 426.7117004394531 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 402.15966796875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 320.25164794921875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 287.8597717285156 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6521, loss_val: nan, pos_over_neg: 305.0792541503906 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.5993, loss_val: nan, pos_over_neg: 409.64935302734375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6246, loss_val: nan, pos_over_neg: 421.1617431640625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6591, loss_val: nan, pos_over_neg: 540.6920776367188 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 1410.076904296875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.5914, loss_val: nan, pos_over_neg: 503.60003662109375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 724.9188842773438 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 422.7146301269531 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 445.24151611328125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 400.1016845703125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 304.65985107421875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.6594, loss_val: nan, pos_over_neg: 203.9466552734375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.6195, loss_val: nan, pos_over_neg: 285.43634033203125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.6297, loss_val: nan, pos_over_neg: 368.03619384765625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.6048, loss_val: nan, pos_over_neg: 488.9598388671875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.647, loss_val: nan, pos_over_neg: 370.38427734375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 706.0946044921875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.604, loss_val: nan, pos_over_neg: 617.018310546875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 816.6939086914062 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.5925, loss_val: nan, pos_over_neg: 719.2114868164062 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.6155, loss_val: nan, pos_over_neg: 411.0060119628906 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.6262, loss_val: nan, pos_over_neg: 547.9131469726562 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 522.3007202148438 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 434.1255187988281 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 385.7974548339844 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.5989, loss_val: nan, pos_over_neg: 336.29437255859375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 574.5848388671875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 449.57281494140625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 345.9192199707031 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 285.78363037109375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.6319, loss_val: nan, pos_over_neg: 427.5120544433594 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 419.613525390625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.6178, loss_val: nan, pos_over_neg: 361.9078674316406 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.6374, loss_val: nan, pos_over_neg: 329.244873046875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 350.5101013183594 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6402, loss_val: nan, pos_over_neg: 305.180908203125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 469.2615661621094 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.5724, loss_val: nan, pos_over_neg: 641.7230224609375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 363.3957214355469 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 510.92462158203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.5907, loss_val: nan, pos_over_neg: 352.13446044921875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6278, loss_val: nan, pos_over_neg: 494.9072570800781 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 361.86517333984375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 377.7200012207031 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 542.1873779296875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.6233, loss_val: nan, pos_over_neg: 463.4195556640625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 408.7193603515625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 503.3924255371094 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.642, loss_val: nan, pos_over_neg: 509.1458740234375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 425.144287109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6128, loss_val: nan, pos_over_neg: 331.7416076660156 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 308.4422607421875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6432, loss_val: nan, pos_over_neg: 391.8470153808594 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.5694, loss_val: nan, pos_over_neg: 559.2988891601562 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.6509, loss_val: nan, pos_over_neg: 273.8592529296875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 405.0805969238281 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 775.3563232421875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.563, loss_val: nan, pos_over_neg: 548.9837646484375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 334.320068359375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 413.9194030761719 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.5728, loss_val: nan, pos_over_neg: 489.9040832519531 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.564, loss_val: nan, pos_over_neg: 559.4057006835938 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.5596, loss_val: nan, pos_over_neg: 755.9285278320312 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 363.0682373046875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.5617, loss_val: nan, pos_over_neg: 639.149658203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 256.72283935546875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.5854, loss_val: nan, pos_over_neg: 546.8632202148438 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 403.3757019042969 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 415.3772888183594 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.605, loss_val: nan, pos_over_neg: 441.8714904785156 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 489.0691223144531 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.6272, loss_val: nan, pos_over_neg: 435.1911926269531 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 534.6471557617188 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 456.3538818359375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6076, loss_val: nan, pos_over_neg: 509.3105163574219 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 436.6995544433594 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6082, loss_val: nan, pos_over_neg: 457.26947021484375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.5849, loss_val: nan, pos_over_neg: 404.83599853515625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 518.0565185546875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6137, loss_val: nan, pos_over_neg: 358.46563720703125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.677, loss_val: nan, pos_over_neg: 341.73236083984375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 303.4973449707031 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 353.04345703125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.6577, loss_val: nan, pos_over_neg: 210.28358459472656 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.6137, loss_val: nan, pos_over_neg: 363.76922607421875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 377.1916809082031 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 664.2667846679688 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.5764, loss_val: nan, pos_over_neg: 457.3058776855469 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 451.60302734375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 373.02490234375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 522.0779418945312 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 719.0724487304688 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.6609, loss_val: nan, pos_over_neg: 272.5700988769531 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 538.0107421875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 454.9426574707031 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 665.2610473632812 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 704.2758178710938 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 436.8724060058594 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 378.5062255859375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 396.9919128417969 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6398, loss_val: nan, pos_over_neg: 458.28460693359375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.6569, loss_val: nan, pos_over_neg: 298.1845397949219 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 321.1441345214844 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6248, loss_val: nan, pos_over_neg: 429.3751220703125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 362.0794372558594 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.6474, loss_val: nan, pos_over_neg: 360.9828796386719 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.5791, loss_val: nan, pos_over_neg: 504.7641296386719 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.5756, loss_val: nan, pos_over_neg: 712.5140991210938 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.6069, loss_val: nan, pos_over_neg: 689.2090454101562 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 306.91033935546875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.6018, loss_val: nan, pos_over_neg: 583.8897094726562 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 329.3725280761719 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 580.653076171875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.5979, loss_val: nan, pos_over_neg: 434.927978515625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.6396, loss_val: nan, pos_over_neg: 290.00836181640625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 315.4117126464844 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.5809, loss_val: nan, pos_over_neg: 325.8252258300781 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 378.8598327636719 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6548, loss_val: nan, pos_over_neg: 268.2504577636719 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.6366, loss_val: nan, pos_over_neg: 255.36170959472656 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 435.5697326660156 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.6468, loss_val: nan, pos_over_neg: 384.9024658203125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.5925, loss_val: nan, pos_over_neg: 496.4419250488281 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 403.8277893066406 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 493.20751953125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 353.6361389160156 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 1034.17919921875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 500.4456787109375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 423.43756103515625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 280.2783203125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 537.5792846679688 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 409.52789306640625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 317.1340026855469 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6181, loss_val: nan, pos_over_neg: 260.5276184082031 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6357, loss_val: nan, pos_over_neg: 322.4476013183594 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 941.1183471679688 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 694.7565307617188 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.5807, loss_val: nan, pos_over_neg: 496.90313720703125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.5833, loss_val: nan, pos_over_neg: 595.7022705078125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 691.5145263671875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.645, loss_val: nan, pos_over_neg: 427.8436584472656 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.5935, loss_val: nan, pos_over_neg: 355.8281555175781 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 928.0958862304688 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 589.7682495117188 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.5419, loss_val: nan, pos_over_neg: 563.5322875976562 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.6174, loss_val: nan, pos_over_neg: 299.2296142578125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.5813, loss_val: nan, pos_over_neg: 449.5912170410156 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 289.5333251953125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.6321, loss_val: nan, pos_over_neg: 378.9977111816406 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.6173, loss_val: nan, pos_over_neg: 525.8308715820312 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 597.022216796875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 351.4404602050781 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 529.460205078125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 708.4822998046875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 442.7645568847656 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.5785, loss_val: nan, pos_over_neg: 387.6481628417969 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 278.27777099609375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.6074, loss_val: nan, pos_over_neg: 393.3690185546875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 676.4715576171875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.5567, loss_val: nan, pos_over_neg: 936.1264038085938 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 379.56170654296875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.5675, loss_val: nan, pos_over_neg: 606.2659301757812 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.6595, loss_val: nan, pos_over_neg: 353.72918701171875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.5898, loss_val: nan, pos_over_neg: 472.42974853515625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.5816, loss_val: nan, pos_over_neg: 539.219970703125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 364.801025390625 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.6531, loss_val: nan, pos_over_neg: 318.85247802734375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.6005, loss_val: nan, pos_over_neg: 395.5715026855469 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 330.8209533691406 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.5942, loss_val: nan, pos_over_neg: 431.7758483886719 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 903.2879028320312 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 427.6739196777344 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.6375, loss_val: nan, pos_over_neg: 414.9295959472656 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 588.6718139648438 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.6286, loss_val: nan, pos_over_neg: 387.9593811035156 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 427.1883239746094 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6526, loss_val: nan, pos_over_neg: 380.2127380371094 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.6344, loss_val: nan, pos_over_neg: 332.353759765625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.6165, loss_val: nan, pos_over_neg: 347.2447509765625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6051, loss_val: nan, pos_over_neg: 582.3225708007812 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 347.1661376953125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 380.5370178222656 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.5923, loss_val: nan, pos_over_neg: 488.873779296875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.5718, loss_val: nan, pos_over_neg: 602.840576171875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.5899, loss_val: nan, pos_over_neg: 443.365478515625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.5572, loss_val: nan, pos_over_neg: 542.333251953125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 417.53289794921875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 663.18994140625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.575, loss_val: nan, pos_over_neg: 666.9603271484375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 566.5474243164062 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.5543, loss_val: nan, pos_over_neg: 653.2481079101562 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 320.25701904296875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 357.2868957519531 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.645, loss_val: nan, pos_over_neg: 225.0564422607422 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.6002, loss_val: nan, pos_over_neg: 351.5649719238281 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 447.1167907714844 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.6159, loss_val: nan, pos_over_neg: 353.2276611328125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.6165, loss_val: nan, pos_over_neg: 598.6180419921875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 659.5831298828125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.6108, loss_val: nan, pos_over_neg: 707.57763671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 464.55523681640625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 567.4959106445312 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 682.0841064453125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 432.9268798828125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.6199, loss_val: nan, pos_over_neg: 327.6308898925781 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6089, loss_val: nan, pos_over_neg: 541.576171875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 308.6145935058594 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.6028, loss_val: nan, pos_over_neg: 623.5114135742188 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.6469, loss_val: nan, pos_over_neg: 432.5195007324219 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.6596, loss_val: nan, pos_over_neg: 283.26495361328125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.6112, loss_val: nan, pos_over_neg: 677.7352294921875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 503.25189208984375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 585.5062866210938 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 773.9913940429688 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6428, loss_val: nan, pos_over_neg: 347.2597961425781 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6082, loss_val: nan, pos_over_neg: 516.3712158203125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 398.606201171875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.5865, loss_val: nan, pos_over_neg: 455.5344543457031 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 287.8408508300781 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.5958, loss_val: nan, pos_over_neg: 472.27392578125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 607.2655029296875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 304.5538330078125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 340.9338684082031 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 757.0620727539062 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.6003, loss_val: nan, pos_over_neg: 475.0718688964844 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 455.3419494628906 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.5667, loss_val: nan, pos_over_neg: 405.6702575683594 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.5791, loss_val: nan, pos_over_neg: 406.4256896972656 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 934.2109985351562 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 403.80999755859375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 327.05926513671875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.5834, loss_val: nan, pos_over_neg: 812.435546875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 636.6333618164062 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 433.8780517578125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.6516, loss_val: nan, pos_over_neg: 436.0529479980469 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 368.04730224609375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 442.7192077636719 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6136, loss_val: nan, pos_over_neg: 390.2488098144531 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.6556, loss_val: nan, pos_over_neg: 363.3705749511719 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.6073, loss_val: nan, pos_over_neg: 325.7711486816406 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.5951, loss_val: nan, pos_over_neg: 294.63671875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.6214, loss_val: nan, pos_over_neg: 365.8994140625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 409.2525634765625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 555.6681518554688 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6255, loss_val: nan, pos_over_neg: 411.9938049316406 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.6267, loss_val: nan, pos_over_neg: 341.7683410644531 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 482.88018798828125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 648.5401611328125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 506.1739196777344 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 492.8701477050781 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.6722, loss_val: nan, pos_over_neg: 325.92181396484375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 356.55499267578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 487.4018859863281 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.602, loss_val: nan, pos_over_neg: 835.5833129882812 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 327.5711364746094 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 309.8179626464844 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.6346, loss_val: nan, pos_over_neg: 394.0744323730469 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.6174, loss_val: nan, pos_over_neg: 347.5820007324219 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 604.4356079101562 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 628.80517578125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.6093, loss_val: nan, pos_over_neg: 514.795654296875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.6443, loss_val: nan, pos_over_neg: 350.2886657714844 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 422.0982971191406 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.6218, loss_val: nan, pos_over_neg: 529.6278076171875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.6234, loss_val: nan, pos_over_neg: 409.5113525390625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 326.8672790527344 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.6057, loss_val: nan, pos_over_neg: 492.1654052734375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 570.5753784179688 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6168, loss_val: nan, pos_over_neg: 447.845458984375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 286.38397216796875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 519.8223266601562 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 403.4627685546875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 340.9649963378906 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.5489, loss_val: nan, pos_over_neg: 464.6114807128906 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 346.8481750488281 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 580.1390380859375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.5833, loss_val: nan, pos_over_neg: 605.5542602539062 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.5607, loss_val: nan, pos_over_neg: 463.5799560546875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 375.12542724609375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.6091, loss_val: nan, pos_over_neg: 362.63677978515625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 392.3586730957031 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.6299, loss_val: nan, pos_over_neg: 359.0161437988281 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 399.9091491699219 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 550.0888671875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 477.7149658203125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 406.94439697265625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.5739, loss_val: nan, pos_over_neg: 628.6073608398438 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 345.34735107421875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 382.4571228027344 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.6374, loss_val: nan, pos_over_neg: 302.9714660644531 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.5807, loss_val: nan, pos_over_neg: 557.3895263671875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 301.5728759765625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.5971, loss_val: nan, pos_over_neg: 367.8941650390625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 576.0907592773438 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 613.2813720703125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.5798, loss_val: nan, pos_over_neg: 954.8058471679688 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6287, loss_val: nan, pos_over_neg: 475.4913635253906 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 410.0384826660156 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 330.986083984375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 594.120849609375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6018, loss_val: nan, pos_over_neg: 523.9130249023438 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 331.02325439453125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 399.91845703125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.6432, loss_val: nan, pos_over_neg: 361.6385498046875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 551.3438110351562 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.5944, loss_val: nan, pos_over_neg: 668.9276733398438 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.5569, loss_val: nan, pos_over_neg: 720.6205444335938 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.569, loss_val: nan, pos_over_neg: 699.989013671875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.556, loss_val: nan, pos_over_neg: 881.2301025390625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 790.2900390625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 460.8399353027344 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6215, loss_val: nan, pos_over_neg: 316.7055969238281 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.645, loss_val: nan, pos_over_neg: 281.02081298828125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.5665, loss_val: nan, pos_over_neg: 393.2181396484375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 469.7890319824219 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 477.76641845703125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.6048, loss_val: nan, pos_over_neg: 299.83221435546875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6343, loss_val: nan, pos_over_neg: 353.60711669921875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 551.1360473632812 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.6249, loss_val: nan, pos_over_neg: 412.7394714355469 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6389, loss_val: nan, pos_over_neg: 536.2366943359375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 490.6412048339844 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 517.365478515625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.5757, loss_val: nan, pos_over_neg: 511.1334533691406 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.6004, loss_val: nan, pos_over_neg: 597.8357543945312 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 627.8409423828125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 460.20257568359375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 408.8829040527344 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 347.9297180175781 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.6435, loss_val: nan, pos_over_neg: 411.7552795410156 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6192, loss_val: nan, pos_over_neg: 382.25946044921875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 417.18426513671875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 416.5752868652344 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 419.0497131347656 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.5951, loss_val: nan, pos_over_neg: 512.1455078125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 565.05029296875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.6128, loss_val: nan, pos_over_neg: 598.5869750976562 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.6468, loss_val: nan, pos_over_neg: 412.7079162597656 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.5967, loss_val: nan, pos_over_neg: 574.3038330078125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 405.51324462890625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 513.8051147460938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6212, loss_val: nan, pos_over_neg: 369.5149230957031 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.6522, loss_val: nan, pos_over_neg: 322.0588073730469 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 477.2853088378906 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.6319, loss_val: nan, pos_over_neg: 369.0472717285156 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.5884, loss_val: nan, pos_over_neg: 582.0729370117188 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 766.7876586914062 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.5912, loss_val: nan, pos_over_neg: 358.68048095703125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 391.7123107910156 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.565, loss_val: nan, pos_over_neg: 673.5528564453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6274, loss_val: nan, pos_over_neg: 578.749755859375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6332, loss_val: nan, pos_over_neg: 360.50592041015625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.5823, loss_val: nan, pos_over_neg: 352.76239013671875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 312.6076965332031 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 418.4912109375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 398.223876953125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 390.2375793457031 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 562.4989013671875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.5671, loss_val: nan, pos_over_neg: 450.858642578125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 529.1731567382812 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.6585, loss_val: nan, pos_over_neg: 570.0208129882812 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 399.3702087402344 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.5898, loss_val: nan, pos_over_neg: 518.0789794921875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.5643, loss_val: nan, pos_over_neg: 877.9991455078125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 637.64599609375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.5896, loss_val: nan, pos_over_neg: 526.3031005859375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6421, loss_val: nan, pos_over_neg: 370.44866943359375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.616, loss_val: nan, pos_over_neg: 323.0102844238281 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.5931, loss_val: nan, pos_over_neg: 647.0308227539062 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 569.6459350585938 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 374.9182434082031 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6205, loss_val: nan, pos_over_neg: 502.1618957519531 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 586.751708984375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 654.7249755859375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 628.9234008789062 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.5985, loss_val: nan, pos_over_neg: 415.65667724609375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.5567, loss_val: nan, pos_over_neg: 569.2384643554688 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.5829, loss_val: nan, pos_over_neg: 516.2050170898438 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.6026, loss_val: nan, pos_over_neg: 480.0684509277344 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.5984, loss_val: nan, pos_over_neg: 384.4616394042969 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.6007, loss_val: nan, pos_over_neg: 315.7066345214844 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 319.5154724121094 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.591, loss_val: nan, pos_over_neg: 407.49456787109375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 443.7919616699219 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 539.5677490234375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.6348, loss_val: nan, pos_over_neg: 356.5931396484375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.627, loss_val: nan, pos_over_neg: 550.87060546875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 500.00799560546875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 571.4279174804688 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 440.97540283203125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.6161, loss_val: nan, pos_over_neg: 536.7647705078125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 629.2056884765625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.5978, loss_val: nan, pos_over_neg: 517.7647705078125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.5935, loss_val: nan, pos_over_neg: 387.7069396972656 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 295.94744873046875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6513, loss_val: nan, pos_over_neg: 225.06680297851562 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.6107, loss_val: nan, pos_over_neg: 407.7244873046875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 535.9441528320312 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6139, loss_val: nan, pos_over_neg: 335.0950012207031 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.6041, loss_val: nan, pos_over_neg: 403.1915588378906 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.6136, loss_val: nan, pos_over_neg: 463.6498107910156 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 1373.607177734375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 1246.72314453125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.5553, loss_val: nan, pos_over_neg: 578.94970703125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 637.375732421875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6328, loss_val: nan, pos_over_neg: 403.9327087402344 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.6567, loss_val: nan, pos_over_neg: 212.37220764160156 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 501.6728820800781 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.6456, loss_val: nan, pos_over_neg: 352.3849182128906 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.6364, loss_val: nan, pos_over_neg: 340.51556396484375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6513, loss_val: nan, pos_over_neg: 252.94874572753906 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 511.01507568359375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 353.00067138671875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.586, loss_val: nan, pos_over_neg: 1123.1058349609375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.5825, loss_val: nan, pos_over_neg: 888.3311157226562 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 720.729736328125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.649, loss_val: nan, pos_over_neg: 380.0658264160156 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 367.1455383300781 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 308.69488525390625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 360.6445007324219 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6142, loss_val: nan, pos_over_neg: 340.5655822753906 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 242.64134216308594 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 540.4569091796875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 641.1559448242188 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.5732, loss_val: nan, pos_over_neg: 958.0252685546875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.5888, loss_val: nan, pos_over_neg: 485.16558837890625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 650.9165649414062 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.6498, loss_val: nan, pos_over_neg: 355.106201171875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 318.7453918457031 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 415.2531433105469 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.6489, loss_val: nan, pos_over_neg: 353.5950012207031 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.628, loss_val: nan, pos_over_neg: 309.363037109375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 342.81231689453125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 486.34423828125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 412.2310791015625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 513.7152099609375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 455.7264404296875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.6447, loss_val: nan, pos_over_neg: 272.2146301269531 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.6295, loss_val: nan, pos_over_neg: 492.3530578613281 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 527.2528686523438 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 498.3183898925781 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.6354, loss_val: nan, pos_over_neg: 355.760986328125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 534.2904052734375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6565, loss_val: nan, pos_over_neg: 382.625732421875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.5931, loss_val: nan, pos_over_neg: 510.8185729980469 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.5927, loss_val: nan, pos_over_neg: 459.5148620605469 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 582.6719970703125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 517.4710083007812 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 449.6574401855469 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.5788, loss_val: nan, pos_over_neg: 410.62139892578125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 440.1907043457031 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.5739, loss_val: nan, pos_over_neg: 410.37652587890625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 404.7724609375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 540.682861328125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.555, loss_val: nan, pos_over_neg: 748.5260009765625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 447.3749694824219 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 365.5440368652344 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 775.2713623046875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.5749, loss_val: nan, pos_over_neg: 538.4843139648438 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 487.50604248046875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 788.1669921875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.6375, loss_val: nan, pos_over_neg: 422.8885498046875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.5763, loss_val: nan, pos_over_neg: 468.3065490722656 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.5917, loss_val: nan, pos_over_neg: 568.2056884765625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.5643, loss_val: nan, pos_over_neg: 635.0050048828125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.5753, loss_val: nan, pos_over_neg: 361.45721435546875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.5572, loss_val: nan, pos_over_neg: 531.3053588867188 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.607, loss_val: nan, pos_over_neg: 506.3669738769531 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6022, loss_val: nan, pos_over_neg: 391.99761962890625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 321.7508850097656 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 366.5316467285156 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 270.63360595703125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 357.4353942871094 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 438.1977844238281 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.6302, loss_val: nan, pos_over_neg: 400.05462646484375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 506.2408752441406 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.6225, loss_val: nan, pos_over_neg: 496.0184020996094 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.6313, loss_val: nan, pos_over_neg: 358.5367126464844 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.5667, loss_val: nan, pos_over_neg: 558.7266845703125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.5958, loss_val: nan, pos_over_neg: 543.503662109375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 457.2147521972656 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.654, loss_val: nan, pos_over_neg: 283.3731384277344 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 426.334228515625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 338.1883850097656 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6211, loss_val: nan, pos_over_neg: 325.8487548828125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 404.6927185058594 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 468.70367431640625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 502.5230407714844 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.563, loss_val: nan, pos_over_neg: 547.7131958007812 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 308.9444274902344 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 325.37103271484375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.6336, loss_val: nan, pos_over_neg: 483.565185546875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.5939, loss_val: nan, pos_over_neg: 377.7906494140625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 456.6863098144531 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 498.1541442871094 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 327.0246276855469 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 425.2633361816406 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 523.3244018554688 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.617, loss_val: nan, pos_over_neg: 338.959716796875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.6075, loss_val: nan, pos_over_neg: 500.46295166015625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6055, loss_val: nan, pos_over_neg: 475.0614013671875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.6073, loss_val: nan, pos_over_neg: 373.503662109375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 377.1248779296875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 515.1341552734375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 415.3112487792969 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.5915, loss_val: nan, pos_over_neg: 541.150390625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6329, loss_val: nan, pos_over_neg: 354.45001220703125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 327.25396728515625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.654, loss_val: nan, pos_over_neg: 272.0053405761719 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.5594, loss_val: nan, pos_over_neg: 399.1725769042969 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.6239, loss_val: nan, pos_over_neg: 403.1856994628906 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.6331, loss_val: nan, pos_over_neg: 368.788330078125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6424, loss_val: nan, pos_over_neg: 391.5538635253906 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.6269, loss_val: nan, pos_over_neg: 341.4332580566406 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.6413, loss_val: nan, pos_over_neg: 342.0009460449219 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 620.7554931640625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 516.7451782226562 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.5781, loss_val: nan, pos_over_neg: 916.3233032226562 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/300000 [3:16:22<75501:45:04, 906.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 364.817626953125 lr: 0.00031623\n",
      "epoch: 13\n",
      "Iter: 0/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 568.4603881835938 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 277.5488586425781 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.6259, loss_val: nan, pos_over_neg: 352.3114929199219 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 394.5813903808594 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6142, loss_val: nan, pos_over_neg: 559.482666015625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.6209, loss_val: nan, pos_over_neg: 382.8017272949219 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.5693, loss_val: nan, pos_over_neg: 662.4328002929688 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.6117, loss_val: nan, pos_over_neg: 387.96905517578125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.5906, loss_val: nan, pos_over_neg: 522.381103515625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6352, loss_val: nan, pos_over_neg: 340.78436279296875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6587, loss_val: nan, pos_over_neg: 373.67559814453125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.5797, loss_val: nan, pos_over_neg: 635.6013793945312 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 450.1226501464844 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.5753, loss_val: nan, pos_over_neg: 407.2485046386719 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 535.2337036132812 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.5795, loss_val: nan, pos_over_neg: 385.72332763671875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.5905, loss_val: nan, pos_over_neg: 413.55096435546875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.5758, loss_val: nan, pos_over_neg: 388.89752197265625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 504.17022705078125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 443.2720642089844 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6387, loss_val: nan, pos_over_neg: 324.1285400390625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 755.1389770507812 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 668.7586059570312 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 441.48614501953125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6454, loss_val: nan, pos_over_neg: 438.0556335449219 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.638, loss_val: nan, pos_over_neg: 291.5384216308594 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6568, loss_val: nan, pos_over_neg: 391.4316711425781 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 398.3719482421875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6125, loss_val: nan, pos_over_neg: 696.1026000976562 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.6196, loss_val: nan, pos_over_neg: 365.7835998535156 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 322.5165710449219 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 484.5045471191406 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 712.996337890625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6286, loss_val: nan, pos_over_neg: 522.3463745117188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.5886, loss_val: nan, pos_over_neg: 410.0797119140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 417.2554016113281 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6049, loss_val: nan, pos_over_neg: 510.7898864746094 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.5797, loss_val: nan, pos_over_neg: 546.2024536132812 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 440.6600646972656 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.561, loss_val: nan, pos_over_neg: 427.00732421875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.5609, loss_val: nan, pos_over_neg: 456.377685546875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 464.29217529296875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 504.2650146484375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.6367, loss_val: nan, pos_over_neg: 392.01812744140625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 476.828125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6085, loss_val: nan, pos_over_neg: 437.0576171875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.6337, loss_val: nan, pos_over_neg: 467.3155212402344 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.5944, loss_val: nan, pos_over_neg: 509.4796142578125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 1194.9786376953125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 772.9779663085938 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.5628, loss_val: nan, pos_over_neg: 553.7919311523438 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 604.1046142578125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 390.12176513671875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 314.8165283203125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 441.8231201171875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.6548, loss_val: nan, pos_over_neg: 424.5884704589844 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 735.0374755859375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.5797, loss_val: nan, pos_over_neg: 398.79132080078125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.5606, loss_val: nan, pos_over_neg: 485.27923583984375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 491.3624572753906 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 548.6888427734375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.6345, loss_val: nan, pos_over_neg: 295.5760192871094 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 351.316162109375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6516, loss_val: nan, pos_over_neg: 221.9211883544922 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.6557, loss_val: nan, pos_over_neg: 277.230224609375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.6382, loss_val: nan, pos_over_neg: 270.76214599609375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.6359, loss_val: nan, pos_over_neg: 329.3602294921875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 344.32415771484375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.5988, loss_val: nan, pos_over_neg: 408.7000732421875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 552.3060913085938 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 479.2467346191406 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 714.3414306640625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 548.5733032226562 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.6372, loss_val: nan, pos_over_neg: 394.953125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 422.8772888183594 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 397.9066467285156 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.6337, loss_val: nan, pos_over_neg: 532.3296508789062 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.639, loss_val: nan, pos_over_neg: 427.4210205078125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.566, loss_val: nan, pos_over_neg: 449.76947021484375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6152, loss_val: nan, pos_over_neg: 363.1335754394531 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.5746, loss_val: nan, pos_over_neg: 1023.2465209960938 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.5472, loss_val: nan, pos_over_neg: 429.3419494628906 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.6465, loss_val: nan, pos_over_neg: 292.0098571777344 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 484.3602600097656 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 449.5955505371094 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.644, loss_val: nan, pos_over_neg: 358.32513427734375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.6006, loss_val: nan, pos_over_neg: 415.0681457519531 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.6485, loss_val: nan, pos_over_neg: 671.9490966796875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.6184, loss_val: nan, pos_over_neg: 562.991943359375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.5703, loss_val: nan, pos_over_neg: 503.6463623046875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.5771, loss_val: nan, pos_over_neg: 523.1724853515625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6007, loss_val: nan, pos_over_neg: 429.5360412597656 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.568, loss_val: nan, pos_over_neg: 648.2054443359375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 408.2521667480469 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6521, loss_val: nan, pos_over_neg: 348.0746765136719 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.5575, loss_val: nan, pos_over_neg: 620.9627075195312 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 405.0306396484375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.6161, loss_val: nan, pos_over_neg: 297.9912109375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 478.0673522949219 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 466.78961181640625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 483.89996337890625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.657, loss_val: nan, pos_over_neg: 311.30303955078125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 423.8033142089844 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 488.6545104980469 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.6161, loss_val: nan, pos_over_neg: 495.70391845703125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.5993, loss_val: nan, pos_over_neg: 512.0761108398438 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 519.8206176757812 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6066, loss_val: nan, pos_over_neg: 382.88525390625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 326.97991943359375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.6006, loss_val: nan, pos_over_neg: 527.4044799804688 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 470.4424743652344 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 365.9757080078125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.5839, loss_val: nan, pos_over_neg: 333.0797119140625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 521.8170166015625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.5892, loss_val: nan, pos_over_neg: 460.9822082519531 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.606, loss_val: nan, pos_over_neg: 510.5673522949219 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.5756, loss_val: nan, pos_over_neg: 324.3511047363281 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 275.4648742675781 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.6084, loss_val: nan, pos_over_neg: 363.23614501953125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.5529, loss_val: nan, pos_over_neg: 688.8972778320312 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.6084, loss_val: nan, pos_over_neg: 420.77850341796875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.6376, loss_val: nan, pos_over_neg: 414.3514709472656 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 506.9333801269531 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.5542, loss_val: nan, pos_over_neg: 467.0385437011719 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.6483, loss_val: nan, pos_over_neg: 322.5121154785156 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 474.99700927734375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.5985, loss_val: nan, pos_over_neg: 311.4035949707031 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 457.95257568359375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.6213, loss_val: nan, pos_over_neg: 342.1445617675781 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.5899, loss_val: nan, pos_over_neg: 435.2759094238281 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.6445, loss_val: nan, pos_over_neg: 416.2156982421875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 574.5675659179688 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 420.5086975097656 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.5712, loss_val: nan, pos_over_neg: 426.0910339355469 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 486.7877197265625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6438, loss_val: nan, pos_over_neg: 314.5995178222656 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 392.2751159667969 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.5771, loss_val: nan, pos_over_neg: 494.3841857910156 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 309.9134826660156 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 422.76287841796875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.5967, loss_val: nan, pos_over_neg: 303.45806884765625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.6161, loss_val: nan, pos_over_neg: 492.68109130859375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6201, loss_val: nan, pos_over_neg: 425.39044189453125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.646, loss_val: nan, pos_over_neg: 350.5541076660156 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.5572, loss_val: nan, pos_over_neg: 622.5159301757812 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 358.2724609375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.6188, loss_val: nan, pos_over_neg: 368.72784423828125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.6069, loss_val: nan, pos_over_neg: 437.5570983886719 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.5356, loss_val: nan, pos_over_neg: 545.0112915039062 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 316.208251953125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.582, loss_val: nan, pos_over_neg: 448.6270446777344 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.609, loss_val: nan, pos_over_neg: 347.6869201660156 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 287.955810546875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6186, loss_val: nan, pos_over_neg: 410.8930358886719 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 872.9547119140625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6005, loss_val: nan, pos_over_neg: 534.1148681640625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6384, loss_val: nan, pos_over_neg: 692.3875732421875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 373.0386047363281 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 537.341552734375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6128, loss_val: nan, pos_over_neg: 503.78082275390625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 453.0951232910156 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.5291, loss_val: nan, pos_over_neg: 680.198486328125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 339.5646667480469 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 299.36175537109375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 607.1401977539062 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.6439, loss_val: nan, pos_over_neg: 358.93743896484375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 461.38702392578125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.5722, loss_val: nan, pos_over_neg: 497.25433349609375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 608.7613525390625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 504.25177001953125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 363.0108642578125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 407.45281982421875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 819.7110595703125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 560.7158813476562 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.6452, loss_val: nan, pos_over_neg: 357.0190124511719 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.5681, loss_val: nan, pos_over_neg: 461.7667541503906 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6244, loss_val: nan, pos_over_neg: 572.0032958984375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 466.00738525390625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 499.8828430175781 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 413.8938293457031 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.5781, loss_val: nan, pos_over_neg: 473.3283386230469 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.6371, loss_val: nan, pos_over_neg: 276.88385009765625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.5912, loss_val: nan, pos_over_neg: 563.2139282226562 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 494.58935546875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 468.5768737792969 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 613.5265502929688 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 376.63946533203125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 280.4428405761719 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 476.0406799316406 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.5739, loss_val: nan, pos_over_neg: 427.73797607421875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 389.47906494140625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.5783, loss_val: nan, pos_over_neg: 527.8805541992188 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 489.33203125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.5889, loss_val: nan, pos_over_neg: 666.2137451171875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.606, loss_val: nan, pos_over_neg: 367.35894775390625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 554.8333740234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.6278, loss_val: nan, pos_over_neg: 401.558349609375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 717.508544921875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 388.6728515625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 377.20697021484375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6233, loss_val: nan, pos_over_neg: 334.3821716308594 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.5872, loss_val: nan, pos_over_neg: 407.017333984375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6599, loss_val: nan, pos_over_neg: 336.7884826660156 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.5843, loss_val: nan, pos_over_neg: 670.0720825195312 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.604, loss_val: nan, pos_over_neg: 709.9649047851562 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 836.2303466796875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 696.1119995117188 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 309.6072692871094 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 502.5224304199219 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 371.1513671875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6214, loss_val: nan, pos_over_neg: 331.1474304199219 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 526.5328979492188 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.5946, loss_val: nan, pos_over_neg: 395.5594482421875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.6186, loss_val: nan, pos_over_neg: 395.91064453125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 638.8712768554688 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.5789, loss_val: nan, pos_over_neg: 719.09326171875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.5662, loss_val: nan, pos_over_neg: 577.8272705078125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.5925, loss_val: nan, pos_over_neg: 392.1966247558594 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.6369, loss_val: nan, pos_over_neg: 348.17999267578125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.5656, loss_val: nan, pos_over_neg: 458.9888610839844 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.5797, loss_val: nan, pos_over_neg: 434.2184143066406 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 595.963623046875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 523.4221801757812 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6056, loss_val: nan, pos_over_neg: 365.5240173339844 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 283.9831237792969 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 326.7276611328125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6056, loss_val: nan, pos_over_neg: 476.3917541503906 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.5683, loss_val: nan, pos_over_neg: 442.6252136230469 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 488.9873962402344 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6023, loss_val: nan, pos_over_neg: 324.4705505371094 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 303.1866760253906 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 384.1795349121094 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 422.12518310546875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 343.83465576171875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 380.1924743652344 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.566, loss_val: nan, pos_over_neg: 660.361328125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 394.7799987792969 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 551.69677734375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.6192, loss_val: nan, pos_over_neg: 449.58306884765625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 374.81719970703125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.5918, loss_val: nan, pos_over_neg: 464.18377685546875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 386.7779541015625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 397.69720458984375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 472.90185546875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.6152, loss_val: nan, pos_over_neg: 533.654052734375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6295, loss_val: nan, pos_over_neg: 370.7182922363281 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 490.5511169433594 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.5872, loss_val: nan, pos_over_neg: 485.6231384277344 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 415.6459045410156 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.6073, loss_val: nan, pos_over_neg: 429.8179626464844 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.5874, loss_val: nan, pos_over_neg: 397.1934509277344 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 388.60369873046875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6056, loss_val: nan, pos_over_neg: 429.1977233886719 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 745.8848876953125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.5627, loss_val: nan, pos_over_neg: 1142.4022216796875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.5668, loss_val: nan, pos_over_neg: 651.802490234375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.5917, loss_val: nan, pos_over_neg: 458.5150451660156 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6186, loss_val: nan, pos_over_neg: 422.9270324707031 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.5766, loss_val: nan, pos_over_neg: 398.67864990234375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.5969, loss_val: nan, pos_over_neg: 300.5233154296875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.5896, loss_val: nan, pos_over_neg: 355.1036376953125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 324.71441650390625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 310.19927978515625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.5826, loss_val: nan, pos_over_neg: 491.79022216796875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 475.31658935546875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.613, loss_val: nan, pos_over_neg: 299.8541259765625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.602, loss_val: nan, pos_over_neg: 582.0003662109375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.6159, loss_val: nan, pos_over_neg: 393.85589599609375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.6259, loss_val: nan, pos_over_neg: 560.9336547851562 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.5987, loss_val: nan, pos_over_neg: 558.7298583984375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 389.9917297363281 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.5947, loss_val: nan, pos_over_neg: 505.5133056640625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.576, loss_val: nan, pos_over_neg: 626.2006225585938 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 391.76220703125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 308.43475341796875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.5872, loss_val: nan, pos_over_neg: 357.85882568359375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 340.46356201171875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 329.92919921875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5991, loss_val: nan, pos_over_neg: 263.70623779296875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 252.4388427734375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6444, loss_val: nan, pos_over_neg: 456.5590515136719 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 516.0250244140625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 723.9561157226562 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.641, loss_val: nan, pos_over_neg: 531.8550415039062 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 522.1406860351562 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6237, loss_val: nan, pos_over_neg: 322.7792053222656 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.5656, loss_val: nan, pos_over_neg: 502.40606689453125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 522.2528686523438 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 367.96612548828125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 406.3791198730469 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 334.5081481933594 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.5752, loss_val: nan, pos_over_neg: 530.7344970703125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 463.48712158203125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.5605, loss_val: nan, pos_over_neg: 391.75726318359375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 534.7240600585938 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 483.02899169921875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 538.410888671875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 384.2113952636719 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.6199, loss_val: nan, pos_over_neg: 309.84478759765625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 399.38671875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 469.4768981933594 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.5889, loss_val: nan, pos_over_neg: 298.816650390625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.6308, loss_val: nan, pos_over_neg: 295.5714111328125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6222, loss_val: nan, pos_over_neg: 312.58563232421875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 584.4471435546875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.6781, loss_val: nan, pos_over_neg: 313.36163330078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.6364, loss_val: nan, pos_over_neg: 475.26190185546875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.5786, loss_val: nan, pos_over_neg: 757.0125732421875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.6117, loss_val: nan, pos_over_neg: 428.6537780761719 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 351.8068542480469 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.6156, loss_val: nan, pos_over_neg: 361.9652099609375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.5823, loss_val: nan, pos_over_neg: 504.5206604003906 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 434.2597961425781 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.5888, loss_val: nan, pos_over_neg: 406.2261047363281 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 355.0549011230469 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 414.9678955078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 382.4780578613281 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 460.0988464355469 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 433.89898681640625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 582.2181396484375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 559.1427612304688 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.5775, loss_val: nan, pos_over_neg: 536.3135375976562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 395.1533508300781 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 326.2023620605469 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 359.97479248046875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 330.3129577636719 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 366.5059814453125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 459.9478759765625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 395.60601806640625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 324.96844482421875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 728.1647338867188 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6026, loss_val: nan, pos_over_neg: 825.8506469726562 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 894.6296997070312 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.5985, loss_val: nan, pos_over_neg: 317.2027282714844 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 300.1913146972656 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6075, loss_val: nan, pos_over_neg: 325.3546142578125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.6256, loss_val: nan, pos_over_neg: 251.318115234375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.6096, loss_val: nan, pos_over_neg: 287.127685546875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 662.8043823242188 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6157, loss_val: nan, pos_over_neg: 502.1379089355469 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.6586, loss_val: nan, pos_over_neg: 288.6330871582031 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 538.5195922851562 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 798.472900390625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 541.142333984375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.6163, loss_val: nan, pos_over_neg: 437.748779296875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 390.8760986328125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6177, loss_val: nan, pos_over_neg: 375.9619140625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 520.0739135742188 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 606.7091064453125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.6415, loss_val: nan, pos_over_neg: 397.5328063964844 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.5915, loss_val: nan, pos_over_neg: 453.2884826660156 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.6005, loss_val: nan, pos_over_neg: 514.0380859375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.5898, loss_val: nan, pos_over_neg: 671.9857788085938 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 482.7894287109375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 528.1453247070312 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 353.13360595703125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.5817, loss_val: nan, pos_over_neg: 532.5465087890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 457.13623046875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 491.806396484375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.6056, loss_val: nan, pos_over_neg: 321.5755615234375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 294.3114929199219 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 404.79095458984375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 615.3795166015625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.5824, loss_val: nan, pos_over_neg: 501.2208557128906 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.5963, loss_val: nan, pos_over_neg: 522.8731689453125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 409.8079528808594 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 358.48382568359375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6196, loss_val: nan, pos_over_neg: 351.5084533691406 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.5451, loss_val: nan, pos_over_neg: 608.1616821289062 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.5516, loss_val: nan, pos_over_neg: 1048.932373046875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.6298, loss_val: nan, pos_over_neg: 450.6410217285156 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.6165, loss_val: nan, pos_over_neg: 345.3674011230469 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.5808, loss_val: nan, pos_over_neg: 555.31494140625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.5703, loss_val: nan, pos_over_neg: 451.89739990234375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.5617, loss_val: nan, pos_over_neg: 531.64013671875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.5963, loss_val: nan, pos_over_neg: 448.4297790527344 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.589, loss_val: nan, pos_over_neg: 477.5322570800781 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.6407, loss_val: nan, pos_over_neg: 585.9442749023438 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 730.79541015625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.5622, loss_val: nan, pos_over_neg: 795.90234375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.5693, loss_val: nan, pos_over_neg: 505.51318359375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.5802, loss_val: nan, pos_over_neg: 755.4295043945312 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 665.3966674804688 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 342.96002197265625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.6579, loss_val: nan, pos_over_neg: 292.2552185058594 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 501.45458984375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.6026, loss_val: nan, pos_over_neg: 398.7030334472656 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 805.8154296875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.5783, loss_val: nan, pos_over_neg: 1022.1211547851562 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.6173, loss_val: nan, pos_over_neg: 636.5101928710938 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.6359, loss_val: nan, pos_over_neg: 511.83587646484375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 704.045166015625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 439.2647705078125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6379, loss_val: nan, pos_over_neg: 524.14208984375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.5946, loss_val: nan, pos_over_neg: 503.1048889160156 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6436, loss_val: nan, pos_over_neg: 405.0463562011719 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 361.233154296875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.5657, loss_val: nan, pos_over_neg: 529.7211303710938 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.5423, loss_val: nan, pos_over_neg: 852.1470336914062 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.6199, loss_val: nan, pos_over_neg: 478.9173278808594 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.5674, loss_val: nan, pos_over_neg: 743.2474365234375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 540.6444091796875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.6208, loss_val: nan, pos_over_neg: 341.504150390625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.5462, loss_val: nan, pos_over_neg: 690.447509765625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.6315, loss_val: nan, pos_over_neg: 321.0633239746094 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 505.86932373046875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 346.70147705078125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 332.7297058105469 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 506.6634521484375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.5802, loss_val: nan, pos_over_neg: 642.6674194335938 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.5993, loss_val: nan, pos_over_neg: 299.7708435058594 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 300.5706481933594 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.5619, loss_val: nan, pos_over_neg: 538.712890625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 409.3753356933594 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.6316, loss_val: nan, pos_over_neg: 325.79400634765625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.5438, loss_val: nan, pos_over_neg: 617.55322265625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 588.0259399414062 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.6101, loss_val: nan, pos_over_neg: 416.67913818359375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 445.0313720703125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 432.4701843261719 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 446.790771484375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 342.0982360839844 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 464.5330810546875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.5862, loss_val: nan, pos_over_neg: 387.8259582519531 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 450.4201354980469 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 458.11846923828125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 556.3130493164062 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6037, loss_val: nan, pos_over_neg: 415.8536682128906 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 460.11578369140625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 468.1769714355469 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.628, loss_val: nan, pos_over_neg: 524.3294677734375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 501.5807800292969 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.5713, loss_val: nan, pos_over_neg: 553.2107543945312 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 891.4087524414062 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 783.7839965820312 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.6131, loss_val: nan, pos_over_neg: 393.5416564941406 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.5537, loss_val: nan, pos_over_neg: 634.6453247070312 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.573, loss_val: nan, pos_over_neg: 474.1117858886719 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 603.438232421875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 615.8629150390625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 411.41534423828125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 329.274169921875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.6232, loss_val: nan, pos_over_neg: 289.9943542480469 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.5957, loss_val: nan, pos_over_neg: 501.4039001464844 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 654.3602905273438 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 622.4688720703125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 413.57452392578125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.5505, loss_val: nan, pos_over_neg: 829.2698364257812 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.6257, loss_val: nan, pos_over_neg: 317.9101867675781 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.5978, loss_val: nan, pos_over_neg: 403.09930419921875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 386.8249816894531 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 406.8597106933594 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.5818, loss_val: nan, pos_over_neg: 596.9362182617188 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 423.36328125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.5505, loss_val: nan, pos_over_neg: 512.2970581054688 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 465.5021057128906 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.5784, loss_val: nan, pos_over_neg: 541.9490356445312 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.5841, loss_val: nan, pos_over_neg: 681.5421752929688 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.5795, loss_val: nan, pos_over_neg: 493.54888916015625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 341.4161682128906 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 491.5986633300781 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6563, loss_val: nan, pos_over_neg: 224.41339111328125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 371.02740478515625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 369.7065124511719 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.6273, loss_val: nan, pos_over_neg: 333.95770263671875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 470.3146667480469 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 655.6987915039062 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.562, loss_val: nan, pos_over_neg: 508.338623046875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.6096, loss_val: nan, pos_over_neg: 534.203857421875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6365, loss_val: nan, pos_over_neg: 869.8104248046875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.6347, loss_val: nan, pos_over_neg: 378.28802490234375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 418.5487365722656 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 607.0840454101562 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 341.3632507324219 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 870.6529541015625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 435.9573059082031 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 299.57550048828125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.5771, loss_val: nan, pos_over_neg: 640.4591064453125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 945.8450317382812 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 437.4402160644531 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.6084, loss_val: nan, pos_over_neg: 439.9120178222656 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.5609, loss_val: nan, pos_over_neg: 550.2310791015625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 515.2955932617188 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 1021.9832153320312 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.5835, loss_val: nan, pos_over_neg: 629.0390014648438 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.5442, loss_val: nan, pos_over_neg: 639.4478759765625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 530.8158569335938 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6439, loss_val: nan, pos_over_neg: 314.1802062988281 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.5624, loss_val: nan, pos_over_neg: 679.2868041992188 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 341.26605224609375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.5984, loss_val: nan, pos_over_neg: 487.6949768066406 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.5802, loss_val: nan, pos_over_neg: 755.24658203125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.5757, loss_val: nan, pos_over_neg: 556.6343383789062 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 318.93829345703125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 430.5102233886719 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.6433, loss_val: nan, pos_over_neg: 473.3515930175781 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.5504, loss_val: nan, pos_over_neg: 1028.0009765625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.5914, loss_val: nan, pos_over_neg: 470.8542785644531 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.583, loss_val: nan, pos_over_neg: 328.9881286621094 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6071, loss_val: nan, pos_over_neg: 347.69830322265625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.6315, loss_val: nan, pos_over_neg: 238.26231384277344 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 310.23565673828125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 468.1016845703125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.57, loss_val: nan, pos_over_neg: 488.6211242675781 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 418.18487548828125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.5667, loss_val: nan, pos_over_neg: 442.57275390625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.5681, loss_val: nan, pos_over_neg: 704.7195434570312 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.5601, loss_val: nan, pos_over_neg: 797.8211059570312 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 353.93389892578125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 303.5616760253906 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.5683, loss_val: nan, pos_over_neg: 391.3348083496094 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 281.6496887207031 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.6173, loss_val: nan, pos_over_neg: 322.8729248046875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 380.0050354003906 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.5312, loss_val: nan, pos_over_neg: 499.4768981933594 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6074, loss_val: nan, pos_over_neg: 790.3367309570312 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 459.7826843261719 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 351.1107177734375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 383.4846496582031 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 425.67755126953125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 378.5968322753906 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.6141, loss_val: nan, pos_over_neg: 308.87481689453125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 606.0955200195312 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 571.7446899414062 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.618, loss_val: nan, pos_over_neg: 678.2855224609375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.6004, loss_val: nan, pos_over_neg: 496.6976623535156 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.5901, loss_val: nan, pos_over_neg: 622.2601318359375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.5641, loss_val: nan, pos_over_neg: 508.5787353515625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 484.76776123046875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 683.1715698242188 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 448.1475830078125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 306.0130310058594 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.6136, loss_val: nan, pos_over_neg: 325.5814514160156 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 523.1328735351562 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.5647, loss_val: nan, pos_over_neg: 609.4229736328125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.5743, loss_val: nan, pos_over_neg: 1387.165283203125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 673.3933715820312 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 752.3599853515625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.6567, loss_val: nan, pos_over_neg: 320.7659912109375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 387.1454162597656 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.5772, loss_val: nan, pos_over_neg: 641.4130859375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 540.0421142578125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 717.747802734375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.6082, loss_val: nan, pos_over_neg: 343.6925048828125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 418.39581298828125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 384.27471923828125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.5736, loss_val: nan, pos_over_neg: 390.0123596191406 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 302.6697998046875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 358.8635559082031 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 401.7604064941406 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 445.818603515625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 506.3684997558594 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 360.0026550292969 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 443.3706970214844 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.574, loss_val: nan, pos_over_neg: 460.20123291015625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.5813, loss_val: nan, pos_over_neg: 455.9745788574219 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 409.7944641113281 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 358.9542541503906 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 588.2288818359375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.6315, loss_val: nan, pos_over_neg: 311.5535583496094 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.578, loss_val: nan, pos_over_neg: 453.0249938964844 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 351.5218505859375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 316.37884521484375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 406.5556945800781 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.5727, loss_val: nan, pos_over_neg: 597.140380859375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6164, loss_val: nan, pos_over_neg: 522.5917358398438 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 454.65887451171875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 677.031982421875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 436.95294189453125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 496.9246520996094 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.574, loss_val: nan, pos_over_neg: 388.09808349609375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 319.791259765625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.6183, loss_val: nan, pos_over_neg: 342.9151611328125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 596.925537109375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.6408, loss_val: nan, pos_over_neg: 339.6353759765625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 296.28228759765625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 468.0032958984375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 392.4453430175781 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.6487, loss_val: nan, pos_over_neg: 479.7557373046875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.5645, loss_val: nan, pos_over_neg: 703.439208984375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.5969, loss_val: nan, pos_over_neg: 515.9638671875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.5533, loss_val: nan, pos_over_neg: 774.74951171875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 813.9788818359375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 391.14361572265625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 503.7811279296875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.5709, loss_val: nan, pos_over_neg: 534.0375366210938 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6098, loss_val: nan, pos_over_neg: 355.8898010253906 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 651.6648559570312 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 342.85760498046875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 287.15771484375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.5926, loss_val: nan, pos_over_neg: 406.2947998046875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 465.3721618652344 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 867.9664916992188 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.601, loss_val: nan, pos_over_neg: 549.4830932617188 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 611.7778930664062 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.6261, loss_val: nan, pos_over_neg: 489.4521179199219 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.633, loss_val: nan, pos_over_neg: 344.88677978515625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 378.7939453125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 453.3196105957031 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.5969, loss_val: nan, pos_over_neg: 392.8883972167969 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 490.9194641113281 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.6217, loss_val: nan, pos_over_neg: 394.6790771484375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.6157, loss_val: nan, pos_over_neg: 457.0228576660156 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.5698, loss_val: nan, pos_over_neg: 514.8024291992188 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 578.7890014648438 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 505.72589111328125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.5947, loss_val: nan, pos_over_neg: 436.90667724609375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.5727, loss_val: nan, pos_over_neg: 571.9639892578125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 462.009765625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 305.5261535644531 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.6397, loss_val: nan, pos_over_neg: 232.43832397460938 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.5888, loss_val: nan, pos_over_neg: 299.3021545410156 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 502.8670349121094 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.5849, loss_val: nan, pos_over_neg: 641.3631591796875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 723.660888671875 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 617.7677612304688 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 613.6668701171875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 570.183349609375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.631, loss_val: nan, pos_over_neg: 381.5455017089844 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.6122, loss_val: nan, pos_over_neg: 278.89276123046875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 347.6514892578125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 425.4403991699219 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 431.73651123046875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.635, loss_val: nan, pos_over_neg: 314.3815002441406 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6398, loss_val: nan, pos_over_neg: 340.0078430175781 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.5662, loss_val: nan, pos_over_neg: 796.8424072265625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.6382, loss_val: nan, pos_over_neg: 333.9794006347656 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.6263, loss_val: nan, pos_over_neg: 320.1960754394531 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 511.1328125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 457.1133728027344 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6167, loss_val: nan, pos_over_neg: 485.2217102050781 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 337.9242248535156 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 327.3872375488281 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6198, loss_val: nan, pos_over_neg: 412.7195739746094 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.5443, loss_val: nan, pos_over_neg: 940.545166015625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.5422, loss_val: nan, pos_over_neg: 643.462158203125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6321, loss_val: nan, pos_over_neg: 326.0317077636719 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.5495, loss_val: nan, pos_over_neg: 583.7797241210938 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6189, loss_val: nan, pos_over_neg: 327.85369873046875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.5774, loss_val: nan, pos_over_neg: 338.9430236816406 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.6041, loss_val: nan, pos_over_neg: 357.3772277832031 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 481.6232604980469 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.5506, loss_val: nan, pos_over_neg: 1017.0087280273438 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.5678, loss_val: nan, pos_over_neg: 525.103515625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 371.4372253417969 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.61, loss_val: nan, pos_over_neg: 415.7478942871094 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.5968, loss_val: nan, pos_over_neg: 497.7117004394531 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 680.5940551757812 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 611.7207641601562 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.5901, loss_val: nan, pos_over_neg: 446.6154479980469 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.5786, loss_val: nan, pos_over_neg: 468.02264404296875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.5569, loss_val: nan, pos_over_neg: 440.2762756347656 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.5925, loss_val: nan, pos_over_neg: 575.7013549804688 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 335.51666259765625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 474.4394836425781 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 386.68170166015625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.6057, loss_val: nan, pos_over_neg: 393.99822998046875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.613, loss_val: nan, pos_over_neg: 628.37841796875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 410.0115966796875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 876.2570190429688 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6037, loss_val: nan, pos_over_neg: 513.9915161132812 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 479.9892578125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 318.1739196777344 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 736.4996337890625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 621.68701171875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 923.4324951171875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 1020.1288452148438 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 423.2275390625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.6093, loss_val: nan, pos_over_neg: 386.86737060546875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.6211, loss_val: nan, pos_over_neg: 340.9874267578125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6122, loss_val: nan, pos_over_neg: 375.2359313964844 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 430.0441589355469 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 287.19744873046875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.6401, loss_val: nan, pos_over_neg: 300.77801513671875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 440.3530578613281 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.6178, loss_val: nan, pos_over_neg: 520.4863891601562 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6222, loss_val: nan, pos_over_neg: 854.3104248046875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.6144, loss_val: nan, pos_over_neg: 406.8092041015625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.5608, loss_val: nan, pos_over_neg: 728.6527709960938 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 419.645751953125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 733.0507202148438 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6435, loss_val: nan, pos_over_neg: 393.3319091796875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 534.64453125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.5783, loss_val: nan, pos_over_neg: 487.4207763671875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.5577, loss_val: nan, pos_over_neg: 271.4330749511719 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 470.7651672363281 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.6323, loss_val: nan, pos_over_neg: 314.77947998046875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 559.9853515625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.5947, loss_val: nan, pos_over_neg: 470.006103515625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 349.024169921875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 521.4336547851562 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 520.6417236328125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.6183, loss_val: nan, pos_over_neg: 327.7066345214844 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.557, loss_val: nan, pos_over_neg: 462.8520202636719 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/300000 [3:31:27<75481:51:14, 905.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 722.7366943359375 lr: 0.00031623\n",
      "epoch: 14\n",
      "Iter: 0/695, loss_train: 3.55, loss_val: nan, pos_over_neg: 706.462646484375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.6174, loss_val: nan, pos_over_neg: 397.46685791015625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.58, loss_val: nan, pos_over_neg: 349.9332275390625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.5711, loss_val: nan, pos_over_neg: 513.1766357421875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 372.22979736328125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 382.0314025878906 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 298.2902526855469 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 383.3020324707031 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6402, loss_val: nan, pos_over_neg: 256.89453125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.5796, loss_val: nan, pos_over_neg: 583.4702758789062 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 512.2279663085938 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 895.3433837890625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 631.1363525390625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.5825, loss_val: nan, pos_over_neg: 658.1735229492188 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 400.0062561035156 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.6226, loss_val: nan, pos_over_neg: 406.89935302734375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.5484, loss_val: nan, pos_over_neg: 549.3263549804688 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 343.8901062011719 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.5641, loss_val: nan, pos_over_neg: 457.75872802734375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 460.91046142578125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 666.9894409179688 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 526.367919921875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.636, loss_val: nan, pos_over_neg: 396.2359619140625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.6693, loss_val: nan, pos_over_neg: 295.3187561035156 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 361.028564453125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.5997, loss_val: nan, pos_over_neg: 506.3457336425781 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.5766, loss_val: nan, pos_over_neg: 447.1620178222656 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 477.75018310546875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 344.52264404296875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.6357, loss_val: nan, pos_over_neg: 352.6816711425781 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 393.5285339355469 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.575, loss_val: nan, pos_over_neg: 506.739501953125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.5576, loss_val: nan, pos_over_neg: 619.927001953125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.6049, loss_val: nan, pos_over_neg: 363.1040954589844 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.6122, loss_val: nan, pos_over_neg: 344.9661560058594 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.5552, loss_val: nan, pos_over_neg: 658.7843017578125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 582.83251953125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 336.3184814453125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.639, loss_val: nan, pos_over_neg: 327.5985107421875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.5963, loss_val: nan, pos_over_neg: 400.4459533691406 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.5538, loss_val: nan, pos_over_neg: 494.6991882324219 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.5599, loss_val: nan, pos_over_neg: 322.4837341308594 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 332.30657958984375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 321.198486328125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.5834, loss_val: nan, pos_over_neg: 607.999755859375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.5644, loss_val: nan, pos_over_neg: 561.67626953125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.5676, loss_val: nan, pos_over_neg: 598.2005004882812 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 497.2220153808594 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.6077, loss_val: nan, pos_over_neg: 526.844482421875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.5997, loss_val: nan, pos_over_neg: 758.6994018554688 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 703.7593383789062 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.546, loss_val: nan, pos_over_neg: 664.7483520507812 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.587, loss_val: nan, pos_over_neg: 578.751953125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.5914, loss_val: nan, pos_over_neg: 448.4268493652344 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6173, loss_val: nan, pos_over_neg: 334.4163818359375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 478.9275817871094 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 429.1563415527344 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 406.0329895019531 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.5817, loss_val: nan, pos_over_neg: 521.5123291015625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.5522, loss_val: nan, pos_over_neg: 422.4219665527344 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.5993, loss_val: nan, pos_over_neg: 453.4471740722656 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 951.518798828125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 441.6079406738281 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.5979, loss_val: nan, pos_over_neg: 411.9716796875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 344.251953125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.5917, loss_val: nan, pos_over_neg: 444.60784912109375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 1248.1771240234375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.5688, loss_val: nan, pos_over_neg: 871.9747314453125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 698.6218872070312 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.5899, loss_val: nan, pos_over_neg: 471.9583435058594 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6183, loss_val: nan, pos_over_neg: 428.2554626464844 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 359.1712646484375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 430.3309326171875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 349.43359375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6312, loss_val: nan, pos_over_neg: 330.6282043457031 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.6041, loss_val: nan, pos_over_neg: 425.39447021484375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 474.2066650390625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 826.938232421875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 555.5236206054688 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 431.249755859375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.606, loss_val: nan, pos_over_neg: 383.6190185546875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.5288, loss_val: nan, pos_over_neg: 597.1210327148438 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 737.33349609375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 729.6673583984375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.5827, loss_val: nan, pos_over_neg: 532.395751953125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6048, loss_val: nan, pos_over_neg: 436.6785583496094 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 337.4288330078125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.5977, loss_val: nan, pos_over_neg: 528.39794921875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.576, loss_val: nan, pos_over_neg: 667.6306762695312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.5665, loss_val: nan, pos_over_neg: 562.0379028320312 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.6125, loss_val: nan, pos_over_neg: 476.64813232421875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 402.7845764160156 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6122, loss_val: nan, pos_over_neg: 315.0980224609375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 414.3713073730469 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6178, loss_val: nan, pos_over_neg: 425.7054138183594 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 539.749755859375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.5567, loss_val: nan, pos_over_neg: 690.4628295898438 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.5974, loss_val: nan, pos_over_neg: 359.2903137207031 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 565.5831909179688 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.6301, loss_val: nan, pos_over_neg: 414.510498046875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 365.7995910644531 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 384.3443298339844 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 402.11907958984375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.5732, loss_val: nan, pos_over_neg: 809.0606079101562 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.5567, loss_val: nan, pos_over_neg: 557.6325073242188 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.5977, loss_val: nan, pos_over_neg: 461.48712158203125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 471.3583068847656 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6241, loss_val: nan, pos_over_neg: 427.3869323730469 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.6292, loss_val: nan, pos_over_neg: 348.51171875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 344.5846252441406 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6142, loss_val: nan, pos_over_neg: 441.7324523925781 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.5728, loss_val: nan, pos_over_neg: 688.13916015625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.6066, loss_val: nan, pos_over_neg: 385.64178466796875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.5939, loss_val: nan, pos_over_neg: 390.8476257324219 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 695.0798950195312 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.5413, loss_val: nan, pos_over_neg: 523.3343505859375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.563, loss_val: nan, pos_over_neg: 454.0917663574219 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.5872, loss_val: nan, pos_over_neg: 392.54833984375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 426.1557312011719 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.601, loss_val: nan, pos_over_neg: 432.0044250488281 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.5596, loss_val: nan, pos_over_neg: 357.1232604980469 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.5758, loss_val: nan, pos_over_neg: 400.5440979003906 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 500.7693176269531 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 338.3165283203125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.5651, loss_val: nan, pos_over_neg: 332.86517333984375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6173, loss_val: nan, pos_over_neg: 602.9256591796875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.5571, loss_val: nan, pos_over_neg: 552.4775390625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.6449, loss_val: nan, pos_over_neg: 302.09234619140625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 361.91876220703125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 526.2885131835938 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 396.744384765625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 437.7262878417969 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 305.4198303222656 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.6043, loss_val: nan, pos_over_neg: 378.4425964355469 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 489.9322204589844 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6118, loss_val: nan, pos_over_neg: 413.3680114746094 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.5709, loss_val: nan, pos_over_neg: 488.35992431640625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.5844, loss_val: nan, pos_over_neg: 692.5994262695312 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.6148, loss_val: nan, pos_over_neg: 424.05194091796875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.5644, loss_val: nan, pos_over_neg: 319.8157653808594 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 332.09503173828125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.6135, loss_val: nan, pos_over_neg: 361.796142578125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 538.2208251953125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.583, loss_val: nan, pos_over_neg: 458.3527526855469 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 349.7867736816406 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 399.404052734375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.5657, loss_val: nan, pos_over_neg: 688.3707885742188 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.6172, loss_val: nan, pos_over_neg: 363.54058837890625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 396.1718444824219 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.5969, loss_val: nan, pos_over_neg: 296.8305969238281 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 462.61676025390625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 435.9969177246094 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.6344, loss_val: nan, pos_over_neg: 349.38323974609375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.5995, loss_val: nan, pos_over_neg: 349.8526916503906 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 443.2024841308594 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6173, loss_val: nan, pos_over_neg: 293.37750244140625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 316.4068298339844 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.5773, loss_val: nan, pos_over_neg: 545.7549438476562 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.5942, loss_val: nan, pos_over_neg: 491.6676940917969 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.6177, loss_val: nan, pos_over_neg: 506.99676513671875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 541.9301147460938 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 373.9668884277344 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 557.230224609375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 542.1494140625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 566.8507690429688 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.601, loss_val: nan, pos_over_neg: 324.46356201171875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.6071, loss_val: nan, pos_over_neg: 279.10394287109375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.6254, loss_val: nan, pos_over_neg: 245.80606079101562 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 310.45733642578125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 335.8500061035156 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 404.4271545410156 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.6367, loss_val: nan, pos_over_neg: 394.4509582519531 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.5681, loss_val: nan, pos_over_neg: 437.9508361816406 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 573.1307983398438 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 500.29998779296875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 531.0079956054688 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 537.7689208984375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 420.57373046875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6268, loss_val: nan, pos_over_neg: 380.8627014160156 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 652.5686645507812 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.5944, loss_val: nan, pos_over_neg: 386.7602844238281 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.5397, loss_val: nan, pos_over_neg: 612.4130249023438 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.5816, loss_val: nan, pos_over_neg: 608.3538818359375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.5715, loss_val: nan, pos_over_neg: 457.80010986328125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 329.460205078125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 535.5219116210938 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 514.5059204101562 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 397.88629150390625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 458.5241394042969 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.5841, loss_val: nan, pos_over_neg: 455.1420593261719 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.5796, loss_val: nan, pos_over_neg: 414.37786865234375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.5675, loss_val: nan, pos_over_neg: 325.0082092285156 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.5488, loss_val: nan, pos_over_neg: 422.3760681152344 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 737.62890625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 359.3067626953125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 290.56341552734375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.5737, loss_val: nan, pos_over_neg: 551.4317016601562 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.5687, loss_val: nan, pos_over_neg: 743.437255859375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.6054, loss_val: nan, pos_over_neg: 663.5374755859375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.5371, loss_val: nan, pos_over_neg: 879.3521118164062 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.6202, loss_val: nan, pos_over_neg: 498.5345764160156 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 715.373291015625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 763.38916015625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.5657, loss_val: nan, pos_over_neg: 561.462646484375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 414.8128356933594 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 481.94915771484375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.5583, loss_val: nan, pos_over_neg: 687.5960083007812 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.5743, loss_val: nan, pos_over_neg: 472.6033935546875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 375.462890625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.5901, loss_val: nan, pos_over_neg: 385.90576171875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 399.7879333496094 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.5553, loss_val: nan, pos_over_neg: 871.412109375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.5508, loss_val: nan, pos_over_neg: 832.8666381835938 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.5517, loss_val: nan, pos_over_neg: 721.2233276367188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.5752, loss_val: nan, pos_over_neg: 840.38232421875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.5946, loss_val: nan, pos_over_neg: 578.5703735351562 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.6079, loss_val: nan, pos_over_neg: 353.34063720703125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 357.81488037109375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.5877, loss_val: nan, pos_over_neg: 348.7927551269531 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.6321, loss_val: nan, pos_over_neg: 281.76910400390625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 307.2850341796875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.5688, loss_val: nan, pos_over_neg: 306.93756103515625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.5657, loss_val: nan, pos_over_neg: 429.97088623046875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6309, loss_val: nan, pos_over_neg: 283.76300048828125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 331.1031799316406 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 577.1898193359375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.5968, loss_val: nan, pos_over_neg: 422.32073974609375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 659.2606201171875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 602.9249877929688 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.5845, loss_val: nan, pos_over_neg: 601.1273803710938 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 434.6972961425781 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.6417, loss_val: nan, pos_over_neg: 328.84625244140625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.5575, loss_val: nan, pos_over_neg: 629.3777465820312 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 451.760009765625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 369.19549560546875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6135, loss_val: nan, pos_over_neg: 333.4785461425781 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 435.13287353515625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.5836, loss_val: nan, pos_over_neg: 351.31292724609375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 330.5934143066406 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.554, loss_val: nan, pos_over_neg: 597.7385864257812 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.5663, loss_val: nan, pos_over_neg: 676.0172729492188 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.6307, loss_val: nan, pos_over_neg: 361.62982177734375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.6163, loss_val: nan, pos_over_neg: 562.6196899414062 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 543.1936645507812 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 509.85809326171875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6342, loss_val: nan, pos_over_neg: 385.433349609375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.6167, loss_val: nan, pos_over_neg: 401.60540771484375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.5662, loss_val: nan, pos_over_neg: 486.9902038574219 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 515.4599609375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 399.3949890136719 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 310.99365234375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.5985, loss_val: nan, pos_over_neg: 468.7113342285156 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6644, loss_val: nan, pos_over_neg: 235.57308959960938 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 506.5856628417969 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 329.6027526855469 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 513.622802734375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.648, loss_val: nan, pos_over_neg: 351.6190490722656 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.5716, loss_val: nan, pos_over_neg: 752.5943603515625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 578.85498046875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 590.4915771484375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.5781, loss_val: nan, pos_over_neg: 622.2845458984375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 508.0229187011719 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.5746, loss_val: nan, pos_over_neg: 500.331787109375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 392.5798645019531 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 279.6903076171875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.5889, loss_val: nan, pos_over_neg: 416.44488525390625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6159, loss_val: nan, pos_over_neg: 329.7381591796875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.6082, loss_val: nan, pos_over_neg: 421.51983642578125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 450.5937194824219 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.559, loss_val: nan, pos_over_neg: 488.60015869140625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.5561, loss_val: nan, pos_over_neg: 521.2326049804688 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 550.9469604492188 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.548, loss_val: nan, pos_over_neg: 1158.9462890625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 399.3450622558594 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.5845, loss_val: nan, pos_over_neg: 378.09503173828125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.6168, loss_val: nan, pos_over_neg: 360.37744140625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 504.5351257324219 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.6053, loss_val: nan, pos_over_neg: 382.0303955078125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 509.73974609375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 781.8736572265625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6368, loss_val: nan, pos_over_neg: 316.6839599609375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 618.1279907226562 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 676.0409545898438 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 365.7855529785156 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 506.9338073730469 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 405.3200378417969 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 437.2022705078125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 393.5193176269531 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.6219, loss_val: nan, pos_over_neg: 264.4964599609375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 336.2522888183594 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 538.6303100585938 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.6273, loss_val: nan, pos_over_neg: 391.0711975097656 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 612.9964599609375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 595.0606079101562 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.5721, loss_val: nan, pos_over_neg: 986.865234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.5745, loss_val: nan, pos_over_neg: 692.6932373046875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 452.6968994140625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.6066, loss_val: nan, pos_over_neg: 460.90460205078125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.5903, loss_val: nan, pos_over_neg: 530.5590209960938 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 620.1314086914062 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.6258, loss_val: nan, pos_over_neg: 370.4407043457031 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.5816, loss_val: nan, pos_over_neg: 386.95025634765625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 337.745849609375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.5775, loss_val: nan, pos_over_neg: 379.2853088378906 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.5335, loss_val: nan, pos_over_neg: 470.7530212402344 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 473.8020935058594 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 315.91412353515625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 455.9759521484375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 489.6500549316406 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 330.0215759277344 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.5653, loss_val: nan, pos_over_neg: 435.8155822753906 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.57, loss_val: nan, pos_over_neg: 500.4137878417969 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 437.9454345703125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 599.0999145507812 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.5802, loss_val: nan, pos_over_neg: 485.0679626464844 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.5697, loss_val: nan, pos_over_neg: 433.33123779296875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6054, loss_val: nan, pos_over_neg: 343.2502746582031 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.5951, loss_val: nan, pos_over_neg: 329.4055480957031 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 336.9190979003906 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6496, loss_val: nan, pos_over_neg: 359.74212646484375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 326.3487548828125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 262.4996643066406 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 419.0438537597656 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.5655, loss_val: nan, pos_over_neg: 443.52197265625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 378.6520080566406 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 495.66180419921875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.5877, loss_val: nan, pos_over_neg: 592.28173828125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 491.9237976074219 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 668.6100463867188 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.5726, loss_val: nan, pos_over_neg: 550.5759887695312 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.5589, loss_val: nan, pos_over_neg: 353.60980224609375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 523.7659301757812 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.5723, loss_val: nan, pos_over_neg: 347.55279541015625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.5682, loss_val: nan, pos_over_neg: 353.5436096191406 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 408.5716857910156 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.5834, loss_val: nan, pos_over_neg: 300.61083984375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.5644, loss_val: nan, pos_over_neg: 466.7196960449219 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.5498, loss_val: nan, pos_over_neg: 554.8757934570312 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 541.7603149414062 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 884.6385498046875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.5953, loss_val: nan, pos_over_neg: 545.8907470703125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.5593, loss_val: nan, pos_over_neg: 493.6106262207031 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 815.6871948242188 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 592.7117309570312 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.6193, loss_val: nan, pos_over_neg: 499.90911865234375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.5564, loss_val: nan, pos_over_neg: 618.8137817382812 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 427.6324462890625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.5319, loss_val: nan, pos_over_neg: 1064.0355224609375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 500.95806884765625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.6458, loss_val: nan, pos_over_neg: 311.59625244140625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.5917, loss_val: nan, pos_over_neg: 421.3049621582031 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.5953, loss_val: nan, pos_over_neg: 538.8742065429688 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6177, loss_val: nan, pos_over_neg: 545.76513671875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.5968, loss_val: nan, pos_over_neg: 990.9077758789062 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.5579, loss_val: nan, pos_over_neg: 512.5543212890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.6404, loss_val: nan, pos_over_neg: 347.2675476074219 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 503.22076416015625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.5608, loss_val: nan, pos_over_neg: 708.0223999023438 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 465.675048828125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.6139, loss_val: nan, pos_over_neg: 320.1897277832031 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 598.4708862304688 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 600.71142578125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.6228, loss_val: nan, pos_over_neg: 473.2088317871094 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 367.1684265136719 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6094, loss_val: nan, pos_over_neg: 433.9996643066406 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 563.2421875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.5891, loss_val: nan, pos_over_neg: 614.5223388671875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 332.3290100097656 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 441.3040771484375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.5571, loss_val: nan, pos_over_neg: 456.4139099121094 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 559.0313720703125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 368.26025390625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 454.3409423828125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 739.8818969726562 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 476.1049499511719 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.5926, loss_val: nan, pos_over_neg: 509.8825988769531 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 404.2950439453125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.5745, loss_val: nan, pos_over_neg: 490.9812316894531 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.5642, loss_val: nan, pos_over_neg: 482.4303283691406 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.622, loss_val: nan, pos_over_neg: 418.8865051269531 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.5718, loss_val: nan, pos_over_neg: 712.0142822265625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 328.7861022949219 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.5656, loss_val: nan, pos_over_neg: 466.7347717285156 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 523.7308349609375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 513.2142944335938 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.5665, loss_val: nan, pos_over_neg: 828.4757080078125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.5433, loss_val: nan, pos_over_neg: 1459.93310546875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 404.5487976074219 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 414.9725036621094 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 435.9896545410156 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.5618, loss_val: nan, pos_over_neg: 424.921875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 371.4114990234375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.6131, loss_val: nan, pos_over_neg: 301.2036437988281 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.6062, loss_val: nan, pos_over_neg: 423.8659973144531 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.6113, loss_val: nan, pos_over_neg: 566.35400390625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.5647, loss_val: nan, pos_over_neg: 440.598876953125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 427.5921630859375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.5721, loss_val: nan, pos_over_neg: 624.3876953125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.5877, loss_val: nan, pos_over_neg: 643.2235107421875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.5873, loss_val: nan, pos_over_neg: 596.12255859375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 437.57171630859375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.6252, loss_val: nan, pos_over_neg: 261.8013916015625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 574.4035034179688 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 455.7621765136719 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 556.1978149414062 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.5621, loss_val: nan, pos_over_neg: 525.3264770507812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.5796, loss_val: nan, pos_over_neg: 496.0291442871094 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.586, loss_val: nan, pos_over_neg: 452.5404968261719 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 390.3175964355469 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.5855, loss_val: nan, pos_over_neg: 334.48931884765625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.5746, loss_val: nan, pos_over_neg: 552.5030517578125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.5991, loss_val: nan, pos_over_neg: 358.9171447753906 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 352.03521728515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 530.7421875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 590.8477783203125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 475.3020324707031 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 496.4334411621094 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.6179, loss_val: nan, pos_over_neg: 546.5844116210938 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.5599, loss_val: nan, pos_over_neg: 1318.9698486328125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 595.6700439453125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 806.9970092773438 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.6143, loss_val: nan, pos_over_neg: 529.70654296875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 475.0772399902344 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 305.8342590332031 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 373.977783203125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 380.84832763671875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.5659, loss_val: nan, pos_over_neg: 363.70135498046875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.6118, loss_val: nan, pos_over_neg: 317.8044128417969 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.5618, loss_val: nan, pos_over_neg: 328.1583557128906 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 306.080078125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.6572, loss_val: nan, pos_over_neg: 463.2139587402344 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6114, loss_val: nan, pos_over_neg: 492.4356689453125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 739.6113891601562 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 492.53656005859375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.5896, loss_val: nan, pos_over_neg: 567.010009765625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.5695, loss_val: nan, pos_over_neg: 453.4855651855469 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.5553, loss_val: nan, pos_over_neg: 419.144775390625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.5696, loss_val: nan, pos_over_neg: 546.3229370117188 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.5704, loss_val: nan, pos_over_neg: 364.6992492675781 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.6129, loss_val: nan, pos_over_neg: 289.3730163574219 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 347.1053771972656 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.6076, loss_val: nan, pos_over_neg: 358.1765441894531 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.5854, loss_val: nan, pos_over_neg: 552.9306640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 518.3763427734375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.5989, loss_val: nan, pos_over_neg: 731.2479858398438 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 426.4808654785156 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.5599, loss_val: nan, pos_over_neg: 434.0861511230469 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.6236, loss_val: nan, pos_over_neg: 338.665283203125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.6088, loss_val: nan, pos_over_neg: 276.0040588378906 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.6415, loss_val: nan, pos_over_neg: 304.7652587890625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6541, loss_val: nan, pos_over_neg: 345.0068054199219 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 264.7213134765625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.6251, loss_val: nan, pos_over_neg: 221.13790893554688 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 427.1502380371094 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.604, loss_val: nan, pos_over_neg: 334.3431091308594 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 358.78564453125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 417.54931640625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 506.3343505859375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.6392, loss_val: nan, pos_over_neg: 563.87744140625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.6272, loss_val: nan, pos_over_neg: 450.1222229003906 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.6065, loss_val: nan, pos_over_neg: 833.4644775390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 643.1138305664062 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.5674, loss_val: nan, pos_over_neg: 547.2548828125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 546.2457885742188 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.5729, loss_val: nan, pos_over_neg: 581.1583862304688 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.6551, loss_val: nan, pos_over_neg: 238.2670440673828 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.5668, loss_val: nan, pos_over_neg: 358.9826354980469 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.6202, loss_val: nan, pos_over_neg: 180.51634216308594 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 411.08282470703125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.5997, loss_val: nan, pos_over_neg: 372.2301330566406 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.5715, loss_val: nan, pos_over_neg: 561.9014892578125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 586.7853393554688 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.5818, loss_val: nan, pos_over_neg: 382.7435607910156 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 521.7428588867188 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.583, loss_val: nan, pos_over_neg: 708.283935546875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.5874, loss_val: nan, pos_over_neg: 532.7332153320312 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6301, loss_val: nan, pos_over_neg: 563.5595703125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 361.6169738769531 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.6325, loss_val: nan, pos_over_neg: 239.50054931640625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.5506, loss_val: nan, pos_over_neg: 378.1452941894531 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.563, loss_val: nan, pos_over_neg: 360.98388671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 382.24005126953125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 430.0976867675781 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.5512, loss_val: nan, pos_over_neg: 432.00836181640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.6188, loss_val: nan, pos_over_neg: 472.71551513671875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.5805, loss_val: nan, pos_over_neg: 566.46533203125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 448.6251525878906 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 419.9408264160156 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 748.1107177734375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.5845, loss_val: nan, pos_over_neg: 504.8092956542969 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.6101, loss_val: nan, pos_over_neg: 400.5162353515625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 320.3974609375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.5845, loss_val: nan, pos_over_neg: 501.24932861328125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 443.60540771484375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.5713, loss_val: nan, pos_over_neg: 429.9574279785156 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.5627, loss_val: nan, pos_over_neg: 391.7413635253906 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.5854, loss_val: nan, pos_over_neg: 465.7669982910156 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.5824, loss_val: nan, pos_over_neg: 391.53656005859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 592.83056640625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 335.3623046875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 629.6907348632812 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 516.9896850585938 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 517.480712890625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.6184, loss_val: nan, pos_over_neg: 508.7889709472656 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 413.677001953125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.5935, loss_val: nan, pos_over_neg: 297.7508544921875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 377.3502197265625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.5668, loss_val: nan, pos_over_neg: 477.890380859375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6161, loss_val: nan, pos_over_neg: 433.92523193359375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 352.1762390136719 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 542.4765014648438 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.5843, loss_val: nan, pos_over_neg: 681.191162109375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 442.1463928222656 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 410.4801330566406 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 432.67413330078125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.5843, loss_val: nan, pos_over_neg: 590.3973999023438 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6172, loss_val: nan, pos_over_neg: 782.7932739257812 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.6115, loss_val: nan, pos_over_neg: 413.8544921875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 629.2196655273438 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.5668, loss_val: nan, pos_over_neg: 326.60565185546875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 361.1348571777344 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.606, loss_val: nan, pos_over_neg: 489.5306091308594 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 374.3979187011719 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6204, loss_val: nan, pos_over_neg: 244.35284423828125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 425.7786865234375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.5407, loss_val: nan, pos_over_neg: 668.8355102539062 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6215, loss_val: nan, pos_over_neg: 1102.52490234375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.5309, loss_val: nan, pos_over_neg: 1088.272216796875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.601, loss_val: nan, pos_over_neg: 742.49658203125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.5963, loss_val: nan, pos_over_neg: 650.2315063476562 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 770.4443359375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 405.6560363769531 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.6093, loss_val: nan, pos_over_neg: 538.3802490234375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 814.8005981445312 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.6194, loss_val: nan, pos_over_neg: 360.23681640625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 444.9733581542969 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 335.13818359375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.5915, loss_val: nan, pos_over_neg: 360.7244873046875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 465.6973571777344 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 379.14324951171875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 407.6790466308594 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 611.1077880859375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 745.33935546875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 582.7289428710938 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.5602, loss_val: nan, pos_over_neg: 821.2066040039062 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.5829, loss_val: nan, pos_over_neg: 770.7977294921875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 400.9768371582031 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 469.67242431640625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.5426, loss_val: nan, pos_over_neg: 539.3873291015625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.5926, loss_val: nan, pos_over_neg: 434.7586364746094 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.5727, loss_val: nan, pos_over_neg: 325.8920593261719 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6085, loss_val: nan, pos_over_neg: 378.7677001953125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.6373, loss_val: nan, pos_over_neg: 266.0745849609375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.6096, loss_val: nan, pos_over_neg: 486.9103088378906 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 534.0381469726562 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.5688, loss_val: nan, pos_over_neg: 469.6286315917969 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.5708, loss_val: nan, pos_over_neg: 491.6753234863281 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.6179, loss_val: nan, pos_over_neg: 473.8042297363281 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.6159, loss_val: nan, pos_over_neg: 748.1932373046875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.543, loss_val: nan, pos_over_neg: 951.2586059570312 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.6171, loss_val: nan, pos_over_neg: 412.7982482910156 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 537.3121948242188 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.6073, loss_val: nan, pos_over_neg: 315.6112976074219 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.5739, loss_val: nan, pos_over_neg: 379.54241943359375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.5743, loss_val: nan, pos_over_neg: 310.3816223144531 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.5612, loss_val: nan, pos_over_neg: 497.0958251953125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.5888, loss_val: nan, pos_over_neg: 314.1719665527344 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 487.4654235839844 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.536, loss_val: nan, pos_over_neg: 854.4754638671875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 646.4279174804688 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.6658, loss_val: nan, pos_over_neg: 358.5351867675781 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 364.8509216308594 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.5627, loss_val: nan, pos_over_neg: 542.2927856445312 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 359.691650390625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 386.93316650390625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 297.31805419921875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 350.7076416015625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 407.801025390625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.5862, loss_val: nan, pos_over_neg: 440.1418762207031 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 340.1790466308594 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 579.8338012695312 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.5687, loss_val: nan, pos_over_neg: 388.3645935058594 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.5371, loss_val: nan, pos_over_neg: 461.41107177734375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 481.3148498535156 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 335.981689453125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 408.4943542480469 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.647, loss_val: nan, pos_over_neg: 417.9097900390625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.5919, loss_val: nan, pos_over_neg: 369.2527160644531 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 273.5304870605469 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.5507, loss_val: nan, pos_over_neg: 467.512939453125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 527.7530517578125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.5607, loss_val: nan, pos_over_neg: 567.5791625976562 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.5716, loss_val: nan, pos_over_neg: 427.3004150390625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.595, loss_val: nan, pos_over_neg: 546.6951904296875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 376.4875793457031 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.5712, loss_val: nan, pos_over_neg: 285.5677185058594 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 394.9753112792969 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6167, loss_val: nan, pos_over_neg: 457.3317565917969 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 378.21923828125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.5289, loss_val: nan, pos_over_neg: 657.9771728515625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.5948, loss_val: nan, pos_over_neg: 479.4259948730469 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.5313, loss_val: nan, pos_over_neg: 533.5824584960938 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 508.15142822265625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.5729, loss_val: nan, pos_over_neg: 535.4616088867188 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.5279, loss_val: nan, pos_over_neg: 610.709228515625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.5977, loss_val: nan, pos_over_neg: 365.4833068847656 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.5493, loss_val: nan, pos_over_neg: 369.5235900878906 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.5665, loss_val: nan, pos_over_neg: 342.15777587890625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 374.14691162109375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.6124, loss_val: nan, pos_over_neg: 301.6431884765625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.5892, loss_val: nan, pos_over_neg: 313.3191833496094 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.5736, loss_val: nan, pos_over_neg: 400.2769775390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 497.08209228515625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 526.7120361328125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.5772, loss_val: nan, pos_over_neg: 752.5166625976562 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 709.5737915039062 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.571, loss_val: nan, pos_over_neg: 906.4591674804688 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.5861, loss_val: nan, pos_over_neg: 935.4404907226562 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 1578.9835205078125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 614.5610961914062 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.5427, loss_val: nan, pos_over_neg: 669.6510620117188 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.6454, loss_val: nan, pos_over_neg: 202.3794708251953 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 408.9322509765625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 308.0862731933594 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6063, loss_val: nan, pos_over_neg: 309.1419372558594 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.5581, loss_val: nan, pos_over_neg: 290.1341552734375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.5533, loss_val: nan, pos_over_neg: 475.6263122558594 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.5743, loss_val: nan, pos_over_neg: 772.6354370117188 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 714.5028686523438 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 769.7766723632812 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 414.81256103515625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 361.92486572265625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.5835, loss_val: nan, pos_over_neg: 550.9750366210938 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.5907, loss_val: nan, pos_over_neg: 596.7408447265625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 366.4948425292969 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.6004, loss_val: nan, pos_over_neg: 427.864013671875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 324.6375732421875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.5783, loss_val: nan, pos_over_neg: 386.3876037597656 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 384.5887756347656 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.5588, loss_val: nan, pos_over_neg: 724.4873657226562 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.5772, loss_val: nan, pos_over_neg: 365.9520568847656 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.6284, loss_val: nan, pos_over_neg: 428.11553955078125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.5508, loss_val: nan, pos_over_neg: 598.4896240234375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.5809, loss_val: nan, pos_over_neg: 448.2869873046875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6305, loss_val: nan, pos_over_neg: 330.3311462402344 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 430.8362121582031 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 546.67919921875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.5568, loss_val: nan, pos_over_neg: 575.5009765625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.5818, loss_val: nan, pos_over_neg: 616.1420288085938 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.6356, loss_val: nan, pos_over_neg: 429.33392333984375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 449.2165832519531 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 354.1489562988281 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.5764, loss_val: nan, pos_over_neg: 354.5523986816406 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 330.1904602050781 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.5886, loss_val: nan, pos_over_neg: 482.6972351074219 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 541.1886596679688 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 365.1388244628906 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 737.6183471679688 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.5622, loss_val: nan, pos_over_neg: 470.1555480957031 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.6115, loss_val: nan, pos_over_neg: 374.4795227050781 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 396.3728332519531 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.562, loss_val: nan, pos_over_neg: 417.26959228515625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.5412, loss_val: nan, pos_over_neg: 736.2986450195312 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.6075, loss_val: nan, pos_over_neg: 280.46331787109375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 326.5150146484375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.5771, loss_val: nan, pos_over_neg: 398.6456604003906 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 584.7054443359375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.5516, loss_val: nan, pos_over_neg: 783.3781127929688 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.5757, loss_val: nan, pos_over_neg: 607.8117065429688 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.5862, loss_val: nan, pos_over_neg: 637.9935302734375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 450.3668212890625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.5726, loss_val: nan, pos_over_neg: 649.5088500976562 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.5544, loss_val: nan, pos_over_neg: 472.39361572265625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.5881, loss_val: nan, pos_over_neg: 438.6909484863281 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.5881, loss_val: nan, pos_over_neg: 405.3035888671875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.5772, loss_val: nan, pos_over_neg: 330.95379638671875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.5585, loss_val: nan, pos_over_neg: 439.1998291015625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.5682, loss_val: nan, pos_over_neg: 364.045654296875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 358.43621826171875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.5683, loss_val: nan, pos_over_neg: 920.7052612304688 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 743.2733764648438 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 847.90234375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 805.5491333007812 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.5774, loss_val: nan, pos_over_neg: 619.2320556640625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.5531, loss_val: nan, pos_over_neg: 523.43701171875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 544.032958984375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.5796, loss_val: nan, pos_over_neg: 711.6624755859375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.5529, loss_val: nan, pos_over_neg: 544.8646850585938 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6006, loss_val: nan, pos_over_neg: 333.6878356933594 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.5354, loss_val: nan, pos_over_neg: 501.4151611328125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.5494, loss_val: nan, pos_over_neg: 419.4427185058594 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.5881, loss_val: nan, pos_over_neg: 475.459228515625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.5426, loss_val: nan, pos_over_neg: 732.6561889648438 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6015, loss_val: nan, pos_over_neg: 575.6752319335938 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/300000 [3:46:27<75344:07:49, 904.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 390.33453369140625 lr: 0.00031623\n",
      "epoch: 15\n",
      "Iter: 0/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 485.8166809082031 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 621.5151977539062 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.5386, loss_val: nan, pos_over_neg: 1028.3017578125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 631.7943725585938 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6277, loss_val: nan, pos_over_neg: 422.9772033691406 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 473.6568908691406 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 657.5759887695312 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.5971, loss_val: nan, pos_over_neg: 403.3689880371094 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.5946, loss_val: nan, pos_over_neg: 384.8455810546875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.6126, loss_val: nan, pos_over_neg: 557.8307495117188 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.5802, loss_val: nan, pos_over_neg: 499.84100341796875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.595, loss_val: nan, pos_over_neg: 459.7269592285156 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.5944, loss_val: nan, pos_over_neg: 301.47760009765625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 405.3870849609375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 538.6880493164062 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 676.3969116210938 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.5824, loss_val: nan, pos_over_neg: 493.7627258300781 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 531.1859741210938 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.607, loss_val: nan, pos_over_neg: 470.9243469238281 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 669.2268676757812 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 618.8352661132812 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 528.4940185546875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.583, loss_val: nan, pos_over_neg: 502.4849548339844 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.5652, loss_val: nan, pos_over_neg: 310.2955627441406 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 314.75091552734375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.5644, loss_val: nan, pos_over_neg: 551.6637573242188 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 405.23748779296875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 592.97607421875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.591, loss_val: nan, pos_over_neg: 402.4449768066406 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.5918, loss_val: nan, pos_over_neg: 594.6834716796875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.5833, loss_val: nan, pos_over_neg: 457.8023986816406 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.6461, loss_val: nan, pos_over_neg: 275.78436279296875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.5948, loss_val: nan, pos_over_neg: 439.4719543457031 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 418.40850830078125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.5632, loss_val: nan, pos_over_neg: 360.624267578125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.5676, loss_val: nan, pos_over_neg: 300.0392761230469 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.5763, loss_val: nan, pos_over_neg: 420.8581848144531 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 355.0859680175781 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.5572, loss_val: nan, pos_over_neg: 414.5289611816406 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.5602, loss_val: nan, pos_over_neg: 482.0677795410156 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.5789, loss_val: nan, pos_over_neg: 570.5138549804688 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.5722, loss_val: nan, pos_over_neg: 641.2330322265625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.6005, loss_val: nan, pos_over_neg: 428.42498779296875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.5888, loss_val: nan, pos_over_neg: 942.33740234375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.5753, loss_val: nan, pos_over_neg: 961.2688598632812 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.5388, loss_val: nan, pos_over_neg: 1492.4510498046875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 606.5096435546875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6023, loss_val: nan, pos_over_neg: 378.4681091308594 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.5477, loss_val: nan, pos_over_neg: 365.7071838378906 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.6245, loss_val: nan, pos_over_neg: 280.9539489746094 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 307.79931640625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.5704, loss_val: nan, pos_over_neg: 295.35821533203125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.5715, loss_val: nan, pos_over_neg: 432.1163330078125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 524.2062377929688 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.6248, loss_val: nan, pos_over_neg: 470.88037109375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 774.8385620117188 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 811.9487915039062 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 627.1716918945312 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.5901, loss_val: nan, pos_over_neg: 598.7000732421875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.6428, loss_val: nan, pos_over_neg: 313.3326416015625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.5655, loss_val: nan, pos_over_neg: 572.6683959960938 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.5687, loss_val: nan, pos_over_neg: 748.1148681640625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 483.3826904296875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6717, loss_val: nan, pos_over_neg: 297.8658447265625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.573, loss_val: nan, pos_over_neg: 412.1801452636719 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 381.006591796875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 431.4537048339844 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.6227, loss_val: nan, pos_over_neg: 368.748291015625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.6073, loss_val: nan, pos_over_neg: 310.51605224609375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 391.97796630859375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6353, loss_val: nan, pos_over_neg: 424.3393859863281 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 526.6629638671875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 476.68072509765625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.6256, loss_val: nan, pos_over_neg: 367.6503601074219 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.5419, loss_val: nan, pos_over_neg: 719.914306640625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.5837, loss_val: nan, pos_over_neg: 463.0192565917969 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.566, loss_val: nan, pos_over_neg: 619.0144653320312 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.5514, loss_val: nan, pos_over_neg: 627.5044555664062 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.5941, loss_val: nan, pos_over_neg: 437.0580139160156 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 686.9017333984375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 614.0715942382812 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 568.0126342773438 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 470.6412048339844 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 448.0415344238281 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.5745, loss_val: nan, pos_over_neg: 426.9707946777344 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 461.20684814453125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.5718, loss_val: nan, pos_over_neg: 640.7437133789062 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.5724, loss_val: nan, pos_over_neg: 433.1396179199219 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 377.53948974609375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6023, loss_val: nan, pos_over_neg: 511.74029541015625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.5718, loss_val: nan, pos_over_neg: 462.9908142089844 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.5948, loss_val: nan, pos_over_neg: 494.3661804199219 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6174, loss_val: nan, pos_over_neg: 542.5271606445312 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.5877, loss_val: nan, pos_over_neg: 450.8757019042969 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 556.9443969726562 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.542, loss_val: nan, pos_over_neg: 2417.224365234375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 703.4761962890625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 509.468994140625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 374.9788818359375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.5625, loss_val: nan, pos_over_neg: 381.9613037109375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 480.9164733886719 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 430.2988586425781 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 279.224853515625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.5683, loss_val: nan, pos_over_neg: 375.9005432128906 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.5892, loss_val: nan, pos_over_neg: 373.8099365234375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.5602, loss_val: nan, pos_over_neg: 481.3531799316406 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.5974, loss_val: nan, pos_over_neg: 528.1890258789062 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.5939, loss_val: nan, pos_over_neg: 792.1128540039062 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.5862, loss_val: nan, pos_over_neg: 545.1796875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.5334, loss_val: nan, pos_over_neg: 679.5701293945312 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6245, loss_val: nan, pos_over_neg: 522.1698608398438 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.5405, loss_val: nan, pos_over_neg: 624.4746704101562 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 275.0391845703125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.5766, loss_val: nan, pos_over_neg: 413.6748352050781 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 321.4199523925781 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 338.8269958496094 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.6322, loss_val: nan, pos_over_neg: 309.4629211425781 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6071, loss_val: nan, pos_over_neg: 312.49578857421875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.5618, loss_val: nan, pos_over_neg: 423.76361083984375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 513.6124877929688 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.5866, loss_val: nan, pos_over_neg: 464.93548583984375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.5861, loss_val: nan, pos_over_neg: 397.1846008300781 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.5789, loss_val: nan, pos_over_neg: 372.4886474609375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 382.47467041015625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 531.576904296875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.5708, loss_val: nan, pos_over_neg: 472.962890625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 454.1363525390625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.5591, loss_val: nan, pos_over_neg: 577.27685546875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 506.2183532714844 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 424.6739196777344 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 315.49310302734375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 614.8692016601562 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 399.65838623046875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 470.96356201171875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 484.7503967285156 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.5887, loss_val: nan, pos_over_neg: 503.02899169921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.5833, loss_val: nan, pos_over_neg: 363.2198486328125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 442.9922790527344 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.5866, loss_val: nan, pos_over_neg: 431.5556335449219 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 411.2810974121094 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 380.5506286621094 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 354.9433898925781 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.5467, loss_val: nan, pos_over_neg: 637.4432373046875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 425.8580322265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.5714, loss_val: nan, pos_over_neg: 412.9191589355469 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.5583, loss_val: nan, pos_over_neg: 1252.38330078125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 647.6702270507812 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 619.82373046875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.5763, loss_val: nan, pos_over_neg: 789.3740844726562 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.551, loss_val: nan, pos_over_neg: 694.505615234375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 574.0181884765625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 642.1111450195312 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.5477, loss_val: nan, pos_over_neg: 780.811279296875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.5435, loss_val: nan, pos_over_neg: 452.4625549316406 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.5349, loss_val: nan, pos_over_neg: 771.8323364257812 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.5581, loss_val: nan, pos_over_neg: 358.7955322265625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 450.4000549316406 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.5873, loss_val: nan, pos_over_neg: 320.3338928222656 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 431.81988525390625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 537.420166015625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 473.684814453125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 946.54736328125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.5743, loss_val: nan, pos_over_neg: 608.8018798828125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.5791, loss_val: nan, pos_over_neg: 546.0567626953125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.619, loss_val: nan, pos_over_neg: 418.8218994140625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 435.6678161621094 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 585.514892578125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.5582, loss_val: nan, pos_over_neg: 485.5110778808594 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 477.4756774902344 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 429.5103759765625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.6122, loss_val: nan, pos_over_neg: 322.7641296386719 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 303.5554504394531 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.536, loss_val: nan, pos_over_neg: 743.7682495117188 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 403.3052978515625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.5941, loss_val: nan, pos_over_neg: 400.0680236816406 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.5805, loss_val: nan, pos_over_neg: 573.93212890625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6265, loss_val: nan, pos_over_neg: 288.3253173828125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.5683, loss_val: nan, pos_over_neg: 520.6491088867188 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6366, loss_val: nan, pos_over_neg: 364.36016845703125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.5599, loss_val: nan, pos_over_neg: 533.371826171875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.5582, loss_val: nan, pos_over_neg: 717.1477661132812 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.5817, loss_val: nan, pos_over_neg: 382.1914978027344 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.5704, loss_val: nan, pos_over_neg: 410.5259704589844 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.595, loss_val: nan, pos_over_neg: 413.95916748046875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.5925, loss_val: nan, pos_over_neg: 648.0439453125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 391.8445739746094 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.5564, loss_val: nan, pos_over_neg: 464.1773986816406 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 357.3808898925781 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.5472, loss_val: nan, pos_over_neg: 659.4778442382812 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 525.2282104492188 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 413.1816101074219 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.5402, loss_val: nan, pos_over_neg: 520.4212646484375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 375.4430236816406 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 479.4649353027344 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 476.98797607421875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.5614, loss_val: nan, pos_over_neg: 689.577880859375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.5736, loss_val: nan, pos_over_neg: 702.3800048828125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.5515, loss_val: nan, pos_over_neg: 520.7208251953125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.6099, loss_val: nan, pos_over_neg: 372.5484924316406 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 343.9046325683594 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.564, loss_val: nan, pos_over_neg: 532.7509155273438 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.5758, loss_val: nan, pos_over_neg: 528.5991821289062 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.6102, loss_val: nan, pos_over_neg: 434.20831298828125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.5708, loss_val: nan, pos_over_neg: 677.7620239257812 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.5884, loss_val: nan, pos_over_neg: 490.8612976074219 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 432.6112365722656 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 376.45782470703125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.5786, loss_val: nan, pos_over_neg: 475.72076416015625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 472.7067565917969 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.58, loss_val: nan, pos_over_neg: 599.1605834960938 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.6238, loss_val: nan, pos_over_neg: 362.7230224609375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.6279, loss_val: nan, pos_over_neg: 352.38641357421875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.6146, loss_val: nan, pos_over_neg: 274.28399658203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 468.67059326171875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 479.2826843261719 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.5652, loss_val: nan, pos_over_neg: 431.5694580078125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.5536, loss_val: nan, pos_over_neg: 554.0245971679688 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 527.3855590820312 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.5642, loss_val: nan, pos_over_neg: 795.495849609375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.5889, loss_val: nan, pos_over_neg: 435.89288330078125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 283.1084899902344 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 528.2968139648438 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 403.8066101074219 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.6071, loss_val: nan, pos_over_neg: 462.6561584472656 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.5976, loss_val: nan, pos_over_neg: 420.7078857421875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.6225, loss_val: nan, pos_over_neg: 288.517333984375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 261.40399169921875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.5783, loss_val: nan, pos_over_neg: 548.7079467773438 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 423.2708740234375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6337, loss_val: nan, pos_over_neg: 280.76275634765625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 397.017578125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 695.3771362304688 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.603, loss_val: nan, pos_over_neg: 578.4300537109375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.6137, loss_val: nan, pos_over_neg: 394.7117919921875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.563, loss_val: nan, pos_over_neg: 454.03741455078125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 494.0030517578125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.6069, loss_val: nan, pos_over_neg: 307.4880676269531 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.5744, loss_val: nan, pos_over_neg: 428.0067138671875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 345.27349853515625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.5728, loss_val: nan, pos_over_neg: 389.4841003417969 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 358.2341003417969 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.5807, loss_val: nan, pos_over_neg: 426.6611022949219 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 464.6283264160156 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.5465, loss_val: nan, pos_over_neg: 562.6512451171875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.6198, loss_val: nan, pos_over_neg: 352.7176513671875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.6165, loss_val: nan, pos_over_neg: 324.8771057128906 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.5875, loss_val: nan, pos_over_neg: 565.4549560546875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 346.9715270996094 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6121, loss_val: nan, pos_over_neg: 387.1285400390625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 412.93408203125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 401.49945068359375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.5799, loss_val: nan, pos_over_neg: 357.36651611328125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.5513, loss_val: nan, pos_over_neg: 453.0287170410156 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.563, loss_val: nan, pos_over_neg: 478.8487243652344 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.5923, loss_val: nan, pos_over_neg: 289.7063293457031 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6112, loss_val: nan, pos_over_neg: 380.8330383300781 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.5875, loss_val: nan, pos_over_neg: 463.4966125488281 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 537.6583862304688 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 483.9367980957031 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.5824, loss_val: nan, pos_over_neg: 412.2991027832031 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 380.6393127441406 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 452.0877380371094 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 269.412109375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 537.4932861328125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 433.0745849609375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.6006, loss_val: nan, pos_over_neg: 580.5386962890625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.6041, loss_val: nan, pos_over_neg: 405.3904724121094 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.5568, loss_val: nan, pos_over_neg: 576.8507690429688 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.568, loss_val: nan, pos_over_neg: 460.4956359863281 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 320.7790832519531 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.5459, loss_val: nan, pos_over_neg: 468.8085021972656 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.5326, loss_val: nan, pos_over_neg: 614.547119140625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.6032, loss_val: nan, pos_over_neg: 235.4129638671875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 295.538818359375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 451.1910400390625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.566, loss_val: nan, pos_over_neg: 738.24462890625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.5463, loss_val: nan, pos_over_neg: 581.06396484375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.6453, loss_val: nan, pos_over_neg: 452.1198425292969 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5676, loss_val: nan, pos_over_neg: 617.6052856445312 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.6491, loss_val: nan, pos_over_neg: 416.1248474121094 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.551, loss_val: nan, pos_over_neg: 460.35333251953125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.5711, loss_val: nan, pos_over_neg: 531.94482421875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 430.7696838378906 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6455, loss_val: nan, pos_over_neg: 203.87905883789062 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 322.2735900878906 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.6042, loss_val: nan, pos_over_neg: 382.1476745605469 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.5703, loss_val: nan, pos_over_neg: 430.9438171386719 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.5887, loss_val: nan, pos_over_neg: 466.9500732421875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.549, loss_val: nan, pos_over_neg: 382.61932373046875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.5674, loss_val: nan, pos_over_neg: 706.0973510742188 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.6349, loss_val: nan, pos_over_neg: 412.9219970703125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 392.61785888671875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 590.8200073242188 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6082, loss_val: nan, pos_over_neg: 754.4089965820312 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.5813, loss_val: nan, pos_over_neg: 705.8992919921875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.6132, loss_val: nan, pos_over_neg: 286.0900573730469 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 531.8095703125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 439.3074951171875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 292.7366638183594 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.555, loss_val: nan, pos_over_neg: 351.70501708984375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.5969, loss_val: nan, pos_over_neg: 265.61431884765625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.5718, loss_val: nan, pos_over_neg: 318.2740173339844 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.5625, loss_val: nan, pos_over_neg: 427.5498352050781 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 401.3032531738281 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 305.58837890625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.6201, loss_val: nan, pos_over_neg: 294.722900390625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 789.9166259765625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.5534, loss_val: nan, pos_over_neg: 663.5020751953125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 616.94091796875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 378.79705810546875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.547, loss_val: nan, pos_over_neg: 535.7317504882812 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6089, loss_val: nan, pos_over_neg: 271.2817687988281 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.6351, loss_val: nan, pos_over_neg: 461.3489074707031 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 517.4169311523438 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 322.6947326660156 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.5726, loss_val: nan, pos_over_neg: 398.1289367675781 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 292.482666015625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 440.9124450683594 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.5594, loss_val: nan, pos_over_neg: 388.7591247558594 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.6205, loss_val: nan, pos_over_neg: 303.9408874511719 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 388.7535705566406 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 899.8412475585938 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 632.1367797851562 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.5671, loss_val: nan, pos_over_neg: 595.804443359375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 467.840576171875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.5462, loss_val: nan, pos_over_neg: 601.1289672851562 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.5674, loss_val: nan, pos_over_neg: 592.1591796875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.5816, loss_val: nan, pos_over_neg: 450.166259765625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 434.018310546875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.5612, loss_val: nan, pos_over_neg: 442.85174560546875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 594.7207641601562 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.6339, loss_val: nan, pos_over_neg: 358.4685363769531 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 460.2880554199219 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.5377, loss_val: nan, pos_over_neg: 909.5728149414062 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 619.4386596679688 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.5687, loss_val: nan, pos_over_neg: 516.4049682617188 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 680.9190673828125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 518.5206298828125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.5629, loss_val: nan, pos_over_neg: 389.64398193359375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 261.9661560058594 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.5835, loss_val: nan, pos_over_neg: 375.1492919921875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.6096, loss_val: nan, pos_over_neg: 561.94873046875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.5836, loss_val: nan, pos_over_neg: 382.9464111328125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 471.22906494140625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.5805, loss_val: nan, pos_over_neg: 866.0413208007812 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.5763, loss_val: nan, pos_over_neg: 544.816162109375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.5854, loss_val: nan, pos_over_neg: 475.2469787597656 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.5515, loss_val: nan, pos_over_neg: 503.5054016113281 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.6109, loss_val: nan, pos_over_neg: 378.7151794433594 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 291.7905578613281 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.5614, loss_val: nan, pos_over_neg: 358.7319030761719 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 268.52996826171875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 334.6099853515625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 351.378173828125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.631, loss_val: nan, pos_over_neg: 316.2313537597656 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.5651, loss_val: nan, pos_over_neg: 380.2813415527344 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.6142, loss_val: nan, pos_over_neg: 419.6057434082031 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.6177, loss_val: nan, pos_over_neg: 977.38232421875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 337.0464172363281 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.5623, loss_val: nan, pos_over_neg: 375.81103515625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.574, loss_val: nan, pos_over_neg: 578.3847045898438 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 302.1989440917969 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.533, loss_val: nan, pos_over_neg: 485.7999267578125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.576, loss_val: nan, pos_over_neg: 440.4942321777344 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 433.6802673339844 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.6394, loss_val: nan, pos_over_neg: 221.78445434570312 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.5746, loss_val: nan, pos_over_neg: 431.9346923828125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.5632, loss_val: nan, pos_over_neg: 548.4962158203125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.6036, loss_val: nan, pos_over_neg: 412.8443908691406 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.5887, loss_val: nan, pos_over_neg: 386.04461669921875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.5437, loss_val: nan, pos_over_neg: 740.7998046875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 365.3960266113281 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 734.8955688476562 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 347.1847229003906 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.5935, loss_val: nan, pos_over_neg: 415.9622802734375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.5759, loss_val: nan, pos_over_neg: 312.6600646972656 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.5606, loss_val: nan, pos_over_neg: 408.4066162109375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 534.803955078125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 617.6087036132812 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 637.2846069335938 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 556.7123413085938 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.6278, loss_val: nan, pos_over_neg: 515.7944946289062 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 438.66510009765625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.5903, loss_val: nan, pos_over_neg: 452.10430908203125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.5905, loss_val: nan, pos_over_neg: 333.4917907714844 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.656, loss_val: nan, pos_over_neg: 228.73365783691406 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 542.7109375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.5497, loss_val: nan, pos_over_neg: 538.5233154296875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.5454, loss_val: nan, pos_over_neg: 706.90625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 557.1778564453125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.5711, loss_val: nan, pos_over_neg: 340.2154541015625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 388.7925720214844 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.5914, loss_val: nan, pos_over_neg: 545.1173095703125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.5595, loss_val: nan, pos_over_neg: 568.4593505859375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 620.1390991210938 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.5744, loss_val: nan, pos_over_neg: 534.7506103515625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 550.9158325195312 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 805.717529296875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.5375, loss_val: nan, pos_over_neg: 588.5379638671875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.5723, loss_val: nan, pos_over_neg: 552.4559326171875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 421.1463317871094 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 393.6512145996094 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.5785, loss_val: nan, pos_over_neg: 432.14080810546875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.5622, loss_val: nan, pos_over_neg: 488.3835754394531 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.5813, loss_val: nan, pos_over_neg: 555.8729858398438 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.537, loss_val: nan, pos_over_neg: 559.3906860351562 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.5859, loss_val: nan, pos_over_neg: 372.2184753417969 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.5645, loss_val: nan, pos_over_neg: 528.5503540039062 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.5611, loss_val: nan, pos_over_neg: 715.3462524414062 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.6084, loss_val: nan, pos_over_neg: 445.490478515625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.5501, loss_val: nan, pos_over_neg: 787.5404663085938 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.5812, loss_val: nan, pos_over_neg: 593.0855102539062 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 512.8524169921875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.5628, loss_val: nan, pos_over_neg: 588.185791015625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 490.3547058105469 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.5854, loss_val: nan, pos_over_neg: 498.2961730957031 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.5329, loss_val: nan, pos_over_neg: 556.29736328125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.5775, loss_val: nan, pos_over_neg: 423.74609375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.6231, loss_val: nan, pos_over_neg: 547.4736328125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 570.1624755859375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 411.74542236328125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 476.2149353027344 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.6009, loss_val: nan, pos_over_neg: 385.6786804199219 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 408.7411804199219 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 444.8019104003906 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.5989, loss_val: nan, pos_over_neg: 529.2132568359375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.5969, loss_val: nan, pos_over_neg: 366.68756103515625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 538.4854125976562 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.5604, loss_val: nan, pos_over_neg: 557.4795532226562 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.5935, loss_val: nan, pos_over_neg: 664.6546020507812 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.5534, loss_val: nan, pos_over_neg: 495.353271484375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.5419, loss_val: nan, pos_over_neg: 423.7820739746094 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 462.6492919921875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.6191, loss_val: nan, pos_over_neg: 392.9452819824219 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 638.5870361328125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 552.5382690429688 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 396.1258239746094 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 514.7783203125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 458.1941223144531 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.5619, loss_val: nan, pos_over_neg: 633.9747924804688 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.5776, loss_val: nan, pos_over_neg: 365.9415283203125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 475.17431640625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.5841, loss_val: nan, pos_over_neg: 456.1612548828125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.606, loss_val: nan, pos_over_neg: 462.512939453125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.6323, loss_val: nan, pos_over_neg: 497.08160400390625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.624, loss_val: nan, pos_over_neg: 568.2340698242188 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.5441, loss_val: nan, pos_over_neg: 833.523681640625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.6429, loss_val: nan, pos_over_neg: 331.507080078125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.5367, loss_val: nan, pos_over_neg: 395.8089904785156 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.5528, loss_val: nan, pos_over_neg: 370.69879150390625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 334.90667724609375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.5862, loss_val: nan, pos_over_neg: 489.47808837890625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.6077, loss_val: nan, pos_over_neg: 345.7164306640625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 522.4248657226562 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 1013.7767333984375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 809.641845703125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.5617, loss_val: nan, pos_over_neg: 525.3035888671875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.5533, loss_val: nan, pos_over_neg: 572.8720703125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.5799, loss_val: nan, pos_over_neg: 406.09320068359375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.5712, loss_val: nan, pos_over_neg: 384.31781005859375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 522.5259399414062 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 325.6694641113281 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.6117, loss_val: nan, pos_over_neg: 334.08697509765625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 501.3906555175781 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.53, loss_val: nan, pos_over_neg: 465.1181335449219 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.5802, loss_val: nan, pos_over_neg: 638.4824829101562 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 816.0420532226562 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.5438, loss_val: nan, pos_over_neg: 803.228515625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.5651, loss_val: nan, pos_over_neg: 1371.47705078125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.5749, loss_val: nan, pos_over_neg: 1152.0238037109375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 579.916259765625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 624.061279296875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.5829, loss_val: nan, pos_over_neg: 428.72503662109375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6124, loss_val: nan, pos_over_neg: 432.71490478515625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.5577, loss_val: nan, pos_over_neg: 640.8648681640625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 375.579833984375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.586, loss_val: nan, pos_over_neg: 300.416259765625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.5788, loss_val: nan, pos_over_neg: 523.230224609375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.5923, loss_val: nan, pos_over_neg: 340.73846435546875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 381.6946716308594 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.5486, loss_val: nan, pos_over_neg: 801.548095703125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 986.895263671875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 631.5486450195312 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.6149, loss_val: nan, pos_over_neg: 356.13897705078125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.5785, loss_val: nan, pos_over_neg: 513.9169921875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 655.1891479492188 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.5795, loss_val: nan, pos_over_neg: 473.437255859375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.5724, loss_val: nan, pos_over_neg: 603.2972412109375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 368.318359375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 382.5444641113281 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.5628, loss_val: nan, pos_over_neg: 410.64385986328125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 361.68548583984375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 342.6796875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 324.80657958984375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.6085, loss_val: nan, pos_over_neg: 330.54241943359375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 433.1754455566406 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.5818, loss_val: nan, pos_over_neg: 427.07794189453125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.5658, loss_val: nan, pos_over_neg: 636.9786376953125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.5497, loss_val: nan, pos_over_neg: 885.267822265625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.5918, loss_val: nan, pos_over_neg: 758.7694091796875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.5243, loss_val: nan, pos_over_neg: 623.8650512695312 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 581.9281616210938 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.5875, loss_val: nan, pos_over_neg: 570.2584838867188 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 719.1871337890625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.5509, loss_val: nan, pos_over_neg: 438.1470031738281 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.5892, loss_val: nan, pos_over_neg: 488.80609130859375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.5682, loss_val: nan, pos_over_neg: 587.0661010742188 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.6205, loss_val: nan, pos_over_neg: 378.7099304199219 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 578.2667846679688 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 654.5157470703125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 759.296142578125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 532.8418579101562 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 532.9852294921875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.5861, loss_val: nan, pos_over_neg: 490.5242614746094 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 504.35772705078125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.5597, loss_val: nan, pos_over_neg: 608.00146484375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 445.1199035644531 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.551, loss_val: nan, pos_over_neg: 669.818115234375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.555, loss_val: nan, pos_over_neg: 467.7350158691406 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.5977, loss_val: nan, pos_over_neg: 487.7156066894531 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.5976, loss_val: nan, pos_over_neg: 410.1924743652344 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 387.0805358886719 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.587, loss_val: nan, pos_over_neg: 542.66845703125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.5798, loss_val: nan, pos_over_neg: 445.201171875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.5841, loss_val: nan, pos_over_neg: 419.0601501464844 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 368.3799133300781 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 864.041748046875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 406.7291259765625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 360.6655578613281 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6448, loss_val: nan, pos_over_neg: 201.9159393310547 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.571, loss_val: nan, pos_over_neg: 653.81591796875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.6409, loss_val: nan, pos_over_neg: 213.1555633544922 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.5923, loss_val: nan, pos_over_neg: 304.80596923828125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 393.52239990234375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 614.6658325195312 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 343.27288818359375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 563.416015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 685.8712768554688 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.5466, loss_val: nan, pos_over_neg: 999.274169921875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.5681, loss_val: nan, pos_over_neg: 763.350341796875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.5634, loss_val: nan, pos_over_neg: 740.0825805664062 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.5494, loss_val: nan, pos_over_neg: 641.1146240234375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.6125, loss_val: nan, pos_over_neg: 461.82177734375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.5683, loss_val: nan, pos_over_neg: 505.9418640136719 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.5759, loss_val: nan, pos_over_neg: 577.1941528320312 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 361.2494201660156 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.6003, loss_val: nan, pos_over_neg: 310.6871643066406 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 262.1134948730469 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.5572, loss_val: nan, pos_over_neg: 466.32989501953125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 471.918212890625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 406.49371337890625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.5602, loss_val: nan, pos_over_neg: 953.4475708007812 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 609.4808349609375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.5711, loss_val: nan, pos_over_neg: 773.7142944335938 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.5874, loss_val: nan, pos_over_neg: 609.3092651367188 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 698.327392578125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 837.8269653320312 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.5656, loss_val: nan, pos_over_neg: 459.90203857421875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.5585, loss_val: nan, pos_over_neg: 372.6336364746094 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.5829, loss_val: nan, pos_over_neg: 328.8866271972656 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.569, loss_val: nan, pos_over_neg: 595.8021850585938 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.5728, loss_val: nan, pos_over_neg: 499.2942810058594 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.5584, loss_val: nan, pos_over_neg: 513.0218505859375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.545, loss_val: nan, pos_over_neg: 411.77740478515625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.554, loss_val: nan, pos_over_neg: 482.1474304199219 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6009, loss_val: nan, pos_over_neg: 414.5712890625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 834.6642456054688 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 424.9650573730469 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 553.29931640625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.5485, loss_val: nan, pos_over_neg: 741.51025390625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.5733, loss_val: nan, pos_over_neg: 446.7976379394531 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 539.6672973632812 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.5405, loss_val: nan, pos_over_neg: 395.5571594238281 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.5947, loss_val: nan, pos_over_neg: 315.7199401855469 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 474.8916015625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.575, loss_val: nan, pos_over_neg: 698.0855102539062 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.5573, loss_val: nan, pos_over_neg: 535.0331420898438 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 533.785400390625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.5861, loss_val: nan, pos_over_neg: 549.6443481445312 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 457.3747253417969 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 516.0853271484375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.6204, loss_val: nan, pos_over_neg: 478.4842834472656 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.5497, loss_val: nan, pos_over_neg: 558.732177734375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 624.30908203125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 533.9193115234375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 660.9934692382812 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.5726, loss_val: nan, pos_over_neg: 612.546875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.5861, loss_val: nan, pos_over_neg: 383.3191223144531 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 522.47705078125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 514.8347778320312 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 502.2815246582031 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 795.0862426757812 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.5466, loss_val: nan, pos_over_neg: 999.3931274414062 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.6278, loss_val: nan, pos_over_neg: 435.4407958984375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.5968, loss_val: nan, pos_over_neg: 558.4744262695312 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 536.5673217773438 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 585.436279296875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 579.5625610351562 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.5816, loss_val: nan, pos_over_neg: 541.0038452148438 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 456.829345703125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.5671, loss_val: nan, pos_over_neg: 447.38189697265625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.6077, loss_val: nan, pos_over_neg: 411.10382080078125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.6125, loss_val: nan, pos_over_neg: 382.90509033203125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 340.2403564453125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.5855, loss_val: nan, pos_over_neg: 288.414306640625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 401.4579162597656 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.5729, loss_val: nan, pos_over_neg: 489.4040222167969 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.5867, loss_val: nan, pos_over_neg: 507.98382568359375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.5844, loss_val: nan, pos_over_neg: 543.3451538085938 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.5674, loss_val: nan, pos_over_neg: 565.514404296875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.5906, loss_val: nan, pos_over_neg: 425.6572265625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.5896, loss_val: nan, pos_over_neg: 855.49169921875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 509.0895080566406 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.5575, loss_val: nan, pos_over_neg: 880.869384765625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 537.2015991210938 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.5594, loss_val: nan, pos_over_neg: 485.37249755859375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.586, loss_val: nan, pos_over_neg: 416.388427734375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 451.3023681640625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 342.76177978515625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.5696, loss_val: nan, pos_over_neg: 281.9353332519531 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.5394, loss_val: nan, pos_over_neg: 569.730224609375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.5616, loss_val: nan, pos_over_neg: 716.3583374023438 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.5511, loss_val: nan, pos_over_neg: 845.2980346679688 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 536.6416625976562 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 645.9395141601562 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 806.1331787109375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 328.96673583984375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.5839, loss_val: nan, pos_over_neg: 339.3822326660156 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.6196, loss_val: nan, pos_over_neg: 424.4526672363281 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.557, loss_val: nan, pos_over_neg: 576.552490234375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.5643, loss_val: nan, pos_over_neg: 563.8414306640625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.5947, loss_val: nan, pos_over_neg: 497.2013854980469 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.5566, loss_val: nan, pos_over_neg: 453.1129150390625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 372.4270935058594 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.5808, loss_val: nan, pos_over_neg: 362.1745300292969 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.5712, loss_val: nan, pos_over_neg: 345.34027099609375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.5455, loss_val: nan, pos_over_neg: 593.083984375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 465.1492004394531 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 520.8991088867188 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.5492, loss_val: nan, pos_over_neg: 625.6246337890625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 414.2587890625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.5879, loss_val: nan, pos_over_neg: 348.3247985839844 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 574.109619140625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.5727, loss_val: nan, pos_over_neg: 596.612548828125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.5797, loss_val: nan, pos_over_neg: 575.081787109375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.5559, loss_val: nan, pos_over_neg: 408.6129455566406 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 278.8819885253906 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 480.2305908203125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.5865, loss_val: nan, pos_over_neg: 363.9208984375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.56, loss_val: nan, pos_over_neg: 528.7726440429688 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 445.750244140625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 643.7620849609375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 427.8435974121094 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.5504, loss_val: nan, pos_over_neg: 431.6734313964844 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.5632, loss_val: nan, pos_over_neg: 594.3424072265625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 318.3960876464844 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.5734, loss_val: nan, pos_over_neg: 406.63134765625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 419.8949279785156 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.5744, loss_val: nan, pos_over_neg: 367.89923095703125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 626.3933715820312 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 391.8834228515625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.6567, loss_val: nan, pos_over_neg: 547.7824096679688 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.5879, loss_val: nan, pos_over_neg: 467.87530517578125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.5467, loss_val: nan, pos_over_neg: 855.1581420898438 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.5594, loss_val: nan, pos_over_neg: 493.400634765625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.5768, loss_val: nan, pos_over_neg: 429.8603210449219 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.5534, loss_val: nan, pos_over_neg: 540.8876342773438 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 481.8138427734375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 322.7439270019531 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 441.6514587402344 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.6056, loss_val: nan, pos_over_neg: 335.5252685546875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 444.7862854003906 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.565, loss_val: nan, pos_over_neg: 744.6444091796875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.5233, loss_val: nan, pos_over_neg: 751.6986694335938 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.5737, loss_val: nan, pos_over_neg: 435.04248046875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.5469, loss_val: nan, pos_over_neg: 613.1873168945312 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 301.32171630859375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 396.89837646484375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 472.1370849609375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.5696, loss_val: nan, pos_over_neg: 500.630859375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.5652, loss_val: nan, pos_over_neg: 550.4421997070312 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 490.4645690917969 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 397.67578125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.5507, loss_val: nan, pos_over_neg: 841.7818603515625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.6519, loss_val: nan, pos_over_neg: 241.0442657470703 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.5484, loss_val: nan, pos_over_neg: 504.9397888183594 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.5653, loss_val: nan, pos_over_neg: 438.4671630859375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 423.4219055175781 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.6166, loss_val: nan, pos_over_neg: 445.54315185546875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.5978, loss_val: nan, pos_over_neg: 530.8316040039062 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.5323, loss_val: nan, pos_over_neg: 703.6526489257812 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.6054, loss_val: nan, pos_over_neg: 461.1497497558594 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 569.8170166015625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.5915, loss_val: nan, pos_over_neg: 492.4916687011719 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/300000 [4:01:31<75335:09:35, 904.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 349.70611572265625 lr: 0.00031623\n",
      "epoch: 16\n",
      "Iter: 0/695, loss_train: 3.5849, loss_val: nan, pos_over_neg: 396.61883544921875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.5788, loss_val: nan, pos_over_neg: 405.5224609375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.5805, loss_val: nan, pos_over_neg: 302.805419921875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.5547, loss_val: nan, pos_over_neg: 618.8518676757812 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 823.79345703125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.551, loss_val: nan, pos_over_neg: 811.4124755859375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.5577, loss_val: nan, pos_over_neg: 589.1451416015625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.6124, loss_val: nan, pos_over_neg: 503.84246826171875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 543.9886474609375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.547, loss_val: nan, pos_over_neg: 1145.6395263671875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 551.616455078125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.5568, loss_val: nan, pos_over_neg: 388.2113037109375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 504.2732849121094 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.5534, loss_val: nan, pos_over_neg: 391.1296691894531 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.5849, loss_val: nan, pos_over_neg: 341.52032470703125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 407.8898620605469 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.6206, loss_val: nan, pos_over_neg: 396.7862243652344 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 539.0037841796875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.5474, loss_val: nan, pos_over_neg: 455.0520935058594 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.5785, loss_val: nan, pos_over_neg: 473.90185546875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.5622, loss_val: nan, pos_over_neg: 647.5046997070312 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.5456, loss_val: nan, pos_over_neg: 863.9876708984375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.5866, loss_val: nan, pos_over_neg: 528.5020141601562 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 352.4622497558594 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.5809, loss_val: nan, pos_over_neg: 696.7229614257812 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.5771, loss_val: nan, pos_over_neg: 703.62060546875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.5694, loss_val: nan, pos_over_neg: 421.6050109863281 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 403.26739501953125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.5521, loss_val: nan, pos_over_neg: 475.6661071777344 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 450.7094421386719 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 500.38616943359375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.5694, loss_val: nan, pos_over_neg: 755.6046752929688 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.5737, loss_val: nan, pos_over_neg: 702.8107299804688 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 471.67230224609375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 768.1089477539062 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 353.50860595703125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.5983, loss_val: nan, pos_over_neg: 250.7952117919922 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.5985, loss_val: nan, pos_over_neg: 300.0021057128906 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 345.85113525390625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.6162, loss_val: nan, pos_over_neg: 489.4075927734375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 504.05780029296875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.5576, loss_val: nan, pos_over_neg: 624.2017211914062 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.5833, loss_val: nan, pos_over_neg: 323.85791015625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.5504, loss_val: nan, pos_over_neg: 536.376220703125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.5288, loss_val: nan, pos_over_neg: 895.9574584960938 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 342.8655090332031 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 505.39447021484375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.5479, loss_val: nan, pos_over_neg: 388.04791259765625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 343.2146301269531 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.5697, loss_val: nan, pos_over_neg: 374.3215026855469 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 621.606201171875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.5903, loss_val: nan, pos_over_neg: 344.6332092285156 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.5798, loss_val: nan, pos_over_neg: 358.3563537597656 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.6055, loss_val: nan, pos_over_neg: 313.36822509765625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.5808, loss_val: nan, pos_over_neg: 448.5568542480469 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 459.2336730957031 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.5951, loss_val: nan, pos_over_neg: 384.8397521972656 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 270.2458801269531 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.5924, loss_val: nan, pos_over_neg: 453.6756896972656 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 290.48370361328125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 481.5509033203125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 779.2227783203125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.5272, loss_val: nan, pos_over_neg: 656.5038452148438 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.5374, loss_val: nan, pos_over_neg: 584.73388671875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 485.20196533203125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 309.43975830078125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.5784, loss_val: nan, pos_over_neg: 542.533447265625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 364.6967468261719 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.553, loss_val: nan, pos_over_neg: 425.72515869140625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 311.7523498535156 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.5797, loss_val: nan, pos_over_neg: 373.81378173828125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.5567, loss_val: nan, pos_over_neg: 531.7410888671875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.5471, loss_val: nan, pos_over_neg: 481.2319641113281 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.6207, loss_val: nan, pos_over_neg: 549.9463500976562 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.6169, loss_val: nan, pos_over_neg: 427.0782165527344 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 694.3394775390625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 538.9761962890625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 462.1222229003906 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.5832, loss_val: nan, pos_over_neg: 771.4974975585938 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.6068, loss_val: nan, pos_over_neg: 424.8413391113281 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 237.8107452392578 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.5515, loss_val: nan, pos_over_neg: 443.1628112792969 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 433.6012878417969 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 370.114990234375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.5695, loss_val: nan, pos_over_neg: 613.5023803710938 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.5539, loss_val: nan, pos_over_neg: 512.7240600585938 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.5584, loss_val: nan, pos_over_neg: 720.3851928710938 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.5957, loss_val: nan, pos_over_neg: 327.87457275390625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.6288, loss_val: nan, pos_over_neg: 384.8506774902344 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.5797, loss_val: nan, pos_over_neg: 599.2052612304688 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.6266, loss_val: nan, pos_over_neg: 294.318115234375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.5984, loss_val: nan, pos_over_neg: 428.76617431640625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 454.3559875488281 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.5552, loss_val: nan, pos_over_neg: 1200.3270263671875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 686.3638305664062 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.5573, loss_val: nan, pos_over_neg: 464.3255920410156 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 722.4940185546875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 420.9507751464844 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.5285, loss_val: nan, pos_over_neg: 618.7383422851562 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.589, loss_val: nan, pos_over_neg: 394.5953674316406 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.6002, loss_val: nan, pos_over_neg: 262.5364990234375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.5733, loss_val: nan, pos_over_neg: 519.9380493164062 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.5942, loss_val: nan, pos_over_neg: 241.5796661376953 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.5509, loss_val: nan, pos_over_neg: 465.1253662109375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.5522, loss_val: nan, pos_over_neg: 728.5730590820312 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.5775, loss_val: nan, pos_over_neg: 564.30224609375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.5753, loss_val: nan, pos_over_neg: 759.9073486328125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.6072, loss_val: nan, pos_over_neg: 671.74267578125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.5892, loss_val: nan, pos_over_neg: 550.4685668945312 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 469.1688232421875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6377, loss_val: nan, pos_over_neg: 465.5736999511719 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 526.5420532226562 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.5697, loss_val: nan, pos_over_neg: 984.162109375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.5935, loss_val: nan, pos_over_neg: 530.6392822265625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.61, loss_val: nan, pos_over_neg: 322.30572509765625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.5715, loss_val: nan, pos_over_neg: 618.4907836914062 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 348.85577392578125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.57, loss_val: nan, pos_over_neg: 676.7527465820312 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.5554, loss_val: nan, pos_over_neg: 543.5197143554688 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 815.7396240234375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 539.786865234375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.6249, loss_val: nan, pos_over_neg: 501.7472839355469 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 473.95672607421875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.5387, loss_val: nan, pos_over_neg: 561.803955078125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.5533, loss_val: nan, pos_over_neg: 384.4087829589844 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 494.3417663574219 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 457.0777893066406 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.5886, loss_val: nan, pos_over_neg: 358.81280517578125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.5467, loss_val: nan, pos_over_neg: 412.0672912597656 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.5784, loss_val: nan, pos_over_neg: 286.9804382324219 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 270.5663146972656 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.5497, loss_val: nan, pos_over_neg: 406.4144592285156 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.5695, loss_val: nan, pos_over_neg: 509.96490478515625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 520.306396484375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.5449, loss_val: nan, pos_over_neg: 560.2637329101562 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.5721, loss_val: nan, pos_over_neg: 265.9872741699219 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.5359, loss_val: nan, pos_over_neg: 480.17523193359375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.575, loss_val: nan, pos_over_neg: 357.98822021484375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.5859, loss_val: nan, pos_over_neg: 350.2770690917969 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.5906, loss_val: nan, pos_over_neg: 365.5444030761719 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 306.389404296875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.6547, loss_val: nan, pos_over_neg: 310.39453125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 452.85308837890625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.5986, loss_val: nan, pos_over_neg: 493.9531555175781 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.619, loss_val: nan, pos_over_neg: 355.0335693359375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.6282, loss_val: nan, pos_over_neg: 272.59747314453125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 471.3404235839844 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.5788, loss_val: nan, pos_over_neg: 375.4778137207031 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.6084, loss_val: nan, pos_over_neg: 293.36114501953125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 287.2398376464844 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.5678, loss_val: nan, pos_over_neg: 414.85638427734375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.5351, loss_val: nan, pos_over_neg: 868.0272216796875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.5573, loss_val: nan, pos_over_neg: 784.7086181640625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.5502, loss_val: nan, pos_over_neg: 651.6648559570312 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.5642, loss_val: nan, pos_over_neg: 387.9022216796875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.5588, loss_val: nan, pos_over_neg: 363.0451965332031 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.5397, loss_val: nan, pos_over_neg: 390.85552978515625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.5554, loss_val: nan, pos_over_neg: 444.1427307128906 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 397.78704833984375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.5469, loss_val: nan, pos_over_neg: 454.5355224609375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 299.347900390625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.5578, loss_val: nan, pos_over_neg: 376.6048583984375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 289.224609375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.5855, loss_val: nan, pos_over_neg: 339.9549560546875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.5496, loss_val: nan, pos_over_neg: 479.6965637207031 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.5844, loss_val: nan, pos_over_neg: 745.6856689453125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.6041, loss_val: nan, pos_over_neg: 478.0079345703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.5351, loss_val: nan, pos_over_neg: 608.6266479492188 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.5514, loss_val: nan, pos_over_neg: 608.7062377929688 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.5157, loss_val: nan, pos_over_neg: 1339.87255859375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.5696, loss_val: nan, pos_over_neg: 835.05029296875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.5886, loss_val: nan, pos_over_neg: 723.8082275390625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.5616, loss_val: nan, pos_over_neg: 563.607666015625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.5547, loss_val: nan, pos_over_neg: 644.3395385742188 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 562.24755859375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 346.7600402832031 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.6066, loss_val: nan, pos_over_neg: 335.9175720214844 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.5333, loss_val: nan, pos_over_neg: 683.8663940429688 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 404.5570373535156 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 481.35321044921875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.6319, loss_val: nan, pos_over_neg: 355.64739990234375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.5418, loss_val: nan, pos_over_neg: 797.6792602539062 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.544, loss_val: nan, pos_over_neg: 710.3621215820312 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 736.989013671875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.6177, loss_val: nan, pos_over_neg: 469.50018310546875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.5573, loss_val: nan, pos_over_neg: 611.3639526367188 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 339.25225830078125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 461.59002685546875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.5558, loss_val: nan, pos_over_neg: 544.5287475585938 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 608.8927001953125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.5931, loss_val: nan, pos_over_neg: 403.76031494140625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.569, loss_val: nan, pos_over_neg: 350.4591979980469 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.6293, loss_val: nan, pos_over_neg: 449.5538330078125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 731.7701416015625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 814.555908203125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.6014, loss_val: nan, pos_over_neg: 619.2570190429688 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 623.1319580078125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 409.1500244140625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 361.5151672363281 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.5557, loss_val: nan, pos_over_neg: 562.3019409179688 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 426.7159118652344 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 478.81463623046875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.5995, loss_val: nan, pos_over_neg: 325.816650390625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.5764, loss_val: nan, pos_over_neg: 495.38397216796875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.5817, loss_val: nan, pos_over_neg: 338.10272216796875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 366.103515625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.5736, loss_val: nan, pos_over_neg: 423.7248229980469 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.6471, loss_val: nan, pos_over_neg: 317.9625244140625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 649.7627563476562 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.5726, loss_val: nan, pos_over_neg: 875.0432739257812 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.5698, loss_val: nan, pos_over_neg: 1109.7462158203125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.5577, loss_val: nan, pos_over_neg: 595.5427856445312 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 434.08245849609375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.5695, loss_val: nan, pos_over_neg: 414.8164978027344 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.5607, loss_val: nan, pos_over_neg: 478.2230529785156 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 504.11474609375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.5759, loss_val: nan, pos_over_neg: 372.8459777832031 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.5522, loss_val: nan, pos_over_neg: 415.8352355957031 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.5845, loss_val: nan, pos_over_neg: 296.46630859375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.6011, loss_val: nan, pos_over_neg: 318.84259033203125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 472.06878662109375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 428.4630126953125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 373.497802734375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 554.1824340820312 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 538.4649658203125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 732.6069946289062 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.6377, loss_val: nan, pos_over_neg: 407.5619812011719 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.5874, loss_val: nan, pos_over_neg: 533.8429565429688 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.5317, loss_val: nan, pos_over_neg: 603.03466796875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.5679, loss_val: nan, pos_over_neg: 458.8631896972656 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.571, loss_val: nan, pos_over_neg: 454.40869140625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 358.23095703125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.5497, loss_val: nan, pos_over_neg: 621.0382080078125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.5452, loss_val: nan, pos_over_neg: 763.8833618164062 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.5362, loss_val: nan, pos_over_neg: 665.6326904296875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.5482, loss_val: nan, pos_over_neg: 454.06024169921875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.5619, loss_val: nan, pos_over_neg: 676.66552734375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.6101, loss_val: nan, pos_over_neg: 373.3537292480469 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.5351, loss_val: nan, pos_over_neg: 731.7951049804688 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 554.3433227539062 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.6215, loss_val: nan, pos_over_neg: 639.0148315429688 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.5849, loss_val: nan, pos_over_neg: 767.9970703125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.5866, loss_val: nan, pos_over_neg: 667.6743774414062 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 495.17193603515625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 371.81292724609375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.5546, loss_val: nan, pos_over_neg: 532.539794921875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.5836, loss_val: nan, pos_over_neg: 657.010986328125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.5659, loss_val: nan, pos_over_neg: 638.4309692382812 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 505.1758117675781 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.5822, loss_val: nan, pos_over_neg: 534.787841796875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 583.8251342773438 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.5696, loss_val: nan, pos_over_neg: 529.6259155273438 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.5663, loss_val: nan, pos_over_neg: 809.3026123046875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.5579, loss_val: nan, pos_over_neg: 611.04248046875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6259, loss_val: nan, pos_over_neg: 302.780029296875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.5635, loss_val: nan, pos_over_neg: 547.190185546875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 506.7659912109375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.5708, loss_val: nan, pos_over_neg: 469.3179626464844 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 531.2801513671875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 653.175537109375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.5671, loss_val: nan, pos_over_neg: 669.4370727539062 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.6134, loss_val: nan, pos_over_neg: 536.7537841796875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 426.7802429199219 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.5395, loss_val: nan, pos_over_neg: 513.70654296875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 862.894775390625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.5569, loss_val: nan, pos_over_neg: 462.51678466796875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.5511, loss_val: nan, pos_over_neg: 658.1349487304688 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.565, loss_val: nan, pos_over_neg: 667.5601806640625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.5872, loss_val: nan, pos_over_neg: 385.8184509277344 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 375.3358154296875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.6075, loss_val: nan, pos_over_neg: 455.5002136230469 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.5978, loss_val: nan, pos_over_neg: 353.54254150390625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.5737, loss_val: nan, pos_over_neg: 482.3038024902344 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.6248, loss_val: nan, pos_over_neg: 423.7410888671875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.558, loss_val: nan, pos_over_neg: 345.8840637207031 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.5663, loss_val: nan, pos_over_neg: 468.1810302734375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6074, loss_val: nan, pos_over_neg: 333.5521545410156 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 691.7122192382812 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 840.4185180664062 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.5698, loss_val: nan, pos_over_neg: 626.9728393554688 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.5547, loss_val: nan, pos_over_neg: 358.192138671875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.57, loss_val: nan, pos_over_neg: 349.2992858886719 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6136, loss_val: nan, pos_over_neg: 398.87701416015625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.5556, loss_val: nan, pos_over_neg: 439.8838195800781 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 624.7230834960938 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.5979, loss_val: nan, pos_over_neg: 340.54534912109375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.5899, loss_val: nan, pos_over_neg: 468.63482666015625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 401.7716369628906 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.5726, loss_val: nan, pos_over_neg: 420.1313781738281 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 416.4877624511719 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.5412, loss_val: nan, pos_over_neg: 484.8125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.5938, loss_val: nan, pos_over_neg: 471.4122009277344 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.5671, loss_val: nan, pos_over_neg: 378.58197021484375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.5739, loss_val: nan, pos_over_neg: 486.7456970214844 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 471.7359313964844 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.5122, loss_val: nan, pos_over_neg: 797.8796997070312 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 275.2460632324219 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.5963, loss_val: nan, pos_over_neg: 333.8247985839844 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 554.3203735351562 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.576, loss_val: nan, pos_over_neg: 486.17822265625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.4937, loss_val: nan, pos_over_neg: 1048.3455810546875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 488.80145263671875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.5759, loss_val: nan, pos_over_neg: 658.2666015625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6363, loss_val: nan, pos_over_neg: 367.02484130859375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.6046, loss_val: nan, pos_over_neg: 586.0245361328125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.5465, loss_val: nan, pos_over_neg: 520.370849609375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.551, loss_val: nan, pos_over_neg: 489.3437805175781 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.5505, loss_val: nan, pos_over_neg: 591.3228759765625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.6016, loss_val: nan, pos_over_neg: 404.9010925292969 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.5623, loss_val: nan, pos_over_neg: 542.3206176757812 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.5577, loss_val: nan, pos_over_neg: 404.1263122558594 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6333, loss_val: nan, pos_over_neg: 216.67529296875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 330.8323974609375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 352.30242919921875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.6248, loss_val: nan, pos_over_neg: 360.0699157714844 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.5571, loss_val: nan, pos_over_neg: 583.0230712890625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6153, loss_val: nan, pos_over_neg: 767.300537109375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.5416, loss_val: nan, pos_over_neg: 647.9324951171875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 873.4738159179688 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 443.99755859375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.6641, loss_val: nan, pos_over_neg: 293.2962646484375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.6037, loss_val: nan, pos_over_neg: 367.418701171875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 679.1912841796875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 413.7947082519531 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.6577, loss_val: nan, pos_over_neg: 297.44097900390625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.5504, loss_val: nan, pos_over_neg: 539.7007446289062 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.5818, loss_val: nan, pos_over_neg: 445.338623046875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.6085, loss_val: nan, pos_over_neg: 369.2471923828125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.5614, loss_val: nan, pos_over_neg: 570.9241333007812 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.6139, loss_val: nan, pos_over_neg: 441.21734619140625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.5526, loss_val: nan, pos_over_neg: 739.1654663085938 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.5742, loss_val: nan, pos_over_neg: 495.60174560546875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.5385, loss_val: nan, pos_over_neg: 906.975830078125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.5807, loss_val: nan, pos_over_neg: 410.0959777832031 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 504.5551452636719 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.6004, loss_val: nan, pos_over_neg: 300.9994812011719 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.6151, loss_val: nan, pos_over_neg: 332.31475830078125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.5967, loss_val: nan, pos_over_neg: 398.7244567871094 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 437.91156005859375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.5583, loss_val: nan, pos_over_neg: 554.7048950195312 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.6031, loss_val: nan, pos_over_neg: 648.423095703125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.5808, loss_val: nan, pos_over_neg: 477.28582763671875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.6148, loss_val: nan, pos_over_neg: 502.6112976074219 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.5992, loss_val: nan, pos_over_neg: 578.709228515625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.632, loss_val: nan, pos_over_neg: 501.1552429199219 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.5836, loss_val: nan, pos_over_neg: 577.9140014648438 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 471.8219909667969 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.5887, loss_val: nan, pos_over_neg: 636.5466918945312 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 656.3135986328125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.5969, loss_val: nan, pos_over_neg: 295.3780822753906 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.6393, loss_val: nan, pos_over_neg: 235.9739532470703 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.5858, loss_val: nan, pos_over_neg: 360.8453674316406 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.5917, loss_val: nan, pos_over_neg: 397.3204040527344 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.5721, loss_val: nan, pos_over_neg: 565.9276123046875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.5475, loss_val: nan, pos_over_neg: 846.849853515625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.5858, loss_val: nan, pos_over_neg: 424.96917724609375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.5662, loss_val: nan, pos_over_neg: 691.3319702148438 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 613.2352294921875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.5299, loss_val: nan, pos_over_neg: 1886.4075927734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.5657, loss_val: nan, pos_over_neg: 600.0848388671875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.578, loss_val: nan, pos_over_neg: 394.5993957519531 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 451.58160400390625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 420.5831604003906 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.6188, loss_val: nan, pos_over_neg: 328.43408203125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.6105, loss_val: nan, pos_over_neg: 480.5953674316406 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 449.036865234375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.6157, loss_val: nan, pos_over_neg: 311.7332458496094 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 506.1144714355469 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.5609, loss_val: nan, pos_over_neg: 645.5955810546875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.5644, loss_val: nan, pos_over_neg: 817.094970703125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 572.8992919921875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.5993, loss_val: nan, pos_over_neg: 571.1323852539062 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.5705, loss_val: nan, pos_over_neg: 342.8013000488281 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 449.4559020996094 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6112, loss_val: nan, pos_over_neg: 367.30987548828125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.5442, loss_val: nan, pos_over_neg: 1092.40234375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.5688, loss_val: nan, pos_over_neg: 484.7552490234375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.5939, loss_val: nan, pos_over_neg: 474.5577697753906 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.5773, loss_val: nan, pos_over_neg: 395.105224609375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 447.36114501953125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.5843, loss_val: nan, pos_over_neg: 507.02044677734375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.5237, loss_val: nan, pos_over_neg: 747.2246704101562 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.5862, loss_val: nan, pos_over_neg: 535.6840209960938 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 416.7034912109375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.5759, loss_val: nan, pos_over_neg: 332.9095764160156 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.5659, loss_val: nan, pos_over_neg: 381.34869384765625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 358.4833984375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.517, loss_val: nan, pos_over_neg: 729.6107788085938 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.5411, loss_val: nan, pos_over_neg: 744.5783081054688 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.5768, loss_val: nan, pos_over_neg: 474.8358459472656 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 376.2646179199219 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.5873, loss_val: nan, pos_over_neg: 465.5928955078125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.5353, loss_val: nan, pos_over_neg: 1144.4639892578125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 513.1414184570312 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 367.1958312988281 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 318.59527587890625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 333.3292541503906 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 492.3485107421875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.5681, loss_val: nan, pos_over_neg: 545.412841796875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.596, loss_val: nan, pos_over_neg: 405.6517028808594 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.5708, loss_val: nan, pos_over_neg: 611.664794921875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.5528, loss_val: nan, pos_over_neg: 593.8447265625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.5675, loss_val: nan, pos_over_neg: 415.0147399902344 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 665.4188232421875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.5732, loss_val: nan, pos_over_neg: 411.30718994140625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.5496, loss_val: nan, pos_over_neg: 467.8904113769531 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.5758, loss_val: nan, pos_over_neg: 492.88702392578125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.5362, loss_val: nan, pos_over_neg: 467.80902099609375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.5757, loss_val: nan, pos_over_neg: 313.80780029296875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.5583, loss_val: nan, pos_over_neg: 535.6470947265625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.5505, loss_val: nan, pos_over_neg: 563.4717407226562 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.5956, loss_val: nan, pos_over_neg: 446.5871276855469 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.5602, loss_val: nan, pos_over_neg: 557.3411254882812 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.6025, loss_val: nan, pos_over_neg: 356.18212890625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.5817, loss_val: nan, pos_over_neg: 719.9160766601562 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.5823, loss_val: nan, pos_over_neg: 593.1560668945312 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 392.0308532714844 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.5711, loss_val: nan, pos_over_neg: 571.2674560546875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 535.2177124023438 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 440.4945373535156 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 503.0380554199219 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.5545, loss_val: nan, pos_over_neg: 557.0914306640625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.5759, loss_val: nan, pos_over_neg: 609.40771484375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 514.3462524414062 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.5823, loss_val: nan, pos_over_neg: 419.9853210449219 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.5917, loss_val: nan, pos_over_neg: 1223.6199951171875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 607.71533203125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.5845, loss_val: nan, pos_over_neg: 341.8246154785156 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.5752, loss_val: nan, pos_over_neg: 715.9400024414062 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 581.0850830078125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 557.7835693359375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.564, loss_val: nan, pos_over_neg: 506.59429931640625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.627, loss_val: nan, pos_over_neg: 371.3984375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 391.6776123046875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.5786, loss_val: nan, pos_over_neg: 689.2688598632812 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.6044, loss_val: nan, pos_over_neg: 343.1463317871094 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 631.1741943359375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.5468, loss_val: nan, pos_over_neg: 508.9031066894531 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.5399, loss_val: nan, pos_over_neg: 728.9957885742188 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 522.2839965820312 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.6104, loss_val: nan, pos_over_neg: 439.8836975097656 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.5891, loss_val: nan, pos_over_neg: 427.2976989746094 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 558.227783203125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 329.00396728515625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.5485, loss_val: nan, pos_over_neg: 379.92999267578125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.5597, loss_val: nan, pos_over_neg: 281.2995910644531 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.6001, loss_val: nan, pos_over_neg: 302.7413024902344 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.5558, loss_val: nan, pos_over_neg: 410.7316589355469 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 334.9153137207031 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.5611, loss_val: nan, pos_over_neg: 389.490478515625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.5697, loss_val: nan, pos_over_neg: 424.1447448730469 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.5438, loss_val: nan, pos_over_neg: 978.2168579101562 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 643.2865600585938 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.6007, loss_val: nan, pos_over_neg: 484.82318115234375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 501.4947814941406 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.5724, loss_val: nan, pos_over_neg: 447.9301452636719 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.5179, loss_val: nan, pos_over_neg: 526.7205810546875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 783.3494262695312 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.5676, loss_val: nan, pos_over_neg: 396.9455261230469 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.5959, loss_val: nan, pos_over_neg: 323.11248779296875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.548, loss_val: nan, pos_over_neg: 403.9503173828125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.5936, loss_val: nan, pos_over_neg: 319.7207336425781 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 413.9256286621094 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.5726, loss_val: nan, pos_over_neg: 412.56158447265625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.591, loss_val: nan, pos_over_neg: 395.3950500488281 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.5722, loss_val: nan, pos_over_neg: 695.4053344726562 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.5544, loss_val: nan, pos_over_neg: 455.49700927734375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 389.8703308105469 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.55, loss_val: nan, pos_over_neg: 1050.30078125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.6098, loss_val: nan, pos_over_neg: 361.2000427246094 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 668.4356689453125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.5698, loss_val: nan, pos_over_neg: 668.2615966796875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 455.0343322753906 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.6054, loss_val: nan, pos_over_neg: 246.84283447265625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.5578, loss_val: nan, pos_over_neg: 454.4812927246094 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 319.93084716796875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.5767, loss_val: nan, pos_over_neg: 379.8381042480469 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.5984, loss_val: nan, pos_over_neg: 338.9071960449219 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.6056, loss_val: nan, pos_over_neg: 360.8655700683594 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 433.1045227050781 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.5825, loss_val: nan, pos_over_neg: 452.0044860839844 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 327.3170166015625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.5588, loss_val: nan, pos_over_neg: 740.0745239257812 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.5492, loss_val: nan, pos_over_neg: 774.3485107421875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.5676, loss_val: nan, pos_over_neg: 944.84375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.5679, loss_val: nan, pos_over_neg: 493.8022766113281 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.5595, loss_val: nan, pos_over_neg: 402.6266174316406 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.5502, loss_val: nan, pos_over_neg: 961.1173706054688 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.5799, loss_val: nan, pos_over_neg: 379.45001220703125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.5716, loss_val: nan, pos_over_neg: 322.0849304199219 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.5523, loss_val: nan, pos_over_neg: 396.12078857421875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.577, loss_val: nan, pos_over_neg: 304.6492614746094 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.5756, loss_val: nan, pos_over_neg: 581.9237670898438 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.5554, loss_val: nan, pos_over_neg: 441.5025634765625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 425.4974060058594 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.5877, loss_val: nan, pos_over_neg: 390.3514404296875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6136, loss_val: nan, pos_over_neg: 447.9564208984375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.5563, loss_val: nan, pos_over_neg: 680.924560546875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.5531, loss_val: nan, pos_over_neg: 670.3980712890625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.5561, loss_val: nan, pos_over_neg: 1220.1959228515625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.552, loss_val: nan, pos_over_neg: 870.53125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 490.17559814453125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 514.2616577148438 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.5298, loss_val: nan, pos_over_neg: 726.5797119140625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.5651, loss_val: nan, pos_over_neg: 433.3865661621094 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.5712, loss_val: nan, pos_over_neg: 478.92095947265625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.5561, loss_val: nan, pos_over_neg: 434.41986083984375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.599, loss_val: nan, pos_over_neg: 283.7747497558594 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.6034, loss_val: nan, pos_over_neg: 274.4252624511719 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 306.9424743652344 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 596.9353637695312 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.5855, loss_val: nan, pos_over_neg: 441.9122619628906 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.5384, loss_val: nan, pos_over_neg: 868.0527954101562 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 1095.1510009765625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.6138, loss_val: nan, pos_over_neg: 324.8955383300781 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.5532, loss_val: nan, pos_over_neg: 464.7267761230469 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.5688, loss_val: nan, pos_over_neg: 670.7781982421875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 570.6282958984375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 440.17205810546875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.546, loss_val: nan, pos_over_neg: 545.4323120117188 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.5438, loss_val: nan, pos_over_neg: 488.2554016113281 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 606.4275512695312 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.5308, loss_val: nan, pos_over_neg: 559.7142333984375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 381.196533203125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.5826, loss_val: nan, pos_over_neg: 351.78253173828125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 440.2537841796875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.6016, loss_val: nan, pos_over_neg: 358.9316711425781 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.6222, loss_val: nan, pos_over_neg: 303.4461669921875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.5858, loss_val: nan, pos_over_neg: 359.2669677734375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 319.86151123046875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.5602, loss_val: nan, pos_over_neg: 437.89703369140625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.5523, loss_val: nan, pos_over_neg: 839.7223510742188 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.5785, loss_val: nan, pos_over_neg: 735.348388671875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.5527, loss_val: nan, pos_over_neg: 555.091552734375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 649.3121948242188 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.5519, loss_val: nan, pos_over_neg: 903.18701171875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.6087, loss_val: nan, pos_over_neg: 471.23138427734375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.5358, loss_val: nan, pos_over_neg: 878.8297119140625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.5306, loss_val: nan, pos_over_neg: 575.5792846679688 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 409.40008544921875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.6156, loss_val: nan, pos_over_neg: 277.1867980957031 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.5667, loss_val: nan, pos_over_neg: 415.484375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 371.0615234375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 348.3677978515625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.5813, loss_val: nan, pos_over_neg: 391.6085510253906 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.5882, loss_val: nan, pos_over_neg: 421.9993896484375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.5698, loss_val: nan, pos_over_neg: 998.8740844726562 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.546, loss_val: nan, pos_over_neg: 735.0000610351562 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.5901, loss_val: nan, pos_over_neg: 446.1827087402344 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 460.7393798828125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.5644, loss_val: nan, pos_over_neg: 500.1806640625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6145, loss_val: nan, pos_over_neg: 481.64508056640625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 1057.795166015625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 582.1036987304688 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.5333, loss_val: nan, pos_over_neg: 898.9212646484375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 692.9227294921875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.5463, loss_val: nan, pos_over_neg: 635.4212036132812 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 663.05859375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 478.7743835449219 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.5632, loss_val: nan, pos_over_neg: 369.47894287109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.5583, loss_val: nan, pos_over_neg: 448.53656005859375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.5749, loss_val: nan, pos_over_neg: 454.37579345703125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.5678, loss_val: nan, pos_over_neg: 308.5570068359375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.5798, loss_val: nan, pos_over_neg: 555.2035522460938 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.6276, loss_val: nan, pos_over_neg: 423.69427490234375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 374.500732421875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 614.3733520507812 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.5274, loss_val: nan, pos_over_neg: 678.599609375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.5988, loss_val: nan, pos_over_neg: 974.1351928710938 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 421.3631896972656 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.5693, loss_val: nan, pos_over_neg: 407.1152648925781 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 375.46502685546875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 512.5235595703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.5422, loss_val: nan, pos_over_neg: 485.0389099121094 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.5541, loss_val: nan, pos_over_neg: 506.67437744140625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.5621, loss_val: nan, pos_over_neg: 514.224365234375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.5662, loss_val: nan, pos_over_neg: 544.3865966796875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.5531, loss_val: nan, pos_over_neg: 484.8132019042969 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.6113, loss_val: nan, pos_over_neg: 304.0905456542969 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 317.6409606933594 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.6155, loss_val: nan, pos_over_neg: 357.85821533203125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.5641, loss_val: nan, pos_over_neg: 530.899169921875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.595, loss_val: nan, pos_over_neg: 332.1717834472656 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.5546, loss_val: nan, pos_over_neg: 689.8306274414062 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 543.1720581054688 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 559.2744750976562 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.5522, loss_val: nan, pos_over_neg: 811.6914672851562 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.54, loss_val: nan, pos_over_neg: 736.34521484375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 618.4768676757812 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.5467, loss_val: nan, pos_over_neg: 762.2996826171875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.6061, loss_val: nan, pos_over_neg: 322.0391540527344 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.5791, loss_val: nan, pos_over_neg: 483.59381103515625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 397.386474609375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.544, loss_val: nan, pos_over_neg: 701.6649169921875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.5517, loss_val: nan, pos_over_neg: 695.7877807617188 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.5202, loss_val: nan, pos_over_neg: 835.4392700195312 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.5616, loss_val: nan, pos_over_neg: 454.9471435546875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.6027, loss_val: nan, pos_over_neg: 448.0616149902344 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.5759, loss_val: nan, pos_over_neg: 413.0182189941406 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.569, loss_val: nan, pos_over_neg: 377.31011962890625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 246.12322998046875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.5419, loss_val: nan, pos_over_neg: 443.75360107421875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 763.9639892578125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.5775, loss_val: nan, pos_over_neg: 420.32861328125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.5445, loss_val: nan, pos_over_neg: 438.074462890625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.612, loss_val: nan, pos_over_neg: 313.5040588378906 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 732.2576293945312 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.626, loss_val: nan, pos_over_neg: 443.62030029296875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.5327, loss_val: nan, pos_over_neg: 519.5333251953125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.566, loss_val: nan, pos_over_neg: 380.72625732421875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.5486, loss_val: nan, pos_over_neg: 756.1497192382812 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 382.7162170410156 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 373.5926818847656 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 438.6231689453125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.5879, loss_val: nan, pos_over_neg: 620.3670654296875 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 462.7785949707031 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.6123, loss_val: nan, pos_over_neg: 340.5379333496094 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.6142, loss_val: nan, pos_over_neg: 332.24468994140625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.5761, loss_val: nan, pos_over_neg: 498.06500244140625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.5546, loss_val: nan, pos_over_neg: 393.21435546875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.5632, loss_val: nan, pos_over_neg: 370.5966796875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.5724, loss_val: nan, pos_over_neg: 304.2813415527344 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.5995, loss_val: nan, pos_over_neg: 347.2796325683594 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.5283, loss_val: nan, pos_over_neg: 950.1817626953125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 453.3915100097656 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.553, loss_val: nan, pos_over_neg: 507.2243957519531 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.5262, loss_val: nan, pos_over_neg: 538.8601684570312 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.5405, loss_val: nan, pos_over_neg: 567.816162109375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.5937, loss_val: nan, pos_over_neg: 348.6761779785156 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.6003, loss_val: nan, pos_over_neg: 339.5365905761719 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.5738, loss_val: nan, pos_over_neg: 567.7921142578125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 403.43939208984375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.5361, loss_val: nan, pos_over_neg: 420.7707824707031 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.5655, loss_val: nan, pos_over_neg: 512.5425415039062 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.6097, loss_val: nan, pos_over_neg: 363.59918212890625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.5539, loss_val: nan, pos_over_neg: 644.0696411132812 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.5795, loss_val: nan, pos_over_neg: 565.0082397460938 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.5395, loss_val: nan, pos_over_neg: 656.2236938476562 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.6221, loss_val: nan, pos_over_neg: 353.5638427734375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.602, loss_val: nan, pos_over_neg: 335.019775390625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.5501, loss_val: nan, pos_over_neg: 417.37451171875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 276.893798828125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.5733, loss_val: nan, pos_over_neg: 372.5535888671875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.5585, loss_val: nan, pos_over_neg: 565.0547485351562 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.5556, loss_val: nan, pos_over_neg: 469.9845275878906 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.6014, loss_val: nan, pos_over_neg: 566.3014526367188 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.5542, loss_val: nan, pos_over_neg: 892.8511962890625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.5523, loss_val: nan, pos_over_neg: 512.735107421875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 402.423828125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.5327, loss_val: nan, pos_over_neg: 665.3657836914062 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.5607, loss_val: nan, pos_over_neg: 347.8090515136719 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.5745, loss_val: nan, pos_over_neg: 292.9017333984375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.5583, loss_val: nan, pos_over_neg: 438.9507141113281 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.5601, loss_val: nan, pos_over_neg: 312.9338684082031 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.6059, loss_val: nan, pos_over_neg: 347.7571105957031 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.6103, loss_val: nan, pos_over_neg: 300.61431884765625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.545, loss_val: nan, pos_over_neg: 687.056396484375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.6283, loss_val: nan, pos_over_neg: 311.1710510253906 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 509.97222900390625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.5544, loss_val: nan, pos_over_neg: 555.4008178710938 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.5756, loss_val: nan, pos_over_neg: 408.092529296875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.5976, loss_val: nan, pos_over_neg: 499.26654052734375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.5768, loss_val: nan, pos_over_neg: 460.8171691894531 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 502.8618469238281 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.5611, loss_val: nan, pos_over_neg: 370.7965393066406 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 315.5704040527344 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.6248, loss_val: nan, pos_over_neg: 328.1827087402344 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.5805, loss_val: nan, pos_over_neg: 408.7665710449219 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 331.0111083984375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.5499, loss_val: nan, pos_over_neg: 436.0464782714844 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.59, loss_val: nan, pos_over_neg: 388.0500183105469 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.5597, loss_val: nan, pos_over_neg: 312.96917724609375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.5962, loss_val: nan, pos_over_neg: 318.2889709472656 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 481.328857421875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.5461, loss_val: nan, pos_over_neg: 782.3914184570312 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.5546, loss_val: nan, pos_over_neg: 480.1207580566406 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 423.4678955078125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.5877, loss_val: nan, pos_over_neg: 593.1803588867188 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.5505, loss_val: nan, pos_over_neg: 500.5169372558594 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.5655, loss_val: nan, pos_over_neg: 416.8581237792969 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.5955, loss_val: nan, pos_over_neg: 343.3124694824219 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.5818, loss_val: nan, pos_over_neg: 303.9256286621094 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 535.2987670898438 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.5613, loss_val: nan, pos_over_neg: 775.1257934570312 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.552, loss_val: nan, pos_over_neg: 666.8800048828125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.5424, loss_val: nan, pos_over_neg: 576.7698364257812 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 281.4312438964844 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.5459, loss_val: nan, pos_over_neg: 454.70703125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 339.2394104003906 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.5542, loss_val: nan, pos_over_neg: 539.6234130859375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.6106, loss_val: nan, pos_over_neg: 276.70916748046875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.5502, loss_val: nan, pos_over_neg: 438.0673828125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.5529, loss_val: nan, pos_over_neg: 524.39990234375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.5398, loss_val: nan, pos_over_neg: 507.3139953613281 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/300000 [4:16:33<75292:50:17, 903.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.5906, loss_val: nan, pos_over_neg: 525.3934326171875 lr: 0.00031623\n",
      "epoch: 17\n",
      "Iter: 0/695, loss_train: 3.6002, loss_val: nan, pos_over_neg: 558.65771484375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.623, loss_val: nan, pos_over_neg: 410.6520690917969 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.5773, loss_val: nan, pos_over_neg: 458.089599609375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.5611, loss_val: nan, pos_over_neg: 688.6539306640625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 521.370361328125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.5995, loss_val: nan, pos_over_neg: 395.28546142578125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.5445, loss_val: nan, pos_over_neg: 580.4561767578125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 352.16302490234375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.6226, loss_val: nan, pos_over_neg: 262.3089599609375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.5578, loss_val: nan, pos_over_neg: 643.7506103515625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 421.673583984375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 464.3486328125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 520.2578125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.5406, loss_val: nan, pos_over_neg: 585.0609130859375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 515.1873168945312 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.605, loss_val: nan, pos_over_neg: 377.0630187988281 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.5531, loss_val: nan, pos_over_neg: 534.8865356445312 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 674.34228515625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.5387, loss_val: nan, pos_over_neg: 597.7833251953125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.5383, loss_val: nan, pos_over_neg: 593.965576171875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 322.30352783203125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 506.52166748046875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.5916, loss_val: nan, pos_over_neg: 509.3896179199219 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.5667, loss_val: nan, pos_over_neg: 330.93389892578125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.555, loss_val: nan, pos_over_neg: 395.7231750488281 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.5504, loss_val: nan, pos_over_neg: 377.1499938964844 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6311, loss_val: nan, pos_over_neg: 331.65020751953125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.5848, loss_val: nan, pos_over_neg: 685.595458984375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.6107, loss_val: nan, pos_over_neg: 525.5921630859375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.534, loss_val: nan, pos_over_neg: 1115.298828125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.5174, loss_val: nan, pos_over_neg: 531.95751953125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 555.1775512695312 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.5918, loss_val: nan, pos_over_neg: 557.0794067382812 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.5855, loss_val: nan, pos_over_neg: 472.4842834472656 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.5528, loss_val: nan, pos_over_neg: 371.4239501953125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 386.95343017578125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 269.892333984375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.5295, loss_val: nan, pos_over_neg: 444.6236267089844 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.6019, loss_val: nan, pos_over_neg: 409.54107666015625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.5573, loss_val: nan, pos_over_neg: 485.45318603515625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 469.0695495605469 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 728.3931274414062 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 622.1140747070312 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.5899, loss_val: nan, pos_over_neg: 327.885009765625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 640.8441772460938 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 408.2424011230469 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.5292, loss_val: nan, pos_over_neg: 505.8263244628906 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.5787, loss_val: nan, pos_over_neg: 538.722412109375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.5948, loss_val: nan, pos_over_neg: 338.1496276855469 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 336.9343566894531 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.5942, loss_val: nan, pos_over_neg: 460.9478454589844 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 585.0287475585938 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 531.7582397460938 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.5456, loss_val: nan, pos_over_neg: 1619.6239013671875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.5469, loss_val: nan, pos_over_neg: 1610.1070556640625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.5844, loss_val: nan, pos_over_neg: 723.8444213867188 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.5682, loss_val: nan, pos_over_neg: 727.264892578125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.563, loss_val: nan, pos_over_neg: 566.6890258789062 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.5578, loss_val: nan, pos_over_neg: 548.1212158203125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 517.645263671875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.5665, loss_val: nan, pos_over_neg: 351.3699951171875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 288.95062255859375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.5948, loss_val: nan, pos_over_neg: 307.9886169433594 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 423.1533508300781 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.5002, loss_val: nan, pos_over_neg: 687.1695556640625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.5523, loss_val: nan, pos_over_neg: 573.8607177734375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 420.3824462890625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.5621, loss_val: nan, pos_over_neg: 814.8944702148438 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.5606, loss_val: nan, pos_over_neg: 698.47265625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 407.8599853515625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.6078, loss_val: nan, pos_over_neg: 372.4454040527344 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.5675, loss_val: nan, pos_over_neg: 486.1521911621094 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.5744, loss_val: nan, pos_over_neg: 405.8942565917969 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 610.5375366210938 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 342.7292175292969 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.5887, loss_val: nan, pos_over_neg: 419.0492858886719 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.5499, loss_val: nan, pos_over_neg: 488.8475341796875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.5885, loss_val: nan, pos_over_neg: 429.5569152832031 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.5921, loss_val: nan, pos_over_neg: 384.2331237792969 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 375.84381103515625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.5841, loss_val: nan, pos_over_neg: 379.6678466796875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 487.094970703125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.5675, loss_val: nan, pos_over_neg: 439.1620178222656 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 395.6160888671875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.593, loss_val: nan, pos_over_neg: 402.9396057128906 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.5628, loss_val: nan, pos_over_neg: 770.4793701171875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.5475, loss_val: nan, pos_over_neg: 916.5276489257812 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.5693, loss_val: nan, pos_over_neg: 495.5433654785156 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.5834, loss_val: nan, pos_over_neg: 453.8028259277344 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 357.13665771484375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.5703, loss_val: nan, pos_over_neg: 514.9820556640625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 458.3598937988281 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 311.0252990722656 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.5512, loss_val: nan, pos_over_neg: 842.2671508789062 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.5823, loss_val: nan, pos_over_neg: 447.5796203613281 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.6108, loss_val: nan, pos_over_neg: 301.05303955078125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.5611, loss_val: nan, pos_over_neg: 620.8642578125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.6127, loss_val: nan, pos_over_neg: 294.2006530761719 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.5979, loss_val: nan, pos_over_neg: 371.0288391113281 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.5896, loss_val: nan, pos_over_neg: 485.92694091796875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 383.121337890625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.5613, loss_val: nan, pos_over_neg: 484.32366943359375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 460.19134521484375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.5767, loss_val: nan, pos_over_neg: 475.005615234375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 366.7477722167969 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 456.650390625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.5616, loss_val: nan, pos_over_neg: 535.6510620117188 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.5569, loss_val: nan, pos_over_neg: 663.69091796875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.5739, loss_val: nan, pos_over_neg: 673.4822387695312 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.589, loss_val: nan, pos_over_neg: 471.7743835449219 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.5659, loss_val: nan, pos_over_neg: 375.81719970703125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.5749, loss_val: nan, pos_over_neg: 402.37042236328125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.5775, loss_val: nan, pos_over_neg: 375.0943298339844 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 368.0886535644531 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 329.42193603515625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 352.728271484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.5524, loss_val: nan, pos_over_neg: 503.10498046875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 509.7401123046875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 477.2314758300781 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.614, loss_val: nan, pos_over_neg: 417.1438293457031 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.5647, loss_val: nan, pos_over_neg: 611.4988403320312 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.6229, loss_val: nan, pos_over_neg: 330.9046936035156 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.5448, loss_val: nan, pos_over_neg: 498.3372497558594 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.5963, loss_val: nan, pos_over_neg: 351.1271057128906 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.552, loss_val: nan, pos_over_neg: 475.6493225097656 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.6184, loss_val: nan, pos_over_neg: 423.21929931640625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.6026, loss_val: nan, pos_over_neg: 637.2903442382812 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.5367, loss_val: nan, pos_over_neg: 608.6580810546875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.5401, loss_val: nan, pos_over_neg: 853.524169921875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 395.00469970703125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 406.81890869140625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 640.1105346679688 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 304.0115661621094 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.5948, loss_val: nan, pos_over_neg: 490.310791015625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 448.84417724609375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.6012, loss_val: nan, pos_over_neg: 239.7541961669922 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.5827, loss_val: nan, pos_over_neg: 292.6787414550781 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.5487, loss_val: nan, pos_over_neg: 565.8134155273438 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.5763, loss_val: nan, pos_over_neg: 532.4848022460938 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.5408, loss_val: nan, pos_over_neg: 602.7462768554688 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 715.9647216796875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.6158, loss_val: nan, pos_over_neg: 307.08905029296875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.5694, loss_val: nan, pos_over_neg: 352.9240417480469 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 461.3069763183594 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.5933, loss_val: nan, pos_over_neg: 338.84423828125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.5665, loss_val: nan, pos_over_neg: 612.6240234375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 545.1557006835938 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.5635, loss_val: nan, pos_over_neg: 319.27337646484375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 333.9901428222656 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.5784, loss_val: nan, pos_over_neg: 285.45745849609375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.5593, loss_val: nan, pos_over_neg: 447.9998474121094 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 426.193603515625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 609.23486328125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.6062, loss_val: nan, pos_over_neg: 281.5692443847656 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.6386, loss_val: nan, pos_over_neg: 413.0926818847656 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.6016, loss_val: nan, pos_over_neg: 430.64447021484375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.5552, loss_val: nan, pos_over_neg: 520.1668090820312 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.5923, loss_val: nan, pos_over_neg: 523.4740600585938 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.5687, loss_val: nan, pos_over_neg: 425.021728515625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 701.9409790039062 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.5593, loss_val: nan, pos_over_neg: 526.6638793945312 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 345.3359069824219 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.5564, loss_val: nan, pos_over_neg: 444.6519775390625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 318.2475891113281 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.5372, loss_val: nan, pos_over_neg: 506.05517578125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 636.859130859375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.5439, loss_val: nan, pos_over_neg: 770.5426025390625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.5333, loss_val: nan, pos_over_neg: 683.7185668945312 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.5581, loss_val: nan, pos_over_neg: 650.5519409179688 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.5644, loss_val: nan, pos_over_neg: 434.0786437988281 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.5206, loss_val: nan, pos_over_neg: 640.2783203125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.5396, loss_val: nan, pos_over_neg: 414.8224792480469 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.5905, loss_val: nan, pos_over_neg: 366.3358154296875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 376.0320739746094 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 416.6086120605469 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 494.1905822753906 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.5906, loss_val: nan, pos_over_neg: 595.257568359375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.542, loss_val: nan, pos_over_neg: 768.262451171875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.5813, loss_val: nan, pos_over_neg: 349.60296630859375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.5876, loss_val: nan, pos_over_neg: 503.90631103515625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.5532, loss_val: nan, pos_over_neg: 723.199951171875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.5269, loss_val: nan, pos_over_neg: 1250.3299560546875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.5424, loss_val: nan, pos_over_neg: 731.834716796875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.538, loss_val: nan, pos_over_neg: 582.2659301757812 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.5781, loss_val: nan, pos_over_neg: 451.58209228515625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.5563, loss_val: nan, pos_over_neg: 712.9229125976562 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.5999, loss_val: nan, pos_over_neg: 374.06622314453125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.5142, loss_val: nan, pos_over_neg: 640.8612670898438 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.5874, loss_val: nan, pos_over_neg: 485.88623046875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 438.44244384765625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 425.5182189941406 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.5735, loss_val: nan, pos_over_neg: 385.81121826171875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.5254, loss_val: nan, pos_over_neg: 481.9853515625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.5587, loss_val: nan, pos_over_neg: 791.1748046875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 480.4943542480469 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 434.398681640625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.503, loss_val: nan, pos_over_neg: 1159.1519775390625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.6377, loss_val: nan, pos_over_neg: 420.9224548339844 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.5638, loss_val: nan, pos_over_neg: 914.6757202148438 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 343.6576232910156 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 640.6165161132812 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.583, loss_val: nan, pos_over_neg: 417.5359191894531 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 504.4352722167969 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.5066, loss_val: nan, pos_over_neg: 598.6475219726562 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.5243, loss_val: nan, pos_over_neg: 764.0099487304688 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 533.100341796875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.6105, loss_val: nan, pos_over_neg: 410.5182800292969 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.5347, loss_val: nan, pos_over_neg: 561.7481689453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.5625, loss_val: nan, pos_over_neg: 775.374755859375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.551, loss_val: nan, pos_over_neg: 624.6199951171875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.5598, loss_val: nan, pos_over_neg: 1137.5628662109375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.5485, loss_val: nan, pos_over_neg: 706.1170654296875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.5979, loss_val: nan, pos_over_neg: 335.6475830078125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 408.1580505371094 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.5409, loss_val: nan, pos_over_neg: 595.34375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.543, loss_val: nan, pos_over_neg: 643.483642578125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.5562, loss_val: nan, pos_over_neg: 456.93212890625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.5329, loss_val: nan, pos_over_neg: 798.7022705078125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.5128, loss_val: nan, pos_over_neg: 722.9347534179688 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.5395, loss_val: nan, pos_over_neg: 531.2914428710938 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.5451, loss_val: nan, pos_over_neg: 728.91650390625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.5554, loss_val: nan, pos_over_neg: 480.625244140625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.5388, loss_val: nan, pos_over_neg: 569.9072875976562 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 379.6601867675781 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 281.069580078125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 366.14849853515625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.5778, loss_val: nan, pos_over_neg: 507.48516845703125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.5575, loss_val: nan, pos_over_neg: 753.8806762695312 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.6009, loss_val: nan, pos_over_neg: 835.4608154296875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 721.2937622070312 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.6289, loss_val: nan, pos_over_neg: 430.2361755371094 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.5542, loss_val: nan, pos_over_neg: 567.5792236328125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.5507, loss_val: nan, pos_over_neg: 641.2727661132812 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 433.49639892578125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.5549, loss_val: nan, pos_over_neg: 521.013671875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.5624, loss_val: nan, pos_over_neg: 428.1117858886719 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 542.2774658203125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.5498, loss_val: nan, pos_over_neg: 392.6882019042969 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 364.6255187988281 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.5516, loss_val: nan, pos_over_neg: 334.8190002441406 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.5401, loss_val: nan, pos_over_neg: 579.4739379882812 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.568, loss_val: nan, pos_over_neg: 662.2484130859375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.5732, loss_val: nan, pos_over_neg: 498.8966064453125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.5449, loss_val: nan, pos_over_neg: 818.7550659179688 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 988.4830322265625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.5477, loss_val: nan, pos_over_neg: 787.9183349609375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 1025.56396484375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.5439, loss_val: nan, pos_over_neg: 596.1434936523438 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 267.8944396972656 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.5563, loss_val: nan, pos_over_neg: 347.5014953613281 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.5892, loss_val: nan, pos_over_neg: 352.6695861816406 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.601, loss_val: nan, pos_over_neg: 406.5066223144531 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.6216, loss_val: nan, pos_over_neg: 258.7809753417969 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.5796, loss_val: nan, pos_over_neg: 353.2524108886719 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.6058, loss_val: nan, pos_over_neg: 337.0584716796875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.6037, loss_val: nan, pos_over_neg: 346.2246398925781 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.5875, loss_val: nan, pos_over_neg: 530.3156127929688 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.6021, loss_val: nan, pos_over_neg: 461.4671325683594 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.6135, loss_val: nan, pos_over_neg: 477.2893371582031 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.6165, loss_val: nan, pos_over_neg: 517.0657958984375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.5527, loss_val: nan, pos_over_neg: 496.2098388671875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.5911, loss_val: nan, pos_over_neg: 564.8668212890625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.5872, loss_val: nan, pos_over_neg: 468.72003173828125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 405.3912658691406 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.6069, loss_val: nan, pos_over_neg: 253.22242736816406 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.5696, loss_val: nan, pos_over_neg: 392.01690673828125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.5537, loss_val: nan, pos_over_neg: 316.1226806640625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.5534, loss_val: nan, pos_over_neg: 470.7875671386719 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.5696, loss_val: nan, pos_over_neg: 340.3823547363281 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 483.86395263671875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.5488, loss_val: nan, pos_over_neg: 616.5840454101562 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.5627, loss_val: nan, pos_over_neg: 1474.869873046875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 610.0655517578125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 353.9627685546875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.5443, loss_val: nan, pos_over_neg: 490.2796325683594 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.5852, loss_val: nan, pos_over_neg: 561.04296875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 413.81622314453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.5587, loss_val: nan, pos_over_neg: 349.858154296875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 372.77728271484375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.5529, loss_val: nan, pos_over_neg: 549.5925903320312 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.5893, loss_val: nan, pos_over_neg: 369.5697021484375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 812.235595703125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.5749, loss_val: nan, pos_over_neg: 420.0663146972656 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 592.3154296875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 593.7173461914062 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 436.26495361328125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.5373, loss_val: nan, pos_over_neg: 566.0306396484375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.6115, loss_val: nan, pos_over_neg: 462.82830810546875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.5772, loss_val: nan, pos_over_neg: 476.2970275878906 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.544, loss_val: nan, pos_over_neg: 475.7527160644531 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 457.80584716796875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.5656, loss_val: nan, pos_over_neg: 404.8634338378906 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.5612, loss_val: nan, pos_over_neg: 340.73504638671875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.6267, loss_val: nan, pos_over_neg: 231.17124938964844 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.5549, loss_val: nan, pos_over_neg: 537.741455078125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.5469, loss_val: nan, pos_over_neg: 778.9171752929688 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.5613, loss_val: nan, pos_over_neg: 633.5597534179688 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.5957, loss_val: nan, pos_over_neg: 423.71624755859375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.5621, loss_val: nan, pos_over_neg: 441.2508544921875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.552, loss_val: nan, pos_over_neg: 638.05029296875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.5554, loss_val: nan, pos_over_neg: 645.7453002929688 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.5523, loss_val: nan, pos_over_neg: 478.5682678222656 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 492.3861083984375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.6345, loss_val: nan, pos_over_neg: 305.76220703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.5526, loss_val: nan, pos_over_neg: 449.1002197265625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.559, loss_val: nan, pos_over_neg: 318.522705078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.5363, loss_val: nan, pos_over_neg: 482.98663330078125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 464.3030700683594 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.5853, loss_val: nan, pos_over_neg: 592.268310546875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.6111, loss_val: nan, pos_over_neg: 479.02392578125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 535.6932983398438 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.6245, loss_val: nan, pos_over_neg: 363.96099853515625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.5731, loss_val: nan, pos_over_neg: 510.3903503417969 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.5668, loss_val: nan, pos_over_neg: 480.12371826171875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.5416, loss_val: nan, pos_over_neg: 740.7559204101562 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.5618, loss_val: nan, pos_over_neg: 447.74273681640625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6206, loss_val: nan, pos_over_neg: 350.10040283203125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 381.8300476074219 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.5531, loss_val: nan, pos_over_neg: 702.649658203125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 547.2322998046875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 548.7875366210938 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.611, loss_val: nan, pos_over_neg: 298.6495666503906 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.5756, loss_val: nan, pos_over_neg: 431.6370544433594 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.6052, loss_val: nan, pos_over_neg: 468.9803771972656 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.5415, loss_val: nan, pos_over_neg: 906.61083984375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.56, loss_val: nan, pos_over_neg: 611.8526000976562 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.608, loss_val: nan, pos_over_neg: 411.7692565917969 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.5559, loss_val: nan, pos_over_neg: 920.9869995117188 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.5634, loss_val: nan, pos_over_neg: 545.0926513671875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.5855, loss_val: nan, pos_over_neg: 581.297119140625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.5545, loss_val: nan, pos_over_neg: 849.635009765625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.5543, loss_val: nan, pos_over_neg: 601.64697265625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.5952, loss_val: nan, pos_over_neg: 393.7768249511719 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.5669, loss_val: nan, pos_over_neg: 510.8217468261719 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 414.6949768066406 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.5326, loss_val: nan, pos_over_neg: 855.78955078125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.545, loss_val: nan, pos_over_neg: 1148.5765380859375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 560.66748046875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.5648, loss_val: nan, pos_over_neg: 488.6710205078125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.574, loss_val: nan, pos_over_neg: 451.03485107421875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.5746, loss_val: nan, pos_over_neg: 461.782470703125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.5788, loss_val: nan, pos_over_neg: 516.8786010742188 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.5512, loss_val: nan, pos_over_neg: 414.6224365234375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.5975, loss_val: nan, pos_over_neg: 414.06561279296875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.5367, loss_val: nan, pos_over_neg: 597.1364135742188 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 527.0267333984375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 324.1781311035156 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.5548, loss_val: nan, pos_over_neg: 688.9404907226562 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.5658, loss_val: nan, pos_over_neg: 555.1721801757812 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.5976, loss_val: nan, pos_over_neg: 570.9367065429688 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.597, loss_val: nan, pos_over_neg: 470.7892150878906 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.5508, loss_val: nan, pos_over_neg: 517.993408203125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.6079, loss_val: nan, pos_over_neg: 340.58441162109375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.5541, loss_val: nan, pos_over_neg: 458.9888000488281 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.5384, loss_val: nan, pos_over_neg: 771.0208740234375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 486.0692138671875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 3.5399, loss_val: nan, pos_over_neg: 1362.8216552734375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 3.5783, loss_val: nan, pos_over_neg: 710.670654296875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 3.5337, loss_val: nan, pos_over_neg: 506.8003845214844 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 3.5607, loss_val: nan, pos_over_neg: 674.5191650390625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 3.557, loss_val: nan, pos_over_neg: 634.68994140625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 3.5698, loss_val: nan, pos_over_neg: 507.6324157714844 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 3.5643, loss_val: nan, pos_over_neg: 466.7933349609375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 3.5594, loss_val: nan, pos_over_neg: 448.3367004394531 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 3.5872, loss_val: nan, pos_over_neg: 344.6824645996094 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 3.5843, loss_val: nan, pos_over_neg: 389.3888854980469 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 3.5605, loss_val: nan, pos_over_neg: 630.0831298828125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 3.5902, loss_val: nan, pos_over_neg: 560.0784301757812 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 3.5436, loss_val: nan, pos_over_neg: 687.161865234375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 609.09326171875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 410.4823913574219 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 675.7977905273438 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 3.5786, loss_val: nan, pos_over_neg: 391.86724853515625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 3.5687, loss_val: nan, pos_over_neg: 595.5980834960938 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 3.6154, loss_val: nan, pos_over_neg: 350.6209716796875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 3.5595, loss_val: nan, pos_over_neg: 717.30712890625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 3.5725, loss_val: nan, pos_over_neg: 468.38238525390625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 740.640625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 3.5965, loss_val: nan, pos_over_neg: 479.07025146484375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 3.5604, loss_val: nan, pos_over_neg: 472.061767578125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 3.547, loss_val: nan, pos_over_neg: 912.178955078125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 3.5217, loss_val: nan, pos_over_neg: 851.8524780273438 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 3.6185, loss_val: nan, pos_over_neg: 507.4032897949219 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 524.8002319335938 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 3.5547, loss_val: nan, pos_over_neg: 496.2273864746094 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 3.5296, loss_val: nan, pos_over_neg: 841.2782592773438 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 3.5786, loss_val: nan, pos_over_neg: 531.1344604492188 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 3.5912, loss_val: nan, pos_over_neg: 472.76251220703125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 3.5627, loss_val: nan, pos_over_neg: 475.98419189453125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 402.1827697753906 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 3.5519, loss_val: nan, pos_over_neg: 463.098876953125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 284.3786315917969 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 434.3062438964844 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 851.126220703125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 3.6369, loss_val: nan, pos_over_neg: 342.6259460449219 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 3.5753, loss_val: nan, pos_over_neg: 460.7320251464844 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 3.62, loss_val: nan, pos_over_neg: 337.3271484375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 3.561, loss_val: nan, pos_over_neg: 358.153564453125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 310.8609619140625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 3.5607, loss_val: nan, pos_over_neg: 361.6031799316406 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 481.3422546386719 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 3.5879, loss_val: nan, pos_over_neg: 330.95928955078125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 3.543, loss_val: nan, pos_over_neg: 507.49749755859375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 3.5981, loss_val: nan, pos_over_neg: 282.7619934082031 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 3.592, loss_val: nan, pos_over_neg: 401.13238525390625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 3.5927, loss_val: nan, pos_over_neg: 412.6983947753906 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 3.5714, loss_val: nan, pos_over_neg: 409.198486328125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 404.5375671386719 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 3.5671, loss_val: nan, pos_over_neg: 330.2671813964844 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 3.5559, loss_val: nan, pos_over_neg: 325.3737487792969 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 470.85589599609375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 3.5625, loss_val: nan, pos_over_neg: 849.1088256835938 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 3.5799, loss_val: nan, pos_over_neg: 411.55133056640625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 3.604, loss_val: nan, pos_over_neg: 287.19927978515625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 324.72406005859375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 3.5397, loss_val: nan, pos_over_neg: 572.8569946289062 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 3.6069, loss_val: nan, pos_over_neg: 414.7173156738281 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 285.42364501953125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 3.5585, loss_val: nan, pos_over_neg: 403.7836608886719 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 465.1570739746094 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 3.6083, loss_val: nan, pos_over_neg: 421.4300842285156 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 3.558, loss_val: nan, pos_over_neg: 537.5800170898438 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 3.543, loss_val: nan, pos_over_neg: 505.2797546386719 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 3.5476, loss_val: nan, pos_over_neg: 1094.3076171875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 579.802490234375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 3.5565, loss_val: nan, pos_over_neg: 790.8470458984375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 3.5722, loss_val: nan, pos_over_neg: 429.6724548339844 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 3.5821, loss_val: nan, pos_over_neg: 337.4443359375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 3.6092, loss_val: nan, pos_over_neg: 364.4629211425781 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 3.5628, loss_val: nan, pos_over_neg: 414.34423828125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 3.5348, loss_val: nan, pos_over_neg: 429.592041015625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 3.6004, loss_val: nan, pos_over_neg: 352.808837890625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 3.5949, loss_val: nan, pos_over_neg: 306.6278381347656 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 3.5661, loss_val: nan, pos_over_neg: 327.34912109375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 3.5487, loss_val: nan, pos_over_neg: 1244.0250244140625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 3.5775, loss_val: nan, pos_over_neg: 474.5936279296875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 3.5833, loss_val: nan, pos_over_neg: 736.2470703125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 3.5347, loss_val: nan, pos_over_neg: 1093.537109375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 747.0914916992188 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 3.5994, loss_val: nan, pos_over_neg: 506.5327453613281 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 3.5708, loss_val: nan, pos_over_neg: 445.30633544921875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 466.4971923828125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 3.5354, loss_val: nan, pos_over_neg: 538.3994140625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 3.5887, loss_val: nan, pos_over_neg: 359.284912109375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 3.56, loss_val: nan, pos_over_neg: 495.2267150878906 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 3.5582, loss_val: nan, pos_over_neg: 444.82379150390625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 3.5596, loss_val: nan, pos_over_neg: 508.6357727050781 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 3.5679, loss_val: nan, pos_over_neg: 579.5569458007812 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 3.5616, loss_val: nan, pos_over_neg: 737.2185668945312 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 3.5033, loss_val: nan, pos_over_neg: 667.9345703125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 539.1007080078125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 3.5471, loss_val: nan, pos_over_neg: 707.2119140625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 3.542, loss_val: nan, pos_over_neg: 472.08795166015625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 3.5628, loss_val: nan, pos_over_neg: 545.28369140625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 3.5958, loss_val: nan, pos_over_neg: 232.52847290039062 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 3.5709, loss_val: nan, pos_over_neg: 327.5852355957031 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 3.6038, loss_val: nan, pos_over_neg: 257.45684814453125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 3.6089, loss_val: nan, pos_over_neg: 390.05804443359375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 3.5446, loss_val: nan, pos_over_neg: 606.50732421875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 3.5558, loss_val: nan, pos_over_neg: 656.4462280273438 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 3.5771, loss_val: nan, pos_over_neg: 617.3958129882812 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 3.5844, loss_val: nan, pos_over_neg: 398.19976806640625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 3.5611, loss_val: nan, pos_over_neg: 604.7918701171875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 3.5376, loss_val: nan, pos_over_neg: 1341.4156494140625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 3.5871, loss_val: nan, pos_over_neg: 540.3841552734375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 3.6182, loss_val: nan, pos_over_neg: 457.1781921386719 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 3.5697, loss_val: nan, pos_over_neg: 369.80401611328125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 3.578, loss_val: nan, pos_over_neg: 329.7447509765625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 305.5089111328125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 3.5628, loss_val: nan, pos_over_neg: 483.9250793457031 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 3.5302, loss_val: nan, pos_over_neg: 390.05340576171875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 3.5439, loss_val: nan, pos_over_neg: 352.07073974609375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 3.594, loss_val: nan, pos_over_neg: 340.8298034667969 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 3.5694, loss_val: nan, pos_over_neg: 461.931884765625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 3.5621, loss_val: nan, pos_over_neg: 468.27044677734375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 3.5434, loss_val: nan, pos_over_neg: 672.416259765625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 3.6003, loss_val: nan, pos_over_neg: 531.7984008789062 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 3.5363, loss_val: nan, pos_over_neg: 853.6970825195312 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 3.5231, loss_val: nan, pos_over_neg: 831.2021484375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 454.5421447753906 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 3.5404, loss_val: nan, pos_over_neg: 543.0040283203125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 3.5478, loss_val: nan, pos_over_neg: 396.0008239746094 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 3.5489, loss_val: nan, pos_over_neg: 595.6394653320312 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 3.5569, loss_val: nan, pos_over_neg: 409.24005126953125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 3.5483, loss_val: nan, pos_over_neg: 310.862548828125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 3.5599, loss_val: nan, pos_over_neg: 459.5571594238281 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 569.7721557617188 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 310.6452331542969 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 3.5393, loss_val: nan, pos_over_neg: 472.9203796386719 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 3.5671, loss_val: nan, pos_over_neg: 363.76385498046875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 3.604, loss_val: nan, pos_over_neg: 416.9666748046875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 3.5668, loss_val: nan, pos_over_neg: 518.6261596679688 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 3.5491, loss_val: nan, pos_over_neg: 548.7546997070312 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 520.9985961914062 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 3.5707, loss_val: nan, pos_over_neg: 595.0819702148438 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 3.5773, loss_val: nan, pos_over_neg: 480.3913879394531 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 3.6053, loss_val: nan, pos_over_neg: 452.62579345703125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 732.4728393554688 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 3.5826, loss_val: nan, pos_over_neg: 595.6629638671875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 3.5491, loss_val: nan, pos_over_neg: 530.5137329101562 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 3.5998, loss_val: nan, pos_over_neg: 375.7503662109375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 3.5866, loss_val: nan, pos_over_neg: 374.492919921875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 368.6458740234375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 3.5544, loss_val: nan, pos_over_neg: 367.0666809082031 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 3.598, loss_val: nan, pos_over_neg: 342.9310607910156 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 3.5558, loss_val: nan, pos_over_neg: 658.3569946289062 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 3.587, loss_val: nan, pos_over_neg: 306.1285400390625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 327.2449645996094 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 482.9375305175781 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 572.9683227539062 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 3.568, loss_val: nan, pos_over_neg: 969.7154541015625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 3.5488, loss_val: nan, pos_over_neg: 939.0853271484375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 3.5283, loss_val: nan, pos_over_neg: 903.9530639648438 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 3.5831, loss_val: nan, pos_over_neg: 593.7908935546875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 3.5504, loss_val: nan, pos_over_neg: 708.4367065429688 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 3.6183, loss_val: nan, pos_over_neg: 460.9507751464844 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 3.5967, loss_val: nan, pos_over_neg: 328.3070068359375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 421.12591552734375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 695.8629760742188 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 3.5529, loss_val: nan, pos_over_neg: 579.17333984375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 3.5567, loss_val: nan, pos_over_neg: 628.6382446289062 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 3.5769, loss_val: nan, pos_over_neg: 473.47137451171875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 3.564, loss_val: nan, pos_over_neg: 441.8407897949219 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 3.6181, loss_val: nan, pos_over_neg: 331.6656799316406 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 3.5444, loss_val: nan, pos_over_neg: 579.521728515625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 3.5434, loss_val: nan, pos_over_neg: 569.5011596679688 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 3.546, loss_val: nan, pos_over_neg: 623.0609130859375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 3.5753, loss_val: nan, pos_over_neg: 609.7174682617188 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 523.8634033203125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 478.95855712890625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 972.8428344726562 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 3.5903, loss_val: nan, pos_over_neg: 1081.3521728515625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 3.5961, loss_val: nan, pos_over_neg: 676.4849243164062 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 3.5589, loss_val: nan, pos_over_neg: 619.7828979492188 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 3.5422, loss_val: nan, pos_over_neg: 469.6991271972656 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 3.552, loss_val: nan, pos_over_neg: 1138.1134033203125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 3.5419, loss_val: nan, pos_over_neg: 943.3123779296875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 3.5982, loss_val: nan, pos_over_neg: 1101.0655517578125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 3.5338, loss_val: nan, pos_over_neg: 633.5931396484375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 3.5717, loss_val: nan, pos_over_neg: 428.66900634765625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 3.5752, loss_val: nan, pos_over_neg: 407.9485778808594 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 3.5659, loss_val: nan, pos_over_neg: 557.8050537109375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 3.5608, loss_val: nan, pos_over_neg: 450.7326354980469 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 3.5434, loss_val: nan, pos_over_neg: 530.4923706054688 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 3.5444, loss_val: nan, pos_over_neg: 675.6314086914062 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 3.5563, loss_val: nan, pos_over_neg: 432.5291442871094 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 3.5517, loss_val: nan, pos_over_neg: 438.6954650878906 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 500.8883361816406 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 3.5997, loss_val: nan, pos_over_neg: 317.24371337890625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 3.5634, loss_val: nan, pos_over_neg: 1209.5623779296875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 3.5564, loss_val: nan, pos_over_neg: 541.988037109375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 3.6055, loss_val: nan, pos_over_neg: 469.56500244140625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 3.5785, loss_val: nan, pos_over_neg: 575.5189208984375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 3.5818, loss_val: nan, pos_over_neg: 350.880126953125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 3.5542, loss_val: nan, pos_over_neg: 375.987548828125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 3.5596, loss_val: nan, pos_over_neg: 427.64764404296875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 3.5581, loss_val: nan, pos_over_neg: 513.8423461914062 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 3.5944, loss_val: nan, pos_over_neg: 498.71685791015625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 3.5979, loss_val: nan, pos_over_neg: 356.1075439453125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 3.5366, loss_val: nan, pos_over_neg: 475.4226989746094 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 3.5412, loss_val: nan, pos_over_neg: 586.9365844726562 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 3.5932, loss_val: nan, pos_over_neg: 460.7830810546875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 3.5359, loss_val: nan, pos_over_neg: 783.7384033203125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 3.5455, loss_val: nan, pos_over_neg: 508.9456481933594 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 3.6013, loss_val: nan, pos_over_neg: 506.57977294921875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 3.5404, loss_val: nan, pos_over_neg: 635.87451171875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 3.5636, loss_val: nan, pos_over_neg: 449.2168884277344 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 3.5878, loss_val: nan, pos_over_neg: 361.789306640625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 383.8893737792969 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 3.613, loss_val: nan, pos_over_neg: 277.1852111816406 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 3.5436, loss_val: nan, pos_over_neg: 694.525390625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 3.5161, loss_val: nan, pos_over_neg: 1335.3525390625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 3.5551, loss_val: nan, pos_over_neg: 964.15380859375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 3.5687, loss_val: nan, pos_over_neg: 639.5107421875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 3.523, loss_val: nan, pos_over_neg: 876.4679565429688 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 3.569, loss_val: nan, pos_over_neg: 631.6744995117188 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 3.5475, loss_val: nan, pos_over_neg: 672.1875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 3.5591, loss_val: nan, pos_over_neg: 662.5978393554688 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 3.5823, loss_val: nan, pos_over_neg: 540.921875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 3.5979, loss_val: nan, pos_over_neg: 269.1251525878906 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 3.5615, loss_val: nan, pos_over_neg: 396.07305908203125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 3.5973, loss_val: nan, pos_over_neg: 351.72833251953125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 3.5443, loss_val: nan, pos_over_neg: 436.7257385253906 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 3.5854, loss_val: nan, pos_over_neg: 580.41552734375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 3.5311, loss_val: nan, pos_over_neg: 910.7473754882812 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 289.0671691894531 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 3.6147, loss_val: nan, pos_over_neg: 402.1070861816406 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 3.5378, loss_val: nan, pos_over_neg: 1065.945068359375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 3.5743, loss_val: nan, pos_over_neg: 497.89886474609375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 3.6112, loss_val: nan, pos_over_neg: 462.20947265625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 3.5517, loss_val: nan, pos_over_neg: 575.3196411132812 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 3.5476, loss_val: nan, pos_over_neg: 591.985595703125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 3.5748, loss_val: nan, pos_over_neg: 530.8081665039062 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 3.575, loss_val: nan, pos_over_neg: 398.9219055175781 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 506.3689270019531 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 676.579345703125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 3.5374, loss_val: nan, pos_over_neg: 559.6047973632812 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 3.5922, loss_val: nan, pos_over_neg: 501.76898193359375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 3.5841, loss_val: nan, pos_over_neg: 317.3396301269531 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 3.5493, loss_val: nan, pos_over_neg: 362.88818359375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 3.565, loss_val: nan, pos_over_neg: 531.9871215820312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 3.5632, loss_val: nan, pos_over_neg: 434.1370544433594 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 402.98345947265625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 3.5605, loss_val: nan, pos_over_neg: 969.5848388671875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 446.5745849609375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 3.5198, loss_val: nan, pos_over_neg: 654.32666015625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 3.5694, loss_val: nan, pos_over_neg: 419.7345886230469 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 3.5693, loss_val: nan, pos_over_neg: 367.6890563964844 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 3.6066, loss_val: nan, pos_over_neg: 308.8607177734375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 3.52, loss_val: nan, pos_over_neg: 747.6928100585938 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 3.5349, loss_val: nan, pos_over_neg: 1176.164794921875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 3.5863, loss_val: nan, pos_over_neg: 710.8530883789062 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 3.5674, loss_val: nan, pos_over_neg: 798.2647094726562 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 3.5909, loss_val: nan, pos_over_neg: 408.5689392089844 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 382.58978271484375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 3.5771, loss_val: nan, pos_over_neg: 377.8128662109375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 348.3900146484375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 3.5516, loss_val: nan, pos_over_neg: 491.3702392578125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 436.1127014160156 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 3.5807, loss_val: nan, pos_over_neg: 656.4893798828125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 3.5792, loss_val: nan, pos_over_neg: 1518.0391845703125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 705.4046020507812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 3.5678, loss_val: nan, pos_over_neg: 610.8094482421875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 3.5221, loss_val: nan, pos_over_neg: 1145.00390625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 3.574, loss_val: nan, pos_over_neg: 577.6885986328125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 3.5588, loss_val: nan, pos_over_neg: 733.6405639648438 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 3.5677, loss_val: nan, pos_over_neg: 630.005126953125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 3.5945, loss_val: nan, pos_over_neg: 477.22265625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 384.7200927734375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 3.5529, loss_val: nan, pos_over_neg: 571.5758666992188 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 412.9725341796875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 3.5682, loss_val: nan, pos_over_neg: 515.5734252929688 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 3.5414, loss_val: nan, pos_over_neg: 461.0107727050781 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 3.6202, loss_val: nan, pos_over_neg: 385.9707336425781 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 3.5697, loss_val: nan, pos_over_neg: 1141.2652587890625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 3.5996, loss_val: nan, pos_over_neg: 514.3621215820312 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 3.5597, loss_val: nan, pos_over_neg: 509.04547119140625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 3.5814, loss_val: nan, pos_over_neg: 627.0958862304688 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 3.5298, loss_val: nan, pos_over_neg: 798.5205078125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 379.84771728515625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 3.5535, loss_val: nan, pos_over_neg: 484.92926025390625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 635.6219482421875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 3.536, loss_val: nan, pos_over_neg: 638.013671875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 3.5732, loss_val: nan, pos_over_neg: 441.76251220703125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 3.6017, loss_val: nan, pos_over_neg: 361.2322082519531 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 3.5331, loss_val: nan, pos_over_neg: 1195.984130859375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 3.6105, loss_val: nan, pos_over_neg: 529.6597290039062 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 3.5605, loss_val: nan, pos_over_neg: 581.0034790039062 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 3.5616, loss_val: nan, pos_over_neg: 400.7652587890625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 3.5466, loss_val: nan, pos_over_neg: 825.997802734375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 3.5507, loss_val: nan, pos_over_neg: 691.837158203125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 3.553, loss_val: nan, pos_over_neg: 633.7666625976562 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 3.5699, loss_val: nan, pos_over_neg: 326.7265319824219 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 3.6362, loss_val: nan, pos_over_neg: 291.255126953125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 3.5904, loss_val: nan, pos_over_neg: 483.5006408691406 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 3.5519, loss_val: nan, pos_over_neg: 393.20635986328125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 3.5721, loss_val: nan, pos_over_neg: 547.197998046875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 3.5789, loss_val: nan, pos_over_neg: 367.1934814453125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 3.5883, loss_val: nan, pos_over_neg: 492.2254333496094 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 3.5926, loss_val: nan, pos_over_neg: 445.11199951171875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 3.5475, loss_val: nan, pos_over_neg: 572.4580688476562 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 3.5623, loss_val: nan, pos_over_neg: 711.2612915039062 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 3.5777, loss_val: nan, pos_over_neg: 388.9667663574219 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 3.5398, loss_val: nan, pos_over_neg: 534.5333862304688 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 3.5895, loss_val: nan, pos_over_neg: 407.4306640625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 331.0434265136719 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 551.0768432617188 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 3.5524, loss_val: nan, pos_over_neg: 759.538330078125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 615.2404174804688 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 3.5541, loss_val: nan, pos_over_neg: 413.436279296875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 3.541, loss_val: nan, pos_over_neg: 613.6722412109375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 3.6291, loss_val: nan, pos_over_neg: 387.2934265136719 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 3.5877, loss_val: nan, pos_over_neg: 542.110107421875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 3.5815, loss_val: nan, pos_over_neg: 512.279296875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 3.5368, loss_val: nan, pos_over_neg: 560.03271484375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 534.6256713867188 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 3.5897, loss_val: nan, pos_over_neg: 406.0479736328125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 3.5464, loss_val: nan, pos_over_neg: 611.239013671875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 3.6035, loss_val: nan, pos_over_neg: 385.18804931640625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 3.5762, loss_val: nan, pos_over_neg: 464.7451477050781 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 3.5078, loss_val: nan, pos_over_neg: 731.3706665039062 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 3.6176, loss_val: nan, pos_over_neg: 320.5897216796875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 3.5754, loss_val: nan, pos_over_neg: 424.7655944824219 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 3.5658, loss_val: nan, pos_over_neg: 526.5927124023438 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 3.5924, loss_val: nan, pos_over_neg: 403.24554443359375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 3.578, loss_val: nan, pos_over_neg: 671.5333862304688 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 3.5663, loss_val: nan, pos_over_neg: 637.5057983398438 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 3.5437, loss_val: nan, pos_over_neg: 670.7445678710938 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 3.5533, loss_val: nan, pos_over_neg: 382.1922912597656 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 3.5689, loss_val: nan, pos_over_neg: 400.2407531738281 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 3.5585, loss_val: nan, pos_over_neg: 955.3153076171875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 3.5413, loss_val: nan, pos_over_neg: 867.2027587890625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 3.5013, loss_val: nan, pos_over_neg: 673.4682006835938 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 3.6247, loss_val: nan, pos_over_neg: 399.9830627441406 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/300000 [4:31:35<75231:46:58, 902.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 694/695, loss_train: 3.5807, loss_val: nan, pos_over_neg: 410.329345703125 lr: 0.00031623\n",
      "epoch: 18\n",
      "Iter: 0/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 722.8974609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 3.5901, loss_val: nan, pos_over_neg: 516.103271484375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 587.323486328125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 3.568, loss_val: nan, pos_over_neg: 297.2255859375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 283.5947570800781 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 3.5715, loss_val: nan, pos_over_neg: 453.1295471191406 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 3.5356, loss_val: nan, pos_over_neg: 580.3215942382812 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 3.5907, loss_val: nan, pos_over_neg: 473.4923400878906 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 3.5498, loss_val: nan, pos_over_neg: 573.6063842773438 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 3.5481, loss_val: nan, pos_over_neg: 407.6631774902344 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 3.5349, loss_val: nan, pos_over_neg: 543.9068603515625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 3.6029, loss_val: nan, pos_over_neg: 485.9453125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 3.5207, loss_val: nan, pos_over_neg: 688.648681640625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 3.6329, loss_val: nan, pos_over_neg: 389.4638977050781 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 3.542, loss_val: nan, pos_over_neg: 658.7935180664062 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 3.5395, loss_val: nan, pos_over_neg: 824.5336303710938 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 3.5681, loss_val: nan, pos_over_neg: 524.870361328125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 3.5361, loss_val: nan, pos_over_neg: 521.3492431640625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 334.6191711425781 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 3.6175, loss_val: nan, pos_over_neg: 489.2477111816406 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 3.5603, loss_val: nan, pos_over_neg: 404.02728271484375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 515.6536254882812 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 3.5609, loss_val: nan, pos_over_neg: 383.1890563964844 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 3.5559, loss_val: nan, pos_over_neg: 423.3218994140625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 3.5692, loss_val: nan, pos_over_neg: 411.5491943359375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 3.5946, loss_val: nan, pos_over_neg: 353.6860656738281 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 3.6119, loss_val: nan, pos_over_neg: 293.5361633300781 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 3.5598, loss_val: nan, pos_over_neg: 532.1578369140625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 3.5408, loss_val: nan, pos_over_neg: 540.6014404296875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 465.3651123046875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 3.5588, loss_val: nan, pos_over_neg: 522.553955078125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 3.5484, loss_val: nan, pos_over_neg: 520.0324096679688 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 3.5468, loss_val: nan, pos_over_neg: 435.4067687988281 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 3.5721, loss_val: nan, pos_over_neg: 692.2763061523438 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 3.5702, loss_val: nan, pos_over_neg: 377.4969177246094 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 3.5913, loss_val: nan, pos_over_neg: 373.34796142578125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 3.6116, loss_val: nan, pos_over_neg: 278.8498229980469 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 3.6033, loss_val: nan, pos_over_neg: 389.24224853515625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 3.583, loss_val: nan, pos_over_neg: 421.9571228027344 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 3.5974, loss_val: nan, pos_over_neg: 362.9754943847656 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 3.6187, loss_val: nan, pos_over_neg: 227.8837127685547 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 3.5568, loss_val: nan, pos_over_neg: 373.316162109375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 3.5925, loss_val: nan, pos_over_neg: 293.7978820800781 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 3.5637, loss_val: nan, pos_over_neg: 439.7800598144531 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 3.536, loss_val: nan, pos_over_neg: 538.9624633789062 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 3.6081, loss_val: nan, pos_over_neg: 810.6681518554688 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 3.5351, loss_val: nan, pos_over_neg: 913.7828979492188 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 3.6425, loss_val: nan, pos_over_neg: 728.7945556640625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 3.5526, loss_val: nan, pos_over_neg: 1042.093994140625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 3.621, loss_val: nan, pos_over_neg: 584.4644775390625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 3.5866, loss_val: nan, pos_over_neg: 348.73675537109375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 560.0181884765625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 3.5703, loss_val: nan, pos_over_neg: 479.7298889160156 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 3.5811, loss_val: nan, pos_over_neg: 421.3617858886719 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 3.5219, loss_val: nan, pos_over_neg: 413.82513427734375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 3.567, loss_val: nan, pos_over_neg: 295.94207763671875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 524.279541015625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 3.5335, loss_val: nan, pos_over_neg: 520.9835815429688 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 3.5788, loss_val: nan, pos_over_neg: 551.6470947265625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 3.5887, loss_val: nan, pos_over_neg: 447.3219299316406 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 3.5265, loss_val: nan, pos_over_neg: 625.8878173828125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 3.6186, loss_val: nan, pos_over_neg: 524.8365478515625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 3.5505, loss_val: nan, pos_over_neg: 670.556884765625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 3.6101, loss_val: nan, pos_over_neg: 498.4570007324219 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 3.5524, loss_val: nan, pos_over_neg: 697.4093627929688 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 3.5691, loss_val: nan, pos_over_neg: 416.18157958984375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 3.5739, loss_val: nan, pos_over_neg: 463.9284362792969 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 3.5351, loss_val: nan, pos_over_neg: 616.0474243164062 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 424.05780029296875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 3.4955, loss_val: nan, pos_over_neg: 678.7843627929688 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 3.535, loss_val: nan, pos_over_neg: 531.4689331054688 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 3.584, loss_val: nan, pos_over_neg: 365.5818176269531 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 3.5964, loss_val: nan, pos_over_neg: 428.63885498046875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 3.5295, loss_val: nan, pos_over_neg: 446.5693664550781 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 3.5678, loss_val: nan, pos_over_neg: 531.28173828125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 3.5799, loss_val: nan, pos_over_neg: 360.46209716796875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 497.8740539550781 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 3.5716, loss_val: nan, pos_over_neg: 820.3375244140625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 3.5437, loss_val: nan, pos_over_neg: 873.0177612304688 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 3.5929, loss_val: nan, pos_over_neg: 487.8092956542969 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 3.558, loss_val: nan, pos_over_neg: 526.2288208007812 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 3.5751, loss_val: nan, pos_over_neg: 525.6278686523438 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 3.5591, loss_val: nan, pos_over_neg: 476.3753356933594 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 354.457275390625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 3.5517, loss_val: nan, pos_over_neg: 456.0345764160156 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 3.6064, loss_val: nan, pos_over_neg: 351.6636962890625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 3.5455, loss_val: nan, pos_over_neg: 629.696044921875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 426.4938659667969 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 3.5779, loss_val: nan, pos_over_neg: 457.20269775390625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 3.5639, loss_val: nan, pos_over_neg: 488.2757873535156 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 3.5646, loss_val: nan, pos_over_neg: 699.9791259765625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 3.5801, loss_val: nan, pos_over_neg: 372.04315185546875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 3.5188, loss_val: nan, pos_over_neg: 706.1917114257812 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 3.5285, loss_val: nan, pos_over_neg: 1775.1153564453125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 3.6008, loss_val: nan, pos_over_neg: 424.0783386230469 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 3.5846, loss_val: nan, pos_over_neg: 502.58160400390625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 3.5577, loss_val: nan, pos_over_neg: 474.5835266113281 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 3.588, loss_val: nan, pos_over_neg: 367.04156494140625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 333.6468505859375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 3.5858, loss_val: nan, pos_over_neg: 314.8749084472656 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 3.5403, loss_val: nan, pos_over_neg: 441.4806823730469 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 3.5634, loss_val: nan, pos_over_neg: 723.1323852539062 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 3.5886, loss_val: nan, pos_over_neg: 467.9952087402344 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 3.5873, loss_val: nan, pos_over_neg: 396.87335205078125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 3.5344, loss_val: nan, pos_over_neg: 313.2701721191406 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 3.6223, loss_val: nan, pos_over_neg: 269.71746826171875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 3.5784, loss_val: nan, pos_over_neg: 657.3560180664062 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 3.5502, loss_val: nan, pos_over_neg: 681.5081787109375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 3.5345, loss_val: nan, pos_over_neg: 782.09375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 585.3348999023438 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 3.6049, loss_val: nan, pos_over_neg: 379.0190734863281 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 3.5755, loss_val: nan, pos_over_neg: 468.6925964355469 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 3.5616, loss_val: nan, pos_over_neg: 519.448486328125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 3.5633, loss_val: nan, pos_over_neg: 484.822509765625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 3.5573, loss_val: nan, pos_over_neg: 349.2876281738281 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 3.5954, loss_val: nan, pos_over_neg: 413.27557373046875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 3.5318, loss_val: nan, pos_over_neg: 418.6661376953125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 3.6327, loss_val: nan, pos_over_neg: 230.1766815185547 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 3.5517, loss_val: nan, pos_over_neg: 509.53253173828125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 3.6049, loss_val: nan, pos_over_neg: 497.0026550292969 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 3.56, loss_val: nan, pos_over_neg: 470.3632507324219 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 3.5472, loss_val: nan, pos_over_neg: 648.873046875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 3.5487, loss_val: nan, pos_over_neg: 527.141357421875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 3.5842, loss_val: nan, pos_over_neg: 958.6299438476562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 3.5425, loss_val: nan, pos_over_neg: 812.7801513671875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 440.824951171875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 3.5583, loss_val: nan, pos_over_neg: 733.890380859375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 3.6002, loss_val: nan, pos_over_neg: 471.8421630859375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 3.5793, loss_val: nan, pos_over_neg: 295.6600036621094 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 3.5774, loss_val: nan, pos_over_neg: 262.8947448730469 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 417.8658447265625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 3.5339, loss_val: nan, pos_over_neg: 597.9498901367188 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 3.5632, loss_val: nan, pos_over_neg: 363.2409362792969 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 3.5497, loss_val: nan, pos_over_neg: 587.654296875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 3.6086, loss_val: nan, pos_over_neg: 344.5715026855469 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 3.5643, loss_val: nan, pos_over_neg: 455.1468505859375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 3.5596, loss_val: nan, pos_over_neg: 472.5032043457031 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 3.5472, loss_val: nan, pos_over_neg: 664.8201904296875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 359.3352355957031 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 3.5991, loss_val: nan, pos_over_neg: 630.4908447265625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 3.552, loss_val: nan, pos_over_neg: 631.6704711914062 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 3.5225, loss_val: nan, pos_over_neg: 708.07421875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 3.6336, loss_val: nan, pos_over_neg: 365.308349609375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 3.5728, loss_val: nan, pos_over_neg: 459.7214050292969 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 3.5613, loss_val: nan, pos_over_neg: 812.4822387695312 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 3.5703, loss_val: nan, pos_over_neg: 448.35284423828125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 3.5993, loss_val: nan, pos_over_neg: 426.6498107910156 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 3.5256, loss_val: nan, pos_over_neg: 614.9490966796875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 561.96630859375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 3.581, loss_val: nan, pos_over_neg: 347.6827392578125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 3.5333, loss_val: nan, pos_over_neg: 811.7794799804688 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 3.5719, loss_val: nan, pos_over_neg: 509.2741394042969 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 3.5469, loss_val: nan, pos_over_neg: 733.627197265625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 3.5675, loss_val: nan, pos_over_neg: 567.1790771484375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 3.5802, loss_val: nan, pos_over_neg: 389.8917236328125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 3.572, loss_val: nan, pos_over_neg: 428.8354797363281 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 3.5723, loss_val: nan, pos_over_neg: 507.5912780761719 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 3.544, loss_val: nan, pos_over_neg: 562.316162109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 3.547, loss_val: nan, pos_over_neg: 507.0346984863281 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 3.5839, loss_val: nan, pos_over_neg: 347.1973571777344 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 3.5588, loss_val: nan, pos_over_neg: 467.4114990234375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 3.5635, loss_val: nan, pos_over_neg: 464.9907531738281 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 3.5418, loss_val: nan, pos_over_neg: 618.9055786132812 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 3.5463, loss_val: nan, pos_over_neg: 703.85546875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 3.5864, loss_val: nan, pos_over_neg: 360.14801025390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 3.5326, loss_val: nan, pos_over_neg: 676.712158203125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 3.5856, loss_val: nan, pos_over_neg: 477.21990966796875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 3.585, loss_val: nan, pos_over_neg: 354.2007141113281 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 3.569, loss_val: nan, pos_over_neg: 524.33544921875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 3.5824, loss_val: nan, pos_over_neg: 416.1620788574219 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 3.5673, loss_val: nan, pos_over_neg: 509.49749755859375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 3.5397, loss_val: nan, pos_over_neg: 789.2413330078125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 3.6024, loss_val: nan, pos_over_neg: 477.2380676269531 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 3.5722, loss_val: nan, pos_over_neg: 401.3866882324219 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 3.5558, loss_val: nan, pos_over_neg: 405.28302001953125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 3.5912, loss_val: nan, pos_over_neg: 594.2112426757812 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 3.5658, loss_val: nan, pos_over_neg: 461.22723388671875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 3.5202, loss_val: nan, pos_over_neg: 595.1671142578125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 3.6045, loss_val: nan, pos_over_neg: 352.78985595703125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 3.5666, loss_val: nan, pos_over_neg: 448.0939025878906 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 3.5845, loss_val: nan, pos_over_neg: 368.6083068847656 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 3.5575, loss_val: nan, pos_over_neg: 462.4307556152344 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 3.5476, loss_val: nan, pos_over_neg: 574.8164672851562 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 3.5574, loss_val: nan, pos_over_neg: 542.39111328125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 566.2927856445312 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 685.45751953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 3.5404, loss_val: nan, pos_over_neg: 731.5493774414062 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 3.5683, loss_val: nan, pos_over_neg: 448.7662658691406 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 448.33258056640625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 3.5701, loss_val: nan, pos_over_neg: 570.8118286132812 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 3.56, loss_val: nan, pos_over_neg: 405.30889892578125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 3.5679, loss_val: nan, pos_over_neg: 372.8654479980469 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 3.5711, loss_val: nan, pos_over_neg: 316.3780822753906 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 3.5609, loss_val: nan, pos_over_neg: 646.568603515625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 3.5753, loss_val: nan, pos_over_neg: 528.039794921875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 3.5608, loss_val: nan, pos_over_neg: 407.241943359375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 3.5608, loss_val: nan, pos_over_neg: 369.4000244140625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 3.5423, loss_val: nan, pos_over_neg: 617.0547485351562 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 3.5784, loss_val: nan, pos_over_neg: 510.5029602050781 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 3.5865, loss_val: nan, pos_over_neg: 630.685791015625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 3.555, loss_val: nan, pos_over_neg: 841.6333618164062 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 3.5474, loss_val: nan, pos_over_neg: 982.87744140625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 3.58, loss_val: nan, pos_over_neg: 460.4040222167969 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 3.5763, loss_val: nan, pos_over_neg: 404.3528137207031 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 3.5631, loss_val: nan, pos_over_neg: 471.7206115722656 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 3.5542, loss_val: nan, pos_over_neg: 373.04290771484375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 3.5584, loss_val: nan, pos_over_neg: 479.4621887207031 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 3.557, loss_val: nan, pos_over_neg: 409.0516052246094 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 3.5377, loss_val: nan, pos_over_neg: 556.4808959960938 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 3.5672, loss_val: nan, pos_over_neg: 378.5914306640625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 3.5629, loss_val: nan, pos_over_neg: 469.2803955078125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 3.5269, loss_val: nan, pos_over_neg: 1052.791015625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 3.562, loss_val: nan, pos_over_neg: 661.4335327148438 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 3.5403, loss_val: nan, pos_over_neg: 960.2054443359375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 3.54, loss_val: nan, pos_over_neg: 792.13818359375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 3.5398, loss_val: nan, pos_over_neg: 873.3906860351562 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 3.5655, loss_val: nan, pos_over_neg: 495.7540283203125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 3.5292, loss_val: nan, pos_over_neg: 562.5372314453125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 3.5495, loss_val: nan, pos_over_neg: 479.8127136230469 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 3.5361, loss_val: nan, pos_over_neg: 511.2328186035156 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 3.5684, loss_val: nan, pos_over_neg: 343.23089599609375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 3.5264, loss_val: nan, pos_over_neg: 550.7647094726562 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 3.5464, loss_val: nan, pos_over_neg: 286.88995361328125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 3.5686, loss_val: nan, pos_over_neg: 351.7287902832031 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 3.5758, loss_val: nan, pos_over_neg: 359.66510009765625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 635.9805908203125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 3.5663, loss_val: nan, pos_over_neg: 619.30712890625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 3.5567, loss_val: nan, pos_over_neg: 709.711181640625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 3.5375, loss_val: nan, pos_over_neg: 1054.048828125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 3.6043, loss_val: nan, pos_over_neg: 567.7019653320312 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 3.5618, loss_val: nan, pos_over_neg: 771.693603515625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 3.5456, loss_val: nan, pos_over_neg: 658.2149047851562 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 3.5393, loss_val: nan, pos_over_neg: 540.5552368164062 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 3.5652, loss_val: nan, pos_over_neg: 317.2604675292969 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 3.5586, loss_val: nan, pos_over_neg: 499.75006103515625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 530.1022338867188 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 3.5505, loss_val: nan, pos_over_neg: 430.407958984375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 3.5534, loss_val: nan, pos_over_neg: 407.3775329589844 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 3.5141, loss_val: nan, pos_over_neg: 888.1080322265625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 3.5803, loss_val: nan, pos_over_neg: 468.224365234375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 3.5547, loss_val: nan, pos_over_neg: 517.7123413085938 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 3.5659, loss_val: nan, pos_over_neg: 637.0930786132812 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 3.5934, loss_val: nan, pos_over_neg: 745.3790283203125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 3.5847, loss_val: nan, pos_over_neg: 559.604736328125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 3.5477, loss_val: nan, pos_over_neg: 770.6407470703125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 3.5829, loss_val: nan, pos_over_neg: 564.2203369140625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 3.5826, loss_val: nan, pos_over_neg: 299.6268310546875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 3.5857, loss_val: nan, pos_over_neg: 377.2142639160156 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 3.6005, loss_val: nan, pos_over_neg: 340.40618896484375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 3.5344, loss_val: nan, pos_over_neg: 485.8901062011719 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 3.5697, loss_val: nan, pos_over_neg: 397.2920837402344 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 3.5524, loss_val: nan, pos_over_neg: 409.3122253417969 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 3.561, loss_val: nan, pos_over_neg: 447.5314025878906 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 3.5581, loss_val: nan, pos_over_neg: 357.19818115234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 3.5555, loss_val: nan, pos_over_neg: 490.93115234375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 375.0066833496094 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 3.5481, loss_val: nan, pos_over_neg: 502.17742919921875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 3.5828, loss_val: nan, pos_over_neg: 542.7413940429688 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 522.73095703125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 3.5564, loss_val: nan, pos_over_neg: 889.0469360351562 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 3.5557, loss_val: nan, pos_over_neg: 710.1278686523438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 3.5682, loss_val: nan, pos_over_neg: 478.5498962402344 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 3.5819, loss_val: nan, pos_over_neg: 327.97918701171875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 3.5385, loss_val: nan, pos_over_neg: 549.6510009765625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 3.5654, loss_val: nan, pos_over_neg: 476.91265869140625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 3.5851, loss_val: nan, pos_over_neg: 368.1750793457031 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 3.5527, loss_val: nan, pos_over_neg: 541.605224609375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 3.5256, loss_val: nan, pos_over_neg: 463.8717346191406 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 3.565, loss_val: nan, pos_over_neg: 467.93463134765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 3.54, loss_val: nan, pos_over_neg: 474.462646484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 3.574, loss_val: nan, pos_over_neg: 518.87646484375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 3.5598, loss_val: nan, pos_over_neg: 460.1362609863281 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 3.5238, loss_val: nan, pos_over_neg: 640.49072265625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 3.5587, loss_val: nan, pos_over_neg: 488.9524841308594 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 3.6062, loss_val: nan, pos_over_neg: 323.8879699707031 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 3.574, loss_val: nan, pos_over_neg: 423.0097351074219 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 3.6, loss_val: nan, pos_over_neg: 348.2423400878906 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 3.6095, loss_val: nan, pos_over_neg: 419.24603271484375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 3.5232, loss_val: nan, pos_over_neg: 500.30517578125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 3.5626, loss_val: nan, pos_over_neg: 503.3735656738281 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 3.6073, loss_val: nan, pos_over_neg: 263.7216491699219 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 3.5711, loss_val: nan, pos_over_neg: 426.5894470214844 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 3.6039, loss_val: nan, pos_over_neg: 241.14715576171875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 3.5371, loss_val: nan, pos_over_neg: 539.5631713867188 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 3.5524, loss_val: nan, pos_over_neg: 415.6381530761719 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 3.5617, loss_val: nan, pos_over_neg: 610.7208251953125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 3.615, loss_val: nan, pos_over_neg: 612.13330078125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 3.5925, loss_val: nan, pos_over_neg: 500.6878967285156 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 3.5743, loss_val: nan, pos_over_neg: 664.994140625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 3.5747, loss_val: nan, pos_over_neg: 626.8072509765625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 3.5664, loss_val: nan, pos_over_neg: 590.235595703125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 3.5598, loss_val: nan, pos_over_neg: 391.73785400390625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 3.5519, loss_val: nan, pos_over_neg: 345.5666198730469 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 3.586, loss_val: nan, pos_over_neg: 323.5089416503906 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 269.55517578125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 3.5724, loss_val: nan, pos_over_neg: 267.7550354003906 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 3.6047, loss_val: nan, pos_over_neg: 288.10174560546875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 3.5309, loss_val: nan, pos_over_neg: 734.7744750976562 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 3.5985, loss_val: nan, pos_over_neg: 465.6539611816406 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 3.5591, loss_val: nan, pos_over_neg: 491.9153137207031 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 3.5459, loss_val: nan, pos_over_neg: 773.5853881835938 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 3.5621, loss_val: nan, pos_over_neg: 423.8729553222656 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 3.5706, loss_val: nan, pos_over_neg: 339.70379638671875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 3.571, loss_val: nan, pos_over_neg: 416.46136474609375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 3.591, loss_val: nan, pos_over_neg: 541.2841796875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 3.569, loss_val: nan, pos_over_neg: 523.3018798828125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 3.5317, loss_val: nan, pos_over_neg: 606.7701416015625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 3.6077, loss_val: nan, pos_over_neg: 355.1853332519531 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 3.5543, loss_val: nan, pos_over_neg: 684.0678100585938 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 3.5966, loss_val: nan, pos_over_neg: 399.4067077636719 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 3.5888, loss_val: nan, pos_over_neg: 382.63702392578125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 3.5511, loss_val: nan, pos_over_neg: 405.2533874511719 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 3.5576, loss_val: nan, pos_over_neg: 420.59423828125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 3.559, loss_val: nan, pos_over_neg: 514.5433959960938 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 3.5798, loss_val: nan, pos_over_neg: 323.1372985839844 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 3.5685, loss_val: nan, pos_over_neg: 506.4656982421875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 3.6066, loss_val: nan, pos_over_neg: 400.11346435546875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 3.5838, loss_val: nan, pos_over_neg: 330.7095947265625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 3.5521, loss_val: nan, pos_over_neg: 467.9472351074219 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 3.5713, loss_val: nan, pos_over_neg: 829.3292846679688 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 3.5741, loss_val: nan, pos_over_neg: 593.8959350585938 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 3.5405, loss_val: nan, pos_over_neg: 566.0670776367188 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 3.5894, loss_val: nan, pos_over_neg: 405.8221130371094 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 3.5928, loss_val: nan, pos_over_neg: 457.624755859375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 3.6053, loss_val: nan, pos_over_neg: 352.03131103515625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 3.5679, loss_val: nan, pos_over_neg: 600.4880981445312 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 3.5344, loss_val: nan, pos_over_neg: 550.3917846679688 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 3.5715, loss_val: nan, pos_over_neg: 330.1326599121094 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 3.5365, loss_val: nan, pos_over_neg: 376.2040100097656 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 3.5812, loss_val: nan, pos_over_neg: 295.06219482421875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 3.532, loss_val: nan, pos_over_neg: 376.1959533691406 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 3.5951, loss_val: nan, pos_over_neg: 211.53712463378906 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 3.5178, loss_val: nan, pos_over_neg: 375.5077209472656 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 3.5794, loss_val: nan, pos_over_neg: 605.4691162109375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 3.5765, loss_val: nan, pos_over_neg: 635.7068481445312 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 3.5612, loss_val: nan, pos_over_neg: 618.5599975585938 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 3.5579, loss_val: nan, pos_over_neg: 631.5135498046875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 3.5484, loss_val: nan, pos_over_neg: 352.73101806640625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 3.5389, loss_val: nan, pos_over_neg: 1326.54052734375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 3.5804, loss_val: nan, pos_over_neg: 423.07958984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 3.5254, loss_val: nan, pos_over_neg: 720.784912109375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 3.5571, loss_val: nan, pos_over_neg: 383.0198974609375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 3.5868, loss_val: nan, pos_over_neg: 304.7947692871094 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 3.5543, loss_val: nan, pos_over_neg: 365.6146240234375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 3.558, loss_val: nan, pos_over_neg: 352.7685852050781 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 3.5649, loss_val: nan, pos_over_neg: 434.7175598144531 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 3.5782, loss_val: nan, pos_over_neg: 352.07806396484375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 3.5908, loss_val: nan, pos_over_neg: 574.4424438476562 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 3.5621, loss_val: nan, pos_over_neg: 891.6367797851562 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 3.5712, loss_val: nan, pos_over_neg: 629.5309448242188 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 3.6067, loss_val: nan, pos_over_neg: 548.9617309570312 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 3.57, loss_val: nan, pos_over_neg: 650.9739990234375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 3.5397, loss_val: nan, pos_over_neg: 421.44415283203125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 3.579, loss_val: nan, pos_over_neg: 363.4754943847656 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 3.5674, loss_val: nan, pos_over_neg: 560.7659301757812 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 3.5168, loss_val: nan, pos_over_neg: 547.6954956054688 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    # model, \n",
    "                                    model_parallel, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.2,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=0, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=0.2'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-3)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
