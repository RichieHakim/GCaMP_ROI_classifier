{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjIUlEQVR4nO2deZhlVXXof6vuvTV0VVfP3TTdDSggEU1A0gGcefLwIcaocUSfYoKiCRrxiYoYA04RDWLQ+NRWUVRwHuMUSavh8VS0IS1gUKY0dNMT1WN1jXdY+eOceu92edaqW7eq7i056/d99dW9e5+99zr7nHWGve5aS1SVIAge/nS0W4AgCFpDKHsQ5IRQ9iDICaHsQZATQtmDICeEsgdBTsiNsovIx0Tk7Q1u+xMReaVRJyLyaRHZJyK/mF0pp4+IvFREfthuOeYrIqIiclybxj5fRP66HWNn8bBUdhF5hYjcVF+mqq9R1XfNQvdPAs4C1qrqqTPpyLuoNIqqXqeqT29wvN+ZlzyTzr+KyEmTyr+Zlp+Rfr9cRD5fV68iMiQih0TkQRG5SkQKk/p4E/AO4I0i8jvnnYgsFZFvpP3cLyIvceQ8T0RuEZGDIrJNRN4vIsXp7u/DUtnnmKOBLao61G5Bmjngc818lGkK7gJePvFFRJYBpwMPTdHuJFXtA84EXgK8qq6P84C/Ap4CPBl4vohcOKn9R4BxYBXwUuCjIvIYY6wFwEXAcuC0dMyLG9i3w1HVtv4BbwEeBAaB3wJnpuWXA18FvpTW3ZpO8ES7S4B707r/AJ6blj8aGAWqwCFgf1r+GeDd6eclwHdIDui+9PPaur5/ArwyQ9bzJ/X9jrT8VcA9wF7g28CRdW2eAPwSOJD+f0Ja/p60n9G0r39KyxX4G+A+YAD4B6AjrXsF8H+BD6ZjvTstu6luPAVeA9yd7ttHAHHm5Zx0/gbT43Cxc6xeBdxZN+enpOVb0uN4GzAGFIE/A34N7E/n89F1/WwB3pr2sQ/4NNCd1p0BbAMuTfd/C/DSurZdwJXAA8Au4GNAT139m4AdwHbgL9P5OM7Yn58Af5eOV0jLXgt8NC07o+5c/PykOT6u7vtX6o7fM9P9WldXvxL4d+AF6fdeEkV/VN02nwOuaFBn/hfwz9PWtTYr+gnAVlLlAI4Bjq2b4DLwfKBEciX7T6CU1r8AOJLk6eRFwBCwuk4pbpo01mf4/8q+DHgeyRVzYXqwvjmVsmf1DTwtPSlPSU/EDwM3pnVL05P5ZSQKcG76fZk1Tnoi/ThtexTJneeVdWNXgNel/fVkyKMkF6/FafuHgLOdedkBPLnuIniKsd8vILkY/AnJxeM44Og65d0MrEtlelR6PM5Kj92bSS6GnXXb35Fuv5TkAjZxbM5I9/GqdD6fmvZ1Qlr/jyQX1KXpsftn4L1p3dkkF4DHkijU9Uyt7K8Efgg8Iy37BfB4GlR24ERgJ3D+NM77xwEjk8oupkEFBr5JgxeG+r92P8ZXSQ7oiSJSUtUtqnpvXf0tqvpVVS2THPxukkcsVPUrqrpdVWuq+iWSO1lD79CqukdVv6aqw6o6SHKXfWqT+/BS4BpVvVVVx0juWI8XkWNIrvJ3q+rnVLWiql8AfgM8a4o+36eqe1X1AZKT+9y6uu2q+uG0vxGj/RWquj9t/2PgZGesMsn896vqPlW91djulcD7VfWXmnCPqt5fV/8hVd2ayvQi4LuqekN67K4kuQg8oW77f0q330sy//X7CPB2VR1T1X8Dvgu8UESE5OniDen8DAJ/D7w4bfNC4NOqeocmr1mXO/tdz2eBl4vICcBiVf1ZA21uFZF9JBebT5I8nTRKH8mTXj0HSC5eLiLyF8B6kjmdFm1VdlW9h+Rd5HJgt4h8UUSOrNtka922NZKr7ZEAIvJyEdksIvtFZD/J1Xx5I+OKyAIR+Xi6MHIQuBFYPHmRpUGOBP7fSa+qh4A9wJrJdSn3p3UeW+s+35/2k1VnsbPu8zDJyWXxPJJH+ftF5N9E5PHGdutIXpss6uWaPCe1tH6Nsf3kfdynh6+JTNSvIHkau6XuuP8gLZ8Yd3K/jfB1kie015E8TjfCKaq6RFWPVdW/TfexUQ4B/ZPK+klej0xE5DnAFSRPIQPTGA+YBwt0qnq9qj6JZOFLgffVVa+b+CAiHcBaYLuIHA18guT9apmqLiZ5LJSJbqcY9o0krxCnqWo/yUIKde2nw/ZU9gk5e0leEx6cXJdyVFrnybmu7vNRaT8TzMRN8XfapnfqZ5O8V34T+LLRditwbIN9T54TIdmnB+u28fZxSTqPk+sHgBHgMaq6OP1bpMlCGSSvJJP7nRJVHQa+T7Ko1qiyz4S7gKKIHF9XdhLJGkcmInI2yTn/LFW9vZlB26rsInKCiDxNRLpIFo9GSB7tJ/hjEfnzdIX3IpLFn5+TvI8p6Ypp+mjz2Lp2u4C1ItJpDL0wHWu/iCwFLpvBblwP/IWInJzux98DN6vqFuB7wKNE5CUiUhSRF5G8432nTs5HZvT5JhFZIiLrgNeTLFLOBofNi4h0pnb6Renj9kEOn/96PglcLCJ/nP7W4Lj0opvFl4FnisiZIlIiubiOAT+t2+ZCEVmbzv+l/O4+viOV78nAnwJfSe+enwA+KCIr031YIyL/o27cV4jIiSKygOkd10uBp6bHbU5Jn1q+DrxTRHpF5InAszEuNCLyNOA64Hmq2vRvO9p9Z+8ieSwZIHn0XEky6RN8i+T9b2KR689Vtayq/wF8APgZyQn8hySLPBP8iOQquVNEsh53/pHkHXKA5OLxg2Z3QFU3Am8HvkZyZzmW9B1SVfeQnKhvJHm0fzPwp3WPYFeTmGX2iciHJu33LSSLXt8FPtWsfJPImpeXAVvS15nXAP8zq6GqfoXk3fp6ksfNb5IskmVt+9u0nw+TzPGzSO5I43WbXU+yMHZf+vfuurqdJMd8O8lJ/hpV/U1a9xaSxb6fpzL/K8lTGqr6fZJj+6N0mx9NMR/1Mm9X1Vb+BuGvSc7B3cAXgL9S1V8DiMhRqQ1/4snk7cAi4Htp+SER+f50B5R0dW/eISKXk6x4Zp58D1dERIHj0/WMhyUisoXEwvCvGXVnkKx8r22xWA972n1nD4KgRYSyB0FOmLeP8UEQzC5xZw+CnNBSp4VO6dJueqfecJ5SWTF92YsP2f4y5SOc/rwHLufXAGrUde6w5aistOVQ53Ygzs9IataZ1YTsAOLMh1ScOktGTw5nn70675h58pd2zp5P1ShDjOtY5t7NSNlTQ//VQAH4pKpe4W3fTS+nyZkzGbKt7H7hEzLLpWYfyRUfs395ue0vs/sDKIybVVStXw9gn4zr3vPT7Apg94tsOcadH3B2Or/3GlmZPSfq/EaxVrLrvAtLzy5bc4vD2XLUSnabsvN7w3Kvfaw7KnafhVG7zzXvs4/NdLlZN5p1TT/Gpz8t/QjwDJIfipwrIic2218QBHPLTN7ZTwXuUdX70h9LfJHkV0BBEMxDZqLsazjc6WAbGQ4eInKBiGwSkU1lxmYwXBAEM2Emyp71gpLlaLFBVder6voSXTMYLgiCmTATZd/G4R5GaznccykIgnnETFbjfwkcLyKPIHFdfDFJLK6HLSv/d/aq6c7X26vZuy+061yaNL1V+rIbbnurLUdHuUGZJjHqRA8YPzr7la1jwDMl2FWFUWfFfchuWF6Y3a7qPGRWnBV3z5pg+gvOE5pWdlWtiMhrgX8hMb1dM+G1EwTB/GNGdnZV/R6Jz3YQBPOc+LlsEOSEUPYgyAmh7EGQE0LZgyAn/L6l6pmXHHG17cgw8GorMrPvOKHOkfFMZYURo0/Py8sbyzEnjSy3KwulbM+VapfjSJLtrAX43mbj/Xa7iuHQ5/Xnmfk6HAclz9nFO0daRdzZgyAnhLIHQU4IZQ+CnBDKHgQ5IZQ9CHJCS1fjK8t72fPc7NXpZZ9sJHHm7x/LP27v167X2c4p1e7mxrPCN5kx4YBap+P44bTTLjtWVK2avaJdGLLvL1p0PGGcqvFF0w8VdfRl9uq45zTkzeOaebDi7hF39iDICaHsQZATQtmDICeEsgdBTghlD4KcEMoeBDmhpYkdu9eu03UXviGzrnjIdj5Y+97WmTQOvPR0s85yCln4xZ/Puhxb326bf4rDdjvLNDS81nZa8RxQOsYdJ5N+J01Lv+GtM2infSkM2veeDifFU80Ja1cazJa/dMhuU3HMns2eiw9cbh9PK+2VZx60uFk3clD3ZvYYd/YgyAmh7EGQE0LZgyAnhLIHQU4IZQ+CnBDKHgQ5Yd6Y3nCsOMXhbNtEccRuI07stFUfbs58su+8bI+9mm1NojRsz+++P7DNWiXHFFlZYI9X6cker9rnTLBDhxOPzTIZAdT6s21lHYdstzFvrFp3cx5xlknXOqcACk6yYc8EOLLSEcSZq2PeNn2Pz6HnnZZZftvGqzm0d2vmaDNycRWRLcAgSZariqqun0l/QRDMHbPhz/7fVHVgFvoJgmAOiXf2IMgJM1V2BX4oIreIyAVZG4jIBSKySUQ2VYeGZjhcEATNMtPH+Ceq6nYRWQncICK/UdUb6zdQ1Q3ABkgW6GY4XhAETTKjO7uqbk//7wa+AZw6G0IFQTD7NH1nF5FeoENVB9PPTwfe6bXpfHDINDNsfZsT5K8ru7zsSO+ZT5plybXZsu97hZ3iaWyRbXMRJ/3TyCrbVOZ5qdWc9EoWaqRqAqgWHPnHHC+1weyDYwXEBNCCU+elr+q2O60Y97OilSYLPzXUyConOKcj4yMutc1r+1+eff5UjfMeME3VNWcOZ/IYvwr4hohM9HO9qv5gBv0FQTCHNK3sqnofcNIsyhIEwRwSprcgyAmh7EGQE0LZgyAnhLIHQU5oqddbvyzV0+TMabfb8q5s04RnqukoOyYjx/xz1DtmN7jlnvNts1y5z5Zx+Ej7uLi52YzLty603bU6SraLoFad3Gyj9gGwcrrVSk5eNie4pee1J86xtjzienbYso8tc8YyctgBLLvd3rdKlyOjUVUctftb9PnsIKcRcDIIglD2IMgLoexBkBNC2YMgJ4SyB0FOmI1INXOOFS/McpABf9W35lzi7v2Anf7JcnTw4sV5q7ejq52AZg7u6rOxb90Lbc+gasWZkKK9Ml12VuOrfcYKf81x4nGsKx7qHGtrNX7kSCdIodNd6YAt/8BJdt2xb5p+nLnZJu7sQZATQtmDICeEsgdBTghlD4KcEMoeBDkhlD0IcsLvhenNolZ0HEKajGfmmYZ0QbYZamxJ2enQwTFdUXAcRpx2R/zhrszyU5ZvNdts3rPWrNu+Z5FZJ92OA40VX6/Tia3XZffX2WmbKUXsuSqPZ5/ilb3ddn9OjL/yIlv+4nBz987tF9vxFy2OvHL6DltxZw+CnBDKHgQ5IZQ9CHJCKHsQ5IRQ9iDICaHsQZATWmp6K6/qZft52WaGaqfdTkvZ5ZY3HECl1zbHVHuai7tXOJR9bdRhW3h1TGiet5Z2OF57K8bNuh0D2aayLT3DZps9hxaYdSUnPl1Hl21yVMO+aZUDrOg/ZNYNDPaadeNjxgkCVEaMU9wx1+Gci6UD9v2xc7+9b156MzVE9OIhjp3zJ9l93WR71015ZxeRa0Rkt4jcUVe2VERuEJG70/9LpuonCIL20shj/GeAsyeVXQJsVNXjgY3p9yAI5jFTKnuab33vpOJnA9emn68FnjO7YgVBMNs0u0C3SlV3AKT/V1obisgFIrJJRDZVR4aaHC4Igpky56vxqrpBVder6vpCj73IEgTB3NKssu8SkdUA6f/dsydSEARzQbOmt28D5wFXpP+/1VArgZoxomVeA6h0Z5tJio4ZR7x4gl2et5xj8rLGctIWWal9ALTHEdLzeivZnlfrVk5eXkkYqdgT3L9g1JbDoepE7hwrZx/oMcMLDWDbTtuo4+1zoeDk82oCL6Cnl6Kq0mP3WXSm+Mj3ZJvYxp6ZbV4D0A7PdTObRkxvXwB+BpwgIttE5HwSJT9LRO4Gzkq/B0Ewj5nyzq6q5xpV08/QGARB24ifywZBTghlD4KcEMoeBDkhlD0IcoKoNucB1gz9slRPk+x1vQcus72Cap3ZMlYdE5qXY80zr7HazolmBTasDDl2Q0eOxasPmnULu205+rtsO84j+wYyywfG+sw2HtsOLTbreoq215s1V7sGF5ptSkXbFFkq2HXjFXudec9A9njqBfv0POKc26OM2pU92+3xltyVvW+ji+3+yv3Z59U9113FyM6tmZVxZw+CnBDKHgQ5IZQ9CHJCKHsQ5IRQ9iDICaHsQZATWhpwcuyoBdz11lMz65bcZps7xpZmmxk6nJxclQXN5YGrDdtT0tmfbQ7r6LHzkNUcE09Hh+2tVXM8+vaM2AEii7I0s7zbMZP90cIHzbqugr1vO4b7zbq+UvZcaZ+9X2NVe+4Xd42YdQ8cWGzWieUR5wQdVStIJX4eOC9P4PgSe7wdT85uJxXH89E4nFbwSog7exDkhlD2IMgJoexBkBNC2YMgJ4SyB0FOaOlqPCp0jGdfXwYfYTeTWvaqZGHEXv0seKumXviuQXv1vDyeHWRMi/aqescCezV7ZMzOM1Sp2nKs7rcdaJZ0Zad5Wt11wGxTcybkqJ7smHYAK7sGzbrVnfszy3eMLzbbbN671qwbGLEjE1tONwBi3M5qxnkIuPH/vLRcVJwTa8Q+noXR7HY1x2rknsMGcWcPgpwQyh4EOSGUPQhyQih7EOSEUPYgyAmh7EGQE1pqepMKdD2UfX2pOfHkakaIt/Ii2+Tl+JG48encdE2Giae0yI4X19lpx05b3mdnte3rtPtcUBw36x6/6N7M8uGabeY7q/dOs+5Ixww1pPb8X3fgcZnlY1b+L6DgOAZ1FW0TphefbnQ8++QZHnFyNTkmNPFMdg6VfltGsRxovKxWRooq77xvJP3TNSKyW0TuqCu7XEQeFJHN6d85U/UTBEF7aeQy9Rng7IzyD6rqyenf92ZXrCAIZpsplV1VbwTsn1EFQfB7wUwW6F4rIrelj/lmrl0RuUBENonIpuqQ/Y4aBMHc0qyyfxQ4FjgZ2AF8wNpQVTeo6npVXV/otX/fHATB3NKUsqvqLlWtqmoN+ASQHWsqCIJ5Q1OmNxFZrao70q/PBe7wtp9ACzC2LNue4MWFM9PxOHYGK0YXgBYdM1+PY+8wPJ7Kg11mk9LSbC80gErNvtZ2F+wd8NptGV2eWf6yJT832xxdtE+DBR22yW60csise2zP1szygbKdhuqYPntpaL/hcQh+LLzHrNqZWb5vsR3H74EB862U8WE71Zd4HnFV+5ipkTbK8hCFKczHBlMqu4h8ATgDWC4i24DLgDNE5GRAgS3Aq6c9chAELWVKZVfVczOKPzUHsgRBMIfEz2WDICeEsgdBTghlD4KcEMoeBDmhtV5vNSgdslLdOJ5GluXNuVSJY0EbX+y5Ezk04Z00Omybrsa7be+1ihNt8Khe20R133C26e23fSvNNsNq93dCadRpZ1axZXxFZvmiop3G6WCl26yrOAf7mIW2/Hftz5aj5HjYLeqzZdxTto9LbchRp5IzWUZdzfHm04oxH45OxJ09CHJCKHsQ5IRQ9iDICaHsQZATQtmDICeEsgdBTmip6a04AstuzzZ5DK7z3N6y6bBjEDK21K7TlXYwx5KTt62yxzAN9dsear0LbdOVFyjxP/fZO/CQk/dsRU92gJCfHPwDs01fwQlu2WGbBxc47e4aPiKzfPvwIrPNUMU2Uw6XbW+zmuP9OFbOPsX3GYEoAcZG7DptMqKjDNnnd4fh9Vawp54OI+CkODoRd/YgyAmh7EGQE0LZgyAnhLIHQU4IZQ+CnNDS1fhqJxw8OntVcnyR7ShQXWA4CnjOBc6qqQzYMeMqTrojM3bdIXsaD6kdO21szF71XbV40KzzUkONGw40407apc2D2c4iAH0le6wO7LkqdmRbGjoL9nLx/fvt2G8Lux0LiuPUUqlmz0exaFtCRsq2Q05xwIlB56yEezEWa51GbEMnJVoz48SdPQhyQih7EOSEUPYgyAmh7EGQE0LZgyAnhLIHQU5oJCPMOuCzwBEk0dY2qOrVIrIU+BJwDElWmBeq6j6vr1oJRlZnm0mqTtolKz5dYdi+Vlmx7gC6HClHbSsUle5sU0h1gS27lm0ZuxbZDjRdRduO88A+20S1vC/bEebO/avMNh1WkD/8NFSHyrYJc3A8u84zky3osj0/yk7Kqy7HoahgjDc2ZpvXPKqrbBOgjjrOLsN2XXEo+1wtDtn7XDMsgDN1hKkAb1TVRwOnAxeKyInAJcBGVT0e2Jh+D4JgnjKlsqvqDlW9Nf08CNwJrAGeDVybbnYt8Jw5kjEIgllgWu/sInIM8DjgZmDVRCbX9L8dqzgIgrbTsLKLSB/wNeAiVT04jXYXiMgmEdlUHcp+nwyCYO5pSNlFpESi6Nep6tfT4l0isjqtXw3szmqrqhtUdb2qri/02hFWgiCYW6ZUdhERkhTNd6rqVXVV3wbOSz+fB3xr9sULgmC2aMTr7YnAy4DbRWRzWnYpcAXwZRE5H3gAeMGUPYmdsqnrIVuUoy/7aQNiHs6e8x9v1lW7nDhiTiqnmmF6w+mOcft6OrhjoV23067r6LPNYQNG+ap+24tu/4hthhocs81rB4fsdr092Saq/U7styW9dtqlUafdzt1OwEHDi7HQZZvrHGc+GLTl6Bh3TgTvHDHqyv22IKWDRgw6R/YplV1Vb7LF4cyp2gdBMD+IX9AFQU4IZQ+CnBDKHgQ5IZQ9CHJCKHsQ5ISWBpzs2jrEcW/4+az1d/Alp5t144udlEBLvKCS9nimR5ET3FKdAJauOcbD6bJSyb5+7x7sM9sMH7LNa109jmdep+1iNTKWncqpULBtmwccE6DlzQewYJ3tLbd7b39meXWPvc9WaiWYwrzm4LWy+qz02Ae60ptdZ5m2Ie7sQZAbQtmDICeEsgdBTghlD4KcEMoeBDkhlD0IckJLTW8eQ88/zawb78u+JnWUbdNEuUnX+Z6dtpFkfFF23fgiJ1imE3CSDseG5jSrHbI9r6qGqa88Zh9qdUyH404+uu5O2yw3OGTkuHNsUF5wzqEHbS9A7XE82GrGgL12m9IOe58LI/YOFGwLoGvuHVuWLYsXUFUc70yLuLMHQU4IZQ+CnBDKHgQ5IZQ9CHJCKHsQ5ISWrsZXlvey5znZseG81cWln/7ZtMda5NTtvOgJZp27ytmED4R2Tj+tFUDXLjtd0NgqeyW5MpbdruDIURt1Vn2dNFRVJyVTsSu7XbVs75djFKDgpELS0SZivx1hp3EqDGc78QCovVCPOqmXOg/YMj7irTdnlm/9W/s8rVpOMs5UxJ09CHJCKHsQ5IRQ9iDICaHsQZATQtmDICeEsgdBTpjS9CYi64DPAkeQJEfaoKpXi8jlwKuAh9JNL1XV77l9KXQY5okl107fvObhmde8GG7iZQUyzBq1Hsde55nrLCcNQG0LFVpyzHmGc03NMfNRteu8WG2D446QhhwlwyQH4IlYWWJXyph9z7JMdoV77Xh3pWFbDs/Byov/VrKzb7Hrddnn6rp3Tz/t2S61Y/U1YmevAG9U1VtFZCFwi4jckNZ9UFWvnLZEQRC0nEZyve0AdqSfB0XkTmDNXAsWBMHsMq13dhE5BngcMPGTn9eKyG0ico2ILJlt4YIgmD0aVnYR6QO+BlykqgeBjwLHAieT3Pk/YLS7QEQ2icimyqj9PhEEwdzSkLKLSIlE0a9T1a8DqOouVa2qag34BHBqVltV3aCq61V1fbG7yfAxQRDMmCmVXUQE+BRwp6peVVe+um6z5wJ3zL54QRDMFqLq2KEAEXkS8H+A20lMbwCXAueSPMIrsAV4dbqYZ9IvS/U0OXNmEs8xA6/O9soDGD7CSNNjpOIBP/2Tl0qoa69dN7zaNr11rst+VRo9YJvQXHczxyzX4cRI004jPVG3Y9scd7zoBm0zX8F2YDM9C71j1uHEklvgxCisGGH3ADrscH0ccfX0TWwWN+tGDmr2ydPIavxNZFuLXZt6EATzi/gFXRDkhFD2IMgJoexBkBNC2YMgJ4SyB0FOmDfpn+YLyz9ue99te2u2d1LBcf4aX2ybeArDjlnLMf8Uxux2xWK2aaur37ZPjQ04NiMnUKV3rygOGnVWOVBzzsZayTGVOe5y/fdmtzvwKHuscr891qDtLEfPbluOmh3DsmXEnT0IckIoexDkhFD2IMgJoexBkBNC2YMgJ4SyB0FOCNPbNFj73mzvpHv/wfaU8yg6pjdxnBG7H3JMPAcWZ5aXVzpBKr3glnaVe6swzYOeJa/bMa85HnaVPrvTA8dly9HjeK8VFjuebY633Ngyu87zYmwVcWcPgpwQyh4EOSGUPQhyQih7EOSEUPYgyAmh7EGQE8L0Ng22vi3b663WZQdR7HDykHXtt001tVJzZjm7URNtgI5R537gVFWNgJOlQVuQ8aX2jhX32O1KB2xBqj3ZfY4vMpvQPWDLseJdtlfkg2+x8wvWHPNmq4g7exDkhFD2IMgJoexBkBNC2YMgJ4SyB0FOmHI1XkS6gRuBrnT7r6rqZSKyFPgScAxJ+qcXquq+uRO1/YytyHa4UCc+WnGfvYrsZV1yPVAcZ5LCqNPOQJyMTF68u2qXl/Yqu7yywInJN9qcA0rnfieWXzm7zlqlBygvbM50seZ9s5fGaS5o5M4+BjxNVU8iye12toicDlwCbFTV44GN6fcgCOYpUyq7JhxKv5bSPwWeDVybll8LPGcuBAyCYHZoND97QUQ2A7uBG1T1ZmDVRNbW9P/KOZMyCIIZ05Cyq2pVVU8G1gKnishjGx1ARC4QkU0isqmMk1s3CII5ZVqr8aq6H/gJcDawS0RWA6T/dxttNqjqelVdX8LJER4EwZwypbKLyAoRWZx+7gH+O/Ab4NvAeelm5wHfmiMZgyCYBRpxhFkNXCsiBZKLw5dV9Tsi8jPgyyJyPvAA8II5lLNl3P9OO55crbeSXWGYdwCKI/ZYoo59reY4wjimt5UfMeLkXXm63cjBMw8Wxp1YeJY50knV5Jneqk58uuKwWcWqD2XPx/Y3204r1ZLd366/sdtZY80XplR2Vb0NeFxG+R7gzLkQKgiC2Sd+QRcEOSGUPQhyQih7EOSEUPYgyAmh7EGQE0Q9889sDybyEHB/+nU5MNCywW1CjsMJOQ7n902Oo1V1RVZFS5X9sIFFNqnq+rYMHnKEHDmUIx7jgyAnhLIHQU5op7JvaOPY9YQchxNyHM7DRo62vbMHQdBa4jE+CHJCKHsQ5IS2KLuInC0ivxWRe0SkbYEqRWSLiNwuIptFZFMLx71GRHaLyB11ZUtF5AYRuTv9v6RNclwuIg+mc7JZRM5pgRzrROTHInKniPxaRF6flrd0Thw5WjonItItIr8QkV+lcrwjLZ/ZfKhqS/+AAnAv8EigE/gVcGKr5Uhl2QIsb8O4TwFOAe6oK3s/cEn6+RLgfW2S43Lg4hbPx2rglPTzQuAu4MRWz4kjR0vnhCQNZ1/6uQTcDJw+0/lox539VOAeVb1PVceBL5JEqs0NqnojsHdSccuj9RpytBxV3aGqt6afB4E7gTW0eE4cOVqKJsx6ROd2KPsaYGvd9220YUJTFPihiNwiIhe0SYYJ5lO03teKyG3pY/6cv07UIyLHkARLaWsE40lyQIvnZC4iOrdD2bNiD7XL/vdEVT0FeAZwoYg8pU1yzCc+ChxLkhBkB/CBVg0sIn3A14CLVPVgq8ZtQI6Wz4nOIKKzRTuUfRuwru77WmB7G+RAVben/3cD3yB5xWgXDUXrnWtUdVd6otWAT9CiORGREomCXaeqX0+LWz4nWXK0a07SsfczzYjOFu1Q9l8Cx4vII0SkE3gxSaTaliIivSKycOIz8HTgDr/VnDIvovVOnEwpz6UFcyIiAnwKuFNVr6qraumcWHK0ek7mLKJzq1YYJ602nkOy0nkv8LY2yfBIEkvAr4Bft1IO4Askj4Nlkied84FlJDnz7k7/L22THJ8DbgduS0+u1S2Q40kkr3K3AZvTv3NaPSeOHC2dE+CPgH9Px7sD+Lu0fEbzET+XDYKcEL+gC4KcEMoeBDkhlD0IckIoexDkhFD2IMgJoexBkBNC2YMgJ/wX9bp5bGZDJEAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOWklEQVR4nO3dcahe9X3H8ffHaLXMDpVcXUiyxcFlLClU5ZI5hOFmmZkdi39USGEuDEfYsGBhMJL+sdI/Av5VxmAywirLWFcJtM7g2m0hq5TBZnp1Wo0x8652ekkwt5bWyoYj2Xd/3CM83tyb5+Te5zH3+e39gss553d+5znfX37cT07O8zwnqSokSW256koXIEkaPcNdkhpkuEtSgwx3SWqQ4S5JDbr6ShcAsHHjxtq2bduVLkOSJspzzz33g6qaWm7fugj3bdu2MTs7e6XLkKSJkuQ/V9rnbRlJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQuviG6lpt2/93qz72+49+aoSVtM8/68mwlnkC5+pyrcffC6/cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qFe5Jvp/kpSQvJJnt2m5KcizJa93yxoH+B5LMJTmd5N5xFS9JWt7lXLn/alXdVlUz3fZ+4HhVTQPHu22SbAf2ADuAXcBjSTaMsGZJ0hBruS2zGzjcrR8G7h9of6Kq3quq14E5YOcaziNJukx9w72Af0zyXJJ9XdstVXUWoFve3LVvBt4cOHa+a/uAJPuSzCaZXVhYWF31kqRl9X1w2F1VdSbJzcCxJK9eom+WaauLGqoOAYcAZmZmLtovSVq9XlfuVXWmW54DnmTxNstbSTYBdMtzXfd5YOvA4VuAM6MqWJI03NBwT/JTST72/jrw68DLwFFgb9dtL/BUt34U2JPk2iS3AtPAiVEXLklaWZ/bMrcATyZ5v//fVNXfJ/kOcCTJQ8AbwAMAVXUyyRHgFeA88HBVXRhL9ZKkZQ0N96r6HvCJZdrfBu5Z4ZiDwME1VydJWhW/oSpJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa1Dvck2xI8m9Jnu62b0pyLMlr3fLGgb4HkswlOZ3k3nEULkla2eVcuT8CnBrY3g8cr6pp4Hi3TZLtwB5gB7ALeCzJhtGUK0nqo1e4J9kCfAr4i4Hm3cDhbv0wcP9A+xNV9V5VvQ7MATtHUq0kqZe+V+5/AvwR8L8DbbdU1VmAbnlz174ZeHOg33zX9gFJ9iWZTTK7sLBwuXVLki5haLgn+U3gXFU91/M1s0xbXdRQdaiqZqpqZmpqqudLS5L6uLpHn7uA30pyH3Ad8NNJ/hp4K8mmqjqbZBNwrus/D2wdOH4LcGaURUuSLm3olXtVHaiqLVW1jcU3Sv+pqn4bOArs7brtBZ7q1o8Ce5Jcm+RWYBo4MfLKJUkr6nPlvpJHgSNJHgLeAB4AqKqTSY4ArwDngYer6sKaK5Uk9XZZ4V5VzwDPdOtvA/es0O8gcHCNtUmSVslvqEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGDQ33JNclOZHkxSQnk3yxa78pybEkr3XLGweOOZBkLsnpJPeOcwCSpIv1uXJ/D/i1qvoEcBuwK8mdwH7geFVNA8e7bZJsB/YAO4BdwGNJNoyhdknSCoaGey16t9u8pvspYDdwuGs/DNzfre8Gnqiq96rqdWAO2DnKoiVJl9brnnuSDUleAM4Bx6rqWeCWqjoL0C1v7rpvBt4cOHy+a1v6mvuSzCaZXVhYWMMQJElL9Qr3qrpQVbcBW4CdST5+ie5Z7iWWec1DVTVTVTNTU1O9ipUk9XNZn5apqh8Bz7B4L/2tJJsAuuW5rts8sHXgsC3AmbUWKknqr8+nZaaS3NCtfxT4JPAqcBTY23XbCzzVrR8F9iS5NsmtwDRwYsR1S5Iu4eoefTYBh7tPvFwFHKmqp5P8C3AkyUPAG8ADAFV1MskR4BXgPPBwVV0YT/mSpOUMDfeq+i5w+zLtbwP3rHDMQeDgmquTJK2K31CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDRoa7km2JvlWklNJTiZ5pGu/KcmxJK91yxsHjjmQZC7J6ST3jnMAkqSL9blyPw/8YVX9InAn8HCS7cB+4HhVTQPHu226fXuAHcAu4LEkG8ZRvCRpeUPDvarOVtXz3fpPgFPAZmA3cLjrdhi4v1vfDTxRVe9V1evAHLBzxHVLki7hsu65J9kG3A48C9xSVWdh8S8A4Oau22bgzYHD5ru2pa+1L8lsktmFhYVVlC5JWknvcE9yPfA14HNV9c6lui7TVhc1VB2qqpmqmpmamupbhiSph17hnuQaFoP9K1X19a75rSSbuv2bgHNd+zywdeDwLcCZ0ZQrSeqjz6dlAnwZOFVVXxrYdRTY263vBZ4aaN+T5NoktwLTwInRlSxJGubqHn3uAh4EXkryQtf2eeBR4EiSh4A3gAcAqupkkiPAKyx+0ubhqrow6sIlSSsbGu5V9c8sfx8d4J4VjjkIHFxDXZKkNfAbqpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBQ8M9yeNJziV5eaDtpiTHkrzWLW8c2HcgyVyS00nuHVfhkqSV9bly/0tg15K2/cDxqpoGjnfbJNkO7AF2dMc8lmTDyKqVJPUyNNyr6tvAD5c07wYOd+uHgfsH2p+oqveq6nVgDtg5mlIlSX2t9p77LVV1FqBb3ty1bwbeHOg337VdJMm+JLNJZhcWFlZZhiRpOaN+QzXLtNVyHavqUFXNVNXM1NTUiMuQpP/fVhvubyXZBNAtz3Xt88DWgX5bgDOrL0+StBqrDfejwN5ufS/w1ED7niTXJrkVmAZOrK1ESdLlunpYhyRfBe4GNiaZB74APAocSfIQ8AbwAEBVnUxyBHgFOA88XFUXxlS7JGkFQ8O9qj6zwq57Vuh/EDi4lqIkSWvjN1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg8YW7kl2JTmdZC7J/nGdR5J0sbGEe5INwJ8BvwFsBz6TZPs4ziVJuti4rtx3AnNV9b2q+h/gCWD3mM4lSVoiVTX6F00+Deyqqt/rth8EfqmqPjvQZx+wr9v8BeD0Gk65EfjBGo5fL1oZBziW9aiVcYBjed/PVdXUcjuuXn09l5Rl2j7wt0hVHQIOjeRkyWxVzYzita6kVsYBjmU9amUc4Fj6GNdtmXlg68D2FuDMmM4lSVpiXOH+HWA6ya1JPgLsAY6O6VySpCXGclumqs4n+SzwD8AG4PGqOjmOc3VGcntnHWhlHOBY1qNWxgGOZaixvKEqSbqy/IaqJDXIcJekBk1MuA97nEEW/Wm3/7tJ7rgSdfbRYyx3J/lxkhe6nz++EnUOk+TxJOeSvLzC/kmak2FjmZQ52ZrkW0lOJTmZ5JFl+kzEvPQcy6TMy3VJTiR5sRvLF5fpM9p5qap1/8Pim7L/Afw88BHgRWD7kj73Ad9k8TP2dwLPXum61zCWu4Gnr3StPcbyK8AdwMsr7J+IOek5lkmZk03AHd36x4B/n+DflT5jmZR5CXB9t34N8Cxw5zjnZVKu3Ps8zmA38Fe16F+BG5Js+rAL7aGZRzNU1beBH16iy6TMSZ+xTISqOltVz3frPwFOAZuXdJuIeek5lonQ/Vm/221e0/0s/TTLSOdlUsJ9M/DmwPY8F09ynz7rQd86f7n7J9w3k+z4cEobuUmZk74mak6SbANuZ/EqcdDEzcslxgITMi9JNiR5ATgHHKuqsc7LuB4/MGpDH2fQs8960KfO51l8ZsS7Se4D/haYHndhYzApc9LHRM1JkuuBrwGfq6p3lu5e5pB1Oy9DxjIx81JVF4DbktwAPJnk41U1+B7PSOdlUq7c+zzOYFIeeTC0zqp65/1/wlXVN4Brkmz88EocmUmZk6EmaU6SXMNiGH6lqr6+TJeJmZdhY5mkeXlfVf0IeAbYtWTXSOdlUsK9z+MMjgK/073jfCfw46o6+2EX2sPQsST5mSTp1neyOE9vf+iVrt2kzMlQkzInXY1fBk5V1ZdW6DYR89JnLBM0L1PdFTtJPgp8Enh1SbeRzstE3JapFR5nkOT3u/1/DnyDxXeb54D/An73StV7KT3H8mngD5KcB/4b2FPd2+nrSZKvsvhphY1J5oEvsPhG0UTNCfQay0TMCXAX8CDwUnd/F+DzwM/CxM1Ln7FMyrxsAg5n8T8yugo4UlVPjzPDfPyAJDVoUm7LSJIug+EuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGvR/82ENSdA3DKAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "#     torchvision.transforms.Resize(size=(224,224), \n",
    "#                                   interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5982, loss_val: nan, pos_over_neg: 1.0550845861434937 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.4398, loss_val: nan, pos_over_neg: 2.5104799270629883 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 7.2474, loss_val: nan, pos_over_neg: 8.204949378967285 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 7.1671, loss_val: nan, pos_over_neg: 15.544561386108398 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 7.0886, loss_val: nan, pos_over_neg: 18.76299476623535 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 7.0679, loss_val: nan, pos_over_neg: 35.98896026611328 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 7.0196, loss_val: nan, pos_over_neg: 61.9755859375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.9937, loss_val: nan, pos_over_neg: 132.55320739746094 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.9979, loss_val: nan, pos_over_neg: 140.61363220214844 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.9655, loss_val: nan, pos_over_neg: 125.67739868164062 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.9554, loss_val: nan, pos_over_neg: 80.19403076171875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.9522, loss_val: nan, pos_over_neg: 92.30541229248047 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.9438, loss_val: nan, pos_over_neg: 151.3818359375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.941, loss_val: nan, pos_over_neg: 162.8236083984375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.9333, loss_val: nan, pos_over_neg: 293.2522888183594 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.9304, loss_val: nan, pos_over_neg: 159.84625244140625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.9328, loss_val: nan, pos_over_neg: 356.19683837890625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.9288, loss_val: nan, pos_over_neg: 162.20005798339844 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.9259, loss_val: nan, pos_over_neg: 186.14805603027344 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.9152, loss_val: nan, pos_over_neg: 259.31524658203125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.9235, loss_val: nan, pos_over_neg: 303.23895263671875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.92, loss_val: nan, pos_over_neg: 885.0030517578125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.9067, loss_val: nan, pos_over_neg: 359.4456481933594 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.9117, loss_val: nan, pos_over_neg: 189.61767578125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.9097, loss_val: nan, pos_over_neg: 189.76951599121094 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.8987, loss_val: nan, pos_over_neg: 454.01116943359375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.9118, loss_val: nan, pos_over_neg: 468.9049987792969 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.8902, loss_val: nan, pos_over_neg: 778.3798217773438 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.8972, loss_val: nan, pos_over_neg: 1357.4012451171875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.8921, loss_val: nan, pos_over_neg: 334.107177734375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.8944, loss_val: nan, pos_over_neg: 604.6907348632812 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.8991, loss_val: nan, pos_over_neg: 214.41043090820312 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.8885, loss_val: nan, pos_over_neg: 277.725830078125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.8847, loss_val: nan, pos_over_neg: 399.68084716796875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.8988, loss_val: nan, pos_over_neg: 625.3892822265625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.8838, loss_val: nan, pos_over_neg: -4605.11962890625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.8763, loss_val: nan, pos_over_neg: 1424.5341796875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.8758, loss_val: nan, pos_over_neg: 1411.6678466796875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.8924, loss_val: nan, pos_over_neg: 978.9677734375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.8753, loss_val: nan, pos_over_neg: 816.5248413085938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.8794, loss_val: nan, pos_over_neg: 296.0098876953125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.878, loss_val: nan, pos_over_neg: 362.0181884765625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.8792, loss_val: nan, pos_over_neg: 412.0280456542969 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.881, loss_val: nan, pos_over_neg: 6891.71533203125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.8726, loss_val: nan, pos_over_neg: -29464.87890625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.877, loss_val: nan, pos_over_neg: -12776.5546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.8772, loss_val: nan, pos_over_neg: 719.6853637695312 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.8709, loss_val: nan, pos_over_neg: 456.0714111328125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.8713, loss_val: nan, pos_over_neg: 546.4715576171875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.8633, loss_val: nan, pos_over_neg: 794.9439086914062 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.8608, loss_val: nan, pos_over_neg: 1546.703125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.8602, loss_val: nan, pos_over_neg: 2006.1732177734375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.8665, loss_val: nan, pos_over_neg: -2403.866943359375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.8651, loss_val: nan, pos_over_neg: 361203.71875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.8684, loss_val: nan, pos_over_neg: 5776.30419921875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.8657, loss_val: nan, pos_over_neg: 2938.461181640625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.8607, loss_val: nan, pos_over_neg: 1095.692138671875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.8629, loss_val: nan, pos_over_neg: 526.9398193359375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.8626, loss_val: nan, pos_over_neg: 629.7684936523438 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.8577, loss_val: nan, pos_over_neg: 585.6128540039062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.8539, loss_val: nan, pos_over_neg: 2039.1702880859375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.8596, loss_val: nan, pos_over_neg: 2220.740234375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.8517, loss_val: nan, pos_over_neg: 2248.787841796875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.8638, loss_val: nan, pos_over_neg: 1813.780517578125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.8508, loss_val: nan, pos_over_neg: 2798.856689453125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.8597, loss_val: nan, pos_over_neg: 591.456298828125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.8574, loss_val: nan, pos_over_neg: 518.89306640625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.8569, loss_val: nan, pos_over_neg: 383.9895324707031 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.8437, loss_val: nan, pos_over_neg: 561.0322265625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.8479, loss_val: nan, pos_over_neg: -175305.125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.8392, loss_val: nan, pos_over_neg: 858.2225341796875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.8572, loss_val: nan, pos_over_neg: 540.4429931640625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.8384, loss_val: nan, pos_over_neg: 7067.4287109375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.8421, loss_val: nan, pos_over_neg: 3008.743896484375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.8549, loss_val: nan, pos_over_neg: 1772.6767578125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.8462, loss_val: nan, pos_over_neg: 1022.7459106445312 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.838, loss_val: nan, pos_over_neg: 3168.41015625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.8421, loss_val: nan, pos_over_neg: 2924.020751953125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.8401, loss_val: nan, pos_over_neg: -2705.466552734375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.8443, loss_val: nan, pos_over_neg: 1191.191162109375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.8479, loss_val: nan, pos_over_neg: 539.8091430664062 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.8336, loss_val: nan, pos_over_neg: 444.49090576171875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.8392, loss_val: nan, pos_over_neg: 714.432373046875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.837, loss_val: nan, pos_over_neg: 7051.96142578125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.8422, loss_val: nan, pos_over_neg: -22951.916015625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.8455, loss_val: nan, pos_over_neg: -4359.107421875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.8436, loss_val: nan, pos_over_neg: 1451.1231689453125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.8416, loss_val: nan, pos_over_neg: 1076.8516845703125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.8449, loss_val: nan, pos_over_neg: 1719.9248046875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.8504, loss_val: nan, pos_over_neg: 793.1513671875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.8589, loss_val: nan, pos_over_neg: 705.3621826171875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.8381, loss_val: nan, pos_over_neg: 600.3796997070312 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.837, loss_val: nan, pos_over_neg: 1384.7772216796875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.8394, loss_val: nan, pos_over_neg: 923.1011352539062 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.8547, loss_val: nan, pos_over_neg: 2040.891357421875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.8365, loss_val: nan, pos_over_neg: -35074.5859375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.8432, loss_val: nan, pos_over_neg: 38697.7890625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.8404, loss_val: nan, pos_over_neg: 6382.8955078125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.8431, loss_val: nan, pos_over_neg: 9458.451171875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.8414, loss_val: nan, pos_over_neg: 1484.3385009765625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.8432, loss_val: nan, pos_over_neg: 450.4328308105469 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.8363, loss_val: nan, pos_over_neg: 1066.400634765625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.8371, loss_val: nan, pos_over_neg: 1041.6348876953125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.8333, loss_val: nan, pos_over_neg: -1772.2796630859375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.8378, loss_val: nan, pos_over_neg: -2694.98828125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.8403, loss_val: nan, pos_over_neg: 1074.305908203125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.8284, loss_val: nan, pos_over_neg: 3095.192626953125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.8307, loss_val: nan, pos_over_neg: 1147.779296875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.8367, loss_val: nan, pos_over_neg: 723.4158935546875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.8403, loss_val: nan, pos_over_neg: -50354.8828125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.8398, loss_val: nan, pos_over_neg: -2253.527099609375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.8431, loss_val: nan, pos_over_neg: 560.4556884765625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.8332, loss_val: nan, pos_over_neg: 6966.40576171875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.8347, loss_val: nan, pos_over_neg: 1439.163818359375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.8327, loss_val: nan, pos_over_neg: 2236.00244140625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.8387, loss_val: nan, pos_over_neg: 1065.5059814453125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.8345, loss_val: nan, pos_over_neg: 1816.7611083984375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.8404, loss_val: nan, pos_over_neg: 684.1168212890625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.8307, loss_val: nan, pos_over_neg: 6790.35205078125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.8408, loss_val: nan, pos_over_neg: 522.078369140625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.8352, loss_val: nan, pos_over_neg: 606.6558837890625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.8374, loss_val: nan, pos_over_neg: 1220.52197265625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.8327, loss_val: nan, pos_over_neg: -5957.57958984375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.8373, loss_val: nan, pos_over_neg: 3342.548095703125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.8336, loss_val: nan, pos_over_neg: 1657.971923828125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.8349, loss_val: nan, pos_over_neg: 371.047607421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.8365, loss_val: nan, pos_over_neg: 262.6270751953125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.834, loss_val: nan, pos_over_neg: 1383.8621826171875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.8323, loss_val: nan, pos_over_neg: 1403.263427734375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.8292, loss_val: nan, pos_over_neg: 2890.981201171875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.8354, loss_val: nan, pos_over_neg: 787.4096069335938 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.834, loss_val: nan, pos_over_neg: 796.1113891601562 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.8314, loss_val: nan, pos_over_neg: 697.7156372070312 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.8273, loss_val: nan, pos_over_neg: 585.3969116210938 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.84, loss_val: nan, pos_over_neg: 489.9184875488281 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.8297, loss_val: nan, pos_over_neg: 649.3230590820312 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.8377, loss_val: nan, pos_over_neg: 693.0384521484375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.8285, loss_val: nan, pos_over_neg: 325.87225341796875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.8373, loss_val: nan, pos_over_neg: 3264.405029296875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.8381, loss_val: nan, pos_over_neg: 642.3505249023438 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.8326, loss_val: nan, pos_over_neg: 199.39418029785156 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.8332, loss_val: nan, pos_over_neg: 358.8052978515625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.8305, loss_val: nan, pos_over_neg: 5653.47216796875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.8358, loss_val: nan, pos_over_neg: 471.5030517578125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.8297, loss_val: nan, pos_over_neg: 705.0119018554688 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.8271, loss_val: nan, pos_over_neg: 5663.81005859375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.8313, loss_val: nan, pos_over_neg: 531.568603515625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.825, loss_val: nan, pos_over_neg: 606.2653198242188 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.8323, loss_val: nan, pos_over_neg: 606.329345703125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.8309, loss_val: nan, pos_over_neg: 958.3739013671875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.8329, loss_val: nan, pos_over_neg: 402.0090637207031 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.837, loss_val: nan, pos_over_neg: 577.81689453125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.8332, loss_val: nan, pos_over_neg: 8439.8037109375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.8285, loss_val: nan, pos_over_neg: 395.9852294921875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.8322, loss_val: nan, pos_over_neg: 642.2991333007812 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.8369, loss_val: nan, pos_over_neg: 680.1788330078125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.8343, loss_val: nan, pos_over_neg: 396.8969421386719 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.8323, loss_val: nan, pos_over_neg: 464.5426330566406 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.8298, loss_val: nan, pos_over_neg: 841.8034057617188 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.8222, loss_val: nan, pos_over_neg: -3215.063720703125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.8295, loss_val: nan, pos_over_neg: 600.261474609375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.8172, loss_val: nan, pos_over_neg: 434.0675048828125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.8228, loss_val: nan, pos_over_neg: 1551.13818359375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.83, loss_val: nan, pos_over_neg: 2659.505126953125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.8233, loss_val: nan, pos_over_neg: 851.2523193359375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.8255, loss_val: nan, pos_over_neg: 2443.909423828125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.8359, loss_val: nan, pos_over_neg: 1176.2667236328125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.8244, loss_val: nan, pos_over_neg: 594.8306274414062 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.826, loss_val: nan, pos_over_neg: 1482.3603515625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.8255, loss_val: nan, pos_over_neg: 1845.4722900390625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.8252, loss_val: nan, pos_over_neg: 2288.36181640625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.8301, loss_val: nan, pos_over_neg: 3933.872314453125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.8311, loss_val: nan, pos_over_neg: 446.6517333984375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.8323, loss_val: nan, pos_over_neg: 2485.634765625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.8227, loss_val: nan, pos_over_neg: -3095.5537109375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.8248, loss_val: nan, pos_over_neg: 889.2216186523438 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.8307, loss_val: nan, pos_over_neg: 1870.1337890625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.8291, loss_val: nan, pos_over_neg: 10806.5283203125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.8213, loss_val: nan, pos_over_neg: 3284.762451171875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.8171, loss_val: nan, pos_over_neg: 6244.884765625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.8278, loss_val: nan, pos_over_neg: 2818.161376953125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.8246, loss_val: nan, pos_over_neg: -8416.8681640625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.8246, loss_val: nan, pos_over_neg: 1304.3673095703125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.8292, loss_val: nan, pos_over_neg: 2409.423828125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.8273, loss_val: nan, pos_over_neg: 1353.0628662109375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.8222, loss_val: nan, pos_over_neg: 2146.733154296875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.8185, loss_val: nan, pos_over_neg: 3983.131103515625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.8203, loss_val: nan, pos_over_neg: -4254.8271484375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.8187, loss_val: nan, pos_over_neg: -4729.45263671875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.8186, loss_val: nan, pos_over_neg: 5117.16748046875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.8236, loss_val: nan, pos_over_neg: 2137.944580078125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.8333, loss_val: nan, pos_over_neg: 566.3950805664062 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.8233, loss_val: nan, pos_over_neg: 674.589599609375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.8113, loss_val: nan, pos_over_neg: 1220.4935302734375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.8213, loss_val: nan, pos_over_neg: 7527.4111328125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.8221, loss_val: nan, pos_over_neg: -2814.7724609375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.8159, loss_val: nan, pos_over_neg: 926.858642578125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.8143, loss_val: nan, pos_over_neg: 1296.00439453125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.8179, loss_val: nan, pos_over_neg: -20364.62890625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.8094, loss_val: nan, pos_over_neg: 7151.21533203125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.8202, loss_val: nan, pos_over_neg: 980.8652954101562 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.8303, loss_val: nan, pos_over_neg: 1937.7601318359375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.81, loss_val: nan, pos_over_neg: -19941.998046875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.818, loss_val: nan, pos_over_neg: 1313.1640625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.8251, loss_val: nan, pos_over_neg: 367.5145263671875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.824, loss_val: nan, pos_over_neg: 492.3554382324219 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.8174, loss_val: nan, pos_over_neg: 1397.5025634765625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.8204, loss_val: nan, pos_over_neg: 2102.572265625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.8205, loss_val: nan, pos_over_neg: 10061.599609375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.8204, loss_val: nan, pos_over_neg: 1050.1822509765625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.8229, loss_val: nan, pos_over_neg: 4376.3115234375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.822, loss_val: nan, pos_over_neg: 465.1174621582031 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.817, loss_val: nan, pos_over_neg: 1184.13134765625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.8163, loss_val: nan, pos_over_neg: 98326.28125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.8132, loss_val: nan, pos_over_neg: -12346.4208984375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.8169, loss_val: nan, pos_over_neg: -7671.26611328125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.8146, loss_val: nan, pos_over_neg: 1953.2518310546875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.8156, loss_val: nan, pos_over_neg: 1395.862548828125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.8247, loss_val: nan, pos_over_neg: 677.095458984375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.8179, loss_val: nan, pos_over_neg: 1266.7420654296875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.82, loss_val: nan, pos_over_neg: 911.4739990234375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.818, loss_val: nan, pos_over_neg: 46047.75 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.8116, loss_val: nan, pos_over_neg: -5182.4775390625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.8223, loss_val: nan, pos_over_neg: -5720.49951171875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.8209, loss_val: nan, pos_over_neg: 9039.80078125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.8198, loss_val: nan, pos_over_neg: 699.3009643554688 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.8153, loss_val: nan, pos_over_neg: -15256.3662109375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.8184, loss_val: nan, pos_over_neg: 2312.548828125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.8217, loss_val: nan, pos_over_neg: 1399.757080078125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.8144, loss_val: nan, pos_over_neg: 3170.576171875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.8198, loss_val: nan, pos_over_neg: 1442.3570556640625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.8255, loss_val: nan, pos_over_neg: 6198.4697265625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.8167, loss_val: nan, pos_over_neg: 2664.970947265625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.8154, loss_val: nan, pos_over_neg: 4293.5595703125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.8135, loss_val: nan, pos_over_neg: -80657.2890625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.819, loss_val: nan, pos_over_neg: 2762.84912109375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.815, loss_val: nan, pos_over_neg: 2144.4912109375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.8147, loss_val: nan, pos_over_neg: 747.40625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.8085, loss_val: nan, pos_over_neg: 4812.4970703125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.8148, loss_val: nan, pos_over_neg: -3683.186279296875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.8129, loss_val: nan, pos_over_neg: 2609.27783203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.8116, loss_val: nan, pos_over_neg: -7328.2001953125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.8109, loss_val: nan, pos_over_neg: 1029.3126220703125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.8142, loss_val: nan, pos_over_neg: 911.0574951171875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.8141, loss_val: nan, pos_over_neg: 4111.7802734375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.8133, loss_val: nan, pos_over_neg: -3382.94580078125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.809, loss_val: nan, pos_over_neg: -4220.646484375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.8095, loss_val: nan, pos_over_neg: 1118.4556884765625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.8135, loss_val: nan, pos_over_neg: 8165.20849609375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.8106, loss_val: nan, pos_over_neg: 2418.22265625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.8083, loss_val: nan, pos_over_neg: 1230.9627685546875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.8208, loss_val: nan, pos_over_neg: 1986.98779296875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.8069, loss_val: nan, pos_over_neg: 2809.158203125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.8127, loss_val: nan, pos_over_neg: 5126.724609375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.822, loss_val: nan, pos_over_neg: 1515.2708740234375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.8144, loss_val: nan, pos_over_neg: 1354.5048828125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.8111, loss_val: nan, pos_over_neg: 6428.78564453125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.8146, loss_val: nan, pos_over_neg: 1658.0576171875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.8123, loss_val: nan, pos_over_neg: 2759.464111328125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.8178, loss_val: nan, pos_over_neg: 829.0051879882812 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.8143, loss_val: nan, pos_over_neg: 1519.1934814453125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.8076, loss_val: nan, pos_over_neg: 2222.4130859375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.8129, loss_val: nan, pos_over_neg: 3543.751953125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.8116, loss_val: nan, pos_over_neg: 1607.9443359375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.8172, loss_val: nan, pos_over_neg: 853.1133422851562 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.8146, loss_val: nan, pos_over_neg: 1799.2615966796875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.8161, loss_val: nan, pos_over_neg: -1481.6278076171875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.8124, loss_val: nan, pos_over_neg: -1957.0892333984375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.8181, loss_val: nan, pos_over_neg: -3466.270263671875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.8174, loss_val: nan, pos_over_neg: 3902.2744140625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.8109, loss_val: nan, pos_over_neg: 2372.1650390625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.8069, loss_val: nan, pos_over_neg: 1029.18359375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.8184, loss_val: nan, pos_over_neg: 1901.6617431640625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.8019, loss_val: nan, pos_over_neg: 4486.130859375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.8138, loss_val: nan, pos_over_neg: -4285.32568359375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.8179, loss_val: nan, pos_over_neg: 3912.230224609375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.8085, loss_val: nan, pos_over_neg: 1022.2138061523438 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.8103, loss_val: nan, pos_over_neg: 583.2945556640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.8099, loss_val: nan, pos_over_neg: 3480.61376953125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.8099, loss_val: nan, pos_over_neg: -4409.86181640625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.817, loss_val: nan, pos_over_neg: 4920.8720703125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.8103, loss_val: nan, pos_over_neg: 7330.1181640625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.8148, loss_val: nan, pos_over_neg: -179018.109375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.8137, loss_val: nan, pos_over_neg: 578.6898193359375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.8163, loss_val: nan, pos_over_neg: 465.7107238769531 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.8134, loss_val: nan, pos_over_neg: 1037.5234375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.8124, loss_val: nan, pos_over_neg: 722.0646362304688 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.811, loss_val: nan, pos_over_neg: 11843.2626953125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.8056, loss_val: nan, pos_over_neg: 13071.4150390625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.8143, loss_val: nan, pos_over_neg: 2460.50048828125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.8106, loss_val: nan, pos_over_neg: 873.1200561523438 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.8081, loss_val: nan, pos_over_neg: -88677.125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.8081, loss_val: nan, pos_over_neg: 3248.1123046875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.8054, loss_val: nan, pos_over_neg: 5769.74462890625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.8031, loss_val: nan, pos_over_neg: -7716.654296875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.8141, loss_val: nan, pos_over_neg: -6412.69970703125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.8137, loss_val: nan, pos_over_neg: 2128.454833984375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.807, loss_val: nan, pos_over_neg: 1610.67822265625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.8098, loss_val: nan, pos_over_neg: 1390.9730224609375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.8144, loss_val: nan, pos_over_neg: 19424.185546875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.8098, loss_val: nan, pos_over_neg: 2724.7666015625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.8103, loss_val: nan, pos_over_neg: 860.9157104492188 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.8191, loss_val: nan, pos_over_neg: 8416.66015625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.8108, loss_val: nan, pos_over_neg: 30232.23046875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.8117, loss_val: nan, pos_over_neg: 1886.3492431640625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.8043, loss_val: nan, pos_over_neg: -5632.5224609375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.8106, loss_val: nan, pos_over_neg: 1342.420166015625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.8127, loss_val: nan, pos_over_neg: 10011.078125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.799, loss_val: nan, pos_over_neg: 3593.157470703125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.8106, loss_val: nan, pos_over_neg: 911.962158203125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.8096, loss_val: nan, pos_over_neg: -17400.498046875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.8077, loss_val: nan, pos_over_neg: 17671.248046875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.8066, loss_val: nan, pos_over_neg: -3098.210693359375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.8156, loss_val: nan, pos_over_neg: 17853.447265625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.8119, loss_val: nan, pos_over_neg: 3041.4775390625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.8104, loss_val: nan, pos_over_neg: 1305.5438232421875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.8152, loss_val: nan, pos_over_neg: 602.3627319335938 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.8145, loss_val: nan, pos_over_neg: -1670.8624267578125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.8182, loss_val: nan, pos_over_neg: 858.0294189453125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.8062, loss_val: nan, pos_over_neg: 3065.171875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.8064, loss_val: nan, pos_over_neg: 1458.446533203125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.8017, loss_val: nan, pos_over_neg: 2032.0006103515625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.8092, loss_val: nan, pos_over_neg: 2913.117919921875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.8052, loss_val: nan, pos_over_neg: 2643.84765625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.8005, loss_val: nan, pos_over_neg: -1903.9818115234375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.8095, loss_val: nan, pos_over_neg: 3267.259521484375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.8114, loss_val: nan, pos_over_neg: 1827.73583984375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.8152, loss_val: nan, pos_over_neg: 570.4420776367188 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.8121, loss_val: nan, pos_over_neg: 616.5777587890625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.8061, loss_val: nan, pos_over_neg: 1780.0238037109375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.8136, loss_val: nan, pos_over_neg: -4667.65380859375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.8063, loss_val: nan, pos_over_neg: -2255.353759765625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.8069, loss_val: nan, pos_over_neg: -2369.1748046875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.8143, loss_val: nan, pos_over_neg: 1717.390869140625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.8046, loss_val: nan, pos_over_neg: 2113.96875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.8051, loss_val: nan, pos_over_neg: 801.7572021484375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7993, loss_val: nan, pos_over_neg: 886.5003662109375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.8078, loss_val: nan, pos_over_neg: 2274.3818359375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.8066, loss_val: nan, pos_over_neg: 15503.744140625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.8048, loss_val: nan, pos_over_neg: 1715.8839111328125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.8037, loss_val: nan, pos_over_neg: 3360.302490234375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.8073, loss_val: nan, pos_over_neg: 2225.372802734375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.808, loss_val: nan, pos_over_neg: 811.1259765625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.8082, loss_val: nan, pos_over_neg: 1957.87451171875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.8019, loss_val: nan, pos_over_neg: 923.719970703125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.8005, loss_val: nan, pos_over_neg: -2487.543212890625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.8063, loss_val: nan, pos_over_neg: 3698.448486328125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.8037, loss_val: nan, pos_over_neg: 2192.850830078125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.807, loss_val: nan, pos_over_neg: -5052.701171875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.8074, loss_val: nan, pos_over_neg: 3956.337646484375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.8136, loss_val: nan, pos_over_neg: 3196.504150390625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7984, loss_val: nan, pos_over_neg: -3780.412353515625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.8086, loss_val: nan, pos_over_neg: 11064.638671875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.8019, loss_val: nan, pos_over_neg: -4062.203125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.8046, loss_val: nan, pos_over_neg: -6449.6337890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.806, loss_val: nan, pos_over_neg: 1336.858154296875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.8019, loss_val: nan, pos_over_neg: 777.4996948242188 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.8054, loss_val: nan, pos_over_neg: 1843.6661376953125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.8146, loss_val: nan, pos_over_neg: 1726.26953125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.8064, loss_val: nan, pos_over_neg: 15063.6943359375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.8103, loss_val: nan, pos_over_neg: 3011.25244140625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.8091, loss_val: nan, pos_over_neg: 888.9376220703125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.8085, loss_val: nan, pos_over_neg: 692.4081420898438 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.8031, loss_val: nan, pos_over_neg: 3390.611328125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.8056, loss_val: nan, pos_over_neg: -9029.7021484375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.8128, loss_val: nan, pos_over_neg: -7170.09326171875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.8071, loss_val: nan, pos_over_neg: 14954.1630859375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.8011, loss_val: nan, pos_over_neg: 1435.727783203125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.8051, loss_val: nan, pos_over_neg: 1314.8367919921875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.8113, loss_val: nan, pos_over_neg: 1998.1065673828125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.8068, loss_val: nan, pos_over_neg: 2256.48046875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.8037, loss_val: nan, pos_over_neg: 1846.2840576171875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.8135, loss_val: nan, pos_over_neg: -3608.817626953125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.8098, loss_val: nan, pos_over_neg: 2296.16357421875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.8073, loss_val: nan, pos_over_neg: 1912.892333984375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.8034, loss_val: nan, pos_over_neg: 5604.31689453125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.807, loss_val: nan, pos_over_neg: 1851.0516357421875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.8031, loss_val: nan, pos_over_neg: -6021.0634765625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.8132, loss_val: nan, pos_over_neg: 1905.0517578125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.8053, loss_val: nan, pos_over_neg: 19271.009765625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.8043, loss_val: nan, pos_over_neg: 3721.911865234375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.8082, loss_val: nan, pos_over_neg: 2263.0947265625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.8075, loss_val: nan, pos_over_neg: 1307.44677734375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7992, loss_val: nan, pos_over_neg: -6825.83203125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.8026, loss_val: nan, pos_over_neg: 916.5908203125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.8009, loss_val: nan, pos_over_neg: 4126.65869140625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.8071, loss_val: nan, pos_over_neg: -9018.0986328125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.8067, loss_val: nan, pos_over_neg: -4444.0986328125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.8026, loss_val: nan, pos_over_neg: 5026.8564453125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.8059, loss_val: nan, pos_over_neg: 772.9072265625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.8071, loss_val: nan, pos_over_neg: 779.1153564453125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.8123, loss_val: nan, pos_over_neg: 908.9207153320312 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7976, loss_val: nan, pos_over_neg: 2969.313720703125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7994, loss_val: nan, pos_over_neg: 1289.4398193359375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.8036, loss_val: nan, pos_over_neg: 3725.39501953125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.8017, loss_val: nan, pos_over_neg: 1131.690185546875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.8068, loss_val: nan, pos_over_neg: 403.80682373046875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.8035, loss_val: nan, pos_over_neg: 1847.14013671875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.804, loss_val: nan, pos_over_neg: -2514.572265625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.8085, loss_val: nan, pos_over_neg: -6304.8896484375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.8032, loss_val: nan, pos_over_neg: -4308.64013671875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.8042, loss_val: nan, pos_over_neg: 4285.23974609375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7996, loss_val: nan, pos_over_neg: 950.35986328125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.8012, loss_val: nan, pos_over_neg: 899.69189453125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.804, loss_val: nan, pos_over_neg: -58834.57421875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.8069, loss_val: nan, pos_over_neg: -2063.67041015625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.8121, loss_val: nan, pos_over_neg: 1430.8216552734375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.8089, loss_val: nan, pos_over_neg: 3473.685546875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.8052, loss_val: nan, pos_over_neg: 1031.7745361328125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.8041, loss_val: nan, pos_over_neg: 450.817138671875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.8116, loss_val: nan, pos_over_neg: 475.2305908203125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.8042, loss_val: nan, pos_over_neg: 6185.17822265625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.8106, loss_val: nan, pos_over_neg: -53925.49609375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.8049, loss_val: nan, pos_over_neg: -2723.1123046875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.8027, loss_val: nan, pos_over_neg: 1856.1220703125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.8067, loss_val: nan, pos_over_neg: 2598.37255859375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.8035, loss_val: nan, pos_over_neg: 709.5321044921875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.8126, loss_val: nan, pos_over_neg: 6825.44482421875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.8018, loss_val: nan, pos_over_neg: 2568.308349609375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.8081, loss_val: nan, pos_over_neg: 1129.819580078125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7923, loss_val: nan, pos_over_neg: -1684.4859619140625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7998, loss_val: nan, pos_over_neg: 6400.8828125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.8059, loss_val: nan, pos_over_neg: 1385.457763671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.8029, loss_val: nan, pos_over_neg: -2252.556640625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.797, loss_val: nan, pos_over_neg: 1941.9765625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.8024, loss_val: nan, pos_over_neg: 1282.5330810546875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.8046, loss_val: nan, pos_over_neg: 6972.31640625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.8012, loss_val: nan, pos_over_neg: 5739.22216796875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7999, loss_val: nan, pos_over_neg: -4252.32421875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.8047, loss_val: nan, pos_over_neg: -4070.772216796875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.8002, loss_val: nan, pos_over_neg: -3292.371826171875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.8054, loss_val: nan, pos_over_neg: 457.3971252441406 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.8104, loss_val: nan, pos_over_neg: 3720.603515625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.8011, loss_val: nan, pos_over_neg: 3850.63330078125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.8092, loss_val: nan, pos_over_neg: 6882.42822265625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.8011, loss_val: nan, pos_over_neg: 1396.8616943359375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.803, loss_val: nan, pos_over_neg: 4427.39501953125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.8052, loss_val: nan, pos_over_neg: 6928.29345703125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.8071, loss_val: nan, pos_over_neg: 6589.0810546875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.8081, loss_val: nan, pos_over_neg: 4687.97021484375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.8078, loss_val: nan, pos_over_neg: -9220.498046875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7965, loss_val: nan, pos_over_neg: -4762.1953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.812, loss_val: nan, pos_over_neg: 1112.838623046875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.8006, loss_val: nan, pos_over_neg: 1215.098876953125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.8063, loss_val: nan, pos_over_neg: 1938.44091796875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.809, loss_val: nan, pos_over_neg: -5874.2353515625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.805, loss_val: nan, pos_over_neg: 8434.31640625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.803, loss_val: nan, pos_over_neg: -7434.18505859375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.8005, loss_val: nan, pos_over_neg: -3006.662841796875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.8009, loss_val: nan, pos_over_neg: 653.4457397460938 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.802, loss_val: nan, pos_over_neg: 1055.7919921875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7977, loss_val: nan, pos_over_neg: 724.4891357421875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.8035, loss_val: nan, pos_over_neg: -9664.5693359375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7997, loss_val: nan, pos_over_neg: 2980.30126953125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.8023, loss_val: nan, pos_over_neg: 2685.262939453125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7986, loss_val: nan, pos_over_neg: 2140.00830078125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.8038, loss_val: nan, pos_over_neg: 1707.665771484375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.8055, loss_val: nan, pos_over_neg: 583.7569580078125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.8013, loss_val: nan, pos_over_neg: 2613.957763671875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.8055, loss_val: nan, pos_over_neg: 1113.4718017578125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.801, loss_val: nan, pos_over_neg: 5439.31787109375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.8018, loss_val: nan, pos_over_neg: 74154.0390625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.8004, loss_val: nan, pos_over_neg: 2811.7197265625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7994, loss_val: nan, pos_over_neg: 3076.767822265625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7959, loss_val: nan, pos_over_neg: 1064.290771484375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.8002, loss_val: nan, pos_over_neg: 816.1598510742188 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.8049, loss_val: nan, pos_over_neg: 4170.67138671875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.8136, loss_val: nan, pos_over_neg: 32713.1953125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7994, loss_val: nan, pos_over_neg: 729.6322631835938 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.8071, loss_val: nan, pos_over_neg: 11247.3984375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.8014, loss_val: nan, pos_over_neg: 901.8961181640625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.8003, loss_val: nan, pos_over_neg: -5119.86474609375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7995, loss_val: nan, pos_over_neg: -9030.251953125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.8041, loss_val: nan, pos_over_neg: 26932.17578125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.8018, loss_val: nan, pos_over_neg: 1437.228515625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7979, loss_val: nan, pos_over_neg: 1032.0263671875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.812, loss_val: nan, pos_over_neg: 1214.705078125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.8007, loss_val: nan, pos_over_neg: 979.0122680664062 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.8033, loss_val: nan, pos_over_neg: 5121.27392578125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.8033, loss_val: nan, pos_over_neg: 841.9797973632812 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7995, loss_val: nan, pos_over_neg: -2482.144775390625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7997, loss_val: nan, pos_over_neg: 2848.867919921875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.8028, loss_val: nan, pos_over_neg: -12356.7890625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.8033, loss_val: nan, pos_over_neg: 1290.7047119140625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.7971, loss_val: nan, pos_over_neg: 937.1392822265625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.8042, loss_val: nan, pos_over_neg: 949.261474609375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.8004, loss_val: nan, pos_over_neg: 1261.5810546875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7953, loss_val: nan, pos_over_neg: 2767.5234375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.8024, loss_val: nan, pos_over_neg: -8024.88818359375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.799, loss_val: nan, pos_over_neg: -2936.12451171875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.8046, loss_val: nan, pos_over_neg: 969.570068359375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.8018, loss_val: nan, pos_over_neg: -17450.91015625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7958, loss_val: nan, pos_over_neg: -3742.72705078125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7985, loss_val: nan, pos_over_neg: 2801.086181640625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7959, loss_val: nan, pos_over_neg: 1644.3480224609375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.8024, loss_val: nan, pos_over_neg: 1484.9111328125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7967, loss_val: nan, pos_over_neg: 16243.8076171875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7965, loss_val: nan, pos_over_neg: 4413.3720703125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.8014, loss_val: nan, pos_over_neg: -34964.16015625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.8011, loss_val: nan, pos_over_neg: 12814.015625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.8001, loss_val: nan, pos_over_neg: 1824.8673095703125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7949, loss_val: nan, pos_over_neg: 3053.066162109375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7887, loss_val: nan, pos_over_neg: -4885.9755859375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7996, loss_val: nan, pos_over_neg: -3575.52294921875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7969, loss_val: nan, pos_over_neg: 7126.26708984375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7962, loss_val: nan, pos_over_neg: 46382.953125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7991, loss_val: nan, pos_over_neg: -8348.30078125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7996, loss_val: nan, pos_over_neg: 1492.925048828125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.799, loss_val: nan, pos_over_neg: 3709.692138671875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7916, loss_val: nan, pos_over_neg: 1531.031494140625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.8022, loss_val: nan, pos_over_neg: 4250.62841796875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.8053, loss_val: nan, pos_over_neg: 4664.78662109375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.8045, loss_val: nan, pos_over_neg: -5407.1337890625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7982, loss_val: nan, pos_over_neg: -3058.697509765625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7983, loss_val: nan, pos_over_neg: -15229.583984375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.8013, loss_val: nan, pos_over_neg: -7155.54052734375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7936, loss_val: nan, pos_over_neg: -9189.6943359375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.8043, loss_val: nan, pos_over_neg: 2538.826904296875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.8042, loss_val: nan, pos_over_neg: 1179.1181640625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7939, loss_val: nan, pos_over_neg: 6185.00048828125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.8003, loss_val: nan, pos_over_neg: 5340.6123046875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7998, loss_val: nan, pos_over_neg: 4691.70654296875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.8039, loss_val: nan, pos_over_neg: -13783.2841796875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7968, loss_val: nan, pos_over_neg: -43329.015625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7925, loss_val: nan, pos_over_neg: 775.8816528320312 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7999, loss_val: nan, pos_over_neg: 5526.75927734375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.8002, loss_val: nan, pos_over_neg: 1047.5291748046875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7973, loss_val: nan, pos_over_neg: 16327.5537109375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7955, loss_val: nan, pos_over_neg: -2323.72265625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7982, loss_val: nan, pos_over_neg: 853.1055297851562 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7951, loss_val: nan, pos_over_neg: 8538.560546875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7971, loss_val: nan, pos_over_neg: 6614.939453125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7981, loss_val: nan, pos_over_neg: 740.1780395507812 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7972, loss_val: nan, pos_over_neg: 10234.65625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7997, loss_val: nan, pos_over_neg: -3063.74560546875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7952, loss_val: nan, pos_over_neg: -63684.828125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7967, loss_val: nan, pos_over_neg: 1909.015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7983, loss_val: nan, pos_over_neg: -1660.0672607421875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7976, loss_val: nan, pos_over_neg: -113070.3046875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7942, loss_val: nan, pos_over_neg: -9979.7724609375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.8044, loss_val: nan, pos_over_neg: 2859.851806640625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.8001, loss_val: nan, pos_over_neg: 35757.74609375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7905, loss_val: nan, pos_over_neg: 5053.6767578125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.8017, loss_val: nan, pos_over_neg: -8465.8798828125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7958, loss_val: nan, pos_over_neg: -8231.0205078125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7997, loss_val: nan, pos_over_neg: 35396.671875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7951, loss_val: nan, pos_over_neg: -3541.335205078125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7928, loss_val: nan, pos_over_neg: 6398.52978515625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7921, loss_val: nan, pos_over_neg: -1359.1702880859375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.8008, loss_val: nan, pos_over_neg: 29148.20703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7921, loss_val: nan, pos_over_neg: 1109.9915771484375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7921, loss_val: nan, pos_over_neg: 78716.03125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7942, loss_val: nan, pos_over_neg: 2352.1748046875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7907, loss_val: nan, pos_over_neg: 2660.738525390625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7936, loss_val: nan, pos_over_neg: 1483.463623046875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.796, loss_val: nan, pos_over_neg: -4601.49609375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.8072, loss_val: nan, pos_over_neg: 3334.31494140625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7965, loss_val: nan, pos_over_neg: 544.3094482421875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.7974, loss_val: nan, pos_over_neg: 854.9016723632812 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.793, loss_val: nan, pos_over_neg: 1617.62353515625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7983, loss_val: nan, pos_over_neg: 7739.41748046875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7972, loss_val: nan, pos_over_neg: -1659.6458740234375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7933, loss_val: nan, pos_over_neg: 7075.23779296875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.8072, loss_val: nan, pos_over_neg: 8528.0087890625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.794, loss_val: nan, pos_over_neg: -4940.4697265625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7976, loss_val: nan, pos_over_neg: 23141.259765625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7986, loss_val: nan, pos_over_neg: 1369.551513671875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.8011, loss_val: nan, pos_over_neg: 1375.0174560546875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7962, loss_val: nan, pos_over_neg: 6107.904296875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.794, loss_val: nan, pos_over_neg: 1905.234619140625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.8019, loss_val: nan, pos_over_neg: 20967.232421875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7917, loss_val: nan, pos_over_neg: 20702.275390625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7951, loss_val: nan, pos_over_neg: -1913.953857421875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7965, loss_val: nan, pos_over_neg: -166949.828125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7938, loss_val: nan, pos_over_neg: -5088.51318359375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.8058, loss_val: nan, pos_over_neg: 1530.06640625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7946, loss_val: nan, pos_over_neg: 4037.931884765625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7988, loss_val: nan, pos_over_neg: 12197.8798828125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7953, loss_val: nan, pos_over_neg: -2415.4072265625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7975, loss_val: nan, pos_over_neg: 3124.864501953125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7994, loss_val: nan, pos_over_neg: -4594.2548828125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7952, loss_val: nan, pos_over_neg: -2801.12548828125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7959, loss_val: nan, pos_over_neg: 20383.201171875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.8008, loss_val: nan, pos_over_neg: -6343.02978515625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7927, loss_val: nan, pos_over_neg: -9524.763671875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7959, loss_val: nan, pos_over_neg: 4387.94580078125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7965, loss_val: nan, pos_over_neg: 1544.964599609375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7949, loss_val: nan, pos_over_neg: 5719.251953125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7921, loss_val: nan, pos_over_neg: -4268.2890625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7925, loss_val: nan, pos_over_neg: -1345.0206298828125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7966, loss_val: nan, pos_over_neg: -2655.3212890625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7957, loss_val: nan, pos_over_neg: 3888.11474609375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.8005, loss_val: nan, pos_over_neg: 807.845703125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7956, loss_val: nan, pos_over_neg: 1423.799072265625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7949, loss_val: nan, pos_over_neg: 1366.8741455078125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7957, loss_val: nan, pos_over_neg: -2898.16552734375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7946, loss_val: nan, pos_over_neg: 22608.046875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.793, loss_val: nan, pos_over_neg: -1483.6083984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7955, loss_val: nan, pos_over_neg: 2389.35009765625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7952, loss_val: nan, pos_over_neg: 599.92431640625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7966, loss_val: nan, pos_over_neg: 732.7495727539062 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7976, loss_val: nan, pos_over_neg: 1430.6683349609375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.798, loss_val: nan, pos_over_neg: 1066.945068359375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7963, loss_val: nan, pos_over_neg: -1820.8475341796875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7945, loss_val: nan, pos_over_neg: -1945.8990478515625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7963, loss_val: nan, pos_over_neg: -3568.552734375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7949, loss_val: nan, pos_over_neg: 1128.6907958984375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7917, loss_val: nan, pos_over_neg: 651.8897705078125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7926, loss_val: nan, pos_over_neg: 3347.342529296875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7918, loss_val: nan, pos_over_neg: -3805.255615234375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7942, loss_val: nan, pos_over_neg: -1605.9261474609375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7901, loss_val: nan, pos_over_neg: -6767.55224609375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7923, loss_val: nan, pos_over_neg: 8219.1435546875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.8, loss_val: nan, pos_over_neg: 892.629150390625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7925, loss_val: nan, pos_over_neg: 3765.7939453125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7958, loss_val: nan, pos_over_neg: 2428.86279296875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7905, loss_val: nan, pos_over_neg: -2414.065673828125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.792, loss_val: nan, pos_over_neg: 2615.242431640625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7907, loss_val: nan, pos_over_neg: -43972.078125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7891, loss_val: nan, pos_over_neg: 2368.644287109375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7904, loss_val: nan, pos_over_neg: 2428.029296875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7892, loss_val: nan, pos_over_neg: 2458.40771484375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7963, loss_val: nan, pos_over_neg: -1500.3709716796875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7856, loss_val: nan, pos_over_neg: 1970.8299560546875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7964, loss_val: nan, pos_over_neg: 9610.181640625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7914, loss_val: nan, pos_over_neg: 866.4075317382812 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7933, loss_val: nan, pos_over_neg: 1043.2686767578125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7973, loss_val: nan, pos_over_neg: 3115.290771484375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7966, loss_val: nan, pos_over_neg: -5290.4501953125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7863, loss_val: nan, pos_over_neg: -7582.68994140625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7871, loss_val: nan, pos_over_neg: -2599.072998046875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7921, loss_val: nan, pos_over_neg: 2210.8095703125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7972, loss_val: nan, pos_over_neg: 3228.018310546875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7911, loss_val: nan, pos_over_neg: 24205.421875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7974, loss_val: nan, pos_over_neg: -3394.9921875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7863, loss_val: nan, pos_over_neg: 2617.024658203125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7986, loss_val: nan, pos_over_neg: 1647.85205078125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7973, loss_val: nan, pos_over_neg: 4584.77587890625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7981, loss_val: nan, pos_over_neg: 1427.3153076171875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7892, loss_val: nan, pos_over_neg: 1031.05517578125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7965, loss_val: nan, pos_over_neg: 2771.088134765625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7958, loss_val: nan, pos_over_neg: -9345.33203125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7891, loss_val: nan, pos_over_neg: 3653.994140625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7927, loss_val: nan, pos_over_neg: 7977.111328125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7902, loss_val: nan, pos_over_neg: -26377.986328125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7908, loss_val: nan, pos_over_neg: 6962.205078125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7927, loss_val: nan, pos_over_neg: 1446.6512451171875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7923, loss_val: nan, pos_over_neg: 3374.642333984375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7903, loss_val: nan, pos_over_neg: 3608.677001953125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7881, loss_val: nan, pos_over_neg: -3651.7177734375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7922, loss_val: nan, pos_over_neg: 1644.668701171875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7895, loss_val: nan, pos_over_neg: -2668.67822265625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7876, loss_val: nan, pos_over_neg: 47235.23828125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7901, loss_val: nan, pos_over_neg: 2762.999267578125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7982, loss_val: nan, pos_over_neg: 721.0763549804688 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.791, loss_val: nan, pos_over_neg: 2080.614013671875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7953, loss_val: nan, pos_over_neg: -2122.855224609375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.793, loss_val: nan, pos_over_neg: -8542.2783203125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7913, loss_val: nan, pos_over_neg: 3155.1787109375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7932, loss_val: nan, pos_over_neg: -28830.6171875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.792, loss_val: nan, pos_over_neg: -4840.15869140625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7926, loss_val: nan, pos_over_neg: -8559.166015625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.7885, loss_val: nan, pos_over_neg: 6598.7705078125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7954, loss_val: nan, pos_over_neg: 3005.4033203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7882, loss_val: nan, pos_over_neg: -2491.106201171875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7985, loss_val: nan, pos_over_neg: 4545.734375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7944, loss_val: nan, pos_over_neg: 1495.7764892578125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7923, loss_val: nan, pos_over_neg: 5621.609375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7901, loss_val: nan, pos_over_neg: 2696.313720703125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7947, loss_val: nan, pos_over_neg: 21568.0234375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7994, loss_val: nan, pos_over_neg: 37149.48828125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7915, loss_val: nan, pos_over_neg: 1383.471923828125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7974, loss_val: nan, pos_over_neg: -2002.0382080078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7922, loss_val: nan, pos_over_neg: 2771.1357421875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.792, loss_val: nan, pos_over_neg: 1814.8260498046875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7905, loss_val: nan, pos_over_neg: 2951.34619140625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7897, loss_val: nan, pos_over_neg: 5372.84130859375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.7943, loss_val: nan, pos_over_neg: 1937.5118408203125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7918, loss_val: nan, pos_over_neg: -6901.845703125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7915, loss_val: nan, pos_over_neg: -19466.169921875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7953, loss_val: nan, pos_over_neg: 13124.298828125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.8031, loss_val: nan, pos_over_neg: 3367.11474609375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.793, loss_val: nan, pos_over_neg: 20583.99609375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7989, loss_val: nan, pos_over_neg: 894.1738891601562 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7897, loss_val: nan, pos_over_neg: 723.1538696289062 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7855, loss_val: nan, pos_over_neg: -5013.77294921875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.791, loss_val: nan, pos_over_neg: -2306.322021484375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7948, loss_val: nan, pos_over_neg: 1482.3697509765625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.7958, loss_val: nan, pos_over_neg: -12040.712890625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7906, loss_val: nan, pos_over_neg: 2388.13037109375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7881, loss_val: nan, pos_over_neg: 891.7206420898438 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7916, loss_val: nan, pos_over_neg: 23772.876953125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.7902, loss_val: nan, pos_over_neg: -2245.815185546875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7908, loss_val: nan, pos_over_neg: -16281.3134765625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7986, loss_val: nan, pos_over_neg: -3861.955810546875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [02:55<14638:30:10, 175.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 6.7954, loss_val: nan, pos_over_neg: 1917.4056396484375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7835, loss_val: nan, pos_over_neg: 3231.62548828125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7862, loss_val: nan, pos_over_neg: 4205.1435546875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7967, loss_val: nan, pos_over_neg: 8296.3447265625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7916, loss_val: nan, pos_over_neg: -2265.168212890625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7869, loss_val: nan, pos_over_neg: -2702.813232421875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7949, loss_val: nan, pos_over_neg: 499633.71875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7962, loss_val: nan, pos_over_neg: 2002.0390625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7824, loss_val: nan, pos_over_neg: 2676.0166015625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.79, loss_val: nan, pos_over_neg: -4110.916015625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7862, loss_val: nan, pos_over_neg: -2261.782958984375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7966, loss_val: nan, pos_over_neg: -8450.4052734375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.7925, loss_val: nan, pos_over_neg: -3802.3544921875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7912, loss_val: nan, pos_over_neg: -23629.05078125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7958, loss_val: nan, pos_over_neg: 1239.7552490234375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7869, loss_val: nan, pos_over_neg: 2329.11962890625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: -1724.1949462890625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7899, loss_val: nan, pos_over_neg: 1345.3074951171875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7915, loss_val: nan, pos_over_neg: -3040.828857421875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7906, loss_val: nan, pos_over_neg: -3261.311767578125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7989, loss_val: nan, pos_over_neg: 7412.64208984375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7955, loss_val: nan, pos_over_neg: -2208.17822265625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7908, loss_val: nan, pos_over_neg: 3054.2685546875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7894, loss_val: nan, pos_over_neg: 847.1805419921875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7932, loss_val: nan, pos_over_neg: 1797.3231201171875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7841, loss_val: nan, pos_over_neg: -2477.44775390625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7933, loss_val: nan, pos_over_neg: -9318.98046875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7868, loss_val: nan, pos_over_neg: -2972.248779296875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7885, loss_val: nan, pos_over_neg: -2239.003662109375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7913, loss_val: nan, pos_over_neg: -4654.39111328125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7863, loss_val: nan, pos_over_neg: 1856.8250732421875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7883, loss_val: nan, pos_over_neg: -5034.24267578125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7944, loss_val: nan, pos_over_neg: 4252.70849609375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7931, loss_val: nan, pos_over_neg: 3366.054931640625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.7911, loss_val: nan, pos_over_neg: -6007.603515625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7927, loss_val: nan, pos_over_neg: -2618.726806640625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7796, loss_val: nan, pos_over_neg: -11543.107421875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7887, loss_val: nan, pos_over_neg: -2655.0771484375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.784, loss_val: nan, pos_over_neg: -2732.515869140625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7956, loss_val: nan, pos_over_neg: 1908.57421875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7928, loss_val: nan, pos_over_neg: 2211.613037109375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7884, loss_val: nan, pos_over_neg: 4725.4736328125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7863, loss_val: nan, pos_over_neg: -5581.43896484375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7916, loss_val: nan, pos_over_neg: 1596.362060546875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7872, loss_val: nan, pos_over_neg: 1259.92041015625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7888, loss_val: nan, pos_over_neg: -2242.119873046875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: 5161854.0 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7866, loss_val: nan, pos_over_neg: -17419.552734375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7931, loss_val: nan, pos_over_neg: 4023.9609375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7886, loss_val: nan, pos_over_neg: -2126.15625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.7905, loss_val: nan, pos_over_neg: 1763.46875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7881, loss_val: nan, pos_over_neg: 24738.6171875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7891, loss_val: nan, pos_over_neg: 3456.8662109375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: 3033.63427734375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7938, loss_val: nan, pos_over_neg: -2846.37841796875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.784, loss_val: nan, pos_over_neg: -1411.5191650390625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7945, loss_val: nan, pos_over_neg: -1592.82470703125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7954, loss_val: nan, pos_over_neg: 7602.05078125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7944, loss_val: nan, pos_over_neg: 2769.167724609375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7911, loss_val: nan, pos_over_neg: 971.6814575195312 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7916, loss_val: nan, pos_over_neg: 1871.7156982421875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7879, loss_val: nan, pos_over_neg: -2530.24755859375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7938, loss_val: nan, pos_over_neg: -2421.816162109375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7936, loss_val: nan, pos_over_neg: -5569.00830078125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7967, loss_val: nan, pos_over_neg: 2728.864013671875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7873, loss_val: nan, pos_over_neg: 109187.234375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7887, loss_val: nan, pos_over_neg: 1322.56396484375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7865, loss_val: nan, pos_over_neg: 24701.419921875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7933, loss_val: nan, pos_over_neg: 7425.693359375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7897, loss_val: nan, pos_over_neg: -2674.49072265625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7909, loss_val: nan, pos_over_neg: 1158.6578369140625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7908, loss_val: nan, pos_over_neg: -4752.791015625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7869, loss_val: nan, pos_over_neg: 2760.29931640625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.785, loss_val: nan, pos_over_neg: 1273.072021484375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7911, loss_val: nan, pos_over_neg: 11059.365234375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.783, loss_val: nan, pos_over_neg: -51269.16796875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7941, loss_val: nan, pos_over_neg: 2758.068359375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: -4854.9208984375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7914, loss_val: nan, pos_over_neg: 6203.8466796875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7917, loss_val: nan, pos_over_neg: -3223.9580078125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7835, loss_val: nan, pos_over_neg: 2354.829833984375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7891, loss_val: nan, pos_over_neg: 3883.10693359375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7867, loss_val: nan, pos_over_neg: -24699.451171875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7886, loss_val: nan, pos_over_neg: 165872.28125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7844, loss_val: nan, pos_over_neg: 2864.4248046875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7968, loss_val: nan, pos_over_neg: -4368.6904296875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7943, loss_val: nan, pos_over_neg: 8425.052734375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.793, loss_val: nan, pos_over_neg: 1286.69921875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7874, loss_val: nan, pos_over_neg: 7750.03515625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7897, loss_val: nan, pos_over_neg: 3014.564208984375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7863, loss_val: nan, pos_over_neg: -1830.79736328125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7931, loss_val: nan, pos_over_neg: 2689.721435546875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.7862, loss_val: nan, pos_over_neg: -15945.50390625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.797, loss_val: nan, pos_over_neg: 5742.3798828125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7866, loss_val: nan, pos_over_neg: 2774.15576171875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7943, loss_val: nan, pos_over_neg: 4397.18115234375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.785, loss_val: nan, pos_over_neg: -2114.73779296875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7999, loss_val: nan, pos_over_neg: 49012.5 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7864, loss_val: nan, pos_over_neg: 5314.5859375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7909, loss_val: nan, pos_over_neg: 40743.16015625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7932, loss_val: nan, pos_over_neg: -4013.945556640625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7873, loss_val: nan, pos_over_neg: 2855.896484375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7951, loss_val: nan, pos_over_neg: -2235.170654296875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7842, loss_val: nan, pos_over_neg: 11082.0830078125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.784, loss_val: nan, pos_over_neg: -2519.786865234375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7853, loss_val: nan, pos_over_neg: 4425.05615234375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.8002, loss_val: nan, pos_over_neg: 11312.7822265625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7883, loss_val: nan, pos_over_neg: 3271.673828125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7895, loss_val: nan, pos_over_neg: -5006.95166015625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7882, loss_val: nan, pos_over_neg: -2312.615234375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7898, loss_val: nan, pos_over_neg: -3477.465576171875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7819, loss_val: nan, pos_over_neg: -1997.4876708984375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7889, loss_val: nan, pos_over_neg: -17047.1953125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7838, loss_val: nan, pos_over_neg: -3576.803466796875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.791, loss_val: nan, pos_over_neg: 3596.34619140625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7882, loss_val: nan, pos_over_neg: 2351.557861328125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7916, loss_val: nan, pos_over_neg: 1751.1500244140625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7881, loss_val: nan, pos_over_neg: -3178.408447265625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7879, loss_val: nan, pos_over_neg: -25552.859375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7879, loss_val: nan, pos_over_neg: -5050.775390625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7871, loss_val: nan, pos_over_neg: 12788.1337890625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7911, loss_val: nan, pos_over_neg: -20743.2578125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.787, loss_val: nan, pos_over_neg: 1905.8538818359375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7862, loss_val: nan, pos_over_neg: -4249.7529296875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7913, loss_val: nan, pos_over_neg: -5722.1005859375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7881, loss_val: nan, pos_over_neg: -1868.570068359375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7896, loss_val: nan, pos_over_neg: -7711.83935546875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7843, loss_val: nan, pos_over_neg: -3359.9189453125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: 1237.763916015625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7868, loss_val: nan, pos_over_neg: 3357.989013671875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7846, loss_val: nan, pos_over_neg: -7453.9521484375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7935, loss_val: nan, pos_over_neg: 4451.97021484375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7887, loss_val: nan, pos_over_neg: 19900.185546875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.785, loss_val: nan, pos_over_neg: 2285.334228515625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: 4547.00244140625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.7856, loss_val: nan, pos_over_neg: -7227.08349609375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7863, loss_val: nan, pos_over_neg: 11581.201171875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7896, loss_val: nan, pos_over_neg: -2705.292236328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.791, loss_val: nan, pos_over_neg: -10287.6708984375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7855, loss_val: nan, pos_over_neg: 5399.22119140625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7854, loss_val: nan, pos_over_neg: -16410.580078125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.79, loss_val: nan, pos_over_neg: 2552.4931640625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7824, loss_val: nan, pos_over_neg: 40678.51171875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7858, loss_val: nan, pos_over_neg: 135344.5625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7865, loss_val: nan, pos_over_neg: -3182.10546875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7814, loss_val: nan, pos_over_neg: 12673.09375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.787, loss_val: nan, pos_over_neg: -15752.740234375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.789, loss_val: nan, pos_over_neg: -1516.15966796875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.783, loss_val: nan, pos_over_neg: 11877.353515625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7933, loss_val: nan, pos_over_neg: -4397.5615234375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7831, loss_val: nan, pos_over_neg: 1322.7430419921875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7909, loss_val: nan, pos_over_neg: -8156.67138671875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7828, loss_val: nan, pos_over_neg: 4006.9912109375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7883, loss_val: nan, pos_over_neg: 50844.578125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7876, loss_val: nan, pos_over_neg: 3595.7119140625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: -1832.7294921875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7893, loss_val: nan, pos_over_neg: -4017.436279296875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7828, loss_val: nan, pos_over_neg: -2432.524658203125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7926, loss_val: nan, pos_over_neg: -4573.234375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7833, loss_val: nan, pos_over_neg: -2476.867431640625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7845, loss_val: nan, pos_over_neg: 5713.251953125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7884, loss_val: nan, pos_over_neg: 15503.669921875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.785, loss_val: nan, pos_over_neg: -1901.9339599609375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: -2670.04736328125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.795, loss_val: nan, pos_over_neg: -2338.35400390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7901, loss_val: nan, pos_over_neg: 3537.302001953125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7847, loss_val: nan, pos_over_neg: -7465.2607421875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7901, loss_val: nan, pos_over_neg: -257832.421875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7923, loss_val: nan, pos_over_neg: 1786.10986328125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7844, loss_val: nan, pos_over_neg: -5483.751953125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.787, loss_val: nan, pos_over_neg: -3807.51904296875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7884, loss_val: nan, pos_over_neg: 3811.585693359375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.7812, loss_val: nan, pos_over_neg: -2441.32470703125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.784, loss_val: nan, pos_over_neg: -3744.33056640625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7805, loss_val: nan, pos_over_neg: -10761.302734375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7836, loss_val: nan, pos_over_neg: -1558.2379150390625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7876, loss_val: nan, pos_over_neg: -2099.8193359375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -15259.02734375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.789, loss_val: nan, pos_over_neg: 2935.18798828125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7837, loss_val: nan, pos_over_neg: 1743.0283203125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7845, loss_val: nan, pos_over_neg: 6916.86181640625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7824, loss_val: nan, pos_over_neg: -8311.041015625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: 41231.59375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7853, loss_val: nan, pos_over_neg: 6300.576171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7918, loss_val: nan, pos_over_neg: 7945.046875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7816, loss_val: nan, pos_over_neg: -3971.140869140625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7895, loss_val: nan, pos_over_neg: 1080.649169921875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.784, loss_val: nan, pos_over_neg: 687.0363159179688 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7832, loss_val: nan, pos_over_neg: 25508.951171875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7891, loss_val: nan, pos_over_neg: 9851.5556640625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.786, loss_val: nan, pos_over_neg: 1249.988037109375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7848, loss_val: nan, pos_over_neg: -1290.2691650390625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7899, loss_val: nan, pos_over_neg: 3614.065185546875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.795, loss_val: nan, pos_over_neg: 356.6009216308594 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7807, loss_val: nan, pos_over_neg: 875.9859619140625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7887, loss_val: nan, pos_over_neg: 794.2250366210938 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.785, loss_val: nan, pos_over_neg: 1013.9736938476562 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7908, loss_val: nan, pos_over_neg: 11251.134765625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7892, loss_val: nan, pos_over_neg: 2301.912353515625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.7867, loss_val: nan, pos_over_neg: 645.8067016601562 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7914, loss_val: nan, pos_over_neg: 680.111328125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: 2889.865234375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.783, loss_val: nan, pos_over_neg: -11971.341796875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7837, loss_val: nan, pos_over_neg: 7825.85498046875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: 2761.515380859375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7887, loss_val: nan, pos_over_neg: 1071.34228515625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7865, loss_val: nan, pos_over_neg: 1297.554931640625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.787, loss_val: nan, pos_over_neg: 1187.66064453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: -6701.6337890625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7909, loss_val: nan, pos_over_neg: 1425.6273193359375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7847, loss_val: nan, pos_over_neg: -8790.7998046875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7852, loss_val: nan, pos_over_neg: 7608.97412109375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7864, loss_val: nan, pos_over_neg: 976.0403442382812 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7864, loss_val: nan, pos_over_neg: -94725.46875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7838, loss_val: nan, pos_over_neg: 4709.83935546875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7883, loss_val: nan, pos_over_neg: -72376.6796875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7865, loss_val: nan, pos_over_neg: -2812.11181640625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7837, loss_val: nan, pos_over_neg: 5786.32421875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7894, loss_val: nan, pos_over_neg: 3651.478515625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.788, loss_val: nan, pos_over_neg: 2089.7958984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7863, loss_val: nan, pos_over_neg: 10089.2333984375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7879, loss_val: nan, pos_over_neg: -2296.279052734375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7897, loss_val: nan, pos_over_neg: -22672.5625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7903, loss_val: nan, pos_over_neg: 1003.4093017578125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7886, loss_val: nan, pos_over_neg: 4136.22509765625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7825, loss_val: nan, pos_over_neg: 1068.1024169921875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7796, loss_val: nan, pos_over_neg: 5273.62255859375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: -3262.861328125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7897, loss_val: nan, pos_over_neg: -8938.8984375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7803, loss_val: nan, pos_over_neg: 122445.2421875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7886, loss_val: nan, pos_over_neg: -3725.27490234375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7787, loss_val: nan, pos_over_neg: 2587.868896484375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7857, loss_val: nan, pos_over_neg: 19490.34765625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7883, loss_val: nan, pos_over_neg: 3296.87646484375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7864, loss_val: nan, pos_over_neg: -3296.714111328125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7877, loss_val: nan, pos_over_neg: 23823.146484375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7901, loss_val: nan, pos_over_neg: -10172.8720703125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: 12198.9990234375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: 3464.09375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7852, loss_val: nan, pos_over_neg: -20037.09765625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7839, loss_val: nan, pos_over_neg: -15349.3291015625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.782, loss_val: nan, pos_over_neg: 13796.85546875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: -3196.64697265625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.784, loss_val: nan, pos_over_neg: 30777.30859375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: 7738.5283203125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7849, loss_val: nan, pos_over_neg: 3884.447021484375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: -2559.27880859375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7828, loss_val: nan, pos_over_neg: -2158.364990234375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7907, loss_val: nan, pos_over_neg: 6762.63720703125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7859, loss_val: nan, pos_over_neg: -2838.01904296875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.781, loss_val: nan, pos_over_neg: 2507.398193359375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7906, loss_val: nan, pos_over_neg: 1062.101806640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: 4932.5361328125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7825, loss_val: nan, pos_over_neg: 2173.131591796875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7831, loss_val: nan, pos_over_neg: -3056.5234375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7902, loss_val: nan, pos_over_neg: 3246.16064453125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7827, loss_val: nan, pos_over_neg: 3568.6767578125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7835, loss_val: nan, pos_over_neg: -17529.953125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.784, loss_val: nan, pos_over_neg: -9115.4658203125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7876, loss_val: nan, pos_over_neg: -3965.060791015625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7829, loss_val: nan, pos_over_neg: 16593.974609375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: -3155.032958984375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.787, loss_val: nan, pos_over_neg: -4424.68701171875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7817, loss_val: nan, pos_over_neg: -2086.939208984375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.78, loss_val: nan, pos_over_neg: -2807.753662109375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7888, loss_val: nan, pos_over_neg: 678.822265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7822, loss_val: nan, pos_over_neg: 1001.6674194335938 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7867, loss_val: nan, pos_over_neg: -2967.22998046875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.784, loss_val: nan, pos_over_neg: -2096.84765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.7886, loss_val: nan, pos_over_neg: 1851.103759765625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: -8211.7373046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.78, loss_val: nan, pos_over_neg: 3181.748046875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7848, loss_val: nan, pos_over_neg: 3165.39794921875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7878, loss_val: nan, pos_over_neg: 1460.012939453125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7808, loss_val: nan, pos_over_neg: 3214.435791015625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7869, loss_val: nan, pos_over_neg: 5336.79345703125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7807, loss_val: nan, pos_over_neg: -2172.216796875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7813, loss_val: nan, pos_over_neg: 4393.26318359375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7831, loss_val: nan, pos_over_neg: -20610.173828125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.782, loss_val: nan, pos_over_neg: -5514.85302734375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7835, loss_val: nan, pos_over_neg: -18429.072265625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: 979.6799926757812 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7866, loss_val: nan, pos_over_neg: 30755.685546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7854, loss_val: nan, pos_over_neg: 2643.240234375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: -622322.0 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: 2971.356201171875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7824, loss_val: nan, pos_over_neg: -7036.978515625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7812, loss_val: nan, pos_over_neg: -7443.68310546875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.7835, loss_val: nan, pos_over_neg: 3509.67724609375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.785, loss_val: nan, pos_over_neg: -51400.84375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7798, loss_val: nan, pos_over_neg: -21677.69921875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7858, loss_val: nan, pos_over_neg: -13057.853515625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7779, loss_val: nan, pos_over_neg: 2754.34130859375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7792, loss_val: nan, pos_over_neg: -2088.75634765625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7854, loss_val: nan, pos_over_neg: -1703.406005859375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7864, loss_val: nan, pos_over_neg: 5117.71533203125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: -13483.6796875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7811, loss_val: nan, pos_over_neg: -4064.818359375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7821, loss_val: nan, pos_over_neg: -2087.787109375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -1389.7564697265625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7794, loss_val: nan, pos_over_neg: -1895.9307861328125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.783, loss_val: nan, pos_over_neg: 4084.83544921875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7874, loss_val: nan, pos_over_neg: -3733.31591796875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7827, loss_val: nan, pos_over_neg: -3631.564453125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7882, loss_val: nan, pos_over_neg: 3380.1689453125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7811, loss_val: nan, pos_over_neg: 2828.708740234375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7803, loss_val: nan, pos_over_neg: -3349.3603515625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7869, loss_val: nan, pos_over_neg: -2128.506103515625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: 48466.79296875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7832, loss_val: nan, pos_over_neg: -5935.39453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: -7573.69775390625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.781, loss_val: nan, pos_over_neg: 2192.433349609375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7886, loss_val: nan, pos_over_neg: 1304.7420654296875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7925, loss_val: nan, pos_over_neg: 1107.606201171875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7811, loss_val: nan, pos_over_neg: 1610.306640625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7816, loss_val: nan, pos_over_neg: -1389.90771484375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7831, loss_val: nan, pos_over_neg: 8363.259765625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7853, loss_val: nan, pos_over_neg: 1855.7943115234375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7807, loss_val: nan, pos_over_neg: -1617.49609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7833, loss_val: nan, pos_over_neg: 1856.053955078125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7734, loss_val: nan, pos_over_neg: 1338.0391845703125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.7814, loss_val: nan, pos_over_neg: 7818.4658203125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7894, loss_val: nan, pos_over_neg: -2804.68408203125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7871, loss_val: nan, pos_over_neg: -13884.224609375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7824, loss_val: nan, pos_over_neg: -1765.2093505859375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.779, loss_val: nan, pos_over_neg: 7674.29296875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7874, loss_val: nan, pos_over_neg: 1148.2413330078125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7828, loss_val: nan, pos_over_neg: 1675.5028076171875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7839, loss_val: nan, pos_over_neg: 3130.0380859375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.783, loss_val: nan, pos_over_neg: 3061.75439453125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7815, loss_val: nan, pos_over_neg: 2128.005126953125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7839, loss_val: nan, pos_over_neg: 2039.645751953125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7843, loss_val: nan, pos_over_neg: 993.7260131835938 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7882, loss_val: nan, pos_over_neg: 14396.6201171875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7785, loss_val: nan, pos_over_neg: 2100.635498046875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7795, loss_val: nan, pos_over_neg: 7018.1689453125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7812, loss_val: nan, pos_over_neg: -57673.11328125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: 1257.426025390625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.7793, loss_val: nan, pos_over_neg: 729.95751953125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7839, loss_val: nan, pos_over_neg: 1069685.5 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7832, loss_val: nan, pos_over_neg: -1597.217041015625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7807, loss_val: nan, pos_over_neg: -2682.23388671875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7815, loss_val: nan, pos_over_neg: -5949.2001953125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: 2804.62646484375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: -10242.2705078125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7874, loss_val: nan, pos_over_neg: 977.4070434570312 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7853, loss_val: nan, pos_over_neg: 52739.72265625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7894, loss_val: nan, pos_over_neg: -4176.951171875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.781, loss_val: nan, pos_over_neg: -4127.1328125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.7825, loss_val: nan, pos_over_neg: 2276.007080078125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7796, loss_val: nan, pos_over_neg: 11615.8662109375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7792, loss_val: nan, pos_over_neg: 2903.74609375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7819, loss_val: nan, pos_over_neg: 7680.03125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: -8595.8740234375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7833, loss_val: nan, pos_over_neg: -6588.58447265625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7813, loss_val: nan, pos_over_neg: -2018.5074462890625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7849, loss_val: nan, pos_over_neg: 4559.44873046875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.781, loss_val: nan, pos_over_neg: -27399.908203125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7795, loss_val: nan, pos_over_neg: -9382.5029296875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.777, loss_val: nan, pos_over_neg: -5523.59423828125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: -45224.37109375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.785, loss_val: nan, pos_over_neg: -92314.3359375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: -2266.439453125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: -2700.886962890625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: 59972.71875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7772, loss_val: nan, pos_over_neg: -4838.8349609375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: -3006.923095703125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7881, loss_val: nan, pos_over_neg: 6344.6474609375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7785, loss_val: nan, pos_over_neg: 65541.1015625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7812, loss_val: nan, pos_over_neg: -1993.91455078125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7772, loss_val: nan, pos_over_neg: 28853.451171875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.7807, loss_val: nan, pos_over_neg: -2866.89599609375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: -1717.6768798828125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7801, loss_val: nan, pos_over_neg: -13000.7001953125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: -2180.6572265625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7849, loss_val: nan, pos_over_neg: -3328.25537109375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7783, loss_val: nan, pos_over_neg: 16396.953125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: 1731.8275146484375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: -11293.28125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.777, loss_val: nan, pos_over_neg: -2989.205078125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: -1656.76220703125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7765, loss_val: nan, pos_over_neg: -1548625.75 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -3030.278564453125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7808, loss_val: nan, pos_over_neg: 8852.390625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7794, loss_val: nan, pos_over_neg: 15814.689453125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: -1459.705078125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: -5702.52294921875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: -1845.7119140625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7853, loss_val: nan, pos_over_neg: 1554.669189453125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: -2931.921875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.782, loss_val: nan, pos_over_neg: -5511.3251953125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.782, loss_val: nan, pos_over_neg: -3754.998046875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: -3116.503662109375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: 2252.273681640625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7796, loss_val: nan, pos_over_neg: -1928.708251953125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7786, loss_val: nan, pos_over_neg: -5004.45849609375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7762, loss_val: nan, pos_over_neg: -1852.2659912109375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7856, loss_val: nan, pos_over_neg: -6050.52099609375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.785, loss_val: nan, pos_over_neg: 5354.13916015625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7785, loss_val: nan, pos_over_neg: 1017.5570068359375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7783, loss_val: nan, pos_over_neg: 2498.109130859375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7829, loss_val: nan, pos_over_neg: 5611.2880859375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: -6401.9521484375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: -13313.1884765625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7757, loss_val: nan, pos_over_neg: 982.8721313476562 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7845, loss_val: nan, pos_over_neg: 1524.726318359375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7829, loss_val: nan, pos_over_neg: -7575.6142578125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -2435.48486328125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.782, loss_val: nan, pos_over_neg: -4492.19287109375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7816, loss_val: nan, pos_over_neg: 2376.93505859375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7811, loss_val: nan, pos_over_neg: 2057.55078125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7775, loss_val: nan, pos_over_neg: 2192.151123046875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7727, loss_val: nan, pos_over_neg: -2814.25244140625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7805, loss_val: nan, pos_over_neg: -2438.652099609375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7866, loss_val: nan, pos_over_neg: -2797.058837890625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -3833.90087890625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7839, loss_val: nan, pos_over_neg: 868.2159423828125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -4595.2119140625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -6256.37744140625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7761, loss_val: nan, pos_over_neg: 7597.91748046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.789, loss_val: nan, pos_over_neg: -3203.65625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: -4850.4833984375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7803, loss_val: nan, pos_over_neg: -28310.54296875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7832, loss_val: nan, pos_over_neg: 12212.306640625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.778, loss_val: nan, pos_over_neg: -2928.092041015625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7846, loss_val: nan, pos_over_neg: 7583.025390625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7819, loss_val: nan, pos_over_neg: -10549.1376953125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7778, loss_val: nan, pos_over_neg: -2852.027099609375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7843, loss_val: nan, pos_over_neg: -2125.2802734375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: -8685.3447265625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: -1995.4637451171875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.78, loss_val: nan, pos_over_neg: 120135.3828125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7848, loss_val: nan, pos_over_neg: -8904.90234375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7839, loss_val: nan, pos_over_neg: -7774.798828125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7768, loss_val: nan, pos_over_neg: -4586.7978515625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7785, loss_val: nan, pos_over_neg: -6792.91552734375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.786, loss_val: nan, pos_over_neg: -2851.607177734375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -5869.14208984375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: -1632.187255859375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7838, loss_val: nan, pos_over_neg: -19507.34375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7788, loss_val: nan, pos_over_neg: 50419.875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.782, loss_val: nan, pos_over_neg: 2171.835693359375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7827, loss_val: nan, pos_over_neg: 6948.53955078125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.779, loss_val: nan, pos_over_neg: -1769.580810546875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.783, loss_val: nan, pos_over_neg: -10475.171875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7816, loss_val: nan, pos_over_neg: 3141.891845703125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.7801, loss_val: nan, pos_over_neg: 2606.965087890625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7852, loss_val: nan, pos_over_neg: 3103.419189453125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: 13938.427734375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7785, loss_val: nan, pos_over_neg: -9096.8486328125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: -1836.2972412109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7808, loss_val: nan, pos_over_neg: -4191.669921875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7745, loss_val: nan, pos_over_neg: -2422.835205078125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7768, loss_val: nan, pos_over_neg: -4450.10498046875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7779, loss_val: nan, pos_over_neg: -4689.6025390625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: -2697.043212890625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: 474704.65625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7788, loss_val: nan, pos_over_neg: 1280.4449462890625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7762, loss_val: nan, pos_over_neg: -2126.210205078125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: -3540.10498046875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: -1870.0819091796875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7828, loss_val: nan, pos_over_neg: -2106.278564453125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: 7278.71630859375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: 7880.84033203125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: 988.4520874023438 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7787, loss_val: nan, pos_over_neg: 10981.46875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.7819, loss_val: nan, pos_over_neg: 50070.2734375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7753, loss_val: nan, pos_over_neg: -3330.797607421875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7785, loss_val: nan, pos_over_neg: -4084.624755859375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7832, loss_val: nan, pos_over_neg: -3549.43896484375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7781, loss_val: nan, pos_over_neg: 8714.0791015625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7829, loss_val: nan, pos_over_neg: 2018.18115234375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: 4349.93603515625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: 1486.5311279296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7769, loss_val: nan, pos_over_neg: 1964.250244140625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: 3244.99267578125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7785, loss_val: nan, pos_over_neg: -5978.77978515625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7786, loss_val: nan, pos_over_neg: 12542.26171875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.7781, loss_val: nan, pos_over_neg: 1874.5506591796875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: 3088.742431640625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7786, loss_val: nan, pos_over_neg: -37988.33984375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7792, loss_val: nan, pos_over_neg: 2494.453857421875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7822, loss_val: nan, pos_over_neg: 2691.04052734375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: 60398.27734375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.78, loss_val: nan, pos_over_neg: 2200.8603515625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: -5644.3681640625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7855, loss_val: nan, pos_over_neg: 1265.4776611328125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7794, loss_val: nan, pos_over_neg: 7955.3203125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: 2182.591796875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7754, loss_val: nan, pos_over_neg: 3863.10400390625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7824, loss_val: nan, pos_over_neg: -2416.146728515625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7794, loss_val: nan, pos_over_neg: -2688.233642578125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7807, loss_val: nan, pos_over_neg: 4247.419921875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: 2971.148681640625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: -10296.2470703125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: -2841.249267578125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: -52622.71875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: 3602.92333984375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.789, loss_val: nan, pos_over_neg: -3520.1513671875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: -5593.39306640625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7845, loss_val: nan, pos_over_neg: 2667.17041015625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7861, loss_val: nan, pos_over_neg: 1355.3599853515625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7881, loss_val: nan, pos_over_neg: 23626.12109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7764, loss_val: nan, pos_over_neg: -21253.669921875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: -1389.0443115234375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7787, loss_val: nan, pos_over_neg: -4825.00146484375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: -18037.9296875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: 8921.5439453125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7816, loss_val: nan, pos_over_neg: -8451.755859375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7833, loss_val: nan, pos_over_neg: -2929.5986328125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7779, loss_val: nan, pos_over_neg: -2698.64306640625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7772, loss_val: nan, pos_over_neg: 5493.0322265625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7832, loss_val: nan, pos_over_neg: 1555.717041015625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7849, loss_val: nan, pos_over_neg: 1714.8323974609375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7793, loss_val: nan, pos_over_neg: -5317.857421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.779, loss_val: nan, pos_over_neg: -1536.406494140625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7766, loss_val: nan, pos_over_neg: -2033.9539794921875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7748, loss_val: nan, pos_over_neg: -2990.91015625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7797, loss_val: nan, pos_over_neg: -4716.033203125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: -14414.328125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: 903.441650390625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7793, loss_val: nan, pos_over_neg: 3484.8955078125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: 10751.7802734375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7871, loss_val: nan, pos_over_neg: -3621.720703125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.771, loss_val: nan, pos_over_neg: -3995.492919921875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7778, loss_val: nan, pos_over_neg: 8190.31201171875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7753, loss_val: nan, pos_over_neg: -1924.9366455078125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: 10349.5517578125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7814, loss_val: nan, pos_over_neg: 1188.7886962890625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: -56445.7734375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: 3558.472412109375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7774, loss_val: nan, pos_over_neg: 46301.58984375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: 21922.45703125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7859, loss_val: nan, pos_over_neg: 1767.472900390625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -2114.078369140625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7875, loss_val: nan, pos_over_neg: -29793.400390625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7813, loss_val: nan, pos_over_neg: 3242.1904296875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: -2558.134765625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7822, loss_val: nan, pos_over_neg: -17265.23046875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7769, loss_val: nan, pos_over_neg: 1371.90673828125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: 8610.140625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: 907.0699462890625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: 1352.7911376953125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7814, loss_val: nan, pos_over_neg: 2618.04736328125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: 38425.3359375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7801, loss_val: nan, pos_over_neg: 1810.8597412109375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: 983.5638427734375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: 9891.8779296875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: 2859.007568359375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7852, loss_val: nan, pos_over_neg: 1974.23779296875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: -2300.177978515625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: -2134.73095703125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7765, loss_val: nan, pos_over_neg: 4588.73779296875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7769, loss_val: nan, pos_over_neg: -14207.1083984375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -68308.0859375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: -2330.14501953125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: 237282.203125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.782, loss_val: nan, pos_over_neg: -4017.069580078125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.7787, loss_val: nan, pos_over_neg: -1533.435546875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7759, loss_val: nan, pos_over_neg: -3580.105224609375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7801, loss_val: nan, pos_over_neg: 1133.4818115234375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7754, loss_val: nan, pos_over_neg: 2185.239501953125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7798, loss_val: nan, pos_over_neg: 14119.654296875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.778, loss_val: nan, pos_over_neg: -4934.87158203125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: -3464.644775390625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7758, loss_val: nan, pos_over_neg: -4595.48974609375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7801, loss_val: nan, pos_over_neg: -61584.3671875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7801, loss_val: nan, pos_over_neg: -30131.91796875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7826, loss_val: nan, pos_over_neg: -3758.765380859375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: -25433.05859375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7866, loss_val: nan, pos_over_neg: 4802.9951171875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7746, loss_val: nan, pos_over_neg: -5654.31396484375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7734, loss_val: nan, pos_over_neg: -8567.685546875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: -19058.130859375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7774, loss_val: nan, pos_over_neg: -7617.06787109375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7826, loss_val: nan, pos_over_neg: -2142.16552734375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7748, loss_val: nan, pos_over_neg: -3407.94384765625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -21176.484375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7796, loss_val: nan, pos_over_neg: -11856.609375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: -99511.25 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7766, loss_val: nan, pos_over_neg: 13923.3955078125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7775, loss_val: nan, pos_over_neg: 1523.7415771484375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7766, loss_val: nan, pos_over_neg: -6286.21533203125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7772, loss_val: nan, pos_over_neg: -2109.791748046875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7791, loss_val: nan, pos_over_neg: 1672.4300537109375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: 3267.60595703125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.778, loss_val: nan, pos_over_neg: -8467.625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7779, loss_val: nan, pos_over_neg: 6538.06591796875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7769, loss_val: nan, pos_over_neg: 6409.8935546875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7827, loss_val: nan, pos_over_neg: 3737.049072265625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7809, loss_val: nan, pos_over_neg: -6830.80078125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7758, loss_val: nan, pos_over_neg: -10841.068359375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7779, loss_val: nan, pos_over_neg: 2531.763671875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7801, loss_val: nan, pos_over_neg: 2290.58642578125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -3786.148193359375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7769, loss_val: nan, pos_over_neg: 8754.4287109375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7729, loss_val: nan, pos_over_neg: 3918.482666015625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7807, loss_val: nan, pos_over_neg: -4545.353515625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: -2797.55859375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7769, loss_val: nan, pos_over_neg: -3924.3330078125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7834, loss_val: nan, pos_over_neg: 5104.3134765625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.772, loss_val: nan, pos_over_neg: 18086.634765625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7814, loss_val: nan, pos_over_neg: 1500.7635498046875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7781, loss_val: nan, pos_over_neg: 1184.428466796875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: -3008.94189453125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7805, loss_val: nan, pos_over_neg: -3854.09326171875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7778, loss_val: nan, pos_over_neg: 1616.103515625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7855, loss_val: nan, pos_over_neg: 2681.373779296875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7728, loss_val: nan, pos_over_neg: -6778.7587890625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -229324.75 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: 1564.7623291015625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7792, loss_val: nan, pos_over_neg: -5409.8779296875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7766, loss_val: nan, pos_over_neg: -5404.7822265625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: -2332.94775390625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7749, loss_val: nan, pos_over_neg: 7593.4521484375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: 55751.08984375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: -23644.96875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.778, loss_val: nan, pos_over_neg: 30938.99609375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7758, loss_val: nan, pos_over_neg: 7017.89404296875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -12844.50390625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: -1690.51318359375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7716, loss_val: nan, pos_over_neg: 57289.18359375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: 1033.7242431640625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7825, loss_val: nan, pos_over_neg: 2756.254638671875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7798, loss_val: nan, pos_over_neg: -11759.3046875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -13217.2421875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.775, loss_val: nan, pos_over_neg: -5620.37744140625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7711, loss_val: nan, pos_over_neg: -1994.914794921875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: -3716.697509765625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7746, loss_val: nan, pos_over_neg: -2304.814697265625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7737, loss_val: nan, pos_over_neg: -2748.5224609375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: 7059.58154296875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7758, loss_val: nan, pos_over_neg: 3137.98486328125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: -6275.72265625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.777, loss_val: nan, pos_over_neg: 1436.5992431640625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7781, loss_val: nan, pos_over_neg: 2107.437744140625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7758, loss_val: nan, pos_over_neg: -20557.416015625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -4877.62060546875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7748, loss_val: nan, pos_over_neg: 77583.375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.775, loss_val: nan, pos_over_neg: 6670.87890625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7749, loss_val: nan, pos_over_neg: -3660.977294921875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: -4206.0244140625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: -2597.37890625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -5547.1728515625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7796, loss_val: nan, pos_over_neg: -2083.42578125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -3905.690185546875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: -1468.5255126953125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7751, loss_val: nan, pos_over_neg: -1500.5386962890625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.785, loss_val: nan, pos_over_neg: -5242.59326171875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.774, loss_val: nan, pos_over_neg: 3527.629150390625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: 3776.59423828125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: 7621.013671875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7845, loss_val: nan, pos_over_neg: -2221.445556640625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7765, loss_val: nan, pos_over_neg: -3175.541259765625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7758, loss_val: nan, pos_over_neg: -5712.134765625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: -2044.3680419921875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7787, loss_val: nan, pos_over_neg: 21098.1796875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: 515.6887817382812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: 6207.46875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: -1722.9913330078125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.7828, loss_val: nan, pos_over_neg: -42317.83984375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.779, loss_val: nan, pos_over_neg: -45125.0234375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: -3359.734375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: 2873.714111328125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: 933.6195068359375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7812, loss_val: nan, pos_over_neg: 4181.81494140625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.772, loss_val: nan, pos_over_neg: 5207.65966796875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7878, loss_val: nan, pos_over_neg: -3014.81689453125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7827, loss_val: nan, pos_over_neg: 1899.685302734375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -33390.3046875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: 5648.8505859375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: 12761.369140625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7776, loss_val: nan, pos_over_neg: 1317.88427734375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: -56053.37890625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7783, loss_val: nan, pos_over_neg: 20466.501953125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: 13253.5302734375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7814, loss_val: nan, pos_over_neg: -3952.06982421875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: -4776.33447265625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.783, loss_val: nan, pos_over_neg: 1871.506103515625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7786, loss_val: nan, pos_over_neg: -2516.645751953125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7745, loss_val: nan, pos_over_neg: -1429.70703125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: -1841.8006591796875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: -3852.31005859375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7774, loss_val: nan, pos_over_neg: -21404.623046875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: 13227.6669921875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7786, loss_val: nan, pos_over_neg: 4085684.0 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7783, loss_val: nan, pos_over_neg: -5081.7373046875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7772, loss_val: nan, pos_over_neg: -23828.734375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.771, loss_val: nan, pos_over_neg: -3082.05859375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.777, loss_val: nan, pos_over_neg: -4660.28857421875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: -114956.9765625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: 10292.234375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.774, loss_val: nan, pos_over_neg: -1584.8753662109375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7798, loss_val: nan, pos_over_neg: -1753.0960693359375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7746, loss_val: nan, pos_over_neg: 1322.46533203125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [05:46<14424:08:38, 173.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 6.775, loss_val: nan, pos_over_neg: 2760.866943359375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7738, loss_val: nan, pos_over_neg: -4401.6943359375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: 56303.59375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: 2847.59521484375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7821, loss_val: nan, pos_over_neg: -202064.703125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7799, loss_val: nan, pos_over_neg: -2491.645263671875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: 1662.2100830078125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7819, loss_val: nan, pos_over_neg: 3906.2451171875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.774, loss_val: nan, pos_over_neg: 7760.57080078125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7778, loss_val: nan, pos_over_neg: -24483.14453125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7776, loss_val: nan, pos_over_neg: -4272.51708984375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7788, loss_val: nan, pos_over_neg: -7419.49072265625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.781, loss_val: nan, pos_over_neg: 2971.832275390625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7711, loss_val: nan, pos_over_neg: 5884.61279296875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7783, loss_val: nan, pos_over_neg: 1347.4932861328125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -17539.384765625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7766, loss_val: nan, pos_over_neg: -2174.0224609375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7762, loss_val: nan, pos_over_neg: -2436.579833984375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7779, loss_val: nan, pos_over_neg: 4135.08837890625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.774, loss_val: nan, pos_over_neg: -11151.4697265625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: 11295.388671875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7759, loss_val: nan, pos_over_neg: 787.2451782226562 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7765, loss_val: nan, pos_over_neg: 5052.994140625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7772, loss_val: nan, pos_over_neg: -150550.40625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: -3546.13427734375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.776, loss_val: nan, pos_over_neg: -9777.0244140625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: 2059.094482421875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: 4869.56640625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7764, loss_val: nan, pos_over_neg: 1235.579345703125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: 3319.4375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: -6082.4775390625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -2056.9267578125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7761, loss_val: nan, pos_over_neg: -12250.7080078125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7688, loss_val: nan, pos_over_neg: 5728.25146484375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.7814, loss_val: nan, pos_over_neg: 1770.03759765625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7787, loss_val: nan, pos_over_neg: -6187.640625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.773, loss_val: nan, pos_over_neg: -4560.99072265625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.77, loss_val: nan, pos_over_neg: -1831.32763671875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -2637.610107421875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.781, loss_val: nan, pos_over_neg: 5192.68359375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -2505.613525390625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: 6630.3876953125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7754, loss_val: nan, pos_over_neg: 13938.5068359375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: -2819.2978515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -1476.751708984375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.771, loss_val: nan, pos_over_neg: -1837.224853515625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7758, loss_val: nan, pos_over_neg: -2815.55712890625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7784, loss_val: nan, pos_over_neg: -3195.5341796875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7711, loss_val: nan, pos_over_neg: 1557.884033203125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: -3234.750732421875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.777, loss_val: nan, pos_over_neg: -5059.35546875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: 1631.0894775390625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7784, loss_val: nan, pos_over_neg: 1086.6904296875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -1357.661376953125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: -2679.1923828125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.7748, loss_val: nan, pos_over_neg: -5046.02734375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7797, loss_val: nan, pos_over_neg: 8572.6845703125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: 5788.490234375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: 2338.605712890625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7779, loss_val: nan, pos_over_neg: 4021.164794921875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: 2638.0302734375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7708, loss_val: nan, pos_over_neg: -8692.826171875 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.77, loss_val: nan, pos_over_neg: -3703.573486328125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7759, loss_val: nan, pos_over_neg: 2723.3515625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: 91063.921875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7797, loss_val: nan, pos_over_neg: -159163.875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -11682.4208984375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7818, loss_val: nan, pos_over_neg: -2802.56005859375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.782, loss_val: nan, pos_over_neg: 1043.3311767578125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: -90180.6875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7823, loss_val: nan, pos_over_neg: 7299.8994140625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: 2997.729736328125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7755, loss_val: nan, pos_over_neg: -10220.9482421875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: 5394.39111328125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: -8450.119140625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.7763, loss_val: nan, pos_over_neg: 3844.947265625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7803, loss_val: nan, pos_over_neg: -2030.4058837890625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: 6781.2353515625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7745, loss_val: nan, pos_over_neg: 8807.7099609375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7761, loss_val: nan, pos_over_neg: -2094.105224609375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7829, loss_val: nan, pos_over_neg: -2509.698486328125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7784, loss_val: nan, pos_over_neg: -30396.677734375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7709, loss_val: nan, pos_over_neg: -4063.1474609375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: -2694.16796875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7815, loss_val: nan, pos_over_neg: 2148.412353515625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: 4701.48193359375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7749, loss_val: nan, pos_over_neg: -8422.38671875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: 5607.28662109375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -2271.58837890625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7751, loss_val: nan, pos_over_neg: -2319.669189453125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.781, loss_val: nan, pos_over_neg: 3875.3017578125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -2534.015869140625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.7771, loss_val: nan, pos_over_neg: 9915.369140625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7797, loss_val: nan, pos_over_neg: 3207.64794921875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: 1603.6512451171875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: -4654.1064453125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7795, loss_val: nan, pos_over_neg: 16900.11328125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -6080.97998046875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -4098.50048828125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7788, loss_val: nan, pos_over_neg: 4010.137451171875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7723, loss_val: nan, pos_over_neg: -2730.63525390625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: 5903.4755859375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7737, loss_val: nan, pos_over_neg: -65962.9296875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7729, loss_val: nan, pos_over_neg: 2946.39306640625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7759, loss_val: nan, pos_over_neg: 3272.53125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.776, loss_val: nan, pos_over_neg: 4544.423828125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -3865.581787109375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7804, loss_val: nan, pos_over_neg: 6209.7021484375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: -3023.195068359375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: -2746.81005859375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7773, loss_val: nan, pos_over_neg: -2275.25146484375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.773, loss_val: nan, pos_over_neg: -2324.742431640625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.77, loss_val: nan, pos_over_neg: -15808.029296875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7675, loss_val: nan, pos_over_neg: -10064.86328125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: 10537.4482421875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -13478.443359375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -3333.218994140625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7802, loss_val: nan, pos_over_neg: -7044.5224609375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: 48417.09765625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7754, loss_val: nan, pos_over_neg: -5590.31640625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.775, loss_val: nan, pos_over_neg: -9316.0986328125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: -12577.015625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7821, loss_val: nan, pos_over_neg: -1780.21484375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7759, loss_val: nan, pos_over_neg: -5293.36669921875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: -58276.87890625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: -6915.64599609375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: -6098.6650390625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7784, loss_val: nan, pos_over_neg: 5149.462890625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: 121869.984375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7746, loss_val: nan, pos_over_neg: -1799.5185546875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.772, loss_val: nan, pos_over_neg: -1599.05908203125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -4114.8154296875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7748, loss_val: nan, pos_over_neg: 3509.045654296875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -2935.817626953125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: 5934.04931640625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: -2881.4638671875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.774, loss_val: nan, pos_over_neg: -26929.3046875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: -2811.287353515625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: -5709.57470703125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: 1807.5048828125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: -9483.8876953125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: -18220.21875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7727, loss_val: nan, pos_over_neg: -2728.8818359375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7753, loss_val: nan, pos_over_neg: -3925.3798828125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.769, loss_val: nan, pos_over_neg: -2079.482177734375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.773, loss_val: nan, pos_over_neg: -6066.77685546875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7704, loss_val: nan, pos_over_neg: 2296.9970703125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7768, loss_val: nan, pos_over_neg: 1875.6500244140625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.773, loss_val: nan, pos_over_neg: 2170.098876953125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -3095.80908203125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7765, loss_val: nan, pos_over_neg: -10940.943359375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -2178.92822265625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7719, loss_val: nan, pos_over_neg: -4609.78076171875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7746, loss_val: nan, pos_over_neg: -60892.1640625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7726, loss_val: nan, pos_over_neg: 13009.67578125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7755, loss_val: nan, pos_over_neg: 2833.249755859375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7781, loss_val: nan, pos_over_neg: -1978.0009765625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -2266.503173828125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: -2681.036376953125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7764, loss_val: nan, pos_over_neg: 2003.2264404296875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -3082.25 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: 11998.40234375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: 95271.671875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7778, loss_val: nan, pos_over_neg: 2314.861328125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: -4870.9208984375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7794, loss_val: nan, pos_over_neg: 14825.3173828125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7808, loss_val: nan, pos_over_neg: 5126.486328125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: 4853.81396484375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: -2131.4716796875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: -6719.421875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: 5676.18994140625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: 13450.75 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.7764, loss_val: nan, pos_over_neg: -9455.63671875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7695, loss_val: nan, pos_over_neg: 14505.994140625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -8141.75341796875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: -2536.090576171875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.775, loss_val: nan, pos_over_neg: -2574.14208984375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.771, loss_val: nan, pos_over_neg: 2615.621337890625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: -49269.14453125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: -6092.77197265625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: -33274.73046875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: -1840.463623046875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7754, loss_val: nan, pos_over_neg: -1972.9500732421875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7721, loss_val: nan, pos_over_neg: -2253.636474609375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7753, loss_val: nan, pos_over_neg: -4200.9482421875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: 2347.94287109375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7793, loss_val: nan, pos_over_neg: 1750.8863525390625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7782, loss_val: nan, pos_over_neg: 13751.7724609375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7755, loss_val: nan, pos_over_neg: -4246.43798828125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.774, loss_val: nan, pos_over_neg: 48252.66015625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: -1927.290283203125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7753, loss_val: nan, pos_over_neg: 8211.80859375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.772, loss_val: nan, pos_over_neg: -3342.914306640625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: -10639.591796875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -2271.275146484375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: -1592.75927734375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.7745, loss_val: nan, pos_over_neg: 6239.60546875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7819, loss_val: nan, pos_over_neg: 14168.4892578125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: -22048.080078125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: 5367.728515625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7805, loss_val: nan, pos_over_neg: -1906.168701171875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: -1312.859130859375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.783, loss_val: nan, pos_over_neg: 686947.9375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: -3925.084716796875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: -2805.253173828125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: -2809.21240234375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7728, loss_val: nan, pos_over_neg: -3077.22021484375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7825, loss_val: nan, pos_over_neg: -1398.3902587890625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: -3382.744384765625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: 3239.435302734375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7765, loss_val: nan, pos_over_neg: 45498.78515625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: -2574.211181640625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7745, loss_val: nan, pos_over_neg: -1757.3748779296875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: -2000.0 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7727, loss_val: nan, pos_over_neg: -2206.323974609375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.776, loss_val: nan, pos_over_neg: -5977.06201171875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -3323.98974609375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: 13669.2421875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7728, loss_val: nan, pos_over_neg: 3064.98095703125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: -2839.8095703125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: 137924.015625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7759, loss_val: nan, pos_over_neg: 5671.18115234375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: 1167.81103515625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7711, loss_val: nan, pos_over_neg: -28375.79296875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: 2164.89404296875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: 2584.798828125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7762, loss_val: nan, pos_over_neg: -2151.347412109375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -2752.19775390625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: 5560.48974609375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -8859.4814453125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: -1444.9852294921875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.772, loss_val: nan, pos_over_neg: -3393.883056640625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -2374.187255859375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7763, loss_val: nan, pos_over_neg: -5245.02099609375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7704, loss_val: nan, pos_over_neg: -2243.377685546875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: -5443.10400390625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: -1579.0389404296875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -2629.000244140625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7775, loss_val: nan, pos_over_neg: -8952.4541015625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: 14377.8701171875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7723, loss_val: nan, pos_over_neg: 6047.17529296875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.777, loss_val: nan, pos_over_neg: -3089.732177734375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: -3415.64208984375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -1960.896728515625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: -2721.3017578125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7751, loss_val: nan, pos_over_neg: 6301.611328125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: 7229.0390625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.776, loss_val: nan, pos_over_neg: -6781.06494140625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: -6307.451171875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.768, loss_val: nan, pos_over_neg: -3500.930908203125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.7727, loss_val: nan, pos_over_neg: -42448.91015625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.767, loss_val: nan, pos_over_neg: -30044.501953125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: -18392.8984375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: -2169.693115234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7757, loss_val: nan, pos_over_neg: -4133.12548828125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7755, loss_val: nan, pos_over_neg: 8941.43359375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7746, loss_val: nan, pos_over_neg: -14920.927734375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: -25919.3515625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: -4688.07763671875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7786, loss_val: nan, pos_over_neg: 2648.488525390625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.77, loss_val: nan, pos_over_neg: 3985.410400390625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: -2573.21142578125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7751, loss_val: nan, pos_over_neg: -1718.3404541015625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: -2798.364990234375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: -4932.39990234375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7762, loss_val: nan, pos_over_neg: -17870.66015625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -6528.18310546875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.773, loss_val: nan, pos_over_neg: -4578.8115234375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -1529.3621826171875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.7755, loss_val: nan, pos_over_neg: -3514.12890625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -1651.5338134765625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -8854.896484375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: -11033.39453125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: -1820.6922607421875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: -2696.30712890625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -2829.383056640625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: -1334.9901123046875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -2114.2333984375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: -2018.880615234375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7698, loss_val: nan, pos_over_neg: 13566.38671875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.776, loss_val: nan, pos_over_neg: 887.5374755859375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7662, loss_val: nan, pos_over_neg: 14029.0849609375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: -2685.47216796875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: -1679.301025390625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7703, loss_val: nan, pos_over_neg: 15214.5302734375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: -3140.67333984375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: -5689.947265625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: 9993.7314453125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.774, loss_val: nan, pos_over_neg: 1579.627197265625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: -6908.74169921875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: -2367.97607421875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: -31189.58203125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.769, loss_val: nan, pos_over_neg: -2089.837646484375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: 2788.134033203125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: -2426.65087890625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: -2073.0927734375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: -1691.376708984375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: -5432.111328125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: -5439.1171875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7763, loss_val: nan, pos_over_neg: 9000.4326171875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7765, loss_val: nan, pos_over_neg: -3082.151123046875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: -3391.4765625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -5860.42578125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.777, loss_val: nan, pos_over_neg: 52848.6328125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: -1728.5576171875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7754, loss_val: nan, pos_over_neg: 2387.28759765625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -19534.150390625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -4474.6845703125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: 13610.69921875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: -3862.51806640625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7734, loss_val: nan, pos_over_neg: -3691.6826171875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: 1617.090576171875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: 2179.946044921875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: -4087.20556640625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: -8393.0478515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -14200.3154296875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: 7628.82421875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: 9897.2333984375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: -77288.9765625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: 40588.49609375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7726, loss_val: nan, pos_over_neg: -22901.708984375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: -2354.03759765625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7729, loss_val: nan, pos_over_neg: -96466.078125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: 2587.68701171875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -2590.207763671875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: -3266.63623046875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.771, loss_val: nan, pos_over_neg: -3491.77294921875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7726, loss_val: nan, pos_over_neg: -5067.29833984375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7721, loss_val: nan, pos_over_neg: -15399.7919921875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: -13719.748046875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.775, loss_val: nan, pos_over_neg: 11076.755859375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: 14309.369140625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -2178.67333984375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -1997.5811767578125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7749, loss_val: nan, pos_over_neg: -2747.376708984375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7781, loss_val: nan, pos_over_neg: -6886.3662109375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -2985.273193359375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7695, loss_val: nan, pos_over_neg: 4115.1767578125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.775, loss_val: nan, pos_over_neg: 4212.76953125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: -5100.4658203125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.768, loss_val: nan, pos_over_neg: -3289.559326171875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -2966.614013671875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7698, loss_val: nan, pos_over_neg: -3446.9033203125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: -18448.880859375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: 24179.02734375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -2937.16357421875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7716, loss_val: nan, pos_over_neg: -16034.2421875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: -4419.69189453125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -1650.2376708984375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: 10600.3359375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: 25609.47265625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7737, loss_val: nan, pos_over_neg: -7862.4189453125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -2282.439697265625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -1602.8048095703125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -1850.233154296875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: -3207.7578125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.771, loss_val: nan, pos_over_neg: -123084.7734375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -4890.80908203125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: -2139.111083984375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -2873.4833984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7767, loss_val: nan, pos_over_neg: -2175.121826171875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: -1376.90185546875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7695, loss_val: nan, pos_over_neg: -1495.36181640625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: -4841.681640625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: 32613.033203125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: 12321.5791015625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7755, loss_val: nan, pos_over_neg: -1512.7904052734375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: -1648.79541015625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -2706.584716796875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -1863.603271484375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7729, loss_val: nan, pos_over_neg: 2388.12939453125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: -1408.2908935546875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7752, loss_val: nan, pos_over_neg: 416689.0625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -1463.0223388671875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: -2381.49169921875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: -4029.846435546875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7698, loss_val: nan, pos_over_neg: -1907.5516357421875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: 5686.0185546875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: -3759.923828125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7703, loss_val: nan, pos_over_neg: 29386.759765625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -4926.40234375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -3379.53369140625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: -2590.9716796875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -3550.212158203125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -3964.849365234375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.7716, loss_val: nan, pos_over_neg: 3496.018310546875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -2234.2265625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7726, loss_val: nan, pos_over_neg: -14158.5927734375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: 6901.177734375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: 6786.90234375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.77, loss_val: nan, pos_over_neg: -4447.62646484375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: 3676.51611328125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -2576.148193359375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: 40047.5546875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: 94636.2890625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: 6934.67578125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7675, loss_val: nan, pos_over_neg: -12902.125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: 4472.32666015625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.77, loss_val: nan, pos_over_neg: 1918.1611328125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -4716.33056640625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7756, loss_val: nan, pos_over_neg: -2977.70263671875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: 2918.060302734375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7762, loss_val: nan, pos_over_neg: -2334.216552734375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: -1664.5142822265625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -3354.34814453125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -1945.043212890625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7744, loss_val: nan, pos_over_neg: -12120.4052734375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: 3310.2158203125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -7848.61376953125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7751, loss_val: nan, pos_over_neg: -2028.0650634765625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -15135.4892578125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7772, loss_val: nan, pos_over_neg: -3257.735595703125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7729, loss_val: nan, pos_over_neg: -3336.96923828125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -6611.51708984375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: 7533.572265625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -1690.682373046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -2484.33642578125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -5517.66943359375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7709, loss_val: nan, pos_over_neg: -6007.29443359375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7721, loss_val: nan, pos_over_neg: -3887.7900390625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: 2857.810302734375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7726, loss_val: nan, pos_over_neg: -3211.384033203125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: 2521.1103515625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: -3046.999267578125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7728, loss_val: nan, pos_over_neg: -2591.5146484375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -12912.0947265625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -5316.48486328125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7719, loss_val: nan, pos_over_neg: -4118.0205078125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: 3214.593994140625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: 23779.35546875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7703, loss_val: nan, pos_over_neg: -2547.4248046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: 1464.5355224609375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: 1391.1220703125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: -4663.21630859375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: -12915.2197265625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: 3535.89306640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: 29777.26953125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: 2023.03076171875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: 6424.94384765625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7723, loss_val: nan, pos_over_neg: 9030.0869140625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: 7337.603515625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -2847.20703125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.767, loss_val: nan, pos_over_neg: 27992.06640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: -2749.982666015625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: 11320.4150390625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -19833.80078125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -4585.78759765625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7719, loss_val: nan, pos_over_neg: 6949.2451171875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.77, loss_val: nan, pos_over_neg: -2320.57568359375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -4830.13916015625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.769, loss_val: nan, pos_over_neg: 4430.17724609375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: -10193.634765625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -9814.400390625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: -2541.431396484375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7738, loss_val: nan, pos_over_neg: 17768.44921875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: -37197.7578125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: -4841.30712890625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -1715.2098388671875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: 1669.127197265625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.764, loss_val: nan, pos_over_neg: 4728.28857421875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: 1619609.0 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: -2124.09814453125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -4878.43017578125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -1922.581298828125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: -1652.3233642578125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -4517.82470703125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: -2385.262451171875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7789, loss_val: nan, pos_over_neg: 8346.103515625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -13869.0791015625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -9166.0234375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: 5895.19091796875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: 3097.9736328125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 3765.35791015625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7675, loss_val: nan, pos_over_neg: -2597.66162109375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7704, loss_val: nan, pos_over_neg: -3543.504638671875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -2855.427734375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7695, loss_val: nan, pos_over_neg: 17485.83984375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: 1584.859375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 2221.627685546875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -4697.5361328125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -114642.53125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -4380.11328125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: -2173.31640625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.768, loss_val: nan, pos_over_neg: -3070.34033203125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.772, loss_val: nan, pos_over_neg: 8578.681640625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: 1002.7525024414062 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: 1357.7491455078125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.767, loss_val: nan, pos_over_neg: -10003.0009765625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: -9236.419921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.769, loss_val: nan, pos_over_neg: 5694.49267578125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: 1317.2913818359375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -5053.27001953125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: 4114.5849609375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.769, loss_val: nan, pos_over_neg: -22775.126953125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: -9635.9814453125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.77, loss_val: nan, pos_over_neg: -2237.177978515625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: -3688.021728515625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -2345.988525390625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: 5582.02783203125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: -3330.53662109375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7774, loss_val: nan, pos_over_neg: 1359.82958984375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: -11746.7138671875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: 1407.6810302734375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.771, loss_val: nan, pos_over_neg: -10032.787109375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: 19242.97265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: 1887.1282958984375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: -3475.663330078125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7776, loss_val: nan, pos_over_neg: -14699.990234375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: 797.3837280273438 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7738, loss_val: nan, pos_over_neg: 804.4987182617188 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -2951.99267578125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7631, loss_val: nan, pos_over_neg: -13315.1865234375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: 27558.701171875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: 4522.87841796875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: -17346.189453125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -6043.0205078125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: 2189.797607421875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: 2902.426025390625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: 40776.42578125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: 929.8369140625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7745, loss_val: nan, pos_over_neg: 4336.27587890625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: -3390.121337890625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7639, loss_val: nan, pos_over_neg: -4785.2060546875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.775, loss_val: nan, pos_over_neg: -2221.318603515625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -4065.033203125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -3093.609130859375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -6267.66943359375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7732, loss_val: nan, pos_over_neg: -19638.9609375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: -2228.429931640625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7755, loss_val: nan, pos_over_neg: -11319.5185546875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: 1137.394775390625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7845, loss_val: nan, pos_over_neg: -4198.806640625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7717, loss_val: nan, pos_over_neg: 4559.04541015625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: -3338.323974609375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -10040.873046875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -2269.98876953125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: -5022.3134765625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7747, loss_val: nan, pos_over_neg: -2471.53662109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7731, loss_val: nan, pos_over_neg: -1848.572021484375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -2356.1337890625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7728, loss_val: nan, pos_over_neg: 1824.404541015625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: 6311.48388671875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -13064.646484375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7729, loss_val: nan, pos_over_neg: -56870.8515625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7675, loss_val: nan, pos_over_neg: -11240.6826171875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7709, loss_val: nan, pos_over_neg: -5938.27978515625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -4564.7685546875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -2336.825927734375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7728, loss_val: nan, pos_over_neg: 166762.671875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: -4966.189453125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: -1726.3392333984375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7711, loss_val: nan, pos_over_neg: -2212.31982421875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -3737.60546875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7735, loss_val: nan, pos_over_neg: 2124.39013671875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7688, loss_val: nan, pos_over_neg: 3134.654296875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: 15730.8974609375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.769, loss_val: nan, pos_over_neg: -4073.929931640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: 4577.53466796875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.772, loss_val: nan, pos_over_neg: -3060.589599609375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -2055.37060546875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: -9424.044921875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -4964.14990234375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: 22688.244140625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: 3176.492919921875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: -3978.851318359375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -2192.810302734375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7662, loss_val: nan, pos_over_neg: -30103.2578125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -3509.448486328125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: -4498.982421875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -2952.838134765625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -2280.25927734375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: 6273.65576171875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: -3901.450439453125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -4872.740234375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: 9236.8291015625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 5899.82666015625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7733, loss_val: nan, pos_over_neg: -2474.262939453125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7741, loss_val: nan, pos_over_neg: -41774.87890625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: -1858.1383056640625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: 6381.59130859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7781, loss_val: nan, pos_over_neg: -6601.86962890625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: -11434.4111328125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: 9324.6513671875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -2579.5830078125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: -3036.386962890625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: 4177.42529296875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7721, loss_val: nan, pos_over_neg: -3879.2978515625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -1877.3994140625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: 6400.6142578125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -2070.674560546875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -2225.825927734375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -2121.925048828125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: 6670.88916015625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.763, loss_val: nan, pos_over_neg: 9168.181640625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: -14395.041015625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -10277.75 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7704, loss_val: nan, pos_over_neg: -5645.41357421875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.772, loss_val: nan, pos_over_neg: -3734.941650390625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: 25049.568359375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 2069.385498046875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -3263.641357421875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: -60475.41796875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: 8144.583984375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -5781.33251953125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7742, loss_val: nan, pos_over_neg: -1907.5233154296875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -4081.0634765625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.775, loss_val: nan, pos_over_neg: 7208.5185546875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: 5996.76953125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: -3742.857666015625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -2081.068115234375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: -12068.9658203125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: 1620.3548583984375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -3094.296875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7711, loss_val: nan, pos_over_neg: -1358.7303466796875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.766, loss_val: nan, pos_over_neg: 2791.986328125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: 2102.423095703125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: 7494.2529296875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: -2177.601318359375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: 5791.80908203125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.768, loss_val: nan, pos_over_neg: -2300.35302734375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: 1140.49169921875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7698, loss_val: nan, pos_over_neg: 3805.542236328125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -1677.4898681640625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -5985.54150390625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: 3124.0224609375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -169116.40625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 13166.8173828125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.771, loss_val: nan, pos_over_neg: 1569.144775390625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: 2433.749267578125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: 1852.1375732421875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7695, loss_val: nan, pos_over_neg: 6204.2109375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -1636.7020263671875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -2124.756103515625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7716, loss_val: nan, pos_over_neg: -2461.4091796875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: 9194.3330078125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: 2636.8994140625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -7260.32763671875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -5276.94677734375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: 1542.24267578125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7719, loss_val: nan, pos_over_neg: -2916.479248046875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: -2620.9892578125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -5411.8359375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: -5705.544921875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: 1979.42919921875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.771, loss_val: nan, pos_over_neg: 1183.4237060546875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7703, loss_val: nan, pos_over_neg: -3894.40869140625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: -2474.35205078125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -1727.4320068359375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: -4108.05615234375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: -5124.2353515625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: 14541.404296875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -9901.515625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: -5148.58447265625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: -4180.18408203125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -3228.79443359375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -2261.14013671875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: -4290.53076171875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -1841.3724365234375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -3042.18505859375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7662, loss_val: nan, pos_over_neg: 16810.478515625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -6583.24365234375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -2095.6318359375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: -4295.794921875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: -1739.0166015625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -6014.404296875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -2232.275390625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: 4478.30126953125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -2406.008544921875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7739, loss_val: nan, pos_over_neg: -1792.3470458984375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -2445.7841796875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -2412.68017578125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: -1839.4715576171875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.772, loss_val: nan, pos_over_neg: -2030.130126953125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: -5705.31982421875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -6525.80712890625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: -1950.9879150390625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: -15286.4990234375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.7712, loss_val: nan, pos_over_neg: 4163.39404296875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 1682.2822265625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: 37525.92578125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -4710.59521484375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -2423.916748046875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: -2837.990234375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: -1333.90380859375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -7814.85107421875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: 2553.148681640625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7723, loss_val: nan, pos_over_neg: 4057.85986328125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.768, loss_val: nan, pos_over_neg: 51650.1171875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -1944.7923583984375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -3275.41943359375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -2471.3583984375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -4381.375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.767, loss_val: nan, pos_over_neg: 16616.681640625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7715, loss_val: nan, pos_over_neg: -3292.3359375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: -3820.740478515625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [08:38<14362:33:23, 172.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: 2297.90283203125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7719, loss_val: nan, pos_over_neg: -64131.08984375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: -2459.50830078125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7716, loss_val: nan, pos_over_neg: -61159.01171875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: 3115.9677734375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -1439.2391357421875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -1716.6224365234375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7738, loss_val: nan, pos_over_neg: -2764.44677734375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: -11379.2939453125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -1914.051025390625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: 19704.005859375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: -1688.0557861328125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -4209.1005859375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -20718.052734375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7718, loss_val: nan, pos_over_neg: 9808.7197265625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: -1510.8416748046875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -4948.48193359375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: 25638.865234375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: -2131.2177734375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -18652.177734375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -1994.545654296875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -44661.55859375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -2415.7998046875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: -4225.98876953125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: 7974.64208984375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -1768.6458740234375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7729, loss_val: nan, pos_over_neg: -3211.453125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -2201.2353515625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7673, loss_val: nan, pos_over_neg: -2392.25830078125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 1876.1217041015625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -13552.32421875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: 2096.967529296875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -1805.5103759765625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -3020.7099609375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: 4554.0869140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -2106.62158203125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -4092.80029296875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: 6544.80322265625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: 1130.19384765625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: -21467.384765625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7639, loss_val: nan, pos_over_neg: 4371.984375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -3280.483642578125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7662, loss_val: nan, pos_over_neg: -25212.626953125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -2172.08740234375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: 6668.77099609375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: -8546.5791015625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7699, loss_val: nan, pos_over_neg: 973.6696166992188 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7675, loss_val: nan, pos_over_neg: 4477.19287109375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -1605.388671875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -1358.3145751953125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -17758.685546875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: 7096.52587890625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: -3115.820556640625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7723, loss_val: nan, pos_over_neg: 3077.54736328125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: 2552.3984375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -1586.2403564453125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7695, loss_val: nan, pos_over_neg: -46755.44140625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.773, loss_val: nan, pos_over_neg: 1811.8192138671875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7777, loss_val: nan, pos_over_neg: 14322.5224609375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -5433.07958984375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: 4758.08740234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7736, loss_val: nan, pos_over_neg: 823.9639282226562 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: 3231.651611328125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: 2717.4599609375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 2206.048828125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: 1030.8818359375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: 2850.81494140625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: 6267.2275390625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: 3052.02734375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: 3938.9755859375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: 1576.8502197265625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.768, loss_val: nan, pos_over_neg: 1083.8575439453125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -3512.016845703125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7688, loss_val: nan, pos_over_neg: -3423.30078125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.766, loss_val: nan, pos_over_neg: 2472.819091796875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: -6829.16064453125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: 17218.115234375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.768, loss_val: nan, pos_over_neg: 2320.876953125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: 4616.36181640625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7704, loss_val: nan, pos_over_neg: 4866.888671875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: 2314.100830078125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -3067.470458984375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: -4701.63671875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 9759.5634765625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: 1737.326904296875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -6516.2392578125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -3530.45361328125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: -1718.6190185546875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: -21688.4375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: 4267.86572265625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: 1327.0732421875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -2699.101318359375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -3996.43896484375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 2049.827392578125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: 5737.08740234375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: -2718.921630859375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -3988.4580078125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: 10153.126953125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: 13754.673828125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -61068.2421875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -4106.1962890625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -6149.3251953125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: 7654.6142578125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 35186.18359375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7698, loss_val: nan, pos_over_neg: -7988.865234375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: -2230.021728515625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: -28838.47265625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -2001.1234130859375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -6047.41162109375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: 129709.5 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -3760.129638671875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7639, loss_val: nan, pos_over_neg: -2099.7724609375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7675, loss_val: nan, pos_over_neg: -2768.05615234375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -1821.8271484375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -1670.3388671875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -4169.60205078125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: 12230.3564453125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7713, loss_val: nan, pos_over_neg: 11466.97265625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -6165.810546875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -2582.140869140625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -5059.49853515625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -2187.63916015625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -1804.7625732421875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -5217.7421875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: -3601.6767578125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7703, loss_val: nan, pos_over_neg: 1670.2762451171875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -3048.476318359375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -3185.432861328125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: -2238.380615234375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -3403.962158203125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -2160.752197265625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -1758.28955078125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -6685.67724609375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: 7442.69970703125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -8440.556640625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -1558.52392578125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -1975.6285400390625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -2568.573486328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -2980.163818359375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -9044.4521484375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -3708.08447265625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -3851.3994140625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7708, loss_val: nan, pos_over_neg: -8106.23779296875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: -10279.0458984375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -3615.255126953125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -2650.1943359375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: -2194.38916015625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -3559.334716796875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -2254.291015625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -5680.48828125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -6255.9365234375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -17311.08984375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: -2985.9775390625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -3338.409912109375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 14419.533203125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -1743.1060791015625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -4018.291015625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: -480228.59375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: 9453.66015625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 4374.21826171875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -198827.15625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -3201.723388671875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -6044.4326171875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -1907.849609375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -7305.55029296875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -3441.0419921875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7725, loss_val: nan, pos_over_neg: -2687.169921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: 5450.283203125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -2361.003662109375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -2561.537109375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -3486.510498046875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: 2816.541015625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -2026.6533203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: -2690.934326171875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -10951.9970703125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7708, loss_val: nan, pos_over_neg: -1878.2557373046875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -2835.031494140625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -4373.20458984375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -4262.8095703125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -2363.935546875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -1457.75439453125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7692, loss_val: nan, pos_over_neg: -37380.26171875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -3029.02392578125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -2904.85498046875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -1605.6834716796875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: 14568.1044921875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.769, loss_val: nan, pos_over_neg: -4040.14990234375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -2240.296142578125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: -2003.1558837890625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: -3789.601806640625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7662, loss_val: nan, pos_over_neg: 3434.96875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: -2331.18017578125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -7896.822265625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: 1012.783447265625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -18226.0703125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: -4612.24365234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -1658.9486083984375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: 16621.97265625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -2110.006591796875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.767, loss_val: nan, pos_over_neg: 25380.431640625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: 1313.0552978515625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: 226367.953125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 16873.78125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -438697.03125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: -11462.654296875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: 2499.130126953125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -4653.2138671875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.765, loss_val: nan, pos_over_neg: 16273.9345703125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: -50913.1953125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -4065.466796875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -5527.85546875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: 4395.33447265625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -2793.9228515625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 2717.338134765625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: 3042.502197265625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: 1394.6298828125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -2194.15869140625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -6567.8564453125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -4951.77099609375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -3932.4296875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: -3160.64111328125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: 801.650390625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: 1457.8760986328125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -4776.17529296875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -15711.6630859375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 11466.2763671875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.77, loss_val: nan, pos_over_neg: 2046.928955078125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -3264.1708984375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: 8666.646484375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.768, loss_val: nan, pos_over_neg: 2874.30615234375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7751, loss_val: nan, pos_over_neg: 957.1171875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7722, loss_val: nan, pos_over_neg: -2603.08447265625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -6691.3125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.769, loss_val: nan, pos_over_neg: 4024.59716796875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -4773.28466796875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -4607.890625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: 1414.72314453125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7728, loss_val: nan, pos_over_neg: 4732.44091796875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: 12131.3974609375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7726, loss_val: nan, pos_over_neg: 1970.62158203125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: 1634.943115234375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -1892.96435546875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: -4001.70556640625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -8318.36328125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -12427.513671875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -2756.116943359375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: 2334.884765625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -30155.38671875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -3892.922607421875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: 9559.9970703125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.769, loss_val: nan, pos_over_neg: 1222.658447265625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -18119.732421875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: 10836.724609375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -4197.2470703125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: 9893.3525390625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7743, loss_val: nan, pos_over_neg: 5537.0146484375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -1776.6175537109375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -1683.843017578125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.769, loss_val: nan, pos_over_neg: -5885.15185546875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -7823.54541015625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: 2914.743408203125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: 6463.0673828125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -1622.6329345703125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -4800.82275390625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -1461.962158203125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -3915.878173828125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -2819.842041015625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: -3551.549072265625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: -2666.80419921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.764, loss_val: nan, pos_over_neg: 10434.8984375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: 6464.5791015625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: -10712.083984375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -3166.75830078125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7706, loss_val: nan, pos_over_neg: -6572.81689453125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -1887.7000732421875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -1496.8675537109375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -1426.1956787109375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7724, loss_val: nan, pos_over_neg: -4760.97216796875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: 7213.98828125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -4269.19775390625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: -1825.675537109375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -2047.4278564453125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -1780.3037109375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.767, loss_val: nan, pos_over_neg: -3138.2041015625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -2100.447265625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: 4104.50634765625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7688, loss_val: nan, pos_over_neg: 5051.10400390625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -3985.298095703125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: 3842.04931640625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -265421.34375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7701, loss_val: nan, pos_over_neg: -2746.62451171875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7675, loss_val: nan, pos_over_neg: -5828.93994140625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -66855.890625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -4575.5556640625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -1715.0008544921875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -6871.80029296875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -1807.4132080078125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7688, loss_val: nan, pos_over_neg: -6838.16943359375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: 31805.3515625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -4602.5146484375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.767, loss_val: nan, pos_over_neg: -11616.25 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -2360.796875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -3784.048828125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -10981.0673828125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: -1739.3984375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: 1910.918212890625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7738, loss_val: nan, pos_over_neg: -16778.984375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7714, loss_val: nan, pos_over_neg: -15122.73046875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: -23937.73828125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -2219.509521484375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: -7116.34130859375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: 1171.6802978515625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.768, loss_val: nan, pos_over_neg: 17431.4453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: 7462.77734375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7679, loss_val: nan, pos_over_neg: -2819.56103515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -3075.429443359375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -3612.137451171875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -6255.326171875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7662, loss_val: nan, pos_over_neg: -4763.38720703125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: 2167.65869140625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: 1013.2037963867188 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -9528.8212890625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -2413.42333984375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -2652.605224609375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: 3688.806640625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -1546.2003173828125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -4803.81884765625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: 10806.283203125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: 1792.846435546875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -2377.686279296875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -3124.54150390625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -2948.889404296875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7685, loss_val: nan, pos_over_neg: -3138.83642578125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: 4584.63525390625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -4791.8310546875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: 7344.365234375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -1692.20166015625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -5689.45361328125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -3473.9599609375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -84649.1796875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -5576.50244140625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7703, loss_val: nan, pos_over_neg: 1240.1146240234375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: 1618.920166015625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: 8294.578125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -225189.9375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: 3823.76220703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.762, loss_val: nan, pos_over_neg: -3097.727783203125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 7600.95166015625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -3919.3251953125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -2679.919189453125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -3783.543701171875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -2493.472412109375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: 1308.8409423828125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: 543138.1875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -13729.25 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7681, loss_val: nan, pos_over_neg: 5190.58740234375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -3128.22705078125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: -3035.892822265625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: 8643.7685546875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -3046.487060546875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -4014.642578125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.761, loss_val: nan, pos_over_neg: 6708.76953125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: 9713.0498046875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -2523.44970703125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: -5493.7900390625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -6773.423828125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -1480.3211669921875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -2582.776611328125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: 60816.75 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -3533.77294921875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -3774.16259765625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.766, loss_val: nan, pos_over_neg: 17357.181640625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.77, loss_val: nan, pos_over_neg: 1325.8021240234375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -6903.53662109375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -2384.8544921875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -1928.92041015625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: -1793.6697998046875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -33751.4609375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7705, loss_val: nan, pos_over_neg: 10143.783203125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -20076.75 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -9671.55859375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: 3606.7470703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -1506.185546875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -3094.685546875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -4119.5205078125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.7631, loss_val: nan, pos_over_neg: -5334.7412109375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: 6433.0751953125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -4049.051513671875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -2464.162109375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7708, loss_val: nan, pos_over_neg: 1642.79443359375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -4563.91650390625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -2072.821044921875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -1873.6768798828125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.767, loss_val: nan, pos_over_neg: 7888.3798828125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.764, loss_val: nan, pos_over_neg: 2836.645263671875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: 1532.5703125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: -2864.538818359375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -1829.7044677734375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 14881.1806640625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: -1653.230224609375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -2011.6756591796875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -2642.06201171875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -2239.070556640625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: 4470.4609375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -13195.154296875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7691, loss_val: nan, pos_over_neg: 8073.453125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: 8525.74609375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: 24448.892578125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: -2928.33203125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -1588.7645263671875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7702, loss_val: nan, pos_over_neg: -3259.295654296875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -2645.8251953125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -73168.0 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -9266.6142578125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -5279.044921875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -4596.380859375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -8362.9501953125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: -1778.0338134765625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -1665.469482421875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -1776.3111572265625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: 12373.052734375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -1986.9481201171875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: -20093.64453125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: 3868.023193359375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -8396.5068359375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -3213.393310546875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -12655.7177734375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -1891.3145751953125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -4501.7568359375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -3027.11376953125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -4690.83642578125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -6965.603515625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -1946.801025390625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -2762.445556640625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -1502.8319091796875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -2261.625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -9699.330078125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: 5768.216796875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 2403.2373046875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -2483.71337890625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -1711.4873046875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -1850.0150146484375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -1624.2144775390625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: -1457.32421875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: 31165.125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: 5325.37939453125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.7689, loss_val: nan, pos_over_neg: 1115.1136474609375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -2158.51025390625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -2614.1708984375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -3287.55126953125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -2022.7069091796875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -2588.6630859375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -2792.119873046875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 3755.280029296875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7688, loss_val: nan, pos_over_neg: 16336.1962890625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -3990.568115234375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -1473.8487548828125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: -4311.158203125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -2760.65625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7709, loss_val: nan, pos_over_neg: -5276.9033203125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -5383.55029296875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -3298.9208984375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.765, loss_val: nan, pos_over_neg: 4438.09033203125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -3248.644775390625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -1293.0498046875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -5775.21630859375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -5532.6630859375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7707, loss_val: nan, pos_over_neg: -2150.1826171875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -5952.775390625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: 1179.5919189453125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: -2079.35546875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: 1662.966064453125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.767, loss_val: nan, pos_over_neg: -2655.908935546875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -2314.27490234375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -3957.664794921875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: -2759.8720703125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -7058.9970703125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7639, loss_val: nan, pos_over_neg: 5502.560546875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: 13241.646484375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7696, loss_val: nan, pos_over_neg: -3315.4765625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -2557.946533203125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -5368.7275390625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -6143.35986328125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 3714.659423828125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -2339.51416015625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: -17418.58984375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: 14993.146484375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: -2779.15625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -1890.2874755859375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: 4044.38671875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -3416.101806640625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -1489.2437744140625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -2040.5848388671875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: -1979.3265380859375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -8529.318359375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2153.718505859375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -3523.184326171875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: 7060.6240234375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.768, loss_val: nan, pos_over_neg: -13320.4140625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: 124260.234375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -1469.2086181640625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -1198.0107421875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -1635.960205078125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: 9190.775390625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: -3920.05859375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: -3371.914794921875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -2415.29833984375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -3371.53564453125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -1484.70458984375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: -2415.512939453125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -3120.159423828125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: 15208.3134765625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -388335.78125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -2268.768798828125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -4468.8818359375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -1617.8631591796875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -11341.884765625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: 4711.138671875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -2251.047607421875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -2136.32568359375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -2699.124755859375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.768, loss_val: nan, pos_over_neg: 1209.4298095703125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -3644.86376953125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: -1953.4898681640625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: -1976.1666259765625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -2383.911865234375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -1330.5806884765625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: -4551.4208984375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -16398.14453125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2535.332275390625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -4157.4658203125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -9892.931640625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -3522.015869140625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -3545.77392578125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: 29210.708984375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: 5854.2216796875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -3318.926513671875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -8824.083984375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -5266.20947265625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -2231.642822265625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7677, loss_val: nan, pos_over_neg: -3509.69482421875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: 1822.5374755859375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -13988.787109375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: -1914.689697265625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -2619.762451171875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: 3011.139892578125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -2678.60791015625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -3548.182861328125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -419445.03125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: 8581.4208984375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -3108.234130859375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7684, loss_val: nan, pos_over_neg: -18935.306640625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -3429.448486328125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -5634.9677734375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -2767.66748046875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7682, loss_val: nan, pos_over_neg: 4855.9130859375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: 15433.27734375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: -4282.61669921875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -2360.865478515625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -3018.439208984375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: 7336.43896484375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 10640.0751953125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -4867.578125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -7765.912109375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -2936.294677734375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -4410.50830078125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: -2133.055419921875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: 14920.4814453125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: -6595.25732421875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -19598.92578125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -2623.837158203125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -3387.3505859375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: 65974.8359375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: -2762.10498046875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -1635.702880859375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -4435.3984375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -5820.2236328125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -15372.98828125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -2845.4755859375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -2623.734130859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: -3280.34716796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -3376.406005859375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: -7857.830078125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -428767.40625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7695, loss_val: nan, pos_over_neg: 2404.77001953125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: -1672.6033935546875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -2322.009521484375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -11262.279296875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -3208.814697265625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -13355.2177734375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: 4866.5 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -8190.7822265625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -1364.3040771484375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -3811.32861328125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -6536.37353515625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: 18677.775390625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: 2658.627197265625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: -80854.625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7658, loss_val: nan, pos_over_neg: -3825.9345703125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: 26655.931640625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -13264.41015625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -3246.155029296875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: -2586.970947265625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 3251.03857421875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 2135.770751953125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7631, loss_val: nan, pos_over_neg: 2063.22900390625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -1794.208251953125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 1470.0430908203125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: 4709.11669921875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7666, loss_val: nan, pos_over_neg: 4839.5283203125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: 5193.31201171875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: 6508.84326171875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -7001.81982421875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -4873.29345703125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -9353.9189453125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: 23486.876953125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -19881.16796875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -2703.637451171875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: 3675.884033203125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 99792.1015625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -2521.798095703125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -3696.10595703125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -3293.919677734375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7686, loss_val: nan, pos_over_neg: 97317.265625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -6434.20654296875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: 1710.19677734375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: -13121.037109375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: 1966.5 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: -3350.310791015625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 18870.08984375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -19017.0546875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: 8697.158203125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -11803.080078125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -3561.179443359375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: 2499.261474609375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: 1879.4346923828125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: 2174.609619140625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: -5422.30517578125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7697, loss_val: nan, pos_over_neg: -2773.68701171875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.771, loss_val: nan, pos_over_neg: 1424.3441162109375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -22266.20703125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: -4543.01953125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -6749.4814453125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -2070.841552734375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 1934.5107421875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -6089.69970703125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2019.9063720703125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -2125.83740234375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1745.982177734375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -1598.1634521484375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7683, loss_val: nan, pos_over_neg: -2300.190185546875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -2834.72607421875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: 1195.443603515625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: 2170.81982421875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: -4734.97265625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -1587.2916259765625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -2775.28662109375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -4680.2275390625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: -6054.57568359375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 39484.11328125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: 2670.025146484375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: 4430.73828125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: 5376.49755859375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: -3510.904052734375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -6407.216796875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: 5081.73828125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: 4457.3369140625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -1822.67138671875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -19327.8671875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: 63858.04296875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: -2270.749755859375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -2355.38330078125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: -2251.734619140625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: -27871.310546875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: -5055.1435546875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2315.1982421875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: -3903.322265625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: -5600.8203125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -3580.755615234375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -2520.3427734375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -3056.4208984375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -5012.20703125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: 7354.0537109375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: 3477.158935546875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: -2424.911376953125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 4948.4189453125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.762, loss_val: nan, pos_over_neg: -4660.32861328125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -17210.72265625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -3013.307861328125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -6302.86181640625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -6767.66552734375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7665, loss_val: nan, pos_over_neg: -2273.80224609375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: 9236.1767578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: 6253.72509765625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -4618.00830078125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [11:29<14323:33:09, 171.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -2176.439697265625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2876.485107421875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.761, loss_val: nan, pos_over_neg: 339367.09375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: 9065.3408203125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -1679.8858642578125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -2458.779296875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -15130.2509765625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -2910.216552734375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: -2223.3837890625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: -2563.283447265625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -3015.06884765625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -2741.5078125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -599447.3125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -2129.3408203125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -3023.82373046875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -5955.17431640625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -2491.869140625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: -2579.468994140625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -1583.9442138671875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: 5527.919921875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -1858.1658935546875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: 2314.8427734375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7687, loss_val: nan, pos_over_neg: -6867.0380859375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: -1964.8619384765625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.764, loss_val: nan, pos_over_neg: 23671.66796875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -5169.373046875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.764, loss_val: nan, pos_over_neg: 3352.184326171875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -10791.2373046875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -1712.58642578125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -5588.9150390625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -7135.52783203125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: -2191.5947265625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -4067.702392578125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -9880.947265625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -30610.9609375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7639, loss_val: nan, pos_over_neg: 11956.5634765625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -3796.200439453125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -4555.16064453125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -38612.47265625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -3260.83984375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -1337.3956298828125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -3660.232421875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: 2418.026123046875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: 1521.968505859375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: 4316.7392578125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -24489.787109375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: 2425.3203125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -7021.248046875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -4340.4462890625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: 1204.2254638671875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: 2257.736083984375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -5282.76806640625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -4638.1943359375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -21695.8125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: 1454.8035888671875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.767, loss_val: nan, pos_over_neg: 6430.10009765625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -3993.74951171875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: -3354.8603515625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7655, loss_val: nan, pos_over_neg: 7695.578125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: 9409.8447265625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: -4686.1884765625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -8165.75244140625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -5299.04345703125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7674, loss_val: nan, pos_over_neg: 3331.24609375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: 2814.000732421875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 3116.457275390625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -3686.9599609375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -3021.85986328125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -3818.3408203125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -1761.51953125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: 3453.918212890625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -1613.700439453125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.762, loss_val: nan, pos_over_neg: 6688.30224609375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: 7155.91357421875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -3952.0556640625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.762, loss_val: nan, pos_over_neg: -2797.298828125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: 147918.25 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -6149.2490234375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -2628.662109375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -2668.7939453125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: 2334.358642578125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: 3190.968017578125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -3646.110107421875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -5403.28662109375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: 8426.7998046875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 5615.48974609375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7668, loss_val: nan, pos_over_neg: -5232.4619140625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -1384.0819091796875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -2308.561767578125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 1062.372314453125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: 4713.71923828125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: -2905.90625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -2064.187255859375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 5754.935546875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 5627.68408203125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -10931.041015625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -2175.4794921875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -24347.78515625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -14188.287109375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.762, loss_val: nan, pos_over_neg: -2747.4130859375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -3254.168212890625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: 32663.7734375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -17672.146484375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7657, loss_val: nan, pos_over_neg: -3607.4169921875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7646, loss_val: nan, pos_over_neg: -3441.628173828125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: 10183.328125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -3604.982666015625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -1582.4786376953125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -2513.48486328125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: -3261.843505859375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -2006.9979248046875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -3272.458984375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -1957.872802734375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7671, loss_val: nan, pos_over_neg: -6559.32373046875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.765, loss_val: nan, pos_over_neg: -3433.159912109375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7667, loss_val: nan, pos_over_neg: -2078.252197265625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -2361.9794921875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7663, loss_val: nan, pos_over_neg: -3223.508056640625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -131718.703125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -3383.53515625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: -2899.5751953125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -9125.4482421875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -1595.5567626953125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -7916.56640625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: 28418.310546875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -3175.482421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -1819.4918212890625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -6415.2060546875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -2413.78955078125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -3221.303466796875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -5098.1142578125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -2651.77685546875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: -68939.609375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -1853.2197265625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -6205.259765625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -5097.16552734375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -8116.72265625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -1728.0068359375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 2535.4326171875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -2086.137451171875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: -6111.4951171875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: 1519.222900390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -19196.24609375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -2072.721435546875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -2190.77099609375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -2331.54296875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: 2888.029541015625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 1638.9969482421875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -6600.51416015625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -2645.0986328125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 6111.0947265625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: 4966.052734375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -6220.73193359375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: 8664.529296875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -30597.298828125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -1697.453857421875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -1397.9703369140625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2281.771728515625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: 48356.828125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: 3101.27294921875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -6193.20556640625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -1310.980712890625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -2556.053466796875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -2275.5517578125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -1797.0357666015625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -8851.2353515625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -4502.1005859375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -10672.0146484375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -1629.8133544921875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2215.66357421875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -2078.231689453125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -12682.8818359375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -1755.8128662109375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: 1639.8265380859375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: 31502.96875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 11134.5966796875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: 4531.91650390625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -7466.322265625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: 1349.229248046875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -3310.206787109375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -4260.64697265625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -23380.072265625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -4627.13037109375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -3950.366943359375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -1912.8773193359375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -3757.928955078125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -2088.9453125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -4820.64501953125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -7168.01416015625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -6723.0830078125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: 5539.79931640625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -6836.6806640625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -6774.19189453125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: 8890.900390625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -10386.673828125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: 2057.022705078125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -7483.9970703125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 17975.822265625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: 560.061279296875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.761, loss_val: nan, pos_over_neg: 2268.976806640625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: 1162.7138671875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -2757.384033203125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -6760.38916015625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: 1838.6502685546875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -2787.595947265625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -27422.005859375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: 158326.0 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: 2788.832763671875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -44988.13671875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -1933.42626953125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7656, loss_val: nan, pos_over_neg: -7198.65576171875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: -1545.2724609375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -1564.837890625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7694, loss_val: nan, pos_over_neg: 2870.447998046875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: 15721.9111328125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: 1379.5040283203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 4583.41845703125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -1324.4581298828125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -1654.291259765625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -3708.4619140625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: 5195.32373046875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1813.8634033203125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: 2610.77587890625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -16453.775390625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -3350.792724609375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7631, loss_val: nan, pos_over_neg: -2680.365234375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -2304.437255859375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -2210.486328125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -2432.7666015625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -7345.11767578125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: 129734.921875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -27515.05859375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -1827.558349609375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -10104.3125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -3422.232666015625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7653, loss_val: nan, pos_over_neg: -3112.7568359375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -3302.9384765625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -2444.750732421875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 1568.6947021484375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -3044.40966796875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: -5819.73388671875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -4207.376953125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -2552.80419921875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -22088.919921875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: 1956.5279541015625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -2023.058837890625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -5446.64599609375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 6534.3037109375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -3928.0908203125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -12475.916015625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: -4546.0537109375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -2858.82080078125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -3247.969970703125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -2525.121337890625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: 34179.16796875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -7036.56689453125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7661, loss_val: nan, pos_over_neg: -9116.146484375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -2293.304931640625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -2150.013427734375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -7605.61328125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -1871.6495361328125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -4581.40966796875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: -8876.7568359375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -4129.9462890625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: 2106.062255859375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: 3075.48193359375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -4445.8642578125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -1716.1778564453125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -2440.1650390625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -2837.84033203125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -5330.8310546875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -2452.320556640625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -66395.3359375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: 66980.5703125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -17302.376953125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -2923.193115234375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -3526.359619140625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -2801.7685546875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -1482.830322265625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -4874.24853515625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -1560.534912109375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2546.021484375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -13184.435546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2845.516845703125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: 6181.73291015625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -7658.529296875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: 1729.69482421875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -7862.6748046875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -15658.560546875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -2161.380126953125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -1582.9979248046875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: 12261.4345703125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -3034.0126953125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -9837.3486328125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: 5689.55224609375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -2137.02099609375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -1364.2764892578125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: 100876.6640625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -8665.8837890625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: 5506.05859375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -4115.83056640625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -3401.79296875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7649, loss_val: nan, pos_over_neg: 66785.6171875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -5659.04541015625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -1898.072021484375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -4926.8994140625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: 2029.3992919921875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -3036.41259765625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -2635.2119140625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -1713.6016845703125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -2832.17724609375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -4135.01025390625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -6152.009765625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -6113.34375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -1950.5704345703125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -3381.9931640625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -1990.546142578125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -16465.734375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -2533.04736328125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -2870.254150390625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7638, loss_val: nan, pos_over_neg: -5303.01123046875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -2576.257080078125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -63635.671875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -6446.41650390625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -1565.4466552734375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -1716.376220703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -2248.5615234375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -2071.389404296875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: 1991.2518310546875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -7479.50146484375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -3774.713623046875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 2971.39697265625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -2121.865478515625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -2691.453125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -4336.62451171875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7645, loss_val: nan, pos_over_neg: -2036.4930419921875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.762, loss_val: nan, pos_over_neg: -1535.170654296875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -1789.470458984375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: 7018.92919921875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 4151.22607421875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -1605.18017578125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -1518.84716796875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -2963.448486328125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -2223.112548828125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -1592.22021484375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -8724.220703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -3473.46923828125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7648, loss_val: nan, pos_over_neg: -2723.737060546875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -4961.04736328125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -2835.234130859375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -3548.063720703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -1491.0113525390625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -1624.5709228515625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: -2066.236083984375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.764, loss_val: nan, pos_over_neg: -5381.9013671875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7676, loss_val: nan, pos_over_neg: 4106447.75 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -7602.80615234375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 4433.88720703125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -1464.8505859375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -3180.588134765625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 1950.3585205078125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 1386.573486328125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7678, loss_val: nan, pos_over_neg: 1565.5831298828125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: 898.8633422851562 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: 1498.215087890625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -16410.193359375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -1945.5111083984375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -40351.71484375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -14870.0029296875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: 6345.64794921875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -8552.3876953125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -19695.458984375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: 4313.2060546875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: 3058.3466796875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -3593.768798828125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -1715.1513671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -1859.3746337890625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -3144.404296875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.759, loss_val: nan, pos_over_neg: 4636.94384765625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -10174.595703125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: 24402.544921875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7669, loss_val: nan, pos_over_neg: -5534.3203125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -5392.42724609375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -5973.02783203125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: 1806.7689208984375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.765, loss_val: nan, pos_over_neg: 2705.19140625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -3900.8896484375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7639, loss_val: nan, pos_over_neg: -3931.0068359375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -3698.465087890625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -2180.076904296875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2635.4638671875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -4077.13525390625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -1599.7481689453125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -3877.6298828125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -5821.72119140625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -5516.58544921875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -7216.658203125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: 5482.8623046875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -5430.06103515625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -2123.066162109375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -10760.1181640625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7662, loss_val: nan, pos_over_neg: -3655.177490234375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -3125.1396484375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: 5396.94091796875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7659, loss_val: nan, pos_over_neg: 2486.0908203125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -5095.92236328125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -2515.60986328125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1542.3172607421875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -2236.8486328125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -1626.166015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -8723.3515625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -2348.734375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: 1491.3873291015625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -10892.5693359375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -4417.03662109375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -6173.541015625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -2775.578857421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: -1744.8760986328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -31137.65234375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -2742.364501953125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: 3466.211181640625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: 6731.994140625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -4597.63525390625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 3068.968017578125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 6839.8505859375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -74618.8359375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2554.88623046875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -1681.1436767578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: 26770.521484375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -2321.35205078125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3001.634033203125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -4157.544921875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 8615.1826171875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: 8815.1103515625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.762, loss_val: nan, pos_over_neg: -2095.34521484375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -4941.0205078125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -6612.4521484375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -5524.66650390625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -1802.756591796875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 13594.619140625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7647, loss_val: nan, pos_over_neg: -3175.7197265625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -9978.080078125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: 2683.070556640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -2599.5302734375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -2901.5478515625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -12333.3544921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -2243.420654296875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 16883.001953125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.759, loss_val: nan, pos_over_neg: 26846.716796875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -4425.98779296875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -4227.63037109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2743.0849609375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -2280.029052734375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -25361.220703125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: -2424.73486328125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -7489.35888671875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -3020.353515625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: 13905.0400390625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -7834.77734375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -4939.60888671875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: -2648.68798828125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -3324.6875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -2870.15283203125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -1995.869873046875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -4633.681640625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: 42242.0546875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: 1846.8743896484375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -1441.9456787109375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -3680.96240234375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -2839.480712890625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -6956.533203125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -3330.3525390625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -6801.48388671875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -3331.537841796875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1767.284423828125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -3227.472412109375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -22754.166015625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: 4008.4716796875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -1676.5045166015625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -1659.9307861328125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: 12305.802734375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: 11413.845703125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 4570.5302734375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -4758.8310546875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -2374.048828125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -15160.2646484375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -3162.475341796875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -803704.1875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -2445.726318359375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1547.604736328125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7639, loss_val: nan, pos_over_neg: 1178.61669921875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: -2585.88623046875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 1510.8953857421875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -63512.23828125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -2683.001953125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -26046.146484375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: 3208.8583984375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -2510.43310546875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -1554.1685791015625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -3867.364501953125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -4268.63818359375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -2752.252197265625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -3759.666748046875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: 1698.4876708984375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -11741.375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -3092.18603515625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -1676.741943359375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -6120.21044921875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -2555.28955078125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -2418.485107421875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -42883.77734375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: 8824.8203125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -6491.53271484375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -3118.061279296875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.766, loss_val: nan, pos_over_neg: -3196.969970703125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -16808.5703125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: 10751.2529296875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 2982.9482421875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 2826.236572265625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -4408.11865234375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -2272.74462890625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7654, loss_val: nan, pos_over_neg: -10048.259765625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -5323.380859375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: -2377.5732421875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -4558.22900390625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -4088.659912109375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -56539.41015625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: 4682.87744140625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: 3411.592041015625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: 2467.42138671875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -5517.7998046875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -2386.584228515625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: 8158.25390625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: 1502.917236328125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -8214.2802734375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -10125.4619140625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -2016.695556640625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -3254.4658203125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -8555.0302734375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 2165.109375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: 3514.618408203125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -3136.0771484375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -4055.982177734375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -1830.1380615234375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -2207.7041015625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -2557.6630859375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -1713.8658447265625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -1675.978759765625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -2760.87744140625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -2327.641845703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -2766.306396484375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -6644.70556640625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -1900.8636474609375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -2772.59912109375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: 1169.799560546875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -9861.8623046875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2468.94970703125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -5915.1923828125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -2517.295654296875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: -1920.133544921875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -2695.512939453125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -2505.8994140625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -80913.734375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -88999.59375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -5955.00439453125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -2426.87109375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -9721.3701171875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -3364.986328125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -11499.15625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -3107.603271484375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -2654.691650390625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -1964.00048828125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -3846.714599609375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -3399.8671875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: 2784.128662109375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -44142.234375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -3643.135986328125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -3258.10302734375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -11197.94921875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -6638.744140625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -5058.8818359375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.761, loss_val: nan, pos_over_neg: 1467.2762451171875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -10419.3837890625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -1913.8173828125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -1497.60498046875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -1988.40185546875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 10394.029296875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: 7515.15673828125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -4053.746337890625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -5620.3955078125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -8053.15869140625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -65546.2421875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -1291.452880859375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -1861.4954833984375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -2306.32861328125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -2220.26171875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: 1244.5279541015625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -11235.451171875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7629, loss_val: nan, pos_over_neg: 3613.572265625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 4774795.5 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -4303.55029296875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -4742.798828125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -4726.169921875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: 18137.30078125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -5860.48779296875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -3286.940673828125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -2837.24267578125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -1632.3092041015625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2710.48779296875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 4280.71337890625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.762, loss_val: nan, pos_over_neg: 1765.943115234375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 9157.328125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -2091.13720703125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: 116004.0703125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -2426.48779296875 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -1434.52001953125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -3455.187255859375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -5763.552734375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -4052.76171875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -6860.01806640625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -21185.017578125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2669.051513671875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -9912.1650390625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -6713.181640625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: 5066.13037109375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -2350.113525390625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.757, loss_val: nan, pos_over_neg: 22722.224609375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -1973.9896240234375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2619.451904296875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -1951.5458984375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -1627.1663818359375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: 4793.615234375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -15084.7998046875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -5623.2314453125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -1607.88525390625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -4483.966796875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -1603.7191162109375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -2156.44091796875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -3984.061279296875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: 4480.060546875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -4031.6640625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -3002.054931640625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -4255.453125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -1826.615478515625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: -2289.72607421875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2804.621337890625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -5416.7060546875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.767, loss_val: nan, pos_over_neg: 15215.115234375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -6011.5546875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -3427.595947265625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -2506.68994140625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 5549.93701171875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -3189.18359375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -3648.587158203125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -3796.92724609375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -25138.26953125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: 16756.36328125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -4242.138671875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -1529.73291015625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -2580.39111328125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: 3792.973388671875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7635, loss_val: nan, pos_over_neg: -8262.9697265625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -27718.9140625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: 1448.5445556640625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2487.14697265625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -1664.158447265625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -2526.45703125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -1782.1961669921875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -2459.847412109375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -11015.28125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.758, loss_val: nan, pos_over_neg: 2242.7431640625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -4776.1845703125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -2954.307861328125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -2886.1630859375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: 5282.13134765625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: 1353.7366943359375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -5228.4638671875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2347.8388671875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: 4563.68603515625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -5423.5966796875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -3818.095947265625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 16457.98828125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -192708.953125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -2544.4443359375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -2664.07373046875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: 26774.240234375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: 12387.630859375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: 241237.515625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -4113.54345703125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2469.2333984375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -6871.48046875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -2910.890625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2622.178466796875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -2162.68896484375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: 16413.5390625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [14:21<14316:20:33, 171.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 3019.457763671875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 5351.662109375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: -2933.058837890625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -2293.825927734375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -46316.38671875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -3929.458740234375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -2374.755859375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -3584.25732421875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -3435.881103515625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1763.195556640625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -1220.847900390625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -2303.740234375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -6847.48779296875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -20952.44921875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -2377.54931640625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: -10194.205078125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -2019.68017578125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2739.583984375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -4126.32666015625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: 15184.849609375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -24846.6796875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: 5551.86865234375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -2635.703125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: -8056.525390625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: 1212.50732421875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -5030.0068359375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -2670.52734375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 30004.1328125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -3918.757080078125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -3379.632080078125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -2683.78662109375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -10824.791015625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -2795.294189453125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -6461.03759765625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -3536.52197265625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -2071.88720703125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -2318.989501953125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.756, loss_val: nan, pos_over_neg: 1973.162353515625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.7632, loss_val: nan, pos_over_neg: -5009.7421875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -2796.318359375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -2708.512939453125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2924.3095703125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -7587.603515625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 4801.8984375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: 3869.002685546875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: 8634.6181640625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -2522.15380859375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -4623.73046875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -5174.525390625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -1492.7154541015625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -3645.925537109375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -7017.37255859375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7618, loss_val: nan, pos_over_neg: -12308.8779296875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -11274.4375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -2599.33642578125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -33557.38671875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -1727.1002197265625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -1364.0609130859375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -1598.3284912109375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3124.226318359375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: 5287.65234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -12363.111328125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -1717.92236328125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -2979.271728515625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -1461.3828125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2688.804931640625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: 2948.146728515625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 10836.48046875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -4317.66064453125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -1614.89599609375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -1450.1842041015625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -4790.306640625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -1419.4080810546875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -12896.0341796875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1625.7425537109375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -3163.75830078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: 17424.83203125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -2087.359130859375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: -2796.914794921875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2220.22314453125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -2103.297119140625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 3518.61474609375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2569.00390625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -2140.8681640625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -1545.8936767578125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -24977.642578125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -2058.981201171875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -2081.978271484375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -4083.389404296875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -2305.94189453125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -1649.51904296875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -3620.855224609375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -3841.366455078125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -1663.5604248046875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -3347.33056640625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7624, loss_val: nan, pos_over_neg: -7174.37255859375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -1212.75146484375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -2054.893798828125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -2048.92138671875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -2913.11279296875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -2163.404296875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -1936.3114013671875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -2640.498046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2757.87451171875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -3743.68603515625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 1666.739990234375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: 3147.6005859375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2409.588623046875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2713.0751953125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -2256.3798828125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -5637.34814453125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -9472.849609375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -5670.57763671875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -10339.6337890625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -3530.9287109375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -1866.2646484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -1626.832275390625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2484.2841796875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -2052.472412109375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -8770.2763671875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: 300327.21875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -2698.05078125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -4416.712890625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -2914.470703125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -1480.391845703125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: 14463.3369140625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 3109.608642578125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.761, loss_val: nan, pos_over_neg: 14007.1103515625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -5968.34130859375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -4474.384765625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: 4600.0537109375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -3616.17529296875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -7006.810546875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -3271.03369140625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -4429.64697265625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: 2607.11181640625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: 4320.29248046875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -4277.26708984375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: 1866.93017578125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 1986.6229248046875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 9533.6728515625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -2179.158935546875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: 2838.224365234375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7625, loss_val: nan, pos_over_neg: 4402.7451171875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 7806.55517578125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7644, loss_val: nan, pos_over_neg: 9183.1337890625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: 9011.384765625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1836.1102294921875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 26255.044921875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -2231.69384765625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -2179.04150390625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -10478.720703125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 15202.5546875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -8371.876953125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -1882.5950927734375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -1972.525146484375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: 13826.56640625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -1758.7659912109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -2818.300048828125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -5197.2197265625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -2501.42822265625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: 4739.66845703125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 359224.15625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2390.20947265625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -2640.124267578125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2288.77783203125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: -56852.3828125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 4839.828125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: 2836.044189453125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.757, loss_val: nan, pos_over_neg: 18724.693359375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: 48127.5859375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7636, loss_val: nan, pos_over_neg: 3688.389892578125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: 36790.62109375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -3019.12890625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -1920385.625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -3567.39599609375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -1945.098388671875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: 16239.3271484375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: 30015.994140625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -3332.86572265625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -2879.79296875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -4630.87109375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2356.25 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: 2380.160888671875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -3976.609130859375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 1185.7186279296875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 2994.973388671875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -1868.152099609375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -1742.5101318359375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -3262.451904296875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -2626.924072265625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -2454.899169921875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: 11201.2666015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -14136.375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -3085.514404296875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -2680.52197265625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -5875.9599609375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -14171.814453125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -1836.3460693359375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1488.791259765625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -460872.9375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -6868.46044921875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -2017.6934814453125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -29200.732421875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -4792.7509765625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: 3253.86865234375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -1991.7667236328125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7643, loss_val: nan, pos_over_neg: 7646.74072265625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 1647.253173828125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: 2785.29736328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 28016.666015625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -4089.69580078125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -13675.9599609375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 7028.9990234375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: 3031.185302734375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: 2829.551513671875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -2929.81689453125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -10613.2685546875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: 2248.213134765625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.7664, loss_val: nan, pos_over_neg: 1722.5185546875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2772.923828125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.756, loss_val: nan, pos_over_neg: 8973.517578125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -8721.830078125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7651, loss_val: nan, pos_over_neg: 6965.98291015625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: 4751.65966796875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -2490.51904296875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -2394.467529296875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -2166.95263671875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.754, loss_val: nan, pos_over_neg: 36170.9765625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -9639.693359375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: 5026.8388671875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7609, loss_val: nan, pos_over_neg: -97710.640625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -1655.373046875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -1742.7725830078125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -3463.7880859375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 1939.493896484375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -1558.1783447265625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: 2320.89892578125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 7657.28271484375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -8069.63671875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -2488.548828125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -1433.1773681640625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1542.0216064453125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -1280.717041015625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -3387.42626953125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -1792.9429931640625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 4674.87353515625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.757, loss_val: nan, pos_over_neg: 2988.778564453125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 8879.859375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -4118.77734375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -2008.543212890625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -1423.5545654296875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -2235.5390625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: 4659.75537109375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7637, loss_val: nan, pos_over_neg: -4322.51708984375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -2383.302978515625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -1923.1195068359375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -4959.0810546875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -1423.3887939453125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -2031.4329833984375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -4122.77490234375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 21934.384765625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -5413.7216796875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -1678.626953125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -3162.748046875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 4109.2900390625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -3705.399658203125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -1674.860107421875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1518.682373046875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.761, loss_val: nan, pos_over_neg: 3828.87890625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: 6100.32763671875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 45264.73828125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -2743.79296875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -3109.64599609375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -2366.84423828125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -1457.806396484375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: -21318.1953125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -2529.13232421875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 1649.1773681640625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7612, loss_val: nan, pos_over_neg: -4840.36962890625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -5933.07568359375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -2180.130859375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -28763.275390625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -1888.2325439453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -19728.66796875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -2881.761962890625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -58500.53515625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -7177.26953125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.76, loss_val: nan, pos_over_neg: -3021.006591796875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -2126.392578125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: 3542.499267578125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -1387.00830078125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -3411.935791015625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -3429.614990234375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -1831.7764892578125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2005.3741455078125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2104.849609375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -2080.307373046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -2065.125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -3234.35693359375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -2522.838134765625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: 18481.009765625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: -2705.787841796875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -2039.801025390625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -5859.28515625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -2770.679931640625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -5572.4951171875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7672, loss_val: nan, pos_over_neg: -10540.9794921875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -4200.8564453125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: -7635.46142578125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -2736.196533203125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -1441.8065185546875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -1446.2230224609375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2342.111328125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -3214.960693359375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.759, loss_val: nan, pos_over_neg: 4632.59033203125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: 4129.861328125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: 1623.5118408203125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -5392.42919921875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -2161.314453125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: 8368.6025390625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -1919.019287109375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7634, loss_val: nan, pos_over_neg: 3989.006591796875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -4514.89990234375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: 33846.3125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -8994.341796875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: 6132.22265625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -3162.105712890625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -2504.238525390625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -1505.584716796875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -1699.2979736328125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -3204.16845703125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -13203.0634765625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: 8450.0048828125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -6992.61474609375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -5847.65771484375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -3531.236572265625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -2356.507568359375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -2388.730224609375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -10034.978515625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -8856.73828125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: 5680.82177734375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -16791.607421875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -3366.8017578125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -4611.32958984375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -7182.11328125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -3016.34912109375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -16222.9501953125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: 2725.20556640625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 2203.895263671875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.759, loss_val: nan, pos_over_neg: 29474.603515625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7603, loss_val: nan, pos_over_neg: -5510.623046875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -3701.066162109375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -2045.7867431640625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -4916.6337890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: 5330.78662109375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -4738.2099609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -3600.71142578125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: 5948.31494140625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7615, loss_val: nan, pos_over_neg: 1261.5716552734375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2251.020751953125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -3204.052734375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -6482.4013671875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -6818.5361328125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: 3865.9697265625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -3897.295166015625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: -12636.8525390625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 29761.640625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 2606.479248046875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -19136.5234375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -3665.712158203125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -3643.474853515625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: -1753.7041015625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -2593.49365234375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -1585.5469970703125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -8928.962890625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 13325.8955078125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -7689.6455078125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: -71678.046875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7623, loss_val: nan, pos_over_neg: 1976.4263916015625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -2281.152099609375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -12821.41015625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -3226.208251953125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2163.3154296875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -1812.3729248046875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2050.08447265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -29144.23828125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -2488.697509765625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.761, loss_val: nan, pos_over_neg: 5105.326171875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -1489.93798828125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 11307.06640625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: -9934.8974609375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -2143.2919921875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -5518.90185546875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: -2080.9228515625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2634.275390625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -3876.63525390625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -21346.91015625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -4063.640869140625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -3933.36865234375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 7921.13818359375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -2644.86474609375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -1780.1876220703125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2273.56689453125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -3970.91943359375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: 10802.666015625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -9540.64453125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.756, loss_val: nan, pos_over_neg: 47274.0546875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -1657.5546875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -24618.62109375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 2766.590087890625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -4746.22705078125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: 5676.66552734375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: 1696.26025390625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -6829.46240234375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -2295.443359375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -19999.205078125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.761, loss_val: nan, pos_over_neg: -4099.25 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -1500.224365234375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -2105.940185546875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -3333.884521484375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: -2339.903076171875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -1628.6676025390625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -4030.24169921875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -11350.6083984375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -6200.2138671875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.757, loss_val: nan, pos_over_neg: 90576.28125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7596, loss_val: nan, pos_over_neg: 19058.142578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -6001.8828125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: -1802.52490234375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -5612.15185546875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -4383.0654296875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -1744.3619384765625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -1822.1392822265625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -5477.10888671875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7602, loss_val: nan, pos_over_neg: 6041.53515625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: -9823.9306640625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -120093.484375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -4162.16845703125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -3770.064208984375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: 9283.5048828125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -2731.398681640625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -3451.685546875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -10240.55078125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -4341.3974609375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -1858.3511962890625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -4856.724609375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: 10146.537109375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -6300.86083984375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -4054.1103515625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 39029.02734375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2140.62451171875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -15991.2001953125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -1553.248046875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: 7567.51953125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -14970.9248046875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -3227.3154296875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 44552.6953125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -5499.78076171875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -2134.136474609375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2524.6513671875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -2591.994140625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -2097.07275390625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: 1827.3096923828125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -17305.140625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2989.337158203125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: 11599.30078125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 4314.4072265625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: 11748.4619140625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -5263.78466796875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1473.79931640625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -3493.525390625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -5322.67431640625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: 1298.2713623046875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: 1561.3809814453125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -2399.91748046875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -4303.77685546875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -3376.10693359375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -5005.17138671875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -3143.81103515625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: 27207.494140625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -40856.5625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: 9756.095703125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -7729.60693359375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -2384.46142578125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3363.95166015625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1888.921630859375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7627, loss_val: nan, pos_over_neg: 2834.578369140625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: 179989.546875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7652, loss_val: nan, pos_over_neg: 2088.03173828125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -2326.63134765625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: 2325.498291015625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -7753.58154296875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -2578.7021484375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2337.813720703125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -4871.4990234375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.763, loss_val: nan, pos_over_neg: -24684.828125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7619, loss_val: nan, pos_over_neg: 2806.458984375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -3800.365966796875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -6946.20263671875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: 18935.748046875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -2767.315185546875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -5609.91845703125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2720.707275390625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -2288.04345703125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7606, loss_val: nan, pos_over_neg: 4898.00634765625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -1659.0550537109375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -1399.3387451171875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -2091.07177734375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -8330.4912109375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -5403.30029296875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7613, loss_val: nan, pos_over_neg: 1326.0335693359375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -3833.52392578125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -2255.826171875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2775.0302734375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -3416.644287109375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: 124423.6015625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -4736.189453125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -3334.324951171875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: 7537.52734375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -2517.630859375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -3698.71337890625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -1595.75341796875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -3265.531005859375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7628, loss_val: nan, pos_over_neg: -1924.2762451171875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -1927.37744140625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -1590.740966796875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -2573.286865234375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -3605.803955078125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -3409.089111328125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7616, loss_val: nan, pos_over_neg: 1840.5655517578125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -3045.9814453125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -3566.837890625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -2295.808349609375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -1703.521728515625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -2885.096435546875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1647.963134765625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -2494.48828125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -2337.4970703125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: 1932.1097412109375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -2900.919189453125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -2571.501708984375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -3532.264404296875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -1391.0186767578125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 8423.28125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -7394.2529296875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.754, loss_val: nan, pos_over_neg: 5756.4658203125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -3896.284423828125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -3987.36669921875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -1897.6497802734375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -1452.930908203125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -2151.648193359375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -3146.18701171875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -985830.375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -10942.6484375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: 10347.56640625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -6565.85107421875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -1655.889892578125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: -5934.78515625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.757, loss_val: nan, pos_over_neg: 34106.77734375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7617, loss_val: nan, pos_over_neg: -2156.19287109375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2789.370361328125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: 1262.2725830078125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -50283.1953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2546.23876953125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -1606.65234375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -6128.9892578125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: 2544.966796875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -2209.63525390625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -21550.763671875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -2918.489013671875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -8322.943359375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -6481.3271484375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -2622.799072265625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -3820.4111328125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 5044.43994140625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 9968.1669921875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -2138.341796875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2846.1826171875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: 87615.7734375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: 8074.94921875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -5398.431640625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: 2271.96142578125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -1741.9642333984375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -10429.373046875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -3988.811767578125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2126.468994140625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -2689.594970703125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -4112.34716796875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7621, loss_val: nan, pos_over_neg: -2777.352294921875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -1852.0308837890625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: 2076.038330078125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -1747.3734130859375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -1788.31201171875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -2643.53662109375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: 10459.22265625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2050.436279296875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 15001.9091796875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2574.347900390625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 20808.27734375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -7857.9501953125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -2046.149658203125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -3696.453369140625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -4043.399169921875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3413.00048828125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -1912.0321044921875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 2436.096923828125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -3459.4423828125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -3307.683349609375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -102619.453125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -3522.427978515625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -1990.648681640625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -2889.549560546875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 2312.65673828125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: 4278.98193359375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: 2091.819091796875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -4198.6416015625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -2839.375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: 69533.2109375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -14690.408203125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -2998.46533203125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: 5522.2783203125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -9728.7314453125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 1591.0460205078125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: 1400.4364013671875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 18905.984375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7597, loss_val: nan, pos_over_neg: 2133.246337890625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -4461.419921875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -2225.509033203125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2092.75146484375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 2492.59375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: 1246.416015625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: 59927.9375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: 18038.16015625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -5255.19384765625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -84890.5234375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -7939.35009765625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 1370.74658203125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: 2632.5556640625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: 1355.5509033203125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 648.3446044921875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -3689.601806640625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 5731.92431640625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -4400.087890625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -3324.30224609375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -3409.091552734375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -7821.142578125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -6103.66796875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 13545.9189453125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.754, loss_val: nan, pos_over_neg: 2334.801513671875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 43923.2890625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -3646.87109375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -2529.5859375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7595, loss_val: nan, pos_over_neg: -1693.7008056640625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -2020.9796142578125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 5965.47802734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 2398.1708984375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -3376.34716796875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 2816.5869140625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 2285.303955078125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -3465.052978515625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -1875.141845703125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1305.3353271484375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 5168.546875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: 4800.25244140625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -1841.6390380859375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -9707.9365234375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 1795.4814453125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -5775.25048828125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: 4869.7412109375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -4641.4072265625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -4654.560546875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -2662.442138671875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -1815.5504150390625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -2337.872314453125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -2029.7025146484375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -6454.1533203125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: 3217.1533203125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -4098.26708984375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -5197.55322265625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -3191.785888671875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: 4306.61328125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -7196.6962890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -1883.5135498046875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -6329.89208984375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: 6047.8203125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: 3386.6337890625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -4742.1767578125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: 17512.224609375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2444.04638671875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1794.2984619140625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -1789.7552490234375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 23350.107421875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: 922.6298217773438 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -9277.853515625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [17:13<14317:36:08, 171.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -64814.87890625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -4350.63623046875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 21089.455078125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -1460.737060546875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2219.89501953125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -5207.96533203125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: 3045.906005859375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -3740.66796875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7641, loss_val: nan, pos_over_neg: 10402.931640625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: 3673.114013671875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -20915.6875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -1309.096435546875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.757, loss_val: nan, pos_over_neg: -144581.671875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: 2162.943603515625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 2604.40673828125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: 1952.779052734375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: 1926.1409912109375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2373.435302734375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: 3728.631591796875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -3931.831298828125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -3028.1240234375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7605, loss_val: nan, pos_over_neg: 2151.30078125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: 1250.9970703125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: 1071.676025390625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -2471.726318359375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: 1623.1351318359375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -5883.47021484375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -5938.0625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -3167.009521484375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1924.6126708984375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -4782.7412109375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -3032.216064453125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: -6386.50634765625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: 9480.15625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.754, loss_val: nan, pos_over_neg: 31481.2421875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 2034.273681640625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -3255.39453125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -2665.053466796875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2687.678955078125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -432400.25 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -4958.732421875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -57858.18359375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -7231.005859375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -4371.1611328125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: 1539.447509765625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -9845.8310546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: 2015.7091064453125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: 4665.8125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -2143.7255859375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -17656.20703125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1910.541748046875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7626, loss_val: nan, pos_over_neg: 56676.19921875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -7190.16748046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2267.3779296875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -2131.126953125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -3297.598876953125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -4571.9599609375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: 9192.44921875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -3390.795166015625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -2020.710205078125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: -1723.556640625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -2731.343017578125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 1748.2410888671875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -2046.826904296875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 21512.904296875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: 3111.343017578125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -7322.59765625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -2774.7646484375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: 144080.09375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -2440.78076171875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -2352.87841796875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -5157.95556640625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: 4249.09619140625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 1684.132568359375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -3644.213134765625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -1849.857666015625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -1719.50537109375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -2375.992919921875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: 1708.939697265625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -5499.6845703125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -3484.966064453125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -4069.42236328125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -1852.6204833984375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -1538.7286376953125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -1199.625244140625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -2055.665283203125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: 10687.0322265625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.7633, loss_val: nan, pos_over_neg: -3352.612060546875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1882.2783203125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -4274.28857421875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -6094.390625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -5085.74658203125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.7642, loss_val: nan, pos_over_neg: -10826.3349609375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -1764.951171875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: 12411.498046875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -5228.50146484375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7611, loss_val: nan, pos_over_neg: -2079.079345703125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -1602.2542724609375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2303.79345703125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -13287.8134765625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -1526.0208740234375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: 2058.956298828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2107.752197265625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -2263.21728515625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -1974.3856201171875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 8091.5234375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -5711.765625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1365.4234619140625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -4901.326171875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -3624.6572265625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2409.405029296875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -89881.375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: 123219.015625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2158.16357421875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -4898.3486328125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3001.321533203125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -3294.142822265625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2707.72998046875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3398.435546875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: 11085.384765625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: 2873.230712890625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -4724.70361328125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -3066.374267578125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2468.376708984375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2345.267578125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -2240.492919921875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: 3871.072998046875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -2020.941162109375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7594, loss_val: nan, pos_over_neg: -25885.13671875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -6658.39501953125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.758, loss_val: nan, pos_over_neg: -51195.30078125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -1632.2332763671875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -1693.119140625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -3584.797119140625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -2306.437255859375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2004.2427978515625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -9793.78515625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: 4310.90771484375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -27361.015625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -3766.45166015625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -45412.6640625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -32247.296875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -1354.989501953125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: 7665.568359375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 8456.5078125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -16262.16015625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2794.263916015625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -2321.4033203125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -2409.211181640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: 6084.732421875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -4536.42626953125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -20775.091796875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1785.647216796875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1650.4671630859375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -2445.710693359375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7607, loss_val: nan, pos_over_neg: -5539.26611328125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -2065.585205078125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -1938.75244140625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -1809.697998046875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -12093.2998046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -3947.56884765625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -2029.7889404296875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -3377.015869140625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2732.988525390625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -1819.9366455078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2265.181640625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2129.4951171875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -4462.67919921875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2214.75244140625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -1451.2989501953125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -1573.915771484375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -2204.268798828125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -4276.32763671875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.76, loss_val: nan, pos_over_neg: 14518.794921875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: 3261.110107421875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: 3840.657470703125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: 17431.171875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7614, loss_val: nan, pos_over_neg: 2411.43994140625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -1853.31787109375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -2437.45751953125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: 4178.6318359375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: 82238.1953125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -2671.333984375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1394.1617431640625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -4755.69189453125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: 2595.358642578125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: 2508.672607421875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: 5781.5986328125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3198.96142578125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -12499.8173828125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: 1435.3436279296875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -6675.26416015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -38541.2265625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -3179.929443359375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: 10733.0859375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -3423.05078125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: 6974.5693359375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2126.473388671875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -3303.825927734375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -2322.6875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1800.1641845703125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -3425.743896484375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -3266.131103515625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -60660.63671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: 7576.896484375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -9387.068359375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -3573.29052734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -23829.3984375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -4315.94287109375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -2887.278564453125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -4912.85107421875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -11832.1923828125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -6216.65625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -3495.188720703125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2375.883544921875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -8040.6298828125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2365.703125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -1880.2484130859375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -3018.314697265625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -1623.1796875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -2447.160888671875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1537.769287109375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -47483.7421875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -1213.72265625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -3477.280029296875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -6027.81787109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -1600.9951171875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2817.900390625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -5662.7587890625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -21313.5390625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1833.8197021484375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2192.073974609375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -4027.1162109375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -2457.189453125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -1557.7496337890625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -19517.146484375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.757, loss_val: nan, pos_over_neg: 911.9408569335938 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 6376.787109375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: 42390.79296875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1554.843505859375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -4446.28955078125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -65075.86328125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 1586.392578125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2309.839599609375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -5571.943359375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: 9041.478515625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: 4304.1376953125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: -2787.489501953125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 11976.193359375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -8250.3798828125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: 1179.68896484375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -91714.234375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -7693.37841796875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -5235.16552734375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: 9486.8037109375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1551.1707763671875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -2170.67138671875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -6147.849609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -2966.98681640625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: 2732.253173828125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: 10279.009765625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: 2365.26513671875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 1945.7684326171875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2794.8173828125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -1914.302001953125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -4733.9755859375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: 19216.4140625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2309.371337890625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -6438.5029296875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2418.87744140625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 4217.642578125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: -12158.5869140625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -3583.353515625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 1559.423828125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: -20935.890625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -2584.1513671875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -2445.82763671875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2150.12646484375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -2060.616455078125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -3181.703125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -2211.885009765625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 3908.664306640625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -5500.77587890625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7589, loss_val: nan, pos_over_neg: -3069.043701171875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -3276.927734375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -13607.2880859375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2118.968994140625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -3594.819091796875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -13661.064453125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 1847.97998046875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -17042.28125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2216.48583984375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2021.0753173828125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -4154.58544921875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2423.455078125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1306.5650634765625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7584, loss_val: nan, pos_over_neg: 3102.17578125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 1759.6768798828125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -367738.875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2627.1376953125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2425.649658203125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -5777.60205078125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -2168.821044921875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: -3652.75244140625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -5184.20068359375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -3153.944091796875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2257.68701171875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: 9294.25390625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -6798.05810546875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: 29362.638671875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: 22868.953125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -15786.9677734375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -2859.67529296875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -11820.8876953125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -1738.31103515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -4064.051513671875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7592, loss_val: nan, pos_over_neg: -4959.73583984375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: 1660.3079833984375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -2740.662109375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1177.2415771484375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -4209.02978515625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2162.894775390625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -3799.914306640625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: 6027.4765625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -26772.373046875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: 6505.6279296875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -4467.4697265625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2071.371337890625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -3152.440673828125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -1422.8857421875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -2021.83642578125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -2248.202880859375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -3662.5546875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: 11946.2841796875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -66895.4296875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -3774.083984375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: 8817.708984375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1856.9376220703125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -1706.80126953125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -1640.7080078125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -3681.6552734375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2107.37451171875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -1744.7513427734375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2655.434326171875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -1437.1597900390625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -4210.5791015625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2568.810546875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -2225.486328125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -2162.240966796875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.7598, loss_val: nan, pos_over_neg: 2355.32177734375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -23283.388671875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -4890.76708984375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1778.0078125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -1588.1654052734375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -2715.151123046875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -2142.1884765625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -4606.58984375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -1617.6798095703125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1536.775146484375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -3064.892822265625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -5311.951171875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -2975.003173828125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -6527.130859375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -2501.626953125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2425.7802734375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1646.6805419921875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -3547.091064453125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -4077.9521484375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -6812.95751953125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -162328.890625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3044.84814453125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.753, loss_val: nan, pos_over_neg: 22190.40234375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -3654.025390625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 11933.98046875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -121895.0859375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2159.541748046875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -2086.82275390625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2670.823974609375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: 5718.587890625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -3684.416015625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -1340.16064453125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -2076.891357421875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -1605.1231689453125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -2103.681640625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -2942.54931640625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: 2359.715087890625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: 985.2935791015625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -4728.97607421875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -3522.2255859375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -3952.9765625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -1258.682861328125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -2059.013916015625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7593, loss_val: nan, pos_over_neg: -1601.025634765625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: 6140.0712890625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -2303.768310546875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -10937.25 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -3447.118408203125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: 29515.7265625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -3919.336669921875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -2829.395751953125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1968.25146484375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: 6910.63232421875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -6787.77783203125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: 40194.97265625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -5390.61865234375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -10235.783203125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -3980.511962890625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.753, loss_val: nan, pos_over_neg: 17360.34375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -1762.1962890625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2867.399658203125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.754, loss_val: nan, pos_over_neg: 3545.435302734375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -3163.928466796875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: 6289.96923828125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.753, loss_val: nan, pos_over_neg: 2326.28564453125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -10172.6611328125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2384.18994140625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2072.504638671875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -5205.88671875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -8606.12890625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -12630.4365234375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -3369.6669921875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -8801.7578125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -3457.33349609375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -6146.751953125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -3325.45751953125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -3876.843017578125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7574, loss_val: nan, pos_over_neg: -4655.0302734375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: 4956.859375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: 191630.40625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.752, loss_val: nan, pos_over_neg: 27394.7578125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2608.398193359375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2501.993408203125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2058.136962890625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -1634.1671142578125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -2478.40478515625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -5303.68359375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -20398.974609375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: 9377.80859375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -3585.817626953125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -4819.03466796875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -2109.75244140625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -2619.718994140625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -1689.4307861328125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2009.743896484375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -8601.4716796875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2659.0537109375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -1750.9710693359375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -1918.206787109375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.7599, loss_val: nan, pos_over_neg: -1769.0247802734375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: 4999.4423828125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: 4860.62548828125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2525.239501953125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1713.9119873046875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -3591.93408203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7601, loss_val: nan, pos_over_neg: 4851.02880859375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1529.708740234375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -1821.5577392578125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -2423.18408203125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -2338.165283203125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -1501.08447265625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2292.42822265625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -7315.80322265625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2013.344970703125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -4053.010009765625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -2038.980712890625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -5125.1923828125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -4824.25634765625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -3345.663818359375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2713.619384765625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -15253.6357421875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -3779.822509765625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7608, loss_val: nan, pos_over_neg: -2282.856201171875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -3003.708251953125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.754, loss_val: nan, pos_over_neg: 10926.1474609375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -3596.001708984375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: 10816.2587890625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2177.478271484375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: 2573.9189453125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: 22991.658203125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -20722.9609375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -1438.106689453125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -4789.865234375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 3943.465576171875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -2415.82177734375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -2238.597412109375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -4375.81201171875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -3584.677734375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -3311.615234375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -12494.97265625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: 2311.392822265625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2694.1630859375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -3081.49609375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2949.689453125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -6131.9609375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: 19948.609375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -3595.0029296875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -1840.130859375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -2863.358154296875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -3263.82763671875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 11193.140625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -3380.399169921875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2265.49072265625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -7366.07568359375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: 6603.10205078125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2823.072265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -13665.8203125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -1813.8553466796875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -2309.292236328125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -2215.8798828125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -9497.1611328125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -3253.77978515625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -1383.00146484375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -24857.8125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -6392.2041015625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -2070.3369140625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2329.38525390625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1807.14990234375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -2712.15869140625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -1976.9654541015625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -2420.31787109375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -1889.07373046875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2073.51953125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -1793.137939453125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -5738.1962890625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: 2375.8974609375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -58802.82421875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -1507.9443359375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2357.10009765625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2384.015869140625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -4846.67626953125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: 15840.5693359375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2802.65869140625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -3160.330322265625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -3520.57177734375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -5594.7978515625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -6632.388671875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -2858.25146484375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: 2234.662353515625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: 9035.9560546875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: 11909.927734375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2646.644775390625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.753, loss_val: nan, pos_over_neg: 4798.08447265625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1733.116943359375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2162.709228515625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -1312.8739013671875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -2201.81201171875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -7167.58154296875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: 3247.9033203125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -4335.10595703125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -3036.701171875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 2383.53857421875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 9531.4912109375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -2363.344970703125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: 4872.80859375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -2016.224365234375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -3612.119384765625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -27484.939453125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -4203.41650390625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2405.014892578125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -10006.880859375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -9838.859375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: 3684.833251953125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -2106.596923828125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -3878.804443359375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -12351.8603515625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -2156.146240234375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 27443.779296875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -3405.764404296875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7578, loss_val: nan, pos_over_neg: -10003.6748046875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -3461.96630859375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: 1180.24755859375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -7283.63427734375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -1439.9736328125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -1655.280029296875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -92592.0859375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: 2601.64794921875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -9237.9921875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -2774.357421875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -10227.8837890625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.756, loss_val: nan, pos_over_neg: 10157.197265625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1995.510498046875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -2830.64501953125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -3306.311767578125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3530.662109375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -10237.6865234375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2389.23486328125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: 4232.67333984375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1835.1551513671875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -1777.09912109375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2252.966552734375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: 95897.859375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: 101695.4296875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -6525.61376953125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -3013.539306640625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -4163.7138671875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2085.94384765625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -3220.41015625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2131.7568359375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.754, loss_val: nan, pos_over_neg: 14193.9716796875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -4052.595947265625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2826.06298828125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -1251.10986328125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -1514.92236328125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -5130.2744140625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.757, loss_val: nan, pos_over_neg: 2850.90576171875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -1395.322265625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -3119.47021484375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: 3606.27685546875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2903.3603515625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2644.24365234375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2256.596435546875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -1792.249267578125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -5121.54052734375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1993.62158203125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -20955.5 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: 2370.03271484375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: 16676.75 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: 23245.2890625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 8575.6337890625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2112.66259765625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -1716.327880859375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -7825.18701171875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 5345.3916015625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: 15395.1865234375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1423.6717529296875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 1248.3985595703125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -7426.07958984375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: 3499.98193359375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -3418.2529296875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -3084.798095703125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 5712.99267578125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: 4972.6455078125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -4467.1669921875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -2378.406494140625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2220.941162109375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -7240.4658203125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -2196.03564453125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7581, loss_val: nan, pos_over_neg: -7630.58203125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: 9874.083984375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -3719.199462890625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: 3010.2802734375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 70632.453125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1770.32568359375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2559.793212890625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -9593.5869140625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -3023.44580078125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: 369242.21875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: 3773.554443359375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -9124.1259765625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: -3190.15576171875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: 6528.2587890625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1808.774169921875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -17422.046875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: -4374.50146484375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2151.492919921875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1840.514892578125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: 188209.8125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -5026.994140625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: 3425.55029296875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: 7193.5537109375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -6091.55859375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -2483.854248046875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -2813.0166015625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2129.948974609375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -1794.019775390625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1809.9876708984375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -3886.287353515625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -4030.503662109375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -1990.915283203125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2075.577392578125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -1491.031005859375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -1911.35498046875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -16722.146484375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.751, loss_val: nan, pos_over_neg: 3015.11865234375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 4451.427734375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3491.331298828125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -7597.26025390625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: -20758.87109375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2342.569580078125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -1654.3887939453125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -6744.08935546875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -6363.4658203125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2970.98583984375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -11889.91015625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1928.807373046875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -4803.4775390625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: 6643.91357421875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -18924.37890625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -37447.078125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7566, loss_val: nan, pos_over_neg: 8345.294921875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2599.06689453125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1942.20458984375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -5638.783203125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -6052.20361328125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -3847.728759765625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [20:04<14304:46:47, 171.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -21141.17578125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -14238.4521484375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 5565.30322265625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -3372.30908203125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -1773.83056640625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3210.351806640625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -2106.831298828125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1829.504638671875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -1727.241455078125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -3214.711669921875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7622, loss_val: nan, pos_over_neg: -6695.36865234375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -3097.40625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -5377.810546875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -962507.625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -6899.74755859375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -1517.189453125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3336.794677734375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1760.0738525390625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -1867.341552734375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -11498.748046875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2144.244384765625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -2488.984130859375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -3618.31494140625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -3169.12451171875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1519.0704345703125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2122.345703125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: -3018.95849609375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -2003.5201416015625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -9362.037109375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -1830.9830322265625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -4657.626953125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -5178.5791015625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -40928.6484375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -7823.61572265625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2324.458251953125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -3738.31689453125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -3994.645263671875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: 4224.25341796875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -4358.89599609375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2546.0849609375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -4480.80419921875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.751, loss_val: nan, pos_over_neg: 3561.77587890625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -3822.486572265625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -3890.1748046875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -4499.75927734375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: 3173.272216796875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -2260.736328125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 33299.890625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -6434.54541015625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -3240.727783203125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -4286.86181640625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: 12136.3984375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -4853.77197265625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -15616.8310546875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2317.26123046875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -3758.261962890625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2286.19921875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -10726.8369140625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: 6772.14501953125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7577, loss_val: nan, pos_over_neg: 12463.7705078125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2998.244140625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -4930.55908203125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -2383.857421875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -3170.37109375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7562, loss_val: nan, pos_over_neg: -2502.1826171875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -4152.140625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2620.327880859375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: 3835.04443359375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -2802.791259765625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: 6254.931640625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1723.3419189453125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1654.501708984375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -1373.716552734375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 22985.46875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: 5350.54736328125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -2309.496337890625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -4230.50927734375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -3882.4404296875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -4685.28955078125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: 7622.1103515625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -2080.93994140625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -5660.8095703125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -2730.998779296875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: 19067.7890625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -2436.76708984375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2731.57177734375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 2118.0771484375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -9149.306640625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -3154.627197265625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2959.6865234375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -4719.64404296875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2796.46337890625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -4844.80810546875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2063.09130859375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -9829.056640625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -14435.748046875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: 3679.455078125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -6200.59716796875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -1734.42041015625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -6760.49658203125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -6102.93212890625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2793.966064453125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 5334.05078125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7583, loss_val: nan, pos_over_neg: 2557.22900390625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: 1688321.0 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -4160.78369140625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2273.709716796875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1997.2562255859375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -1571.064453125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: 20143.4375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -3431.79248046875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2981.943115234375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7573, loss_val: nan, pos_over_neg: -1740.0013427734375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: 57606.58984375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -34555.94140625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: 3516.0029296875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2225.118896484375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -2230.887451171875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -7705.6787109375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -4280.6416015625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -3150.745849609375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -6629.24267578125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: -3729.348876953125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -1528.035400390625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: 45328.05078125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -16743.73046875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -1944.5985107421875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: 6477.3203125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 2286.137451171875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: 2175.733154296875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1845.4644775390625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: 9103.6220703125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -2070.242919921875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -4619.3583984375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7576, loss_val: nan, pos_over_neg: -2686.425537109375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -1429.107421875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2686.852783203125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7563, loss_val: nan, pos_over_neg: 1446.408203125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: 10310.39453125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -2676.95751953125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -4472.84716796875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -3255.666015625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -3371.77734375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -3486.0556640625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1819.468994140625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: 4222.08984375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -15556.8876953125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -3181.3125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 2378.222900390625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -6807.10693359375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: 9266.6728515625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -6814.2509765625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -7104.373046875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -4475.92529296875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -3118.5087890625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -1412.36767578125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -3146.86865234375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7561, loss_val: nan, pos_over_neg: -9759.583984375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -1691.587890625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -7265.9921875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -20004.88671875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -4205.078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2644.763916015625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -26577.619140625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2128.3466796875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 5042.26513671875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -2482.75830078125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: 1895.8033447265625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.752, loss_val: nan, pos_over_neg: 4797.14306640625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: 1409.6058349609375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: 14240.904296875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -5191.44384765625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 41271.34765625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -16927.51171875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -3023.850341796875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1640.97119140625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: 9536.7685546875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -21834.87109375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: 8949.3232421875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -2467.552978515625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -30095.390625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -4798.68505859375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -3234.55419921875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2989.1611328125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -13252.4052734375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: 8208.4951171875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -5203.984375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1840.04345703125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -2733.75830078125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -1792.6754150390625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1509.682373046875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -1383.6488037109375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -4299.29248046875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: 5978.68603515625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: 2454.900634765625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -2953.208984375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -6597.90771484375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -2484.812255859375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1329.5181884765625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1692.7596435546875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -1407.5162353515625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3718.694580078125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -2676.69384765625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -7537.66552734375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 1486.440185546875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4579.02685546875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: 3884.81494140625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -3410.101318359375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2638.637939453125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -2408.995361328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -1558.074462890625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -2066.80908203125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -4058.822998046875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2411.065185546875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2026.6396484375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.75, loss_val: nan, pos_over_neg: 3028.63623046875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: 7918.673828125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 6110.02099609375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -1728.8587646484375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1854.609619140625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -8910.5302734375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -1585.7843017578125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -3561.933349609375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: -1509.45068359375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2229.947021484375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7568, loss_val: nan, pos_over_neg: 4383.50341796875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -4744.94091796875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -3163.95654296875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -4404.7919921875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: 2082.048095703125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -4895.26220703125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2026.163818359375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -3347.8837890625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: 12972.91796875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -5470.09521484375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -3305.228515625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -5756.486328125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: 21020.951171875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2183.164794921875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1645.968994140625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -4168.53857421875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 3326.25146484375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -242635.828125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -5949.7705078125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -4462.8125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: 20301.23828125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1603.396728515625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -1575.134765625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -2006.994384765625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -6100.05126953125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.752, loss_val: nan, pos_over_neg: 6897.140625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -4988.92333984375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -45747.7265625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -7615.2001953125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -2559.957763671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -4623.39453125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: 27023.916015625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -2442.471923828125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2812.512939453125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -7687.849609375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -6550.2646484375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -3254.544677734375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: 6693.4521484375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1882.590576171875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -4698.2197265625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -3653.6953125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1811.6260986328125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -2834.9521484375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -2023.7843017578125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -2057.5517578125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -2255.0068359375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -8533.9404296875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -4620.3642578125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -2453.544921875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -7108.41650390625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -4393.3642578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -1687.1771240234375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -1657.8865966796875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -11102.177734375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 1936.630615234375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -4494.61328125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -1194.6126708984375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: 4265.7822265625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -2938.75634765625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -14300.1005859375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -3798.90234375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -3250.907958984375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -5338.8134765625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -47378.8359375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -5490.83935546875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7567, loss_val: nan, pos_over_neg: -2581.805419921875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -3164.167236328125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -15692.255859375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: 2485.30419921875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -9250.0712890625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -2362.496337890625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -2630.3193359375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -2205.934814453125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -2028.3193359375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1737.475341796875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -1825.4801025390625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2035.4638671875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -7071.537109375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -6252.16796875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -4957.634765625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -1577.6026611328125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -5402.7578125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: 3443.601318359375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -6136.68359375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1746.2822265625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -1300366.5 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: 7408.4423828125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -5318.07177734375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 5533.81982421875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -6395.89404296875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1948.686279296875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -1553.947509765625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -11028.9345703125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: 17609.955078125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -8887.8291015625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1616.8804931640625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2918.67724609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -3022.507568359375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 9622.1015625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 8431.8125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -1817.8631591796875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -1595.7825927734375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -5422.55078125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: 2735.601806640625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3080.1650390625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -2346.20947265625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -5801.73681640625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: 2706.56494140625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: 2366.245361328125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: 65025.2578125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -5208.5576171875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: 2485.6025390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -2388.248291015625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3431.690673828125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -1485.3779296875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -2431.164794921875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -7725.6015625 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: 31928.173828125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7585, loss_val: nan, pos_over_neg: -6344.99267578125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: 3715.7490234375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -3388.191650390625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -3063.972900390625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1236.0142822265625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -3248.455810546875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -3281.470458984375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2383.672119140625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -3412.659912109375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -2750.23974609375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2682.60595703125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -4357.26416015625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7565, loss_val: nan, pos_over_neg: -3459.062255859375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -31879.34765625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: 7409.31689453125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2367.1591796875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -2322.387939453125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -2130.20654296875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -1268.44873046875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: 178504.09375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1554.431396484375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -1399.8330078125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -2308.955322265625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2974.25146484375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: 18816.4609375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: 3287.510986328125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2667.777099609375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -3370.896728515625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -1814.6402587890625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1519.6236572265625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.753, loss_val: nan, pos_over_neg: 8397.5224609375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -4987.62548828125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -2585.414306640625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -6626.3427734375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.759, loss_val: nan, pos_over_neg: 1300.1761474609375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2985.390625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2263.739013671875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.749, loss_val: nan, pos_over_neg: 72920.859375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1792.831298828125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 107530.25 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -3716.261474609375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1929.4276123046875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2286.85986328125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -4672.24560546875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -11249.498046875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -2752.763427734375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -3507.874267578125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: 21443.05859375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1641.4290771484375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -2985.774658203125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -35229.47265625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1635.44921875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -5280.5810546875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -3050.482421875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1747.0357666015625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1593.0811767578125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2002.5987548828125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -3226.23681640625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -3458.010009765625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -5162.95703125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -9511.076171875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4088.45654296875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1505.8675537109375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: 8329.94140625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -1482.6741943359375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -4476.81494140625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2138.061279296875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -3842.43896484375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: 25527.37890625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: 16693.83984375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -284960.96875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: 7070.8466796875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -1727.105712890625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1770.0594482421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -3544.3818359375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1588.19091796875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: 24900.326171875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1520.069580078125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: 4338.79833984375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: 3135.22216796875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 5954.1640625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -4616.0712890625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -2319.772216796875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -3079.898681640625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -5604.41064453125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4065.665771484375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -1555.51318359375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -3419.906494140625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: 11934.755859375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1489.3399658203125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -2549.971923828125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2122.575927734375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2148.281982421875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1857.4320068359375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -24100.236328125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -1860.741943359375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2262.640380859375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -2212.170166015625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -1796.4736328125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1543.713623046875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1474.80810546875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1480.7388916015625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2184.4873046875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1610.7408447265625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1837.810791015625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1906.999755859375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -5093.98974609375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2993.059814453125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -4868.74609375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7553, loss_val: nan, pos_over_neg: -3873.719482421875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -8177.15869140625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -2941.32958984375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -19140.0 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1793.96142578125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 8496.8671875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -1575.6029052734375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1608.6636962890625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -1538.7122802734375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2694.627685546875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -4653.712890625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: 11772.9619140625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -3489.14501953125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 1082.3145751953125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.759, loss_val: nan, pos_over_neg: -2833.1748046875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2828.1572265625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -1679.2469482421875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -2441.219970703125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2381.457763671875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -1739.1417236328125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -1862.951904296875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -5583.90478515625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -1448.1611328125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1698.5245361328125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -10262.0341796875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -21643.48828125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2049.053466796875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7604, loss_val: nan, pos_over_neg: -2589.070068359375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -3039.966552734375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1950.9471435546875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -7456.95703125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -39852.31640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: 1895.6192626953125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7569, loss_val: nan, pos_over_neg: -9805.5732421875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -1506.5665283203125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -1655.4910888671875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -4043.971435546875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2044.200927734375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -22507.111328125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -2344.867919921875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -7719.37353515625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 4655.81103515625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1620.631103515625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2485.3818359375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -37851.9296875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: 11023.525390625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -2016.4705810546875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2033.149658203125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -3372.173583984375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2053.00146484375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: 2705.502197265625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -1695.4188232421875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -9312.9345703125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -1809.636474609375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: 5244.06640625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -3023.415771484375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -2967.421630859375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -1591.3572998046875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2216.69140625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -1557.5936279296875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -4812.56689453125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -5649.646484375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: 15903.958984375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1740.9677734375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1752.8292236328125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7582, loss_val: nan, pos_over_neg: -5553.0244140625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: 40874.6953125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: 3992.340576171875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -4889.53076171875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -5378.46875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -7647.69677734375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -1896.6156005859375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -1647.5291748046875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1697.75244140625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -4309.46728515625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: 36118.1875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -42010.9921875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1210.3150634765625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -2546.31884765625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -4896.31005859375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2018.5054931640625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2252.41845703125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -9070.984375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -3857.190673828125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -4016.38427734375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -3858.5751953125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -9582.62109375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: 7895.314453125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -1440.1639404296875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7572, loss_val: nan, pos_over_neg: -3299.076171875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1988.9180908203125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -1443.9776611328125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2055.15673828125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -1729.852783203125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: 2535.787109375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1915.7786865234375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -3612.18994140625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -4903.6640625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7588, loss_val: nan, pos_over_neg: 5014.5712890625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -1531.59814453125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1854.4444580078125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2155.590576171875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: 14275.2880859375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1837.377685546875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -3801.61083984375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -2364.885498046875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -5912.369140625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2232.338134765625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -3980.114990234375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: 2060.78759765625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.752, loss_val: nan, pos_over_neg: 1680.1436767578125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -3397.8603515625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1881.02392578125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.751, loss_val: nan, pos_over_neg: 5934.4052734375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -10899.666015625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1886.9761962890625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -326552.96875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: 9553.392578125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -3319.561767578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 9161.201171875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -10219.7705078125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: 5044.775390625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -1805.579833984375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -1771.6014404296875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -72783.9375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -5572.89306640625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2983.567138671875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: 5544.177734375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2702.727783203125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2633.82080078125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -6596.310546875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -2895.3935546875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: 2585.40771484375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -2484.52392578125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2399.130615234375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -1548.281982421875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2129.798095703125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -1720.75927734375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -3682.581787109375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -2937.0556640625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -5754.79150390625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -9188.830078125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2404.253173828125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -3064.31689453125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -2245.662841796875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -10199.7099609375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1590.0955810546875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1692.06787109375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2120.47314453125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -11619.02734375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: 3225.851806640625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -14929.78515625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -1823.9346923828125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1947.426025390625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2236.805908203125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -3036.083251953125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2600.322265625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -17071.095703125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -2620.449462890625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -2837.297607421875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -2341.326904296875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -2174.0986328125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -2353.006103515625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -4546.84814453125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: 13758.28125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1684.670654296875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1582.83203125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -1336.4334716796875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -4546.7373046875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -13295.9580078125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -5254.77099609375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: 39910.0859375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -4375.55224609375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -6724.419921875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -18774.025390625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -4183.7373046875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -5020.4189453125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -3769.50537109375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: 1321.0787353515625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -4173.09814453125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7591, loss_val: nan, pos_over_neg: -14033.19140625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.751, loss_val: nan, pos_over_neg: 7288.009765625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -4185.25390625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 128103.84375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2317.7470703125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -15347.9482421875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2878.765625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -4068.46484375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -7686.2548828125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2268.539306640625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3352.28515625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -3430.331298828125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -7680.53125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2896.44580078125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -2756.63427734375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -3488.418212890625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -1686.3677978515625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -3338.84765625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -9284.015625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: 8859.5302734375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: 10558.37890625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -4177.04736328125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1412.8211669921875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -2210.2177734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -2762.390380859375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2132.199462890625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2767.62744140625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -2627.34033203125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1812.177978515625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -2050.13330078125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -1483.10986328125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -2654.416015625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7564, loss_val: nan, pos_over_neg: -1432.07470703125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -6033.6826171875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -3223.4306640625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -7835.400390625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -4584.712890625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -5509.31494140625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1384.6796875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -3067.5810546875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1304.8719482421875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7551, loss_val: nan, pos_over_neg: -5526.53466796875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -2717.34130859375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2638.051513671875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -2732.9033203125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.753, loss_val: nan, pos_over_neg: -2048.186279296875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1750.6744384765625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: 3735.962158203125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -4573.1376953125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -2945.4091796875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1207.3323974609375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -3155.41650390625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -1563.892822265625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -1461.456787109375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: 5124.55224609375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: 4787.599609375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2608.966796875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -3065.52587890625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2174.501953125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -2807.415283203125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1895.8916015625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: 1746.658935546875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2747.37060546875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [22:56<14305:24:20, 171.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3160.866943359375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -4416.4765625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2307.8212890625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -3089.967529296875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -2123.34716796875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2953.25048828125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -100186.875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: 22857.185546875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -2548.52734375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -2307.892578125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -2622.5703125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -5495.1474609375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -1758.7064208984375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -1257.588134765625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -1551.4989013671875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -11084.0673828125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -2978.68798828125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -2004.8126220703125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1386.4388427734375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -2704.071533203125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -16468.904296875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -5150.8115234375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -5944.23388671875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -3902.017333984375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -2572.321044921875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2363.31591796875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -2009.2156982421875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -2041.6644287109375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7545, loss_val: nan, pos_over_neg: -2154.69091796875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2530.201171875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: -2214.10107421875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -5112.97412109375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: 1165.774658203125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -18758.15625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -6371.68798828125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3753.417236328125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -1908.4273681640625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -2902.986083984375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -1701.468017578125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -1589.1912841796875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2536.028076171875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -3262.58837890625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 4324.67724609375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: 18487.849609375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -6367.83984375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -3391.031005859375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -2192.59228515625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -3147.869140625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7579, loss_val: nan, pos_over_neg: 5066.9453125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -1373.562255859375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2923.999755859375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -2627.681884765625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: 8587.349609375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1742.968994140625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -1905.044921875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: 23010.791015625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2800.6435546875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -2309.818359375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -3630.88330078125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -4109.0146484375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -3489.0673828125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -11754.3623046875 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -2970.63037109375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -1540.494873046875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2174.08056640625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: 12585.6689453125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: 2594.4404296875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.753, loss_val: nan, pos_over_neg: 7609.54248046875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2219.046142578125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -3464.929443359375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -78459.0 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -7609.4755859375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1850.1822509765625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -1802.284912109375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7587, loss_val: nan, pos_over_neg: -9924.2294921875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -3381.513916015625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -74663.578125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -2585.58349609375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 7228.11572265625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: 1591.688720703125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -1813.78076171875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2043.3499755859375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: 9066.830078125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -4187.26708984375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1644.594482421875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -2067.418212890625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: 1691.041748046875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.75, loss_val: nan, pos_over_neg: 25306.466796875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -2418.480712890625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: 4272.4189453125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: 6720.0234375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2456.669189453125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -6308.98291015625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -4056.884521484375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -6554.9833984375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -7775.78955078125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: 10622.2041015625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -15896.552734375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -5615.2001953125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7541, loss_val: nan, pos_over_neg: 6854.4296875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: 2819.975830078125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.755, loss_val: nan, pos_over_neg: -2772.789794921875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -7192.72216796875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -2301.403564453125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2274.03271484375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: 4746.54931640625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -5664.01611328125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -2796.962890625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -2310.9794921875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -8619.3349609375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -1960.69970703125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2259.82861328125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -1715.114013671875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -2195.165771484375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2284.8203125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -4486.916015625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.752, loss_val: nan, pos_over_neg: 62087.0546875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.753, loss_val: nan, pos_over_neg: 48824.48828125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -15318.0986328125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -2319.482666015625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: -1556.251953125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1401.973876953125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -3890.470458984375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: 4878.98828125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: 14117.8505859375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -3558.916015625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -1752.684814453125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -1736.601806640625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -5617.74072265625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -4177.8310546875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2282.836181640625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -1823.6783447265625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -1500.40283203125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -3468.43798828125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -29879.375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.751, loss_val: nan, pos_over_neg: 4401.37158203125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2931.347900390625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2319.983642578125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -3650.9833984375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -12021.734375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -4337.810546875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2954.087890625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: 5337.9345703125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -4971.861328125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1951.7958984375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -4574.69287109375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1349.922119140625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -2414.357421875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2879.95166015625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -183393.046875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -1936.582763671875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: 43120.671875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -3076.5947265625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7411, loss_val: nan, pos_over_neg: -1735.830078125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -2268.112548828125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -4587.4228515625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1725.05224609375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -1599.8809814453125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: -1959.9466552734375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2142.924072265625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: 29024.6953125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 3847.414306640625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -11574.880859375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -5015.9677734375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: 10460.640625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7556, loss_val: nan, pos_over_neg: -2140.414794921875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: 5572.65869140625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -1698.1524658203125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -1965.346435546875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -2566.972412109375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -4506.2822265625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: 15460.0908203125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -294480.1875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 4888.2900390625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -6453.421875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2057.655029296875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1484.7005615234375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2665.253662109375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3724.470458984375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -9557.615234375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -69325.234375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -8193.90625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -3069.60009765625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: 314550.0 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -1818.964111328125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -1495.45751953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2556.592041015625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2408.94384765625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -3173.528076171875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -2633.47021484375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1891.5047607421875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1354.23291015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1728.6683349609375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -304690.96875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -5697.56787109375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -2578.70751953125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.748, loss_val: nan, pos_over_neg: 46321.3359375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -4640.37890625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -4859.373046875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -2193.909423828125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2816.69287109375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2836.901611328125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -4037.09033203125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -1619.3153076171875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -4005.0166015625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -1548.1087646484375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: 5216.59130859375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7424, loss_val: nan, pos_over_neg: -2804.90283203125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2742.68505859375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -8208.982421875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2476.86376953125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -3771.9794921875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: 3844.6435546875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -12140.1513671875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1513.8037109375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1589.7242431640625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -8574.4990234375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: 3135.126220703125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -3596.83642578125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: 4508.5126953125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: 2426.24169921875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -3992.118896484375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -4290.6708984375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -4025.169189453125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -3369.390869140625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2549.911376953125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2967.19384765625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -24897.0546875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -1881.5753173828125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -2204.307373046875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -2024.23388671875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -1559.04638671875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -3536.8134765625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -42268.125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -2344.548095703125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2408.697509765625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -1312.6256103515625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1616.2054443359375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -6351.236328125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -5288.5 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -3299.0126953125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -19683.005859375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -3051.68798828125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2102.67822265625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7571, loss_val: nan, pos_over_neg: 45169.41796875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1394.8516845703125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2590.096435546875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1449.97998046875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -1308.8321533203125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -1867.4215087890625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: 41661.296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: 5364.80126953125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2065.341552734375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1806.04638671875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1708.0042724609375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1864.124755859375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: 8093.27685546875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -2221.779052734375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -6108.94482421875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7413, loss_val: nan, pos_over_neg: -2059.900390625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2481.85791015625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2383.31591796875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -4465.25048828125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -2289.59619140625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4842.02978515625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -2405.51123046875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -1290.4422607421875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1575.3486328125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2647.477294921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -1917.5250244140625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -1837.049072265625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: 21645.37109375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7586, loss_val: nan, pos_over_neg: -256173.796875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -3546.362548828125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7575, loss_val: nan, pos_over_neg: -2484.878662109375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -6743.62548828125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2187.16015625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -1613.1258544921875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -1873.6239013671875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -2175.301513671875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -25989.36328125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -3788.23046875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -1942.150146484375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -2250.146240234375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1559.412109375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -13669.3203125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2053.62109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1913.6737060546875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2648.92724609375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -3326.412353515625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -4304.16162109375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -1895.375732421875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -2417.48779296875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -4686.63671875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -3303.631103515625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: 4204.59228515625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -1940.6243896484375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2207.72802734375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -7245.88916015625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1729.1024169921875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1909.24072265625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -2085.605224609375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -1667.7652587890625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -2752.985595703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -1645.29052734375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -2109.12744140625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -3541.94384765625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -1882.5997314453125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1988.77001953125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -1949.59814453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 19227.689453125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -3463.142578125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -10105.296875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -14257.8876953125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -5796.263671875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -3827.341552734375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: 5893.4150390625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: 2356.124267578125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2989.90869140625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -3139.052001953125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -21061.6328125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: 2013.5648193359375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: 2097.513671875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -8416.9609375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -2849.98291015625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -3949.16796875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -6641.51123046875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -3043.052001953125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -4586.24951171875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -16273.630859375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -3295.737060546875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: 234673.703125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -5444.23828125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: 4085.44580078125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -4251.6943359375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -1515.443115234375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -1767.551025390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -3709.837646484375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: -4151.52099609375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: 29281.8671875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -1877.7303466796875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1958.2220458984375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: 4884.43603515625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -5077.44482421875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -4107.88525390625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -14718.8525390625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -4922.3251953125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -3306.859130859375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.741, loss_val: nan, pos_over_neg: -2320.9482421875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -1542.958740234375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2688.1845703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: 19925.740234375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -3707.181884765625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 2658.43212890625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -6113.6220703125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -3823.23095703125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -1912.5592041015625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -1605.015380859375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -4418.375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -1659.51025390625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -1840.589111328125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -8630.908203125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -6433.37939453125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -4049.74072265625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7544, loss_val: nan, pos_over_neg: -2641.300537109375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -1636.191650390625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -5324.583984375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: 86674.5703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -5678.62841796875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -1683.03955078125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -4978.52392578125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -15484.255859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -5700.69921875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -3171.8251953125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -8132.283203125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: 3793.309814453125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2027.3797607421875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1537.2645263671875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -85628.53125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1381.7764892578125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2336.778564453125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -2307.10302734375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.748, loss_val: nan, pos_over_neg: 36633.90234375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -3199.016845703125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2911.724853515625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -13578.3720703125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -2478.4541015625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -2758.7412109375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -3980.1220703125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2223.66162109375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -18039.515625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -7260.3115234375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -91691.78125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1467.3448486328125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -1246.626953125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -4195.0126953125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -1846.24951171875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -30533.98046875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -8709.8701171875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -3079.885986328125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -4653.05419921875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -37212.390625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -1402.02734375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1666.018310546875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2062.539794921875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1704.3436279296875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2767.0146484375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.744, loss_val: nan, pos_over_neg: 9803.2802734375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: 19072.38671875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2598.592041015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -4938.93505859375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: 12438.2958984375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -4227.59130859375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: 15437.9990234375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -2067.4638671875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -1799.723876953125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7546, loss_val: nan, pos_over_neg: 1877.0655517578125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: 3176.90869140625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -1703.3585205078125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -3696.628662109375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -3451.003662109375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2816.931640625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -3489.7080078125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: 4592.59375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -4624.9345703125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2097.066650390625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1868.924072265625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -1479.7996826171875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -4420.25 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -2462.697998046875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1988.89892578125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -8205.3115234375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2692.4619140625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -1729.8570556640625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -2734.487060546875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -2132.228271484375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -7934.37109375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2851.57470703125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3618.032470703125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -2883.001953125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.756, loss_val: nan, pos_over_neg: -76868.9375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: 19328.453125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: -5547.94482421875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1384.5938720703125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -6798.40673828125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -21316.654296875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: -21455.95703125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -3560.60888671875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -4993.72607421875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -6764.04150390625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -3054.1982421875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1809.6397705078125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -3976.07470703125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2237.680419921875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2939.473876953125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2584.02880859375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: 3899.6943359375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -8680.29296875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: 3724.510986328125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3160.31103515625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1781.7144775390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7547, loss_val: nan, pos_over_neg: -3451.530029296875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -3275.7822265625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2587.288330078125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -1546.1427001953125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -5099.02392578125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1776.0228271484375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1888.0068359375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -1572.6837158203125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2570.275390625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2027.275146484375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -2069.31298828125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -3037.679931640625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -1999.2364501953125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1973.7783203125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -1491.816650390625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -7208.666015625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: 4087.738525390625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -12967.0712890625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2891.05322265625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -2189.23193359375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -2007.260498046875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2007.9656982421875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4515.12060546875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -3895.636474609375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -6593.5546875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: 5480.1669921875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -4567.5576171875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -1650.667724609375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1739.812255859375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -2094.52197265625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -1298.6763916015625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1312.1676025390625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -2263.829345703125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: 10082.119140625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: -2334.6552734375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2746.259521484375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -3538.916259765625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2870.586181640625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -5511.3203125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2099.482421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: 11090.0673828125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: 3514.316162109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1988.3883056640625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: 4806.796875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2977.466552734375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2325.356201171875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -2554.890625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7402, loss_val: nan, pos_over_neg: -2949.865478515625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1907.327392578125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1714.456787109375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1936.3272705078125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: 3361.895263671875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1913.2474365234375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -6308.60107421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -3478.56005859375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -1593.809814453125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -2607.5185546875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -6082.32177734375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -4793.40478515625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7555, loss_val: nan, pos_over_neg: -1810.9149169921875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: -2167.517578125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -16008.0947265625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1776.9071044921875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -5181.2158203125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: 9372.783203125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -2145.014892578125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.752, loss_val: nan, pos_over_neg: -2929.658447265625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -9044.025390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -2126.9833984375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1502.8975830078125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: -1478.9495849609375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1900.6317138671875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -2686.708251953125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -2239.910400390625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3086.9609375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -1377.068359375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2580.312255859375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7548, loss_val: nan, pos_over_neg: -3642.68896484375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -1756.2720947265625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 5116.98876953125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -1707.485107421875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -3422.1923828125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -3947.923583984375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3190.623779296875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1809.2861328125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -1692.7071533203125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -3252.621337890625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -1723.523681640625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1906.4830322265625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2010.3753662109375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1607.540771484375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -81157.5546875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -2069.37841796875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -2607.87646484375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1883.6859130859375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -2269.86865234375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: -3529.9443359375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -3182.531982421875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -2909.127197265625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1681.6719970703125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -4570.45849609375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -2701.95556640625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7419, loss_val: nan, pos_over_neg: -2354.23193359375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1975.0389404296875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1769.680908203125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -1451.975830078125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -3651.144775390625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -3721.964111328125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -1738.0897216796875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -2694.678955078125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -4247.955078125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -3588.373046875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -2988.3583984375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -7983.5791015625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1898.960205078125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2141.951904296875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -5627.8203125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -2421.056396484375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 4969.01953125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -4088.24267578125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -5807.89306640625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: 14297.65625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -1583.857666015625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: 30654.98046875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -17757.349609375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -3041.251953125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -1675.9420166015625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: 2850.577392578125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1441.49755859375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2017.692138671875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -3493.068359375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -2278.487548828125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1816.482177734375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: 5348.37158203125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -2689.5625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1711.625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -3252.234130859375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -1788.2069091796875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -26176.96875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2019.7413330078125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -4784.3212890625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -2453.79150390625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2899.155517578125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: 8627.96484375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: 7877.81298828125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: -26513.982421875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -7044.29541015625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -1629.6727294921875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -1478.5126953125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 2193.614501953125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1945.14501953125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1799.1868896484375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -4000.17236328125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4448.0087890625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1718.6842041015625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -3654.87158203125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1852.2880859375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1593.6307373046875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -6757.21923828125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: 10272.88671875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -9884.1640625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -1873.1771240234375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1447.5994873046875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -15370.6435546875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -2877.563720703125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -14610.9453125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: 7468.7109375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7412, loss_val: nan, pos_over_neg: -3135.496337890625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -9398.35546875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -10997.830078125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: 26910.626953125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1530.42822265625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -1523.0596923828125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -9775.1142578125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -4846.98193359375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: 9543.8916015625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -5084.294921875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1866.569580078125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -2643.987548828125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2909.091552734375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -2437.788818359375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -1852.3009033203125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -1756.62109375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2243.087158203125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -1397.3836669921875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -2940.300537109375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3068.085205078125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -9785.1533203125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -4879.15869140625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -2003.2763671875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -3586.791015625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -3217.996826171875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1537.9547119140625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1460.6275634765625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: 2515.880859375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2365.986572265625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2391.54638671875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -2239.249267578125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2252.58837890625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -1553.98974609375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1789.5062255859375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -1885.4974365234375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -2259.40283203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -19673.35546875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -39429.7734375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -3107.869140625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -2102.7001953125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -1387.050048828125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1677.537841796875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: -1852.5654296875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2584.740478515625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2250.237060546875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1862.4205322265625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -4396.92626953125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7414, loss_val: nan, pos_over_neg: -4291.6396484375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -7037.365234375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -1634.516357421875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.748, loss_val: nan, pos_over_neg: 9640.75390625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -10314.53515625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -8880.8173828125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: 22143.71484375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: 10589.873046875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -5263.57177734375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -1351.12646484375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -1208.6690673828125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -30211.806640625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2145.5869140625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -6175.2060546875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: 45102.6484375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: 12268.19140625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: 1274.8399658203125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2604.30517578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -1823.119384765625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -6085.603515625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/300000 [25:46<14280:22:36, 171.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "Iter: 0/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2403.17236328125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -26997.962890625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -3477.888671875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -3918.63427734375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1917.3370361328125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: 9776.71484375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2336.422607421875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -7057.08984375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -3419.58056640625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -3104.533203125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1766.79833984375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -4495.79541015625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2622.41357421875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -1802.5697021484375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.7401, loss_val: nan, pos_over_neg: -2209.3154296875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -4796.2119140625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -2871.18603515625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -5563.78759765625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 60211.93359375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -4681.541015625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -37996.359375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -4154.53173828125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2004.14306640625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -5340.91796875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3902.18017578125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -3056.4853515625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -2126.188232421875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -4387.8056640625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: 8481.9619140625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: 161609.921875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2828.676513671875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -4283.70166015625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1535.025634765625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -3699.44287109375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -5196.41796875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -3304.61328125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -2280.44287109375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -2715.947509765625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -10102.3466796875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.7416, loss_val: nan, pos_over_neg: -2976.811767578125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -27099.95703125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -4102.2724609375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -2470.815673828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2003.068115234375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -1826.770751953125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2533.978515625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -31772.01171875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -3270.324951171875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2526.459228515625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: 5129.81396484375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -4859.294921875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3404.525146484375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -5494.5546875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -6191.3720703125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -22561.251953125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2151.65185546875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7558, loss_val: nan, pos_over_neg: -4863.564453125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3051.866455078125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -2812.49462890625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -2693.7978515625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2030.830322265625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -1671.7728271484375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -1813.2506103515625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: 33272.7734375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -4349.0087890625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -3745.48828125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -5258.388671875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -5018.47900390625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -2404.798583984375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -6735.1083984375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -11006.138671875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -7887.9599609375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -1628.1497802734375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -8604.5703125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -4274.6904296875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1927.588623046875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -3398.500732421875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2092.53515625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -3212.6767578125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -4196.8447265625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2008.6785888671875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -1706.186279296875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -11685.9873046875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -5351.080078125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1598.4671630859375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -3288.828125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -4880.357421875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.752, loss_val: nan, pos_over_neg: 2750.864013671875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -9769.6318359375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -6784.74609375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 17650.83203125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2898.028564453125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -3779.2294921875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2422.21630859375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -1926.6246337890625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1473.2535400390625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -13557.2861328125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -319307.65625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -6173.404296875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -2064.44921875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.748, loss_val: nan, pos_over_neg: 2582.639892578125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2444.392333984375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -118454.8828125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -6936.78955078125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -1762.7554931640625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -1628.911376953125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -80886.28125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -3841.594970703125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -41387.88671875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -8474.4951171875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -4971.8701171875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7529, loss_val: nan, pos_over_neg: -3739.5341796875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: 12580.4990234375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2699.104736328125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -2078.910888671875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: 16051.705078125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -3241.655517578125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -3126.026611328125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2692.78369140625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -1908.8857421875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3905.16357421875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -2614.9580078125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -6074.9853515625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: 61278.765625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: 23056.888671875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -5808.1806640625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -2130.399169921875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -2010.5740966796875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -3689.173828125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -3938.757568359375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7535, loss_val: nan, pos_over_neg: -1575.8785400390625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -2794.2919921875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -5798.43798828125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7543, loss_val: nan, pos_over_neg: 2521.4150390625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2961.8818359375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2014.4976806640625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -2947.65966796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -2072.93603515625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -1688.4876708984375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -2004.6407470703125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7401, loss_val: nan, pos_over_neg: -1286.450927734375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: 4280.32666015625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: 1924.59130859375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1995.752197265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.749, loss_val: nan, pos_over_neg: 5235.3564453125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7419, loss_val: nan, pos_over_neg: -3349.447265625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2377.8134765625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2262.25390625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -2105.09228515625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -2073.240234375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -3080.53662109375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: 14921.1103515625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: 2569.710693359375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 9403.9345703125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -1257.992431640625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -1652.473876953125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: -4362.81298828125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: 47298.90234375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -3100.896728515625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1491.8526611328125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2242.8388671875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -3252.334228515625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: -2663.03173828125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: 5008.1376953125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -1946.2557373046875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1867.26171875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1343.6226806640625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1433.58935546875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -3301.1162109375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -3915.401123046875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -1906.5045166015625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2742.37060546875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.7413, loss_val: nan, pos_over_neg: -3069.317138671875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -14012.5048828125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1563.931396484375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1650.3203125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -4122.1337890625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -5498.2119140625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -2396.331787109375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -1567.291748046875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -1501.6617431640625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -2805.130126953125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -1125.2711181640625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1899.8284912109375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.745, loss_val: nan, pos_over_neg: 16723.33203125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7524, loss_val: nan, pos_over_neg: 3033.443603515625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -2302.913818359375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.75, loss_val: nan, pos_over_neg: 3113.39697265625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -19169.86328125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -2846.10693359375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -2126.291748046875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -1615.291259765625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: 20019.35546875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -3375.926025390625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -1564.1376953125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -1584.2974853515625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.7406, loss_val: nan, pos_over_neg: -2262.73388671875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -4989.611328125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -1897.45361328125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -5266.61279296875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7422, loss_val: nan, pos_over_neg: -5630.896484375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3042.286865234375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -6881.5048828125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: 19621.033203125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -2521.84423828125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -2128.15673828125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -6640.99365234375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -4502.453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7533, loss_val: nan, pos_over_neg: -1841.8375244140625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -1882.966796875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -2170.4033203125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -3233.996826171875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -4793.0703125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -7498.53662109375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 38754.375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: 1449.2620849609375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3616.654052734375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -5480.9638671875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1929.3446044921875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -3835.926513671875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -2057.439208984375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -3915.95361328125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -3514.480224609375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2879.732666015625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -2672.68603515625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -3787.398681640625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: 68827.6875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -16088.6943359375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2369.791748046875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -3348.464599609375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: 7234.5458984375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1812.732666015625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1623.776123046875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -1518.63330078125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -11073.2353515625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -4970.69287109375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7559, loss_val: nan, pos_over_neg: 6454.8017578125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -3851.842529296875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: 5204.7587890625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -1463.522705078125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -7666.533203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2319.06494140625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -1707.724365234375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -6040.5322265625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -1819.8572998046875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3799.890380859375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -6746.20263671875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: 2322.974853515625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -1889.0052490234375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2232.17431640625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: 12052.99609375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2432.855712890625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -1864.1922607421875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -1815.0128173828125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7549, loss_val: nan, pos_over_neg: -8911.6474609375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -7784.35791015625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -4125.0 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: 12583.74609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -3076.440185546875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7525, loss_val: nan, pos_over_neg: -15648.865234375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7531, loss_val: nan, pos_over_neg: -2015.5413818359375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -3347.15478515625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1781.3480224609375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1736.377197265625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: 11238.4638671875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: 8702.150390625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3452.255859375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -2904.096923828125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -3167.642822265625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1768.737548828125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -2629.611572265625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -3283.315185546875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -2655.51513671875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: -3331.699462890625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -12255.5712890625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -5536.08447265625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: 5044.310546875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 3557.72021484375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -1593.8250732421875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1734.913818359375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7534, loss_val: nan, pos_over_neg: -7121.21044921875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -2051.6962890625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -4726.44921875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2336.572265625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -9957.05859375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 25856.00390625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -1630.1226806640625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 4914.01806640625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2163.49072265625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7552, loss_val: nan, pos_over_neg: -2599.877685546875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -1593.5863037109375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -7733.35107421875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3896.959716796875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7554, loss_val: nan, pos_over_neg: 1572.5087890625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: 3947.547119140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -1916.7841796875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3786.64794921875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: -4484.25634765625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: 3815.094970703125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -3483.831787109375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2749.175537109375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -33906.23046875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: 2424.66259765625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -5270.12939453125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: 3764.366943359375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: 3881.326171875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1760.7069091796875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -2933.72998046875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -1810.645751953125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2048.82470703125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -4845.78759765625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -2879.897705078125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -4456.6337890625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.755, loss_val: nan, pos_over_neg: 4045.90576171875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -4452.67333984375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -2948.196044921875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: -1821.93798828125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -6897.21875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -10627.9365234375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2954.311279296875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -11091.6376953125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2745.080322265625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -3336.74462890625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -2495.03857421875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -3030.42138671875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7385, loss_val: nan, pos_over_neg: -1584.186279296875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -2001.286376953125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -1350.3187255859375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1727.9239501953125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -17183.919921875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -10179.896484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: 2263.4306640625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: 2028.5723876953125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2378.3017578125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2263.351318359375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: -3907.540283203125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -2731.1435546875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7408, loss_val: nan, pos_over_neg: -2351.21728515625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -1528.705810546875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1585.737060546875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -1905.84765625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -4526.2568359375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: 15020.7890625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -8504.546875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -4987.71923828125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -2222.304931640625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -9103.845703125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -1998.3316650390625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -3389.65380859375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -5307.12060546875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -2324.31982421875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -2831.080078125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -5623.5634765625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -2832.084716796875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: 17438.896484375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -2570.875244140625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: 17882.751953125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -83579.2578125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7413, loss_val: nan, pos_over_neg: -2060.572265625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -2760.3388671875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -386782.5 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -3499.787353515625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -2282.136474609375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -3175.879150390625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2069.555419921875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1548.07275390625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -5730.76123046875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -2501.19189453125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -1893.1912841796875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3833.587646484375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -1752.6844482421875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1788.859619140625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -6933.33935546875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -2396.41796875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1733.27294921875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2454.023681640625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: 29469.78125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2555.2001953125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2633.4404296875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1761.7056884765625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -11479.3974609375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -1923.671630859375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1825.243896484375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -3601.595458984375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -8495.9482421875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -4548.900390625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -3585.000244140625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -2799.80517578125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: 51753.89453125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -2170.597900390625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7542, loss_val: nan, pos_over_neg: -9849.767578125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: 5756.73486328125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -7991.01025390625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -6723.43505859375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -1694.272216796875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: 10010.962890625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: 2324.78955078125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2780.158203125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2043.30078125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7405, loss_val: nan, pos_over_neg: -2210.061767578125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -4941.376953125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -4034.286376953125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -29959.201171875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -2058.425048828125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -1654.549072265625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -2382.30126953125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2994.94091796875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2101.2216796875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -1460.74072265625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1380.9681396484375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2851.4365234375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -1851.925537109375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -6632.51806640625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7411, loss_val: nan, pos_over_neg: -1915.0987548828125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -5341.93017578125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2396.237548828125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -3831.47607421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -1748.1357421875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -4201.69482421875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -3624.116943359375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -2911.349609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.746, loss_val: nan, pos_over_neg: 304382.1875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: 6762.44189453125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -1275.5609130859375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -12604.2080078125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -1674.997314453125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: 9749.07421875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -5117.92724609375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1460.3748779296875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1886.8558349609375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 8019.78857421875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 20547.6328125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: 4120.2880859375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 6200.68603515625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -1561.6942138671875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -2806.662109375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -12450.9453125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1668.50927734375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1464.3914794921875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -2284.7119140625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -3590.638671875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -3122.676513671875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -3006.994384765625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2221.16162109375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.741, loss_val: nan, pos_over_neg: -19296.91796875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -6576.54833984375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 7192.5634765625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: 5310.19384765625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1590.9395751953125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -24897.759765625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: 17782.515625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: -3387.74267578125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: 2367.60107421875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1922.104248046875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -7353.15673828125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -1829.7744140625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -4134.271484375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -4441.6337890625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.744, loss_val: nan, pos_over_neg: 2980.1298828125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 3046.2666015625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -3757.755859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -3247.072998046875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -5265.48681640625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -3669.811279296875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1595.9827880859375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1504.34716796875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -5584.693359375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -3255.422119140625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1442.6298828125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -3112.93017578125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -2750.011962890625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -3233.796142578125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -11815.1904296875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2204.598388671875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2843.31982421875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -37044.37890625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -2633.462890625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -5255.845703125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2078.515625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -4142.48828125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -3146.210205078125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1904.3922119140625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -3055.31982421875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2300.129150390625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -2283.46337890625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 2418.572998046875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: 2151.532958984375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: 8084.50439453125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -8052.39208984375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -7374.97998046875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.7515, loss_val: nan, pos_over_neg: -4358.2119140625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -2975.91259765625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1652.455322265625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -1836.952392578125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2350.7666015625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: -3867.1640625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 7891.88623046875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: 5392.5341796875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.7411, loss_val: nan, pos_over_neg: -3395.9892578125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -3086.385009765625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -31839.4609375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -2971.65283203125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -1852.7724609375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -2085.446533203125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3401.2685546875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -7028.1005859375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7412, loss_val: nan, pos_over_neg: -2538.993896484375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1409.2239990234375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -1847.9962158203125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -1625.4737548828125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -3697.47705078125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: 4553.70947265625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -3243.0615234375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -3072.6689453125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -5349.03662109375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: 1980.1302490234375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -4039.062744140625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -2160.28564453125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2727.81396484375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -3307.616455078125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: 45704.92578125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -6704.37646484375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -7382.79150390625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -1972.6370849609375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -3238.600830078125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -3329.809814453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -27072.83203125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 19864.0546875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -2271.076904296875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1637.9771728515625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -1855.6016845703125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1540.7154541015625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7403, loss_val: nan, pos_over_neg: -1624.0545654296875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -34012.5859375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: 10394.513671875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: 2166.37060546875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -4176.1435546875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -1479.7489013671875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -2200.841552734375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2916.077880859375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -3529.849365234375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2495.048828125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 3012.388916015625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -3268.423828125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: 3128.156494140625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -4039.341064453125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: 1695.209228515625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7407, loss_val: nan, pos_over_neg: -12259.5029296875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -2678.511474609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2036.06201171875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -2337.260009765625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -5259.10791015625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -2441.964599609375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2359.15185546875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -3326.29541015625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -3490.28759765625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -3658.902587890625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.75, loss_val: nan, pos_over_neg: 2646.69970703125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.752, loss_val: nan, pos_over_neg: 5224.72900390625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -1748.5233154296875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -1943.0264892578125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -1510.8756103515625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -3644.034912109375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -1290.3125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -3351.247314453125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2647.12255859375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -1830.3406982421875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -4823.64892578125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -10896.7373046875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -3582.083984375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: 1506.56640625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1740.181884765625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -1239.44189453125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -1685.967529296875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1765.2572021484375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1400.567138671875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: -8820.6240234375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -9264.1796875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1483.7486572265625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -1653.6549072265625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: 5840.95654296875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1707.93310546875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -4032.6943359375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -3817.0576171875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -3540.10107421875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: 164892.625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -3422.92236328125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -195158.3125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1882.10546875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -1867.2406005859375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 52647.39453125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2559.671630859375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -2932.466064453125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 5408.73974609375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -1982.8695068359375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: 9063.4775390625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -3264.714111328125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -4790.7822265625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 2303.375732421875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2378.726318359375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2539.365234375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: 119181.40625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 5310.6259765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -4055.17236328125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -3425.677001953125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -8299.1328125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -1828.1971435546875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7402, loss_val: nan, pos_over_neg: -11155.173828125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: 9980.6796875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7419, loss_val: nan, pos_over_neg: -4332.609375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: 10084.751953125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: 18271.306640625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2053.500244140625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: 7469.9375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2389.336181640625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1674.5284423828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -1984.2216796875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -3335.62158203125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -58204.57421875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -1866.19091796875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2097.515869140625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -54057.90234375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: 3947.211181640625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -2476.158447265625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1937.280029296875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -1692.6485595703125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -4086.7978515625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -7589.810546875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2148.4052734375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: -2671.091064453125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: 26818.015625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -2426.28466796875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: 2125.26220703125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1649.7232666015625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -3491.137451171875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -8699.7529296875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -2885.84765625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7385, loss_val: nan, pos_over_neg: -5317.880859375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -1511.6494140625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1793.232666015625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -3797.330810546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -2594.7421875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -1248.08984375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -2144.384033203125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -6662.78369140625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -1675.833984375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -12689.2294921875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -8789.58203125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: 2286.349365234375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2229.610595703125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -1936.2391357421875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: -2215.76220703125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: 5876.681640625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2256.615966796875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: 67085.25 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2145.120849609375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2301.472900390625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -5256.74169921875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -2888.06298828125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1383.2742919921875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -1671.075927734375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 6.7409, loss_val: nan, pos_over_neg: -1466.4527587890625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2480.50537109375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -8288.474609375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 6.7511, loss_val: nan, pos_over_neg: -3711.977783203125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -2573.736572265625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1838.4774169921875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 6.7536, loss_val: nan, pos_over_neg: -2595.863037109375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2984.52197265625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2508.84423828125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 6.7386, loss_val: nan, pos_over_neg: -1642.3173828125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 6.748, loss_val: nan, pos_over_neg: 12200.259765625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -1903.0718994140625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -4899.34130859375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: 2636.207275390625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2350.377197265625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -5274.39599609375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2763.262451171875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: -3188.5517578125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -2443.19140625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -3424.9091796875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2386.302978515625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 90636.4296875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -38186.45703125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -2928.7470703125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2511.206298828125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -1314.5894775390625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 6.7539, loss_val: nan, pos_over_neg: 4683.30126953125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -3951.389892578125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -2634.197998046875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -2449.172607421875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -1900.97802734375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 6.7502, loss_val: nan, pos_over_neg: -4020.187744140625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -1194.60107421875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -3059.781005859375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/300000 [28:39<14309:10:19, 171.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "Iter: 0/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -1597.1839599609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: 24171.693359375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -7704.7412109375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -2427.2919921875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7506, loss_val: nan, pos_over_neg: 2799.686279296875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: 90134.6484375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -9120.248046875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: -3039.052001953125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1833.3516845703125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -3012.896240234375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1484.5899658203125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -3435.532958984375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -3075.061767578125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -1834.3333740234375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -2320.418212890625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -2021.1396484375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: 8972.548828125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2468.468505859375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -4126.5380859375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -2157.207275390625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -3305.19091796875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -2767.940673828125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -2061.3779296875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -3304.802001953125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -17222.072265625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -5485.60009765625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -2063.18115234375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -5399.86767578125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -1719.447998046875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1830.37744140625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2570.7802734375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -10250.6181640625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -2175.037353515625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -2135.064453125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 1448.76806640625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -9173.8779296875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: -1469.46240234375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3306.1826171875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -2386.09619140625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -3703.779052734375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2188.575439453125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1375.4285888671875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -3084.940673828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -7229.68603515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -3203.5693359375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -3424.825927734375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -3076.1083984375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.7412, loss_val: nan, pos_over_neg: -4197.60791015625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -1779.7611083984375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -2606.05029296875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -1464.9805908203125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: 4464.8642578125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -2176.795654296875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -4390.55908203125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1919.4708251953125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: 9276.4521484375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: 2953.749267578125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: 8960.96484375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -2791.8837890625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1372.322021484375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.7406, loss_val: nan, pos_over_neg: -2030.3026123046875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -2216.20361328125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -4791.12548828125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -4115.8642578125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.7417, loss_val: nan, pos_over_neg: -337845.1875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.7409, loss_val: nan, pos_over_neg: -2222.0556640625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2667.563720703125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1649.2352294921875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2635.58447265625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.7422, loss_val: nan, pos_over_neg: 33659.73046875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -1835.7359619140625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: -1656.771240234375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -3867.39599609375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: -3247.74365234375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.7417, loss_val: nan, pos_over_neg: -5698.19775390625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -1507.028076171875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -2439.95556640625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -1774.9775390625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1940.8114013671875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -1714.1468505859375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1528.9393310546875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -2786.2568359375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -3129.28125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2832.8232421875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: 5292.556640625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -2235.016845703125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1523.8096923828125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -5328.09716796875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1360.515869140625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: -2839.14501953125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: 17852.1328125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.7416, loss_val: nan, pos_over_neg: -2325.791259765625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: 2339.342529296875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -8772.5625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -1925.043212890625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -2843.753662109375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -4180.96435546875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -1725.6575927734375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -14815.841796875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: 17331.720703125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -1802.1173095703125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2357.927978515625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 237699.890625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.7407, loss_val: nan, pos_over_neg: -2010.6177978515625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2219.8740234375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -1876.9520263671875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -2212.5625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.7409, loss_val: nan, pos_over_neg: -1785.829345703125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -2571.363525390625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.7398, loss_val: nan, pos_over_neg: -2090.408447265625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2284.851318359375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: 8498.2021484375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -3052.937744140625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.7496, loss_val: nan, pos_over_neg: -2297.0537109375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.7501, loss_val: nan, pos_over_neg: 15321.9140625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2181.82763671875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -1794.38818359375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1751.2247314453125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -1883.70751953125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: 8040.611328125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.7503, loss_val: nan, pos_over_neg: 31824.255859375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -2746.68701171875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1989.3131103515625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1988.1180419921875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.7399, loss_val: nan, pos_over_neg: -1847.511474609375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -3208.684326171875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: 16100.0634765625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -3799.799072265625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: 2224.83349609375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -1956.354248046875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.7375, loss_val: nan, pos_over_neg: -2010.1353759765625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -1994.3843994140625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -1584.5244140625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: -1995.710205078125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.7413, loss_val: nan, pos_over_neg: -3438.075927734375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -4923.89306640625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: 2210.81787109375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1948.015869140625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: 37479.44921875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: 22681.57421875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -4434.61328125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -2619.11572265625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -2105.817138671875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -3921.070068359375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -7072.7998046875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: 3165.901611328125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -2851.390869140625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -2933.578369140625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -2921.085205078125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: 29088.033203125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -4143.22998046875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -1787.70556640625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.7485, loss_val: nan, pos_over_neg: -1697.5848388671875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1749.8134765625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.7383, loss_val: nan, pos_over_neg: -1548.7928466796875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -2354.32275390625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -3021.83056640625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -1611.4017333984375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: 2443.906982421875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -3531.600341796875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -2148.266845703125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -3575.400146484375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2314.20849609375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -8063.974609375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -5345.06103515625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: 6612.6845703125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -6359.09912109375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -7888.6298828125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1840.1177978515625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -8635.1767578125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -2258.933349609375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -5205.15185546875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.75, loss_val: nan, pos_over_neg: 5082.24658203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: 1992.7437744140625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 2049.29443359375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.7518, loss_val: nan, pos_over_neg: 2697.632568359375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -2929.849853515625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.7522, loss_val: nan, pos_over_neg: -2931.952392578125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -2602.807861328125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -2937.5537109375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -1838.5599365234375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -6499.244140625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.7508, loss_val: nan, pos_over_neg: 3023.43017578125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: 3600.576171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -129554.625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -4126.72509765625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -4483.65625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -3323.772705078125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.7403, loss_val: nan, pos_over_neg: -5479.53173828125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: 5462.54052734375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -1818.1617431640625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.7419, loss_val: nan, pos_over_neg: -1786.52978515625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -1717.6871337890625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -2489.5595703125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -1541.9886474609375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -2985.646240234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 6402.5146484375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -8918.9130859375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -1987.7808837890625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -2179.08837890625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.7519, loss_val: nan, pos_over_neg: -166084.90625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -3240.150390625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -1675.1112060546875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -2371.9345703125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -3502.6552734375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -2504.46533203125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: 17572.91796875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1781.4510498046875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.7532, loss_val: nan, pos_over_neg: -2717.69677734375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1901.1890869140625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -1428.9365234375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1723.30810546875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -2470.683349609375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.7516, loss_val: nan, pos_over_neg: -1764.7451171875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.7461, loss_val: nan, pos_over_neg: -2220.50390625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: -1686.524658203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -2381.403564453125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -6313.1318359375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -2058.93896484375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -7936.84375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: -3461.691650390625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -5900.77587890625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -1572.659423828125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1723.7999267578125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2829.234375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1880.1666259765625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -9069.4375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.7491, loss_val: nan, pos_over_neg: 5955.158203125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -2234.14794921875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -8896.0546875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -2723.593505859375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.7404, loss_val: nan, pos_over_neg: -5600.287109375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: 3664.9443359375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.7492, loss_val: nan, pos_over_neg: -18486.98828125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1695.674560546875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -1679.43359375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.7405, loss_val: nan, pos_over_neg: -2242.794921875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.7394, loss_val: nan, pos_over_neg: -2029.1490478515625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -62409.83984375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -2894.676025390625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.7413, loss_val: nan, pos_over_neg: -1874.179931640625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -10875.7802734375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.7481, loss_val: nan, pos_over_neg: 19781.994140625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -1750.938232421875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -2918.601318359375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -2257.44970703125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.7388, loss_val: nan, pos_over_neg: -1983.0101318359375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -1731.2620849609375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -3749.07763671875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -2392.944580078125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.7408, loss_val: nan, pos_over_neg: -2720.630126953125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.7404, loss_val: nan, pos_over_neg: 10543.271484375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -5102.857421875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -6341.4931640625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -3502.64404296875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2746.15673828125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -2046.6214599609375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -1623.5477294921875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.7494, loss_val: nan, pos_over_neg: -3429.326904296875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.7424, loss_val: nan, pos_over_neg: -1415.523681640625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -3932.451171875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -2487.482177734375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -11988.7109375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -2298.046630859375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -14341.57421875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2804.728515625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: 12088.1943359375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: -12460.1005859375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.741, loss_val: nan, pos_over_neg: -2066.601318359375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -1717.529052734375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: -14795.28515625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: 4008.146728515625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -1629.6373291015625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.7538, loss_val: nan, pos_over_neg: -1690.364013671875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 51669.7578125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -2075.90478515625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -11889.03515625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -10088.4736328125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: -2853.886474609375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.7408, loss_val: nan, pos_over_neg: -3208.01123046875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.7498, loss_val: nan, pos_over_neg: -7113.04052734375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: -1832.314697265625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: 37875.10546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -1492.7974853515625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1318.5458984375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.754, loss_val: nan, pos_over_neg: -2863.244384765625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -2356.711181640625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.7412, loss_val: nan, pos_over_neg: -2175.670654296875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.7514, loss_val: nan, pos_over_neg: -1962.4200439453125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -1955.569091796875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -2263.319580078125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -3600.126220703125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.7411, loss_val: nan, pos_over_neg: -32931.33984375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -12708.189453125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: -2694.504638671875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1733.8814697265625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2389.4443359375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -1910.580322265625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -2023.76611328125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -2344.1875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.7406, loss_val: nan, pos_over_neg: -1879.582763671875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4419.42578125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -3208.884033203125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -2619.516845703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1646.7039794921875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -1498.462158203125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: 5663.22998046875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -1881.1429443359375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -1973.6221923828125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -2835.71484375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: -7396.947265625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -5551.4580078125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1537.58740234375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.751, loss_val: nan, pos_over_neg: -1764.52685546875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: 3940.1728515625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -8567.80859375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -2599.378662109375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.749, loss_val: nan, pos_over_neg: -9146.96484375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -2033.648681640625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -2032.520263671875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.7419, loss_val: nan, pos_over_neg: -7485.5048828125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -1852.37744140625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -1931.244873046875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -2121.029052734375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -2262.72705078125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.7419, loss_val: nan, pos_over_neg: -1980.29736328125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -8564.6474609375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -1305.337158203125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -3178.513427734375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -3667.701171875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1848.987548828125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -2132.56005859375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -1847.0101318359375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.7528, loss_val: nan, pos_over_neg: 3766.0634765625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -6846.7763671875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -1769.98876953125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.7406, loss_val: nan, pos_over_neg: -1991.2769775390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.7486, loss_val: nan, pos_over_neg: -5204.7578125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1670.05517578125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -3026.69482421875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: -1849.239990234375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -20210.388671875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -5955.0556640625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1966.1519775390625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -2784.9970703125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.7495, loss_val: nan, pos_over_neg: 2960.5263671875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.7527, loss_val: nan, pos_over_neg: 1747.0870361328125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -4531.84765625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -1777.648193359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1655.29443359375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -3421.349365234375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.7387, loss_val: nan, pos_over_neg: -4363.0205078125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.7475, loss_val: nan, pos_over_neg: 4370.609375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -2643.339599609375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.7383, loss_val: nan, pos_over_neg: -2641.86669921875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.7416, loss_val: nan, pos_over_neg: -1704.870849609375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1783.6953125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -1411.4217529296875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -1875.7506103515625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -4706.98583984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -6139.21435546875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -2061.62841796875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -2175.708251953125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.7479, loss_val: nan, pos_over_neg: -1885.56591796875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -9067.171875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -3460.6162109375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -2248.055419921875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -1748.5576171875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.7422, loss_val: nan, pos_over_neg: -1261.33837890625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -1277.303466796875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -3666.412109375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -1824.8074951171875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -4836.87158203125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1519.4527587890625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: 14343.916015625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -1953.190673828125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -8870.5556640625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.7415, loss_val: nan, pos_over_neg: -2810.944091796875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -2715.76025390625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -2007.2744140625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.7476, loss_val: nan, pos_over_neg: -4657.7001953125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -7338.17431640625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -2417.66162109375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.7557, loss_val: nan, pos_over_neg: 940.4422607421875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -3305.602783203125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2968.14111328125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -3904.987060546875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -2016.8502197265625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -1799.07373046875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -1811.4197998046875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.7407, loss_val: nan, pos_over_neg: -24927.26953125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.746, loss_val: nan, pos_over_neg: 3618.34814453125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: 4417.03271484375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -6101.54345703125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -10035.927734375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2967.984375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.7421, loss_val: nan, pos_over_neg: -2847.04931640625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -3031.173095703125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -1640.8289794921875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -2743.0263671875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -2423.150390625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1824.4302978515625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -17496.13671875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 6.7412, loss_val: nan, pos_over_neg: -1957.59716796875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.7422, loss_val: nan, pos_over_neg: -2318.115478515625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.7389, loss_val: nan, pos_over_neg: -2823.4921875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: 14367.2822265625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -1604.8807373046875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -5553.53955078125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.75, loss_val: nan, pos_over_neg: -3728.142333984375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -2663.2373046875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -5089.943359375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1574.7081298828125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -1930.6142578125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -7255.32568359375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -4014.0859375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -2110.647705078125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -1882.655517578125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -1153.9456787109375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -3832.938720703125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.7424, loss_val: nan, pos_over_neg: -3578.7578125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.7442, loss_val: nan, pos_over_neg: -4487.0185546875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -2047.8758544921875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -1828.256103515625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -2790.009033203125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -1633.1275634765625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -1897.69140625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -3591.05029296875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -5689.23046875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -1615.1234130859375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -3358.8623046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1818.977294921875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.7403, loss_val: nan, pos_over_neg: -1614.8895263671875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -3757.990966796875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: 16003.1630859375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.7487, loss_val: nan, pos_over_neg: 7447.49560546875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2773.37841796875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -2691.085205078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.7411, loss_val: nan, pos_over_neg: -2331.8916015625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.7497, loss_val: nan, pos_over_neg: -3048.47314453125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.7385, loss_val: nan, pos_over_neg: -2327.44287109375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.7425, loss_val: nan, pos_over_neg: -1854.1162109375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -13029.7578125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -1517.2083740234375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -1902.1016845703125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -4533.76904296875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.747, loss_val: nan, pos_over_neg: 18058.548828125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: 2510.86669921875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: 16852.73046875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.7407, loss_val: nan, pos_over_neg: -6704.4814453125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: -4084.926513671875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -1696.7232666015625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.7463, loss_val: nan, pos_over_neg: -16213.525390625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -2566.681396484375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -4028.8955078125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: 3898.229248046875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.7408, loss_val: nan, pos_over_neg: -2625.318115234375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -1748.6741943359375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.7403, loss_val: nan, pos_over_neg: -1791.5362548828125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2538.215087890625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: 31753.462890625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2241.64794921875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -3445.201904296875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -3110.27880859375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: -5125.18310546875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.7537, loss_val: nan, pos_over_neg: -1687.340576171875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -18480.58203125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.7482, loss_val: nan, pos_over_neg: -96510.4375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -2504.079345703125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.7424, loss_val: nan, pos_over_neg: -3996.27734375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.7398, loss_val: nan, pos_over_neg: -1687.0579833984375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.7386, loss_val: nan, pos_over_neg: -2135.322998046875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -1595.21337890625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 15860.3427734375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: -2364.15087890625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -1902.3333740234375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -2219.498046875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -2097.186279296875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -3825.0849609375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -8728.19140625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: 7997.44287109375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -6232.4619140625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.7424, loss_val: nan, pos_over_neg: 11140.7412109375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: 31690.7109375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -3106.843994140625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -3779.232666015625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -2947.97021484375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -2940.920166015625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 6.7526, loss_val: nan, pos_over_neg: 3925.2861328125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: 3988.465576171875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.747, loss_val: nan, pos_over_neg: -141631.296875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -2011.934814453125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -6355.06201171875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: 4207.0634765625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: 22527.53515625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -1873.4317626953125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 6.748, loss_val: nan, pos_over_neg: -1842.912353515625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -2716.379150390625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2678.972900390625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -3182.350830078125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: -1764.0079345703125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 6.7446, loss_val: nan, pos_over_neg: -3459.981201171875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -3712.488037109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -2717.59375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -2180.777099609375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.7426, loss_val: nan, pos_over_neg: -1806.6405029296875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.7473, loss_val: nan, pos_over_neg: -1548.2659912109375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -1355.1895751953125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -2271.285400390625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: -1490.6854248046875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: -3449.01708984375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.7398, loss_val: nan, pos_over_neg: -3802.31396484375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -20060.646484375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 6.7424, loss_val: nan, pos_over_neg: -1382.5948486328125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -2699.451171875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.7404, loss_val: nan, pos_over_neg: -2304.76025390625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: -1940.2303466796875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -3151.24853515625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -1829.546630859375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.7401, loss_val: nan, pos_over_neg: -2141.802001953125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 6.7408, loss_val: nan, pos_over_neg: -3514.512939453125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 6.7521, loss_val: nan, pos_over_neg: 3592.358642578125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1560.824951171875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.7436, loss_val: nan, pos_over_neg: 9042.5673828125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.7413, loss_val: nan, pos_over_neg: -11303.3017578125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.7512, loss_val: nan, pos_over_neg: 48377.86328125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.7422, loss_val: nan, pos_over_neg: -1525.730224609375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -2028.5228271484375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -1644.790283203125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.7488, loss_val: nan, pos_over_neg: -1454.470703125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -2705.567626953125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -60028.515625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: 19402.576171875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: -2450.108642578125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -68238.5546875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 6.7489, loss_val: nan, pos_over_neg: -1744.3616943359375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 6.737, loss_val: nan, pos_over_neg: -2928.740234375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -1267.3704833984375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -2271.92578125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.7406, loss_val: nan, pos_over_neg: -2438.190185546875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: 5874.93505859375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -4444.22705078125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2155.563232421875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.7468, loss_val: nan, pos_over_neg: -1412.6224365234375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -1604.296875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -1526.3985595703125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -1901.5430908203125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.7457, loss_val: nan, pos_over_neg: -5611.974609375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -1733.6016845703125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: -2586.122802734375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: 4280.08544921875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -1588.430908203125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -7221.05322265625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -2335.513427734375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 6.7523, loss_val: nan, pos_over_neg: -2292.591796875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 6.7437, loss_val: nan, pos_over_neg: -1865.2745361328125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -5011.2890625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 6.7493, loss_val: nan, pos_over_neg: -54252.51953125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -1804.6993408203125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 6.7397, loss_val: nan, pos_over_neg: -1621.1865234375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 6.7474, loss_val: nan, pos_over_neg: -8290.1650390625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: -1800.48046875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -2146.947509765625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -2134.9267578125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 6.7458, loss_val: nan, pos_over_neg: -4454.51220703125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -1657.14111328125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: 3415.441650390625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 6.7408, loss_val: nan, pos_over_neg: 15987.6953125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -1870.3741455078125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -1775.5009765625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -1523.3817138671875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -4940.951171875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -3602.32177734375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 6.7453, loss_val: nan, pos_over_neg: -12607.419921875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 6.7439, loss_val: nan, pos_over_neg: -9346.556640625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: 61728.0625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -263642.625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.7469, loss_val: nan, pos_over_neg: -2496.8076171875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 6.7423, loss_val: nan, pos_over_neg: -3582.527099609375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: -2867.18994140625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -5336.37744140625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: -7323.59033203125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: 5387.49560546875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -1785.6844482421875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.7448, loss_val: nan, pos_over_neg: -2749.347900390625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.7499, loss_val: nan, pos_over_neg: -7501.7314453125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.7433, loss_val: nan, pos_over_neg: -3815.953369140625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: -3185.446044921875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 6.7452, loss_val: nan, pos_over_neg: -5417.361328125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -3085.870361328125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -3004.6953125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -1652.6038818359375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -1603.39892578125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 6.7441, loss_val: nan, pos_over_neg: -1780.95947265625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.7471, loss_val: nan, pos_over_neg: -2971.950439453125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.7507, loss_val: nan, pos_over_neg: 3984.659912109375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: -5279.189453125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: 4877.33984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -2503.592041015625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.7449, loss_val: nan, pos_over_neg: 1665.4539794921875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: 3327.661865234375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 6.7464, loss_val: nan, pos_over_neg: -3017.210693359375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.7472, loss_val: nan, pos_over_neg: -1622.5799560546875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 6.7462, loss_val: nan, pos_over_neg: -2651.695556640625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: 14604.466796875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.7401, loss_val: nan, pos_over_neg: -1885.3197021484375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 6.7513, loss_val: nan, pos_over_neg: -2345.916259765625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.7429, loss_val: nan, pos_over_neg: 4569.51318359375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.7477, loss_val: nan, pos_over_neg: -2232.4248046875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 6.7465, loss_val: nan, pos_over_neg: 4938.01904296875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -3236.73486328125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 6.7454, loss_val: nan, pos_over_neg: -3818.873046875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 6.743, loss_val: nan, pos_over_neg: -1673.8817138671875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 6.7438, loss_val: nan, pos_over_neg: -1511.710205078125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.7434, loss_val: nan, pos_over_neg: -3890.30517578125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -1433.7789306640625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 6.7444, loss_val: nan, pos_over_neg: -2421.489013671875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 6.7483, loss_val: nan, pos_over_neg: 9506.7802734375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 6.7451, loss_val: nan, pos_over_neg: -5671.92919921875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.7406, loss_val: nan, pos_over_neg: -8899.7861328125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 6.7409, loss_val: nan, pos_over_neg: 5841.3466796875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -4032.067138671875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 6.7393, loss_val: nan, pos_over_neg: -2004.34326171875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.745, loss_val: nan, pos_over_neg: -2089.17333984375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 6.7459, loss_val: nan, pos_over_neg: 4331.662109375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 6.7478, loss_val: nan, pos_over_neg: -5409.10302734375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.7417, loss_val: nan, pos_over_neg: -2348.11669921875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 6.744, loss_val: nan, pos_over_neg: -8197.40625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 6.7386, loss_val: nan, pos_over_neg: -1748.048828125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 6.7467, loss_val: nan, pos_over_neg: -3413.0244140625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.7431, loss_val: nan, pos_over_neg: 18314.748046875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 6.7422, loss_val: nan, pos_over_neg: -7243.822265625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.7447, loss_val: nan, pos_over_neg: -2711.1748046875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 6.7455, loss_val: nan, pos_over_neg: -2647.931396484375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 6.7456, loss_val: nan, pos_over_neg: -3881.748046875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 6.7517, loss_val: nan, pos_over_neg: 3648.89794921875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 6.7411, loss_val: nan, pos_over_neg: -1938.534912109375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 6.7432, loss_val: nan, pos_over_neg: -2182.891357421875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 6.7445, loss_val: nan, pos_over_neg: 7037.8125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 6.7505, loss_val: nan, pos_over_neg: 2176.80078125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 6.7393, loss_val: nan, pos_over_neg: -3711.046875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 6.7427, loss_val: nan, pos_over_neg: -2441.774658203125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 6.7388, loss_val: nan, pos_over_neg: 10328.70703125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 6.7484, loss_val: nan, pos_over_neg: 38954.8046875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.741, loss_val: nan, pos_over_neg: 17914.3984375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 6.7405, loss_val: nan, pos_over_neg: -2539.484130859375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 6.7428, loss_val: nan, pos_over_neg: -1484.290283203125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.7417, loss_val: nan, pos_over_neg: -1643.8165283203125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 6.7422, loss_val: nan, pos_over_neg: -3419.296630859375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 6.7435, loss_val: nan, pos_over_neg: -3042.3583984375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 6.7443, loss_val: nan, pos_over_neg: 13698.857421875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 6.7504, loss_val: nan, pos_over_neg: 5816.01220703125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 6.7418, loss_val: nan, pos_over_neg: -2216.23876953125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 6.7509, loss_val: nan, pos_over_neg: 1465.725341796875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 6.746, loss_val: nan, pos_over_neg: -8154.93212890625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 6.742, loss_val: nan, pos_over_neg: -3969.65234375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 6.7466, loss_val: nan, pos_over_neg: -1368.6866455078125 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=norsz'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
