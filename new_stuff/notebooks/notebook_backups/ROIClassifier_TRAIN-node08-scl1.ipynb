{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUEklEQVR4nO3df6xf9X3f8ecrhhArgYaIC3JsJ0aZ080g1RTLo0LasiQrLqlm0g7JbAWkMpkxkMjWboL80+QPb6nWJB3aYCMDYbYsrqUkw0pCWpcmy9AI5po5GGNYrOBgxx52ShFGmlht3vvjflC/u/na9/r++N5wP8+HdPQ9530+n3M+R5Zf9+jzPd/vN1WFJKkP71joAUiSRsfQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKGvRS/JwSQfn2bbSvLXZnieGfeVRsXQlxZQkmVJdiQ50v5orFroMWlxM/SlhfUm8G3gNxd6IOqDoa+uJFmf5IkkryY5muTfJnnnpGbXJvlRkp8m+ddJ3jHQ/7eT7E/yF0n+OMkHT3Oea5M8l+REkp8k+d1h7arq5aq6F3hq7q5SOj1DX705BfxT4CLgV4CPAf9kUptPAuuAXwY2Ar8NkOQ64NPAbwBjwH8HvnKa8zwA3FpV5wOXA382lxchzZShr65U1e6q+n5Vnayqg8B/AP72pGa/X1WvVNVLwB8CN7T6rcC/qqr9VXUS+JfA2tPc7f8lsCbJBVX1F1X19LxckHSWDH11JcmHk3wjyf9O8hoTwX3RpGaHBtZ/DLy/rX8Q+DdtauhV4BUgwPIhp/pN4Frgx0n+W5JfmcvrkGbK0Fdv7gOeB1ZX1QVMTNdkUpuVA+sfAI609UNMTNm8d2BZWlX/Y/JJquqpqtoIXAz8V2D7HF+HNCOGvnpzPvAa8HqSvw7cNqTNP09yYZKVwJ3AH7X6vwfuTnIZQJJfSHL95M5J3pnkHyb5har6y3a+U6cbUJJ3Aee1zfPatjQvDH315neBfwCcAL7EXwX6oEeA3cAe4JtMvClLVX0d+H1gW5saehb4tdOc50bgYGv3j4HfOsOY/g/welt/vm1L8yL+iIok9cM7fUnqiKEvSR0x9CWpI4a+JHXknIUewFQuuuiiWrVq1UIPQ5LeVnbv3v3TqhqbXP+5D/1Vq1YxPj6+0MOQpLeVJD8eVnd6R5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOvJz/4lcSXq7WnXXN2fc9+DnPjGHI/kr3ulLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNThn6SdyXZleQHSfYl+WyrfybJT5Lsacu1A33uTnIgyQtJrhmoX5lkb9t3T5LMz2VJkoaZzrdsvgF8tKpeT3Iu8HiSR9u+L1bVHww2TrIG2ARcBrwf+NMkH66qU8B9wGbg+8C3gA3Ao0iSRmLKO/2a8HrbPLctdYYuG4FtVfVGVb0IHADWJ1kGXFBVT1RVAQ8D181q9JKkszKtOf0kS5LsAY4BO6vqybbrjiTPJHkwyYWtthw4NND9cKstb+uT68POtznJeJLx48ePT/9qJElnNK3Qr6pTVbUWWMHEXfvlTEzVfAhYCxwFPt+aD5unrzPUh53v/qpaV1XrxsbGpjNESdI0nNXTO1X1KvBdYENVvdz+GLwJfAlY35odBlYOdFsBHGn1FUPqkqQRmc7TO2NJ3tvWlwIfB55vc/Rv+STwbFvfAWxKcl6SS4HVwK6qOgqcSHJVe2rnJuCRubsUSdJUpvP0zjJga5IlTPyR2F5V30jyn5KsZWKK5iBwK0BV7UuyHXgOOAnc3p7cAbgNeAhYysRTOz65I0kjNGXoV9UzwBVD6jeeoc8WYMuQ+jhw+VmOUZI0R/xEriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjkznh9HflWRXkh8k2Zfks63+viQ7k/ywvV440OfuJAeSvJDkmoH6lUn2tn33tB9IlySNyHTu9N8APlpVvwSsBTYkuQq4C3isqlYDj7VtkqwBNgGXARuAe9uPqgPcB2wGVrdlw9xdiiRpKlOGfk14vW2e25YCNgJbW30rcF1b3whsq6o3qupF4ACwPsky4IKqeqKqCnh4oI8kaQSmNaefZEmSPcAxYGdVPQlcUlVHAdrrxa35cuDQQPfDrba8rU+uS5JGZFqhX1WnqmotsIKJu/bLz9B82Dx9naH+swdINicZTzJ+/Pjx6QxRkjQNZ/X0TlW9CnyXibn4l9uUDe31WGt2GFg50G0FcKTVVwypDzvP/VW1rqrWjY2Nnc0QJUlnMJ2nd8aSvLetLwU+DjwP7ABubs1uBh5p6zuATUnOS3IpE2/Y7mpTQCeSXNWe2rlpoI8kaQTOmUabZcDW9gTOO4DtVfWNJE8A25PcArwEXA9QVfuSbAeeA04Ct1fVqXas24CHgKXAo22RJI3IlKFfVc8AVwyp/znwsdP02QJsGVIfB870foAkaR75iVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI1OGfpKVSb6TZH+SfUnubPXPJPlJkj1tuXagz91JDiR5Ick1A/Urk+xt++5Jkvm5LEnSMFP+MDpwEvidqno6yfnA7iQ7274vVtUfDDZOsgbYBFwGvB/40yQfrqpTwH3AZuD7wLeADcCjc3MpkqSpTHmnX1VHq+rptn4C2A8sP0OXjcC2qnqjql4EDgDrkywDLqiqJ6qqgIeB62Z7AZKk6TurOf0kq4ArgCdb6Y4kzyR5MMmFrbYcODTQ7XCrLW/rk+vDzrM5yXiS8ePHj5/NECVJZzDt0E/yHuCrwKeq6jUmpmo+BKwFjgKff6vpkO51hvrPFqvur6p1VbVubGxsukOUJE1hWqGf5FwmAv/LVfU1gKp6uapOVdWbwJeA9a35YWDlQPcVwJFWXzGkLkkakek8vRPgAWB/VX1hoL5soNkngWfb+g5gU5LzklwKrAZ2VdVR4ESSq9oxbwIemaPrkCRNw3Se3rkauBHYm2RPq30auCHJWiamaA4CtwJU1b4k24HnmHjy5/b25A7AbcBDwFImntrxyR1JGqEpQ7+qHmf4fPy3ztBnC7BlSH0cuPxsBihJmjt+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkem88PoK5N8J8n+JPuS3Nnq70uyM8kP2+uFA33uTnIgyQtJrhmoX5lkb9t3T/uBdEnSiEznTv8k8DtV9TeAq4Dbk6wB7gIeq6rVwGNtm7ZvE3AZsAG4N8mSdqz7gM3A6rZsmMNrkSRNYcrQr6qjVfV0Wz8B7AeWAxuBra3ZVuC6tr4R2FZVb1TVi8ABYH2SZcAFVfVEVRXw8EAfSdIInNWcfpJVwBXAk8AlVXUUJv4wABe3ZsuBQwPdDrfa8rY+uT7sPJuTjCcZP378+NkMUZJ0BtMO/STvAb4KfKqqXjtT0yG1OkP9Z4tV91fVuqpaNzY2Nt0hSpKmMK3QT3IuE4H/5ar6Wiu/3KZsaK/HWv0wsHKg+wrgSKuvGFKXJI3IdJ7eCfAAsL+qvjCwawdwc1u/GXhkoL4pyXlJLmXiDdtdbQroRJKr2jFvGugjSRqBc6bR5mrgRmBvkj2t9mngc8D2JLcALwHXA1TVviTbgeeYePLn9qo61frdBjwELAUebYskaUSmDP2qepzh8/EAHztNny3AliH1ceDysxmgJGnu+IlcSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdmc4Poz+Y5FiSZwdqn0nykyR72nLtwL67kxxI8kKSawbqVybZ2/bd034cXZI0QtO5038I2DCk/sWqWtuWbwEkWQNsAi5rfe5NsqS1vw/YDKxuy7BjSpLm0ZShX1XfA16Z5vE2Atuq6o2qehE4AKxPsgy4oKqeqKoCHgaum+GYJUkzNJs5/TuSPNOmfy5steXAoYE2h1tteVufXB8qyeYk40nGjx8/PoshSpIGzTT07wM+BKwFjgKfb/Vh8/R1hvpQVXV/Va2rqnVjY2MzHKIkabIZhX5VvVxVp6rqTeBLwPq26zCwcqDpCuBIq68YUpckjdCMQr/N0b/lk8BbT/bsADYlOS/JpUy8Yburqo4CJ5Jc1Z7auQl4ZBbjliTNwDlTNUjyFeAjwEVJDgO/B3wkyVompmgOArcCVNW+JNuB54CTwO1Vdaod6jYmngRaCjzaFknSCE0Z+lV1w5DyA2dovwXYMqQ+Dlx+VqOTJM0pP5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjU4Z+kgeTHEvy7EDtfUl2Jvlhe71wYN/dSQ4keSHJNQP1K5PsbfvuaT+QLkkaoenc6T8EbJhUuwt4rKpWA4+1bZKsATYBl7U+9yZZ0vrcB2wGVrdl8jElSfNsytCvqu8Br0wqbwS2tvWtwHUD9W1V9UZVvQgcANYnWQZcUFVPVFUBDw/0kSSNyEzn9C+pqqMA7fXiVl8OHBpod7jVlrf1yfWhkmxOMp5k/Pjx4zMcoiRpsrl+I3fYPH2doT5UVd1fVeuqat3Y2NicDU6SejfT0H+5TdnQXo+1+mFg5UC7FcCRVl8xpC5JGqGZhv4O4Oa2fjPwyEB9U5LzklzKxBu2u9oU0IkkV7Wndm4a6CNJGpFzpmqQ5CvAR4CLkhwGfg/4HLA9yS3AS8D1AFW1L8l24DngJHB7VZ1qh7qNiSeBlgKPtkWSNEJThn5V3XCaXR87TfstwJYh9XHg8rManSRpTvmJXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHZlV6Cc5mGRvkj1JxlvtfUl2Jvlhe71woP3dSQ4keSHJNbMdvCTp7MzFnf7fqaq1VbWubd8FPFZVq4HH2jZJ1gCbgMuADcC9SZbMwfklSdM0H9M7G4GtbX0rcN1AfVtVvVFVLwIHgPXzcH5J0mnMNvQL+JMku5NsbrVLquooQHu9uNWXA4cG+h5uNUnSiJwzy/5XV9WRJBcDO5M8f4a2GVKroQ0n/oBsBvjABz4wyyFKkt4yqzv9qjrSXo8BX2diuublJMsA2uux1vwwsHKg+wrgyGmOe39VrauqdWNjY7MZoiRpwIxDP8m7k5z/1jrwq8CzwA7g5tbsZuCRtr4D2JTkvCSXAquBXTM9vyTp7M1meucS4OtJ3jrOf6mqbyd5Ctie5BbgJeB6gKral2Q78BxwEri9qk7NavSSpLMy49Cvqh8BvzSk/ufAx07TZwuwZabnlNSnVXd9c1b9D37uE3M0krc/P5ErSR0x9CWpI4a+JHXE0Jekjsz2w1n6OTObN7x8s0ta/LzTl6SOLOo7fe96Jen/552+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjoy8tBPsiHJC0kOJLlr1OeXpJ6NNPSTLAH+HfBrwBrghiRrRjkGSerZqO/01wMHqupHVfV/gW3AxhGPQZK6laoa3cmSvw9sqKp/1LZvBP5mVd0xqd1mYHPb/EXghRme8iLgpzPs+3blNfeht2vu7Xph9tf8waoam1wc9Y+oZEjtZ/7qVNX9wP2zPlkyXlXrZnuctxOvuQ+9XXNv1wvzd82jnt45DKwc2F4BHBnxGCSpW6MO/aeA1UkuTfJOYBOwY8RjkKRujXR6p6pOJrkD+GNgCfBgVe2bx1POeorobchr7kNv19zb9cI8XfNI38iVJC0sP5ErSR0x9CWpI4sy9Hv8qockDyY5luTZhR7LKCRZmeQ7SfYn2ZfkzoUe03xL8q4ku5L8oF3zZxd6TKOSZEmS/5nkGws9llFIcjDJ3iR7kozP6bEX25x++6qH/wX8XSYeEX0KuKGqnlvQgc2zJH8LeB14uKouX+jxzLcky4BlVfV0kvOB3cB1i/nfOUmAd1fV60nOBR4H7qyq7y/w0OZdkn8GrAMuqKpfX+jxzLckB4F1VTXnH0hbjHf6XX7VQ1V9D3hloccxKlV1tKqebusngP3A8oUd1fyqCa+3zXPbsrju2oZIsgL4BPAfF3osi8FiDP3lwKGB7cMs8jDoXZJVwBXAkws8lHnXpjn2AMeAnVW16K8Z+EPgXwBvLvA4RqmAP0myu30tzZxZjKE/ra960OKQ5D3AV4FPVdVrCz2e+VZVp6pqLROfZl+fZFFP5SX5deBYVe1e6LGM2NVV9ctMfCPx7W36dk4sxtD3qx460ea1vwp8uaq+ttDjGaWqehX4LrBhYUcy764G/l6b494GfDTJf17YIc2/qjrSXo8BX2di2npOLMbQ96seOtDe1HwA2F9VX1jo8YxCkrEk723rS4GPA88v6KDmWVXdXVUrqmoVE/+X/6yqfmuBhzWvkry7PZxAkncDvwrM2VN5iy70q+ok8NZXPewHts/zVz38XEjyFeAJ4BeTHE5yy0KPaZ5dDdzIxJ3fnrZcu9CDmmfLgO8keYaJm5udVdXFI4yduQR4PMkPgF3AN6vq23N18EX3yKYk6fQW3Z2+JOn0DH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkf8HaHzVE381SwwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(1.0, 1.0), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABJr0lEQVR4nO29XagsS3bf+VsRmVW1P8653fd2t9Tu0UjtoQfcnhdrhCywMQZjpi0MPS820oDxg8AvMmODH3xtPfhJIPtBT4MfBG48Ax7JAhumHwRiLDwIw1gjY2RbrUZSS3ZLLfXH/Tzn3LN3VWVGrHmIiKzIyMiq3OfjnrrWXlDs2lmZkZGRK9bHf61YIarKPd3TEjKvugP39NGhe2a5p8V0zyz3tJjumeWeFtM9s9zTYrpnlntaTC+NWUTkCyLymyLyNRF582Xd554+PJKXgbOIiAV+C/iLwDeAXwV+VFV/44Xf7J4+NHpZkuUHga+p6u+q6h74OeCLL+le9/QhUfOS2v0M8PvZ/98A/vTcySvZ6IW5Dv+IxKMKNaGX/YzqcIqka0XCP8O1M+2U9/KhLcmP5+2ndrNmq307JaijJB+fFu5c3nvcx7nmlNEzhkZCf7N7Tp6t1nfgsXvnbVX9ZO2Ul8UstR6NxkdE/gbwNwA2csUPXf5lMAaxUdh5ZVCR3oe/xgyDoM6Bc6jqYWCsRawFa+Md/aEd78cvIN1LTGin71HnQlvD9To6Juk4zLY56m8iYw7nZr+FNuL1Jr7gvI30vXzBmekwjINXxFTGwfvxOGXPlr2M4dl+8dGXvs4MvSxm+QbwPdn//w3wh/kJqvozwM8AvGY/EXrufZgBImNGyV6MWjsaVClsLlVFnAMjhxcBhxeWk1dI71+y2ejcoX1rD4OZXoafER/5C87vmyhnBtJML/sU2zDmwCRlm+m8yAAqBjGxHTHjc+L1x+UTLLFdXxaz/CrwORH5LPAHwI8A/8vRK1TD4ESGyQdInQOv4YHjSxMPWpvJ6Xo9SKFBGpUzPM0yI6CxrTgLcW6YnYMESmR8uH92z6pUyV9yunfBMCO1k1+bT4iK6tCc+Y2AY8oo6fnmVE+NuY/QS2EWVe1F5G8Cv0iYu19S1a8suHD+wdSjKoj3YE1gmDRfkiRKgxe/a6a2wmky0oWqikRVVSWRKaPkqu0Y1V5CLlVyBoaq6hr6boox8VGiGBOki/dopiIPfT0ynnl/SkadoZclWVDVXwB+4U4XFWI3DUhSBYN49hoGML3ENNMrA59e6ki1hR+irvfjY8YEhkz3guMMktsBiRbM1klb8cWnNoOqzV60n6rb/P5Se+HHJEdhAPMK1dDdKFcLxYwST5g15QwRExlGQAUkSAl1Y5UU2ikGzPuxYZyTMdN+eB0MyeGcY/bEKSpU1cDIeZulNIlUZbJEIlMpMTcG6Vj6/VVKljtTrp/nDMn0AvPfTeW8kqJKymnCKLnnUaivCZV2R/UUOa6qMgO+7NuE4vONVG2h0kb3qhrzFVtqAYPkdB7MUs4oMUH0U7io3kPbjH6n09NeCkwH64SnkYxs8dlsTtfl0iBJm5LJklQ8Jd5LWyv1NU2KGqNMmsh+S9KlxjD58z4Dcn8ezAIHPZ3Ui2P8MkrvxRFUjvcjz2eWch2fv9hC3A9qLDOS8z6O8Jm+P6i03G6ZGKSF7VC++HKmFzbQxHjPvLwqE+UMM9eH8v4L1On5MAuZd5IYIWeUYUAVCK70+IEzXKUEytJMzSlnvtjuBGhL5x06OO10MohHbetULdTaS79loOLo/CVe1xyVz2vtuP9pXDJpeYrOh1nSTE7/OzfWz8nVVAU31t0HieShcyNgK6GWI5Avgm7qHIKtM0nqU0mDisjEftm/OWS3RkdspAmjaDYZjtk6BT6jc8xfqNU5o3o4/eivHzYltRMZRTWbWbmoThIn0YmHhIpB6wubyLkI/mVMUNLomjFcPjqnaOuUihxelJEBdKx6PadUbTqvaFPk8BmpnJxRrB3jSRU6H8mS9CyMuF+SjQETFHQYkPSQEjCZHFnVYpYN7eSGahk6SOfOeUwURm+SJF1XVT+TGFfRh8PD5qo2e/6oYseobYEO1+iYwV9eo75+XkbnwywVymfCYMzFARtmQwlcGRmrFucCwpsGJ3c5o6EqGtXfDIo6edmVFzbCYSQLiNZe5Jwnpr4O2YsBC+LcSPWUEitn1ByMXESn4AfOjVlKK7540BHDVCDwyewoRHn+4odB1IOqqGIx0QWGIh6TI8NJgsQBF8txtVHcR1XDPYwcnuGOGMiIMocgBx9HTF+TkifofJgluYS1AFoNR/EajEx3+H/WkGP84jWmHgzBwwpVZ22Jh8S+iSemA0SGvYMNldpVY4LhXpCqBhe9QrPSIw+X6Akv70iwsqTzYJb0MBFqn9giiVIMKA2AK9TD3AxJED4Q3G5/MKZhLAGOeQbV2FBQEUNAT4vEo/RcExUXz0nPkBvt+fXl8Vr8q6LqJn2I7YwYrBZ8PELnwSyF+zmiMi+FAt6uJBUdri2ixSes/TEeUuj/GvCVtSsiIddmLnZTszVKXOhww2WeT2lLlUyTJGlx2SjJ6g50JszCiMsHHW4pXFQz4Bs1KH02ODgkVenB5im7kF03YcRjlEBCOBpzmbruM673oRPj33LV4tx4guT3nXiJbnL9QHdQQXAuzIIgTXPAJwiiVWrSoJaWkBKWyBimFmmGOgBWSwWIjDKkVdYkX+GaizVjF7cWHE1xrRo6O8NsQ5beyGbzh+vLdNDEKEZCRF7NmJGekc6EWTh4ARNV4Kqnw1iyzOEooR0/Ftk1KVbOxNTuMb1+avDjCxruAUP7o76XsaAcuymh/0LaSSF50rm58V/z9J6FzodZgCH7LQ6Mlkjt6NxpgG/4Hn9PWIfCIXxQSbpWYw5Zd0YORueMQVn2eSItspDCgPpW+j6ROACaScwcMU59yMZkxGSDHTRNKx3uxfwEW+I+nwncn8HnUXwecBCtf2poaM4oKV6UvKsa5cZxcnuHbPsDGDhSa+VgJ3Fftll+z59lYiybMdPUMv2z1QyTZ47JWuU9NJ1vMqb0fnL/jxbOork6KJig5hYnqD5fNlI2mUewk3dRwu/l+b4y82rR7KXeSq4Oc/S3ZGD1Y+2XS6jy2Wv3yL+XAdbYXnrGnJFn839n6DyYJdJYtx+wgInrmhglx04ch5dYE/2hwXh6od4i5qIVT2GgfEYe0/9F6D+pimSIj889qNz8mSe5wqU6mjxWoWJKrChnvlMpF0foTNQQowE5PHQNd8lUTBLDYqZiOB+cYwOUq6985t2p7zWwLlMjg4qR8QKwUuUyE3QcmszUUx5MzX6fMvkMgvuRxVkqVEqZyW/qg1uYS4M8cl2/cHzuyIPwdfWSM0Ilg00KaTHJn4ltjAKf1kLTHNpXjzg/NpIL7GTUbxgxSs1orSLcItC29XMX0PkwS6k/o8s5+7sIiA6ycXAPc08kXZcbsuWsTKqg0pfZbLfUjyxZS4oI+BB8TPB6UpvGHr4n5vYetT6mafZjSTNnh5TxspKpSkmS23mSSTwfJ94COh9mOWZkVX7LDdgJZYxSLiy7K816UiUAlr8EHTgFSS/TmgA8Ng00Fm0OzCJeka6PaRKFV5Y9z+yis4LulIppBLzho5PPkoJ6OUqaB+BqD5/P6njNiDFKY7Uy89QU2fvFvUeweTmYYqBpkMYepETynNLzZPaXJCZpG1i1aHtgFnWK7PZIwpYcgJt/6WXcKj1j5f/RWALqPCLRnS48sFN0FsyiJBvFjxkmPyfaBiPEMoUGclc0Y4xRWkLphkev6SiyaW2QGBDso7QSEoI6Wa/QzSosT0kUE8mDDaJDYFEbC22DthZdt/iVRY0gziNew2P3DjoLaYVByeDlmJTMVHqMySYpJ0k4eGdQ7iyYRTgRCc1c0FmqRKdHlA9kJTg3rEfKmaGJhmhiGBHURmZuLO5yhbto8WuLbwSN14pXRIn2QLhUbfhdLeFcK4FJ9ordeRqvyK5Htrtwj5rhXHnOWfuGggFqjLRkvVVGZ8Es1RSFSmh/QrUckTKBKHkXM57Q4KVEo1NS7os1QWW0TVg+2xg0fYzgV5b+yrK/tvQXgm/At4yxGgiSRaEsBCQKpoPmVmlvBHEtZtsFdeU9mH4U2xmeL0XeRzZNYlLqQFuuZk+p1yN0HswCB4PxWHg/M+xGa5oTVRaPSwbSDWK9HLCmQVZtYIwmQudtg1816NriW4tfGdza4FvBt4JbCftroXsg9BfgNhqYpfFo7s0r4CX+BePAdILZQ3MLakC8obk1wY5pLNLJJOl81tCuFQjIkeZTVAmeztF5MEsZ3ocR7qE5tA91sVk5NrFj0oL30o3drNH1Ct0Ew9OvLH5t6TcWtzH0G6HfCG4Dbp3+QnetuOseuepZX3Rcb3Zs2h4rihGl94beG/a9Zd83dJ2l2zbwpKV5YhAvNE/BOEWcgi+eJwtGJmM8hUMO4J0eChKVYYJJKkYlMl2rsDVDZ8IsRI8iq4RQ5pIco8wzAibnT3I9IigmTfRMLtb4qzXussGtLW4dGKS7NPSX0F9K/Kv0lx69dNirjqvLHa9f3fBdl0/4nov3+L7N23zM3rCRDiuerW95313yyF3ydnfN2/trfu+Dj/P7b3+MXi9obkKfTK+Y3g8rGxOKPRj9kYboeIatDHk/2ViMqHC7Rwh5CdqdoPNgloIm2Ij3ywJ3pyiJbJFgm6xadLPCX63pHqzoHkRGWQfV0l0L/RX0V0p/FZhkdb3n4dWWT159wHddPOHT60d8Zv0e/93qO3xv8x5vWGUtho003PiOJ+p53zf8l+51vr7/JCvT8/7thncfrVFjEQ9mp8jeI30KT4zjQeN82gp8nwcKh2ctsJl8bGuS5KPiDaFMg2yFBzSbyV5C9JVBGq71BJEd7RRdtfjNiv66Zf+xhv21wa2DqgnMAv21x1155CKomgeXO964fMp3XTzhU+snfGr1mDfsB7TS02HYak/QJz1GhAcYjOl512xppcerQVVAg0tvOmi2DnvbwW4fPjGbv4wDTZ4bDgHQAogss//y8Ru51MdymAs6E2bRic0hZbS5XCxWA+sqInegIcmJqIoMum7wly3ddcPugWH/MNgjvoX+QukfKP5BT3vZsVr3XK73vH5xw6cio3xX+5jX7Qc8sLdYlBsf4i5X0rMXx5UY1tLQimcjXeiGSmQWEAe2U8zOIbd7ZLtHu25cNChf7VDaJKVkSBMsT6v0imo/xmwSzJ/G81iSWUbnwSxE0K0SRS2xg9FDz6QUjKRQPnBNg6xWARS72tA/WNM9bNk9NHQPhO5BMFx9q8G7uXQ0Fz3rTcem7dk0PY3x2Gi83vgVj9wVHsMT6diYjku/48rseGD2dNLzwAQm2eolW225dS2ds0gviAsMM6gf56Drp4gyLMZCFlOO1yxU8SeZRUS+BPxl4Duq+j/EY68D/xz4PuC/AH9VVd+Lv/094McI/tj/qqq/uKgnKaBWSIdZhkmUnT9KaWzbzJC1wT5ZtfjLNe5qRX/Z0F9b9leG3WuG/YPg3fgW/ErRtUfWDmMjPE6QCp2zPNpv6LzlcX/Bhd1zYTse2C3XdssDs+WBveWh2fLA3PIx3QHwjrvmUX/JTb9ivw/usekyAC+NQfl8zIBrpWrKpOngNcWYTxrD2fVQ2TgeoyWS5Z8C/xvwf2TH3gR+SVV/SsImDm8Cf1dEPk8oY/ongT8G/CsR+e9V9aicG15yGcM5BTaVUVinQzwmhA4k4CfrKE0uV0GaXDd0V4buUugvg0TprxV3qfiVh0aRlce2nqZxWNGBYfbe4rsNW9di4rGN7XjQ7nitveW15pbX7C2v2Rse2A1PzQ1WPG/1D3ivv+Rpv8L1FrMXTM8kDaMK4ZeTJVGBROdtHBimlmtTgHFz7Rd0kllU9ZdF5PuKw18E/nz8/r8D/w/wd+Pxn1PVHfCfReRrhDr+/++C+8SypXVYf5AcNRFtMikUDThJOSNtMzCKu1zRX9oRo/SXUfU0oDamPFgFo2OJ0lucF3Z9g81SP70K1niuV3veay55uLrlYbPjYXPLtd3xwG4x4vm93Rt8a/uQR9sNfmtp+qiCNIQCRklYw3ONUeZaxmBtnIYxdWPbZsBqaiDcC5IsNfouVf0mgKp+U0Q+FY9/Bvi32XnfiMeW0zEXrkRiS0rFjyMCK01AZHXV4NcN/aWlvzADsBaQ16h2bGFgS3JHBedNyNrsx4zsvcF7QUR5ulqxbhzvry64Xu24bndc2I4L22FQvrO75js3D3hys4G9wXQypC+oSKztayZqd8CItCjUPMcopSFc5PJqqb7ya0/QizZwa3esvn3Ja/dzmf8QLjqSKXe0AwnpjSkBWBtQ2XWE7FcyuMd+BW6dYPokWeJHIBkT3svwUW9i1FbwTlAX+tvtG7aN43bV8nS14oPVmtY6WhNm8ePdhsfbNfttg9kZJFdBJpMsZXwrZeb7CMsvURmZSsbHxWipQnklXeNlr0j8toh8OkqVTwPficdP1uxPpHntfvOGTpDaHJUtZ0JmvxxEKyPjTbLBVxPUk1pi1De4x76N31eKX3toPdJ6TKMY47P0FsE7EySJE/CCeglwihOIX1HYp+4rWBNgf6/CzW7FdtvibxuaLsSK0mJBbQRtKvGfOAZS2h21rL0Syq8FWU1MwKokii+p/HBaUdXpy8Bfj9//OvB/Zcd/RETWEur2fw74/062NkR+7YHzVdG+H5UNG85Nn3zmQOYKmvG5ABLEvbeCb2KUuNEDo6w99sLRrBxN29O0DmMUjbiI9wbfC9obdG9gb6AzgVmcQGfwncV1lv3ecrtb8XS74vHNhsdPN9w+XeGetsg2SJUUiVYj+KiCqi8srf9xM8lQmWpOEqREaBMQJ9GGC2kXd6ugAMtc558lGLOfEJFvAP8A+Cng50Xkx4DfA/4KgKp+RUR+HvgNoAd+/JQnFG6SgXAwTmyaS4rKI6t5ZDYGyIbrNHNNiTM5SZgWdKWw8jSbgKdY47EmuMs+BgL73uIlvV1CFNnJQcEGQBY68GpRL3inYbGAj5JoZ5C9weyCvWLc1BMKEqGA8qHuKSaay2GZASzDBDKMo5YswnGWeEM/OvPTX5g5/yeBnzx55wqNLPk8+y2DtkeUBiAxzHCxQfoe9rHu/2aFuCxrzXDAUzaO9jIEBR9sdiM7Y+caOme57RpuZYVqi1dFU4K3B0n5CB4S12hncEZjCmBEazsJ7nIHdgdmH6B+08c83ZQAHmvrTqjM8sthg8lqAn8Yn4KRxPlgk2mFwU7Q2SC4wNjmIHoIWd38kboxMramc4jcOeiiG20t0rmAkqZJZ0AbRVeK2TguLva8fnXDG5unbGxHG6f8rWu56Vc8spvgFTmDeg72ihGUkFog6VhkHlEis4Tv0odYkB0YRrF7xXQhPUGcjqpIwNTw1JjbO9h3xWqC6ksv7cC+r9pG8Qbz74azYZYCWSz29glvN1VWmhHPxTFVDdKl75GuR5zGWUyW6qg01nOxCqDap9YfDPiIFc8Tt+Fxv+Hd9oqVcVijPLUr9gacGPAHtaQuMniC8N0h4UkgeD9OMokSz0sJ3nCI7Rxd6aBghSESXxj1o1BBmR0Yx6fKEh+ZqLNwqMkGY3Qxj1+YUM16VApuJn8jDWYykk3vkV5DolGMy6AgRmmM57rd8bH2hk+v3ue7m0dszJ4n7oL33SXfbl/jqtlx0XS83VzxyG7YmRbvTbBLXFA9KjFQ6Q+qSeLH9IL0ByZJEif91WiAynp9kKKjNISkWjJVXYEXRjhNsU5q1hZK43uCzoNZID4QB+aolTRPWftZrke4tIC9E1bjXGCM3iGdi0lG6YUJogFQa63jwna81tzwmfY9vq95hwem44ltecdf8sBuubQBZGuiPfMIcM7gnMGbCNoBqibcM6mfklEGicKgpkLfCUDieoV2XShjCocXnD13fPixYX+MYnagwqggNfH/k8nwkc6HWVKawrGdxQqqFrKZbdtjeo/dK3armL0ge6HvLXtn2XmLjyKrFU8rDKkF4dNzafZcNnuu2j27vmFLE4C64T5yYIDcA5NMivgYPMzVoYRn0MYG9zaBaKLjnJMycFiuFUpUS9+Yqcu3eAx5dpzlxZIyYpRR6t8xisnXI+wFqgMlTpE+LrvYKs0t2J3gdpbbfTBkb/yKp37NVi2dQqcGVwxRK55107OO6QqqAcVVZwZDd2CEAUsJiPCYYTKwTQIoRxMTto05bO2XJ0Dln1yipEh7wqNyCKKUOoNxbMZtL6AzkSw69naOUe4WlrNizjBMkmXvMDtLe2vobhV7K3Rby812xaP9Be/sr3mrecDH7FM6fcpTDczzxG+48Ss6tXgEQ7BzvAYk1/cG+oCnEBHekRUpHLCY7JHTb8OaIhMYRnoL/QxMkC4rMad8XKpDkDFQOjfbheRVxIaejZRgiGYzZhJhhWHGjCLUp2IlMUtedh3SWpobg1phtZGw3mdl2XHB171w07V8a/uA3734JG+0T8PlCDduxeN+w5Nuw/v7Cx7tNnywXXN7uwrZ+ruYzBRtk2R0DpLEZShyHnJowrIS3wimMWAy78i7id1WFhcaeT5zoGVB1SoPC+ksmEVV0a5H2kp3ytTIOz6kqkLvgvu87bBxULURXBtEuvQN+/0lf3jb8s7VFV+/fJ3r9Y617VlZNyQ97VzD032A8fe7Fre1sDOYvRlc4xxfSYySjFkVhhWJptGMYcywmpGIpQz4Up6mUC4yyydJaaRW4IQ892eEUn2kViRqsleKfJYaZF3LF82lS8IX8kRu76Drhh3VrSqtwIURxFtMJ9idobtZ0V20vH2x4Z2Nw7aedtVjTJrJsN+19DsbpMk+orJ9AOQkdS2qncE2yaH9TCWpSLBnDAE7yW2TvIZ/RbWcrAI1J3FLPCuVj/+oLIw/xCsY3MHRg2Y4wQBalSH2WnJQ+s1rqB6w74In0jsapxFZbWl2FnsrNE8Ft5Gw8nDd4NbKdq1oG/1gA/QSYjwp060H4yKzaPJsogeUGMVlKqnPjnnFOIYwjVoJHlEKqnaHmv0HTKXAYHIj12Q1beaSpPLF/QNDfpQkC2QPkA1EYooyKai2PCIvPZ5dF9r00PdolDDsLaYLyK7ZrmlvVrRPG7ori1uBW6UViIfVh75V1EYG6A/YSZIeSaqntIMUqxPPwBABc2GM9/gUG4KUtEWq3VLQqNRrGTAUmZEkJrtEh8nzLHQ+zJJTKSHm0i3nMuaqbfqQTSgGnEdjMpBxAbCT/ZrmxuJWBt/GbLp1ZJosWSq0VZEaHBhFbWQYzc5xByaxex1iQ3brabYO0zno/UGF1gKDc3TMk7nLGJ2gs2GWmuuWpycMsyqXOPn6IFfMtJxy0QuH730/mHlN79Hbhqa1+FgtwbcmZNdthH4TVir6hoOacYxSDWrMclgMH5mjC8FD0ylmH935vUP2fQh27rvgNvuCYXKYoFTR6fjwvGX6QeFWFx7Vy86Ue6E06Wrm9YzyWeJv5bkDHc3fLRhGPWHxoA8Bx+0+LBkxBpOSkaxBjUE3De6qpbts8GsJ0sdGJkgpBqlZI3gb/gY7Jdglduux28AcIajpofdBqnV97IeD3h2SvvLnyxacjVDd+NsoiJgzVJngXsSW8r+n6CyYZURZdtfA9SPbpAh8lTOqdLVzQzgaiEEsRKMiplzSh7SGYWVAvIeIoNsW2a8xty1+3eBXBt9IeOkusznSva0MdVokSj2zd0gXmcP7UN3JxwpR2f84F+yrPDuwoIFhYOwOD/xRkUjDmBST5g50HsySM0Q2k6Q8Jx6fXl64izMieRhCxwF30PHAaW1Pom0TDOLtCtvYAM0bM9gWkrYO7t1Eug1SJ7dF0p7V+cxO4Q5VdN+NEO0c1k+Bw9GCsaw+y2hZagXVHansYnxO0XkwCxzwkfJYTiZbVTdJYq7UjqtR5m2pZJtvwlR0R3UoTRMWq69axJgQu8n7pzFfOKqPYSd7GG9bkyTWaI+AMTiWEpwGBppdQZjBDeXxpeXS8jFZQOfDLDCWMKX6oTCCyz2CykXjOZWSppxp+XkpqQiCjZCndO7HIZ9hCUdoaOjHLOVbz6V+OK2GNwJzTcdgRAVOMsv0c1vq5RNrgYQ5D2YRqmoIKl5SKXoBcPPbzeTSKDQ4PjdXe/nsLyiEDfopBpRsnGwBfr7ctiz0V8uVnZS+iMZslfx4+cskSapsexJbMlOG+WhVUShqhiyhtMvZsQJ6c22V3sQcQybKMu7VJbc23H9IKlITtujNnyqP5wCTmm9D+7688HhAdZScXkFsh5/Sb1HymIrnmd3vlHQ5E2ZhvEQztxlKCTMS8+ONmuZqo02MuuGiLI6U36ucteGfoR8hR8oy8dRydZDac3HNdFavf1KnP49rpbp3OYyfS5zhXiV2Mo06SzrPzjgHJV5zImPuPJhFYLREEya6tCz8O7o8qgFBZqpYFlIkhQJKjKYsGJS7r9lLkfxYXp2qFOeZkZviPWHxvgfvADeuEJGep0xFSAWjc88uZdMV/RgVMsqR7zyOVD77RyqfBQbRPmukzuS2kPCYKGAlFjUeSZljyUE1L+yUsVd6Q8covaQkSQwsTh2NC/0nwVPvDxIukyi5fTTxDCsOA4UEP8Uw58EsSmCU5HLGAR42dqqoA2D8spKbGTflzqteVlHfPNB4jDlqzFW2Fc+r1cA7SEg97Pw+t/tqJkXTJp8iEurf5e0lKZL1ryzUMzfpqvtGRqY6xb5nwix6YJQkyvMtWY7FfeJgDdIlubMOKJdoVihHQ6uGZtbH8p4TLySrWj3YPenyDISrqYBR7ZlcfeSMoX7cpzuWU59dXLYw2HgezAJV929UjKYc6HwTyXhciYNa7kpq7URnT2rXqc4PWMJejsae4jmp0E+yOWpYRsULEwk7zk9eXO71FVUnRzQH3mV9Hq0zyiTKqG9H6HyYpQyCJYPzBI1mZc4wJGDr7htEjNZca/HCj/Q/SI8M15nbkWTuWawZluwCGcobGSVJpygNT9avKe9VlkEdnXpaOp0PsyTKfP5T61qGl5KrqUz3TgruLdjYYFSjbk6818R2xgxq7SjHdVB1c0hqul+Gx4y8uuL5hvFZ6MWM+lxOwpF0Oc4w58MsJRAlcth/sBZ1Hl16iPICI5c1X+uaSn/NIqpkM6wsVa7ZC0tZaTU7psYEeXJ1skdqLzoycmnoT0Ig2ViNnmd4/tivfPOM3IWu0BBeOELnwywZ3C4QRCbUH27ASg6DNQQES1c5m0WackIosIg5mhPNNZAse6mj1EUzZWJgvHEFjAG6pNJgurR0krJRZ5QRI7bNpP2hP8dCCwWdB7NIRWUMv1WOnRK/lbgIRFBLsuBfKR3yoGDR1tFMtIJRRkboEk9lrmDg6Bb1HJaynYEq6QujAoblPT4y3pDmlnoWmZ1DGotBylMCyLyacgF9KGbjxnGVGjMkr0R1bDeV9lPFnsqllZbt5ufVPL30P0zvSSEtCrf5kChWMAnUcZ3S6F7gTJwHs0SawOU5cEYlMJeQ0EokuGSaod6aYbIXc3nPYWlJBNE0xVkOHTn0b/Zhku3jDueXdlUyyMvIcGo7t02OZLdpjk3lW/fBiJmqdtaMFK7R+TBLDYNYCBYNYfY8mJarlBTPOWbAlfeK4N5ohi/pSy28MPciMiYe7pmuqQUpKwwzAvNGbR+YdRKsDBce7hXDB6fo5NOLyPeIyL8Wka+KyFdE5G/F46+LyP8tIr8d/348u+bvicjXROQ3ReR/OtmL9MDJ+s+jwbVc1OT+ijnsmjFsl2smEgmRUMs/1fOvnQPhPoXaS1U0xYZdz8QudFfn2i7vkdIdciq8naGqhK99Dm2OUhKcQ/f7Q/J3yVT5M6RFbScM3SXJIz3wd1T1TwA/BPy4hBr9qX7/54Bfiv8j4/r9XwD+sUiZ6TEmhbpXUgzsaMBgAL4Sw0j6mw9EGvi4Cfew2i8xZRYuKGepDNfa4T6TVMY5JoCDmM9E/Vwaxeia0oAvmSEfjxmppS6sEkhlUUeSLre5sgl2qqjPSWZR1W+q6r+P358AXyWUWP8ioW4/8e//HL9/kVi/X1X/M5Dq98/SZJ4mMZxze6lbB3ulMjPzdo55ThPJYQ8R62qgL7tXYoJcKh7pQ06TJRhSeDklHWs/f9kjA/iQ2F0ay9M8nGV2y51sFgkbPvwp4Fd4kfX7K7D8KIKbxOgoTK+MPKdpX6f3yVMgcsMxVcXMZv3QojEHT63cgLLEK0qvpgDTpCZNhhhSBrjllEusYpzyNd6lETzEmvJ+lgyV9/dFekMicg38C+Bvq+rjI3q79sPkbUpeu1+upjo+33dYTVijnM6p7eo1XJdFoUeivLILWK5WYloDqcZtjtTWDNtSnUwM5MK9zsIYqX/5s86tyJy47yWV5TOAVBlhBFRm5wN33kwTFjKLiLQERvlnqvov4+Hnqt+vee3+5pMqeQCtRnHWnTQucxtnfMPyoQbcZeJl1AKHWW0U8dQjxFnbs55TNN5HUqYC4S9dJTihIUQSJM0AxB1rb4mXxzJvSIB/AnxVVX86++nLvKj6/UJWuz8DmzQDqMqUhOEJZh4heVKFoTcYe/F3Hdb5TA3bqi2QMvitGRvLRZ9Kz2mwH0qvrfC2auGBajpCtc5/YWfV1hXNHc+R7RlaIln+DPDXgP8kIr8Wj/19XnT9/pyG4NscPnEEjQydmIr7EssoJEcZ8q/O8OFaM2Lk0VKOJP2GF+Km8LxmEegSdJvbwT03lPNxyddZJ5DPmINqSnsw1WySO0L+S2r3/xvqdgi8qPr9ypQxytpp5cufE/U52JR3vOYVpjSIWnYeoU+l3RPusVzPj1Idcrspa3eUlEQlfaDi0s4uJ8nbNxlDJzxqTtosoDNBcKPKyGeXZsxRQuNeD1Fbk/31I9EwzNqq/s+WiE7W0xRrh4HosfjD9/x0kelKxSIfZhZfiXbPyBA9kpJRbafihR08ODkYwaMwyQG3kcp9anQezKKMH662pian5GYuMQiz6gKjW0Z1oVSkR7YH4nhJxrih2Xt7ZbRvzag/0+DouGMzTDO0XYRFau3XotKlRCkn0gKv6DyYBQ66dhgLPejZZHzla48Lz2hiO5yiiIvUZpU6Pw5aGj+ObMdrqqogzepiHfIoVTNjmOE+Xg/ZdLUg5KiDpTtc4DM1IDLl8gKHciMZ9vORCiRmxlk1GFeCUTDxCEb5GolmjOQhuy5zW0tQTkzaIi4Obp65V7rNixi0wDiGzgSGrJbCOKIeJvhLbixXUhQwB4YZPf9CenZr52VRGQvKaARdP2NBmhdKc/EsOI4Z5XTqOWbu8cw4zLH7nFBF8sJv+gwkIm8BT4G3X3Vf7kCf4L/O/n6vqn6y9sNZMAuAiPw7Vf2BV92PpfRHsb/np4bu6WzpnlnuaTGdE7P8zKvuwB3pj1x/z8Zmuafzp3OSLPd05nTPLPe0mF45s4jIF+IqgK+JyJuvuj8AIvIlEfmOiPx6duzFrWZ48f19+Ssw4JBx/io+hMSB3wH+OLAC/gPw+VfZp9ivPwd8P/Dr2bF/BLwZv78J/MP4/fOx32vgs/F57Ifc308D3x+/PwB+K/brhfb5VUuWHwS+pqq/q6p74OcIqwNeKanqLwPvFoe/yAtazfCiST+EFRjw6tXQZ4Dfz/4/vRLg1dFoNQOQr2Y4m2c4tgKD5+zzq2aWRSsBzpzO5hnKFRjHTq0cO9nnV80si1YCnAl9O65i4FlWM7xsOrYCI/7+3H1+1czyq8DnROSzIrIiLHv98ivu0xy9uNUML5g+lBUY8Gq9oWiZ/zDBev8d4CdedX9in34W+CbQEWbhjwFvENZ0/3b8+3p2/k/E/v8m8JdeQX//LEGN/Efg1+Lnh190n+/h/ntaTC9NDZ0j2HZPz0cvRbLEEhu/BfxFghj/VeBHVfU3XvjN7ulDo5clWc4SbLun56OXld1fA33+dH5CXkXB0vyPl/KQ5fBECRNo9ev49PwaLX+sXzw5TQ4H57o6uVc8ebZfx+51ik61e6QfM0P4hPfe1pkc3JfFLCdBH82qKDw0b+gPtV9Yviy0XDCVXaeVrHpJVZtgWvwnX6VXdjit+amtlMzvk63JkZmNLkONmfF9pcjsH93v1DLTsh9lH/Jnq1wzd+9/5f751+du+bLU0B1BHz3NKOU6mLnfTlF6EelTtKdeh09ouhj48oXU7l95FinuW76sdE7t+DGaZYxqpYSCobMxWHLvl8UsLxZsK17UiPI9dioL18tZGspeZKUtstlWSqWjL2LmhdckW7pG8nJkp65RPz+BCqmSXvQiZssnydzzzdBLUUOq2ovI3wR+kZCG8CVV/cqzN+hHKmB0LK+rYmT+ZRUUlrsyKp/x3JQvEa3RzP5AVaZ8UX3K2yqZJH33uohhXtryVVX9BeAX7nxdKf4PP4z/n3m4wDDHl3zOXzdmvNG9F6iadI7GencTJiiYZGQ75C/y1Mub292kdk1tgpW0UPWdz1rnguYkxEitzMy+O+n9tJY5FeeZ3eRp3gCeOzcxrRg/kRrVa2NtuSDxdHR8oDmGPtqPbAI+x7LfVx1IHFE5s6s6OB/03EirfQoa1knnRYdHTT+jcVn+X2PmY2ol6+tRkHRJMaEZL2fSxjPQeUgWnTJK/BL/+LHIntsNoxSzmWifLVyYrj2xcdUxN3o4LmEj8Nnd67N+iSnsiNSXpIbyvvgZSXpMVc1JL/WhuNSkXsvpiXIezAL1F53XcKvUWRuK6RimNdPy+rJL7pfueQSLCC8gq7jti5cezxcVtPZCJjVSKhQZZcScFWxn1LcZNTV95Py3wqZaIHHOh1kiTV5QOcvUDzNt2KSsVo8u/0sm3vP2jjBMTcWoz9zlU4Nbw2PSPSvnLaGJt1f2/1nV6ikvLtL5MEuOW+TeQxWdjbPibjUw65QG6hgTpBdf1gGcYeKRmqneMpcQy17U4fwK89W8v4UI8KJzI50NsxwzDie6N6qDybU1ql47457D2Ds6YRMcpRmPbQocFhw455VVMJiqqsypNgEq5VmX0nkwS3re0maZe5gZ9HSghRZ/OdhDQb6hNFj1omp/JpTjJbV40tx1S2hGVU5ooapcSufBLDkVs+GYlzSc/xzuYEljhhlLgWo4IKmw2uzNK10eY5SFbvWi4zXKEdxjRv8JOhNmyYC2U0ZaDbK+063MwfZI3stQEt2ESpU+FBsswbGRgamekQqpMe2RUMRRqbCEEUoVsxRlTtcsNdQzOhNmKeiI+plQaQOUUiedk2yBphlq5Q8bThmLNJFh+h7tOuh66HsUF1RSYizROo6SD/oMznJX0O8o1QC6cgyOId0VW+4UnQ+zZGDUxHA7xiRQj39UkMyU1yKrFaxaxBhoGmgs2jYgguy7oWK2QCyHLocSqDEvRb2JbRbeSAGGncxNWYrGzqC3VWk1B04+Sx8yOh9myajqHp4Kkh0uPiC22U6s0kaJstkgmzW0Dbpu0daG7WCa6B7vW6RtkP0qSJd9B94dGBlAFXx/XFIsxC6O2i5HZv/J6Pqx+z9jRPt8mCWH3WH6QDU1QyVymzOKtVHtxO9ti25W6KrFX7T4dYNfW3y2K6vtGsxFi+wc0jmk65Guh65H9/sgbVSz6t+FDRVR3ZP7oOTPM0Kqj5yb0RyjTpDfEkO6i4ov6EyYpT5LZpORTFAH6feJ2soZZdUiq1WQIqsWv27RtcVdNPSXlv7CoCZKHwXTK2bfYPY+fneY2x5zswsZuM6Bq8/aIWJcg/rHD1Y/XjOST+FCs7eYx2AWG9oFnQmzMCs5ZqlESZNUSZ7NqkXaFjZr9GKNv1rjNg1+ZfErQ39p6C6F7krwjaA2hA9MD6ZT7A6ardLcetonDa0VjHNIHw3fyosdQgpzgcK8rznNpCs8LyRQtamq58gi1XQ+zJJoDsDKqVRZkdKOZ9I0I0ZxDzb0D1b0lxa3CszRXQrdtdBdg9uAWyvaKuIE04HdCu0HQvtY2DSCeGV1G72k/X6cd5Ig91NGeXohce8fVa2rhly13TGkkSTFWOqeSs/8iMH9sMBoy6kWcIzbzdE0sF4NEqV/sGL/WkN3KbhW8C30l0L3ALprxV17uOpp1j3OGfq9RW4t7n2DbwRRg901NE9XmK5Hdnu0bZGuQ3MVmvo/l0Ix6n/YkUPR42pB6hH3REtUynMZwxmdFbMsAq7K3NtcXItA0yDrFXq5wV9v6B6s2D9s2F8Zuitwa8Gvob+A7oHHXXvMdcfl1Y6LVceua9h3DTvb0rkWvMF0gt0Z7HbFShUTS2bpbg+7Xdx3sWCaMoWg+F0TA1Rc4UF11J6/8J7mGOSULaKnUjcrdCbMIqN4x8kAWT4TRjC7GYxav1kFRnmtobsSuivor4R+A+5S6S8Vf+Ww1x1Xlztev7rhwWrH027Fk90aEeXWGXov7DvB7oRm2yAeWqdY56Nn5GHrxyppCa4x8MoJdZszRC2dotb2sd/jObMT84jaOxNmGdOzIp0SJYuuW3z0dpIR6y6OM8qnLx/z+uqGp27Fo9WGd9sr3hLlhgu6rsXuBLs1GGcx3QrpHEY1eFFeg9Hr+0NnqtLFTEC7WXrGlQLPle5wgs6EWY74/+mMEk/JKR23NqKxFr+2uI3QbwS3Dozi14pvQVuFRmlbx+V6zxubp3zm4n0+s34Pr4atb/nW/iFftd/NH6hwuzd0O4vdCaa3mL0GhtGwP7N6PbjVp4zF6CktXqO0hE6FOsrzKn1aQmfCLHWqzsAKlA8MO3hpZBa3MrhVYBS3Ab8CtwK/UrRRTOto257LtuON9VP+2/W7fG79LVrpsSh/sP44Xg23Xcu3di3dXjB7i+mEZmex2xbpPaZfI15R9SGulEuXkrK0hRrDTHZZLVIcJup3Dumu2UtzqZl3CMw+Y0LFS6SZzPyB1E+y8w+bdh822FYreCtoA96CbwOTqMkGC2iy3TYdgkVppeeBueVhc8vD9Zb1Zo9ufFBhF9BdCO7C4C5a9CIAftKE2NIiKmb4orXN+bXp+iyWpoUBXf2+oC/H6EwkS32wSiNsAiAlr8jKIcVgtHHl9BN+APWCV6FXw61r+Xb3ECOeB+aWjem48Wscho3tWLc9t2uHW9lBUnWXBukt0rc0+z646yWVs794lvSME2kxozrKhOsqDYxUZ74chzn04Thwl+hMmCXSaGCn2WDlDBryVxO+EndpVQEkMohlQGcx4fjALF5w3nDTr3hrf41X4dpe8sBu6dTSe8PKOi5WHY9bj18pfq30G6G/MNhOsXuL37bYxh42p6wlG2VR40WAY0G1ddzZQIzOGcWFyv4MdKjqoP54znCi82KWciYewRQS44iFtI0vqxbdRE/owuDWYztFbfgwBKOFnbM82W8A6L2lWz3FRtW08y3bvqVzNvBYEwxkvwLXgmsFtzLY1qBtg6zaELNybkhlCM81fmkjeGBpwtIcza0iSG0tjdYvoDNhFp0O7AQFLaD9zEAUa4PLvAqR5P6iobswuIto3LagDQOj0HjEerwXdl3DB6J4hN5bGuN4rbnFoNy6ltu+peuj4WkUteCb0KZvBW0EbS20TciT6YKBq7lR6Yo1Ojnj1yTAEjrGJKOhPdH+HbyvM2GWjLKHO7kbfCIjiAmz268b3EZwq6lUwQBGB7Pee2HfN6gKnbN0rWVlex42W1px3LqWp92KzllUJao2RU1gEteCWxvc2mLWLXa9gq4LKQwF04zWHM099xEa2W9zjLIwr7fazkcN7gdGxt7sIrJ8INKD2uQJMRi54kEc4EMKAhq/9+CxILAHnDXsuoZt1wSjN9oq720v6b3BOYPvDLIzmD607Rtwa+icIM5idy3mZoXs9jEdc/pSJn2fO7ZQdRytwlCjAiUPhz5yKQoznH1izbAMiU7JsI27oAuIBkYxveAbRbyABxWNzBMZRsFFaGRvGvre8nTfsmoczht6Z3BO0J3B3hqkC+cGZklMaWhuG+yTJrrQx2fp6cAhVTUcVG+Gu4gZM0yiDC0+Nnaj+y2gM2GWCp0Qo+Mc3cgwVoKKSF5QA77R6A1pYJR8XD2oM2iSOMC2N/SdxTYeEUVE8c6CF8QFJsQEm0UUQOg7cCuDrhpoY2ZeJb3gTlF1OKQxTH7IbJDnSZ38aAYSZwakDIqVq/Wi2xyAPEGNwbeCWwn9hdBdK91Djza5vaJgc7wjSBycgArqhd4LrvcDY/nOBBu8DYawGsUYCdKrJ9gyFnxrMXGVQN7PWWi/OhQVBsjUmVgKl7cwlGsMmX57Ri8o0ZkwC/VBqv0/0e8HNYSNhudK6C+he6DwsA/LN7wEGMZ6jA1ZKL43aCeBUZwgXgJA6ixqzEEKuWintIoKGBP0nPSCNaBCkGhWoLGIMcMKgdDtSvmyioeSS8zZEiFGAAu1pKZjzPCsv2V0JsxyB7i7pCHbLGaGxQw234Bfe1YXwchQL6gKxnqM0WCHpBeqgvTxYySGBUA0iRaCGvLBaMYDGtSQCkEtNeBXFl01cU1S6erP1LsrJkDCjqpVH45NpiL2cywR6lmTpE6+oVe6uWSKg+Sf8nfvw1KN3iG9D+kCEKSCgbZ1bNYdm4s9F5c7Vqsea30I4yRt5iQkOO0F6eKnF+gP/5u9YHYhryV9UljJN+Abwa1NwFyaiMucinOlZ6B4UXn8q1wTlcXEalSrllWWa60xxRJ7asl0/qfAF4pjbwK/pKqfI2xN8iaAiHyeUMb0T8Zr/nGs43+ajBw+FcofeBQvSssynAuMk1BxARrPqum5XO95sNlxtd6zaXusCdJlIE+QKh2YjkHKmJ7AKF3My90TGSae54j2iuBbwa8kSJcmuvGJ7uAdZQdHjDSC++8Ark2Y8DnslpPMoh/25pLHYiBUZo5X1Hm06w6SRYPBKSvPw82O69WeTdOzaXrWkXnatkeMDl5Q8qAwQdWYpJZ8dMMHAySem/09dC6pJRMWquX1do/QqJJDLf2iIi0mNIPDFA09u7rn2W2W0UaNIpJv1Phvs/NmN2rMa/dvuKyK1rIocO13sQSp0vdBFbkoaq3SrHo+tr4FYOsanBpWJvizqsL2doXzhJfcKl6CShIl2CXl/UzAacQIYsHsow2Td10kuM5NA7ZHnBsndU8GYhowXUQLPZyaShrue8flJi/awK09cbU3Ze3+8Y93EJXqQ+CuE0zvDkaoBHvl4eoWrwYjnr1vWJmelXV4hMfths4Exkrur9kH1TNhguwJQ05MBP88gzQDoosec4HbJqrJwuVfSqeY4a6xpGdhyoyelVm+LSKfjlLlxW8uOcQsKsnH2QCNYhrRbjGdx3Zh/Y+q0IrHGIcRj9eOC9uxNgGyfXSxods3uM6gzkAveAmYikGQPqijEdOkiemD3WJ3gcFMkmhG0MYM1RoCEFN/viVj8ExBxmNt5Vl3tbJmLyFh+8uEDRp/iulGjf+niPw08Md4ls0l89zRudV8OSYTRami0DtM5zCdIr3BR3tkbXrWpqcxjkuzZ216jHgeX27onWG7b+k7S9/ZgL94g/gIunUglfdkXGAUu1WanRL5LyLIJiC5u8yFfg5bIaeTKx/mmKumcnKX/EUEEkXkZ4E/D3xCRL4B/AMCk/y8iPwY8HvAXwFQ1a+IyM8DvwH0wI+rnlwifqBja21G64Nm4icu2CzGBU/FOYNHuLB7ru2OS7uLaZNBDX1i8wE71/DEem5twO97L6gTtFOQwDDGETwmZcBXxEVG2So2Mov0ZK77vGd3ko5NklPHnpUW9PUks6jqj8789Bdmzv9J4CdP3rmkOxpbk2sBrA1eSgwkem/YuYZLu+d712/zMXvD1rc89WvWsuG62fPa6havKRVC8M7i9uaQYWdBHViX3GUdotl2B3YXGMZ0GhbTb0P1BfyCl7gk2nxqbGYi1Md2J6kyWC0BvKAzQXAzmhvAEi/IUhYPCdsGtUkMg3fC1jWsped72nf4bvsBb7krvtW/xsZ0PGi23K5afMyFcF7Y7y3O2pBVZ8KCeSQxh0Y7Jtgodh+lyt7TbB1m5zC3HbLvglE798KO4SQ1iTocn0l/LNI28/uEIs+Ve80xzBE6P2ZhRi/X1FCiVPbLGkQV4xTTgW4tT7oNnVo20vGaccBTAByGTi0737BzDbd9S0hdNdCZgNj2gUkkq6xg9xpKcXSK2St25zF7h905ZN8j28gsfR+Q5SUS5pi9kOfw6kz5jeIlT/JocmN2Tm29CDX0KuguuRZhsykL63VwWZ1idkqzBbm1vHtzwTvdFVttWYnwunFcymM20uFUuPErHtkLAPa9xe0t5tbQPA0Qv90nQzaqnJ3PmMRjdj1m76D3iHOHwj+pLl2ei1t6OIlyo74mISorGY9VR6jGoSqqKa93Mxu4zOh8mGUuKlsavbVzbSggqNZAH1SCvW1oboQnTze8u7/iqV9jgEvT8LpYrNzyvn/Ct/vXaMSHHFxn0J2huRWamwDxmy7WadkpzdZjt5nK2fbILjCFpGBm16P7Liyad25RSuiESglSxofi+BzzjMrk9tG1+fh+5JOfSkY5Zfg6h/YO2XeYrcU+NayfrFi/Z/jgYxu++YmHvO8u2aoCDoeyVeGpX3Pj1jzqNrx7c8Ht0zXmxtLcCnYb8BO7V5pbpX3qaT9w2K3D7Hpk24dihbt9qDsHoCH0QLc/ZPkf63tNlZQ5O3NjQzq93n71+iOMsWR/7zNhliNh+GOUkoucQ3a7UGHSKxZYv79m/U7L/oHlO999zdv9Q25U6FTxON5yl7zjrnncb3hvd8mTDy7gUUvzVAaVY7dBqrQ3nvaJo32yR3YdsguMwr5Dd7ton2gszuODhEm2Sg3vyKmynmgugbq65HUYwvH43akU2EIv9DyYZanHnA9IPgBpFmtgOlGlfX/DxbWle2h58njDN/ev8ZYLtkmnDX/Qf5w/2H+cb25f452nl3RPVqweG9oPhOYGmhtob5XmxtM+7Wme7DFPtoFJkk2y79D9Ho3MMulflhIw2WUkT5mcqJm09KSQrBnSOokm18aopOdcQ3QezEI2oOVqvsMJh+MweAl5kpDQHUI0Nx321tPcGHjS8tXH382nVp/Da/CC/nD3MX73yRt88/FDHr97RfNeQ/tYWD1S1o+V1RNPc+OwNz32Zo/c7JCbbWCMvo8M6ueTymszO/NKhpULZXL1XErlobEhxjObTMUCaZL6cwc6G2aBUnQuyNko9Lw6QPfh1N2e5tbR3DQ0jw1ff+/j/Ir9LF6FvW94++aKd96/pn+8on3f0j4KjLJ5X1m/19M+7rBPd4FJtnt0u8Xv9hM1s0TX5883u2WMVDCUOyzpyMenXNCWdWBezX8k1w1FWrSuZZJl5sIuYvs9Zt9htz3tTcvqseWDdy75bfnkEPu5fbpC312zedewegztY2X92LN+5Fi9u8U8vkVud+h2h+52aN+jXT/1KAoMZLLOWMZu79EyYHeB76WQRrlhnL/4uTZrntEJOltmSTS7Jjj7PX45zCwRcB6z7YPN8djSvNPyWK7AaEhi+6Bh/a5h8xZs3vOs33es3t9jn+wwT29huzvYJS4D18pAZq2/w7Z49cDdsBD9GFC28GUO0kXM/DiV0eYafVRBuTvTZLabEFTc97Qf9KwfWbr3DNAOdVraJ4b1u3D5luPirY7VW0+Rdx8Fg7XrDwZzTd08T3I5EDKrbEUy5rfIXl6NOctUjeHZZ5DdjGGOVtE6QmfFLFUg6XBgxp08bPotKYc3VVByHuk97a2yej8kLKUCP+0HUaK819O+d4u8/wT/6PEUGzmF9czlz1a3opu+oDlX+GQqwqlzTiR1j2hhdPw8mEWmurZq4JYMk9ROtn+QNDH3NaoCUcVuPasPQvZb8kLaG1i/72mehnhOcH8rwb9jHll5/Ngjlkx913XH+bMnG0izBfe1gOupvhWe5Sk6D2ZJ64ZGIvSIYZYhvDleMazXsYfsenGRWZ7E5RsK4sHeelbJ49kFrOSoZzNnwM4kSo/KsZcpjbXMtSWUq60l11XU3KxdswCYOxNmmafqrMte1mgxlniGPYG8D2kCvcfuHAjY1oSigU4xW4d9usN8sIXdPgT95oAu5tVF6YmUa3NyCTk6NpKiJ2Z18XJrC+SP9W1SPOhY1twROi9mycVhJmqJQTMoBrZ4uRp4IqQ19j3SO8Q5zE5AFbuVUL927wJsv93D7RaNkiW1c2hyJkpbUqE+y+hw3sfwxxwYprKqsNZueTwwoR3H0UoUOf1VH6RuPG+070C875JFZufFLFCf3bkkqc3O4WEDmip00IeSXbLrQ3ZbF7ybSVxnH+wVLVMJSldzqdcghrxe2zF1BRRhixmooBZPOvZ7tU/UUxEWMgqcI7PkdEcXdTTznYNd3ExKQmWoIYUg4ScDbF+JDktWD6VGlaTxvB+T60q1cwTWn+Sq+ApjlGGRErWdYfKR2h7196MoWWB4Eam8RA14G1EBletQpmsb4jjxHNSHqkwxKWlY+lq0NdmV7EhAcI5OZuAvcIsP5A4MVmOqMo5WSsbc63mOBO/zYZZS5yaS7IUdkzT5b14PzNBl63YiyKbOz4rf2srsOy3MKhm6ZofU1MpS72YufFAZm4n3VgPl7kDnwywnQLi7PpzGuvoYM8qDTYwyf2GWInDyJtmsfo4Zm1NeAGhc3ar0iIrdUypSd+JB1o7fgc6HWWp0CgQ7FkX1sYBhUjNlhLp4GSMpcyohqZY/QmUmp77PpFeM7lXQEjh+8tIr43EUYzkSbqjR+TDLLLgl00FY6q3M4CZ3FcPPsz44NjA9dgTTmVwzt9SjRsXvS43XJfSMEbEXTKqTBzrA2TOxl7ljWlEz2QyaY5SJyF6Kecy1kSjFq8r+5J9TtOBlL2aIyn2XTobzkSwcZnwJZC2tLT+nHlLbVZpBYKt0IlXyKBXudQ1knI0RLbSH5vJuZ+9xx3DDWTHLUHwvf2knZvTRl3YKEKv8NlvPBKqLuY7l2tSumVU5mWScrCI8ll6QMfoz0R0Y5jzUkEwftnwJtfTB6m/p95kXOLR7yg0v7z16mTPeT02VVVTOIpVRUYNV1bwEuMyeZ1EVqRk6E8lSmZ1ZlHbJ4D4rdjDtyhiUW7yxcrqmUFXHb5U9txy5Vxn4K937pdLhrlHugs6EWTI6EsgbURn2P9HW5HjJnMck0dz98/b8+P+T2W9lXxZ4dSmMMGfTVG2WQhXP2kofuahzRotEdW1gKx7GdI2wjM9ZAL/n59aMbV2wz8Asslsw1zEGnSDMOROMntuOGKVcwlqtrnBipcJZMssz5aQ+SzvHaKEruoSpZ5eSLrFhSqa+K5UeYmKI2jCIcGzB33kxy9IZFs+NJy0SoSfbygVD5UVOoPcl4NjClztrm83gRXNtTCLdVc+rjjHNRtczOi9mSZQGeg7mP3WMiv6eU1n5+cMLdgf3PX/pM1jJMToZfS7AsSo4ubDdo25/cb+q9wlH7fnzcJ1Lkpldv4oEqKV0yoCrtVfFeQq9XxvwozGdBZLi6AqHE30eJlgloDjq10LjvqSTZ4nI94jIvxaRr4rIV0Tkb8XjL6x+vwBpR7KRSC6N1RoEXxmguZc/yoYbNSMj6ZP+zz/D/SvqSUwsKBT7IbESVZIUVfUy9yzWzr7s8vPckg3GSVMnmGYJS/XA31HVPwH8EPDjsUb/i6vfL/mOZAXDhC8zvZdJ7KWUIuWLPgZoVQGrbBDTi5+0UQb9zOFZ0r0XSUOTjUPZ/9q9KCbGCTR68mw5NlSLYZXdO9F9VPWbqvrv4/cnwFcJJda/yIuq319x2aoi/Ug6QnHxoelcnRxTbzURfpf0gLwv6fOcVFUdM9H50cRaYMdN+rqgz3cycEXk+4A/BfwKz1m/v6zdr25GelRe2DCIk4DjjDeRM8kz6uvRfWv3mJ48gIaTHceO0JAjW0tReI70g8XR+yO0eKRE5Br4F8DfVtXHx06tHJv0VFV/RlV/QFV/oJVN0UJllp9wfZ8ZxMvbP+W5lP2bdKOY4cPpC6VZbYbnqqK8x1zbR/qY9/WutEiyiEhLYJR/pqr/Mh5+ufX7YYylzM2MzCgd/6RTPOQYpD53/zlJZGRSRbK0tRYBgKdmd4Z/lOuo8ij9qG9lf094XUtpiTckwD8BvqqqP5399GVC3X6Y1u//ERFZi8hnuUv9/jlD61iiUI7JFNHV+CWeVvFKclqCb+SMsoBqmMmcbZEbz6NP9OBG4YQS+U1elJHDHkd3CF/MudwlLZEsfwb4a8B/EpFfi8f+Pi+6fv+LiBgnKoKMuYdUTaTK1UCp0krpcqKfxxjyzqsEYt8WeYUwLCCrbfF7lwj+HC2p3f9vqNsh8CLr98/N5Jn4ycmE5sx7KOHsiZo4Bs2Xx1M/Ki/+Tn084uYOKjT7rTw2uialRMwFdvK2EoiR5efM9rGg84P7y8DZjN0RoqZ2fPyIzq7NrBHTLInllDbUHCJbGsPFzA8lUE39nlovw3HUHjpl99TUpzeDalsq8c4T7s+pGgybwUuOYAxzdHKgavZS8f9JKTfXbv730NjJfi3K9uOEynmGJKjzkyy5JCnzMMoBmgOfnKsO9tHkoFEX6jB9Fd1NX4eltlNJMSlnOsmkq/d3EVRf1rGRzEPTLNl9xqMcGG+BdDkfyVLC2szD/ccGUbMXUfs+un6JJCjiQRMmyj2QKPFylTEKPyQMpZwENZxoBscZfjPjEMnommPPlT3PJBxwYrOH82EWmHebM7ordnFSOpwA+07Rkt00lra1hGHuTEvUjamU4qiQ3KXo78siEXmLsBHQ26+6L3egT/BfZ3+/V1U/WfvhLJgFQET+nar+wKvux1L6o9jf81JD93TWdM8s97SYzolZfuZVd+CO9Eeuv2djs9zT+dM5SZZ7OnN65cwiIl+Iid1fE5E3X3V/AETkSyLyHRH59ezYC0tQfwn9felJ9UAIar2qD2F19+8AfxxYAf8B+Pyr7FPs158Dvh/49ezYPwLejN/fBP5h/P752O818Nn4PPZD7u+nge+P3x8AvxX79UL7/Kolyw8CX1PV31XVPfBzhITvV0qq+svAu8XhF5eg/oJJP4ykel69GvoM8PvZ/9Xk7jOhUYI6kCeon80zHEuq5zn7/KqZZVFy95nT2TzDi06qL+lVM8uLS+5++fTtmJjOS0tQfw46llQff3/uPr9qZvlV4HMi8lkRWRFWMn75Ffdpjl58gvoLog8tqf4MPI8fJljvvwP8xKvuT+zTzwLfBDrCLPwx4A3CMt3fjn9fz87/idj/3wT+0ivo758lqJH/CPxa/Pzwi+7zPYJ7T4vpVauhe/oI0T2z3NNiumeWe1pM98xyT4vpnlnuaTHdM8s9LaZ7ZrmnxXTPLPe0mP5/X/K1FgqchBsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAy7ElEQVR4nO29X6gs233f+fmtVVXdvff5c++5/yzZwpZnFIgSBqIRtiEhBIYQRQSUl4R4IMyDQC8OSSAPvo4f8mRw8uCnkAdBRBLw2DGTwPjBYGyTYAJJRiLYiWUh61q2oqt7dXX/n3323t1dVes3D2tV9arqVdW1zzn77D731Bea7q6/q6q+6/d/rRJVZcaMKTA33YAZTw9mssyYjJksMyZjJsuMyZjJMmMyZrLMmIxrI4uIfE5Evikir4nIq9d1nhlPDnIdcRYRscAfAX8VeB34KvDTqvqHj/1kM54Yrkuy/ATwmqp+W1W3wK8CX7imc814Qsiu6bg/DHw3+v868JNDGxey1JWccnUZp6CAdBe1EGEcnY27x+wvjk6SPGp8LtXRZu3tmlzfP8LI/s25w3kPbj/SnjN97x1VfSm17rrIkrrKTvtE5EvAlwCWcspPLT/PqEp0Cka6/9Xtn6RZLgaxdrePGzh2cwwx7X8N20qzr0QCOCyTPhFNtI1z7bU026nqfhuiY7XX3mwTt6t/3b1jiLXd86buVe+cQ/it9S9/Z2jddZHldeAT0f8fAd6IN1DVLwNfBrhrXtC9m9WHOnA9rRk/xOghI8Y/aCPdB5GC7GtiMdI5VgfhQfSPKW6fuECXBH1iTsXQPWk7guueK0WI0IGSxzpAoAbXRZavAp8SkU8C3wP+DvB/Dm281/xIYmjn4mog9HgxiPU3XWvX2+4AUjent39zjo5katrVJy2gqeffkxKtxLK0RB7bfu+3b1h7DU1H0Drapn9t8XUlJHFM3L329HAtZFHVSkT+HvCbgAW+oqpfH9q+bWL0QCY/fGNAFVHpqo+r9t62MSZ9Uw9hQMV0JF4Ce1Kvd+5431Yt9okWS6y+CkocM4VDRIHrkyyo6m8AvzF1exHxvbMO/3sXrE73bAhVhbrdATG9HulMt8fHonhI9PZ7dSxFempvj5CpZe2uPfvHabdtAzZY5z70j21Me/3+/ky4vgGo6kHCHF8EV3oPJ/xPGptOd58hadKsbx5Eq0qmSS6t690+qR4bfw4hZaz22qFO05Kop346BnW/Tc2xh9oQf+JdD8TcjoYsrRU/dtOjG+Z36j2kPmH6BOkvb27oVeydVJtS5zxkSCYItkeSAfL7ezXgCfa27RznYVVzwLWpoatAYfyBpTySzgESnlK8Lj7GnoRJeFjRQ+zYP2MPMz5+jwQSG8kj6qHjhcVt7F9Ho8LqutMmiTtRv11DRBkKBSRwFGTxgbCERBmxAZLbwq6nj0iojiEcn8NIlxAP2RPjWE97nphwfWl4tYOH4/TIP9CO1o7px5uuel6OhSy6L0I7vSRlSD5uRJ5GUkr1bY0hIgVDW12CKCk3uCdt9gzaQ9catuk7BHvH6rvlLuogHLZX4FjIAuMBq4YwU27coeOP7NMG8Aaivm00ttklSInWU4tEftujr4L4GlPqo7+uLwkPoROdHgnSDeBoDNw+9oy1wUBdQOqmjqDT6+KeLVEMo7e88UBi/b7XlpTXFiHZ9r7qG5FaqTYnt+97PA1Rgk2lfXU4AcchWQTE2r0bOSkw9xAqKiWy291jcZz0aAw418aFRPfjJ0kDMz73oTYPkSJWfyljObXfngFs9889UQIeB1kQsBah7jJ+aoALri6S4/3HEm8pxISxNhkr2Z2iZ3gPJEOTRvchIzSVHoBugtJE2z2s+moOe+U9rgFCEO0pV7TBIYkxFoRKLXvEmEMnKNaPn8TfnX2uQMhHxKAr3Ab1nlJvSOmJ/yZrHAw+jcTvnhifEsQ7hP6NS/Q8VR3MLPdtjsYb2ktGxtv2pKfY/WWTCZW4zs79HLpH8XVOUN9HQRYgreuDjm3U0xhRrpRETOjppifueUTxblNqVELbG2+jyYzH+3c8KIjsiwnSdEoHiGMp4Xcbcwk2y8HSjQSOQg0dRCNpGhzwODp4iJhMUoRHeRxVPXyjexJlLJ2RPN+Udl/12lL36wrq6HgkSx9xCD9VNdb0mlRQaqxHxjbFgBF5sGJvKvqZ5aYdYw95KD0RL4uXp9YnvCQxbryGZgKOR7KkkmpxnmWsV/Szp0OZ26HjsCPIJNF8lUxznBlPIVGK0cFeWcL+9p19UvZXXzWnMtYTjP4jkSwTRDqMi8wpEV7Y3ya60QfLna+Svxrr8a5Xe0O8aaJCb8ijGwjzJxGH9ZsaoGb5ROlyJGTZIVnsM9XlHLuBQw/6CugYpf3jjHloYX0nXdA8L6dge9fcj5P0Eavgq2Ao2jyRMMejhvqYIk3G9mt+T7yhjThPqbB42aB6m0q8hAroFFr3SgYGbYxm24G0xSCG1Gf/WAkcnWQZwkFXL+7dfYLExuJgVHjfZmqkyF6dycHGpu2rfhnoLgO876rvXWt8/qG6mKFI9JRM9oTre2rIsodUJvpQmNyNhdCjsP0BKZFURwP5nE5MBtL1Mo3nd0jd9gN5TQdKOQF9cnXa1ssPTewIR6KGRgJpUWxjD72gXCqT2hHjvQfcZJhTWea9Fo7VnAxJq5QKGSrxHMIhzytWj+rGH/yQNJqI45IssaSIVUYjXvu9speEa1HX7b57FfQwInl29Sl77SKE5PsYkQZJgo/Vpxx6cP0QgUZEmdKu1PU/ld5QqtF9whyCxhVqI8U9Qzo/hQOGdkfNpK4hLrGM3eGh+FHCrkqnBg6rzI73lVJL/WMewPGQ5SqY6gaP1WqE6Gqr9/fqPva338PYTT70MA/FZqLtWuPbTBiO27N/BvNXTRuftnB/v7mD8Qy/8vDxxjLTA6J8aP3guYPUGh22OnS8MemwF8DrxlM6bnZY18lYR23oBBmHOs4h1RzhKMgCoYdPnHRjkExNwrGdVWDAKD6k2vrH7tkynQKllEhPqZle/AQS6uFhE31Gunm0Qx1qSF0ewFGQRZlgDELngY1Kn6uI11SP6+/flErI1VL6neOHY04uCxi7hod40HtteQgcBVmATm88mO/oPWAfNBt54C4Rj+mj7z30YhNiLWQCbbzE9coObMeeGJVCzfma/32V2Zd8fbU1cB8mId4nZXCP4EjIkmjsFW/CXonCXn2u6Uim1H7JIFcgimTZTr018Yy6Rqtqv2gphT2b4oBtNNV2Gtl2ci6LkTRGhCMhS2SQHmB4I0U6RduPSLTBc1kDee6JUoTvOLez3sB6DWW5K4gKvXSvqHzI5tm/wHRjxryrVDQbkm0YOm67bb2/SYMjIQv7mdQUAeKscixKY+PuCkQZdZHF+BEHRY7kOSwKtMghs6i1iCqS2XBah9S1n1QnFT8xsnsI6rpkT92HoTaNScZJRm0v6DlFIkY4HrLAvh5P3fhUDiaVso/T/JFReTApGB6sWIMsF8jJCbpaoMscXeS43KC5AQdZbnf5ks0GtltUD9hbY7hqhn0KpnaeCfVAR0GWwVtkd/H1VPCsgyuErSEhVaKYiWSZJ8rtW7jnblGfFlQrS72yuFxwFkShWFmKzGAyi1wWsF4j2zLM6aJdI3h34v04UNSGJGGueG2T0ZfET0elXMK4tHZv0hpxzsdihtRNIlHYWecSxVG9TG1rpyyXuDsnbO+t2N7NqFZCuRI0A2cFcYrLBFhSiGDyDLEGtRukqtBt6Y8ZVaV1bIih0P8ht3+szHRo+zF3+Qqq+2jI0qqKRhw24fhmrI5J2CgHbuzeUNQhMRt7PKslslzgbp+yefmUy5dzNneE6lSolqAZoV1CvRTqIqdeGPKznOwsxzzYIOuNN363pVdNUzymfnnB0HU9jPE+VOtzxUrBg2QRka8AfwP4gar++bDsHvBvgR8D/hT426r6flj3c8AX8Sbd31fV3zx0jj3OR3EGxd+4zgCvuFf2e8wVEm3t6bIMWS2R5RK9fUJ1e8n23pKLlzMuXhbKO0p16qhPHBj1erMWqpX1EufUsvjQsPjQkt8vyM4KzH0L2da3Zw2oevXUzwDH19u0P76u2C6r413SEez4moeGxO5U+mE7JcYUyfKvgH8O/Jto2avA76jqL4aXOLwK/KyIfBo/jemfAz4O/LaI/BlVHXHIeojC5e3FYnf2RL+YKJXmjzyElDHbVwdSFDui3Dtlc6/g8gXLxSvC+iVH/VxFflpy+2SDMQ5Voawt5ydLLk9yqhPjP0thuTAsMiFXH+MQF6RkVYWZreu0Sphi3MZJxStImDbeYqIJluvpj6TBQbKo6u+KyI/1Fn8B+Cvh978G/iPws2H5r6rqBvgTEXkNP4//f75yywLaXhFXkkWzNIaNujtNqDLDWn/jrEVOVuidU6rnVqxfKrh40XL5krB+2SGvrHnp7jkvnz7gleUZmampnOWyznnj9C5v3zrl4nRFvcqoloZqaakLYWmFfJFh8ww5s7DZQrmFLSCuO3ftWLsHamsO3LR926w93sMXrj+szfKKqr4JoKpvisjLYfkPA/8l2u71sOwwEjdlaLhE54JToWvopA725pIVs7NRihw9WVLfWbK5t+DiBcvly8LlK47slQt+/OV3+V9vv8OPrt7hR4t3sCilWu67FX968iLfPn2R754+xw9Wd7g8KXALi2YGNd7NLqwhw7viegE0JOl7SWPq9KpG7ci2B2cyH8HjNnBTsjTZqs7c/Zz09nC7IFzzP0iXpgalM51Evwft3dyeyG0y00Xwek6XbO8WbJ4zbF7wqid7+ZJPvvQen3n+u/xvJ9/lx/J3+JHsEgusFS7U8sP5e7xc3OfF4hW+mZe8nj/H2qwAC2rwIZcCUcWK96BwNboNnl0/65wiRayOw8zc0Nuvn9caiDF1tunhOsP9b4nIx4JU+Rjwg7D84Jz9beOiufvvyD3Vut7P7wygcwPGDL32b5QiCOpHsgxZLNCTJdXtBdu7GevnDZvnFHe34oU7F/zI6Qf8L8sf8In8XV6yl9w2FouwVIfVmpfsGeu8YLPM+aBccXE7563Sst0skUoQNYiC6ILciJcwAGbj3/lRlmCj+NFg6iIie6pzxMZwtC4pReJtonNeZ7j/14H/C/jF8P3/Rsv/bxH5JbyB+yng/5t0RO1N2jewzXiFPume1xQINQZekCosCnRVUN7O2NwRts9BdddxcveSj9+6zydP3uET+bv8kL3grhFOpMDhH+YJym0peSm7z7kreG91yoNywcVpwYdby7bOEWe8n62gUoD6Gy5GvIQBb3sdMDZTAcTJGFNjQ1n5AUxxnX8Fb8y+KCKvA/8ET5JfE5EvAv8T+FsAqvp1Efk14A+BCviZK3lCUzGlXiWOeprdrNSSZZBnSFGgqwX1aUF5aihPhepU0ZOK26sNLyzOuWsvuWPWLMRP/FFqjcOx1pq1Kg6DxbE0JSdmy618w+liy+VJzmZrKCtB6vBRi2hQSeBVklz6WAzsjN5UYdYhW6VXM9O5T1PxOML9qvrTA6v+j4HtfwH4hYONiyHsXLrO8t4FdJKHIzo7RkOUxvsp8lb91LcXlHcKqpXBLcBlilhFRHEIpVrOteBDt+VCHDlb1mrYaMa55py5FffdkrfK57hfLdk6ixElz2vKVUW9NVSlwdTB3ReLmgWFNVjj0wSEAJ53rct9L+lQp4gN+imFTQ2hUrUxB3AkEdyRUsgh0Rmj36P2ShrDi6pyb6c06qc6yb1UOYG68NFZMYoRpVZh7TwhcqmxQf2cuwUXuuDcLTirl5y5Je+Xp9yvVmzrzJPF1mRFzfakpqwlmPhNbMeiRigEMis+YdkUTNX1zluKb4Ek7JAUBgq+R0MLjzkod+0QRiz3q+BQrwqqSDOLKyz1wgfSXC4hjK8IUNaWi6rg3fKU79oX+MCeUKuhxvCgXnJWL7lfLbl0BZd1znlVcH+75KIsON/mbKsMVU88LRz1ylC65koBNYjLEQVbK1JWniRVtTfFOowQ5OD1TiygOrRtwFGQRUnckH6xUErCpOIUA8acqkavOxTUGtQKLgtEaZ5jLWzKjA+3K940d6kx5FJTqmXrMu6XS+6XS87Lgk2VUTnDtrKUtaWqLHVlcM7gakGdgFXcUqlckDAqmBpMbTGVImWObAukqj1ZyrLrUh8ixJSHPhCD2pt67YDKOwqyAF33z3WHoqqLZqyOVc6h1L3pv8QpbGu9R6UZuJAYVBNUgRO2ZcaH6yVOhYuqwIhjXedcVjnn24KLTcF2k7WkwAViKC0hdm1QNHe4JdRqECeYUjAl2K3BbjLMRY5sS9+uPlJ5nqkYKqmMjje6bQ/HQxbokMC/maxn4E5Jt8foZ65d5glTux1xAkRBakFLQ7nNODcFZWU5zwucCmWQHttNRrXJ0I0BJ4gTcL1opPiPSkRs8Qa0K6Be+Ax2tTRkC4tdWHRtR1XBJJJMSXtIE564WsYZjo0sRIlC6b4wocUhgyxxw1rCNDEN57zrqiAufGpBakVKQ722rF3BxirnxoEKdW3QyqAbg2wNdutd4laatAQhMA/UhCizqF8hnq8uD4RZQF0YXJFh4jfF9q/nqtc8dL/GCp2enoLtBELdStMDUu8KHB07FMPt3s8jqkjtPDFqTxQc/nclyNYT1dX+4dbNg64FqQSzFexWMFu8VGmaJV6VeXJ4kqgABtRGv433ulxBK2XcwqJ5hjEGbZKkqaEiQ95OCmMESwX5nh7XeYfUq22TL5Vstu8TZuiGOg01Jc67qs5hKsVUit2C2wq6Fh/tVoNmCgaaN2ZLJUgFpvJEMaUgNZ3MlxrpSBi1oJmiTlq7SJrmCTgLLhfqhSFb5bBcIJc5lOLb2sTe+3XJicF3yXs51Jl69uFUHBdZEv7/4BtAUl7AkDHYbOuahGSQLJXDlhazBbv2Dw/1Nohm0koKcZ4sJkgfU3pJZGp2hBF2454jsrhacLn6+XOMeNumcUqMUOdQLw31wmKKHBYLRExb/0Iq4Qh7Hs7BRGBKOk14wVWM4yJLg4HGj86ePeGCNaggH9MIkqVU7Fa8BLBC5c+EU/U2hwQTJEgVqTx5GqLE0qUzd0+wT5qAnKqi1v/vE8zlgissuswxee4lgjrUuf3anYfBkBpLZfdHcJxkiRp/JVexc4iRZGRVIWWF2dbYrcWWggvurPHVBRgJkqUli7Q2jqkBx86wbTp13Lkj49lUoCo+yas7kkmtSCtlQHOLLgt/SPVGuFbVrt00qsX57HBUV9vMpDAYm5pi7zwt1f1JHGh8U9jU182j9alNiWNZYUqHKTV8xH9b8eqjOb5ED9jtvjvEaOya2DMCP/6o+a2+fBf15DFVdBwFtYJmxmfDqxrJMk+U/ouwNDFhUZw93r0Ye3iMdApPXZzlKohslEEpstezLFrXSG2hrJBNib3MyDJpo7nWaIjzeBsmftiS6JwaDFqJJU28XIHYe3V4CVaq/64UU3tDW+pdDGgwxJ/oQG24gYneYf84E/c5DrKEyvdB0Zm6sL4ReyDWoE4Rap+wrH1oXdYlNrfemLWCs8a7vY1qyYPd0RwmODvNdwcJA7f9blRSMIrN1n/sVrFbxWwUu66Ry9IPI9mW3fFG4SVYIj0XN5Yezb3pvShzD48wDvw4yAKteE1GFnvDGIDuLNX9m9YfRlE3h9EwOaFAWSKXG4wI1hqcFbI8io8gLQE6JMDv3vb7hliGXswlbv9OfZkKbOnddbtV7EaxG4fZVMh6g643PkfUGQ5iojLM/YetqpPeyzxIlCn1QRwTWYaGdfYHctvERcUVdH2ihGMTxyzqGraljxIDNrNobv04ZuMTjGoVzcSrl8YVbiRKc+hGckA33hKkSmzHtHZP5dWPJ4yXKPaywlyWyKb0IxnrevdW+HgYbArhQXemD2uWp+5h755fBcdDlgZ70UWzUyNhzM7g+3lGRv3FMRet/M33wbkasQabW/JMUJtT54rk0not2gTnGonSSJGILMFD7uQQ2+gw4TtIFtMSxZFd1tiLLWy2aDN6MaQkgHQdbXO9/sKGC8F65NgLbsYBvqcm3C9dV29v3rWanZoSTQepIB3dhM44587kwk6hLCHLkDzDZoYsMz4En/vYi8sECe50a7Ows10aIrWEatsSLi36blIMpgKzDepnXSKXW+Ry46VKmOtldOzT0LLEtQ8F6yanSiIcB1l6M2z3Xy/n/5hdb0hJkEPWfZRraquCNUiqbYmst5g8ayVMKMP3meMgUjp1L03Ox9LGY9pTtdVxoWmxKnJ476d0mG2NbGo/80KQKHtEmVrJlqz3SefW/Oayv/0BHAlZIkQ2RzxYvmP4DvWmxAUnK/B6g9C1qmC9QYxgjQkqpzGkdzrHCX5IUBN5zQiR37j9weB0sou7EBHG0UaOzbb2VXJlBVXVHQvdv84p6NfvWPEdIzrOwwY54RjJ0qBvoUukRkhEKB/hJlDXsPFjeXypAMGFzn1yMFI+TcFUE0OJ+ODbBaDSxl3aQF6rgtSnDkqHVG43L10zpwsMk2RCWH7PMzISxekmRHVHcLxkgaTeHpy5aSDRljIQ98fhBIO3qmCz9RJGhLzliUGc8XOy5IJkXiW1VXGZtnZLY5NI3QTfwmcL2SbEVcpQTxPqanTgWge9lQMucEqSttHePqaOCuBoyKJd4/RAmHr0TWVj1v1QKl/VzwkHsNkg4SFmTQ/VPKgRg6vUFzAFt7pWkKYEwaovoqq6RPHBN0JMxasgqV3ILw0Ys1OQii0NHesqxdsDOBKyJNAfD/OwaGIvKTsnHkNN7VXI1nsjYgxyHga1K4jLkJqWKC6DOtS1uLyp5ZUonL8L6dutkm2U7NJhLx12XWHWla+7repgaGvrLk+63qsOJnvU+8jRkKWnS/sPd4j5B9zGbmG3SR4zzuLu5oMBlU2r+61zSJmTrTPq3OBygyuEuhDqtVDnEryi0JTGPa4UG0hj144sDsBdbpEQW/ER2wO9e6++uBeEHCJD/zqvqtoiHAVZvJcaqZ5HqWj3O3ZiNqlhJvHxtX0V3c6obk3aug6udYG5yH1xdW5xucUVhnrhieNsU28rbX3vzutx2I2XKLKJ4ipl6SXZoeTh0P3oD38ZIEycrU4SZqLEOQqyjAnRwURh3zOKlrffYYqOMdtHnbaeFoRtxfnE5hbvqZSlJ0yRw4VFM4vNM19/kltc5lMEWEEjT8R7QN7rkdJhtsGAXm/RMLmP1m4XiIuRKHmc/FbZ1LpDCdoJOAqyAN0bEw9XGEoSjqCNTup+kVD/5oq1ba8LO/tAXV3vJI4pUbv1M1laC8b6YafWgjFYa/xIx5C19iMfe22uaz+QrAwzWW42YWLCxCxQY5J0aJKekYjswbrliTgesqTQyzQDo5IiRjcPMtCTYhL15kBpplDdqaiGwLJH4GbgvR9TvSNU23aRnZqpqkCY7Y4kY1NiJDrRldCTKFd6i2wPx02WfqwE0vGS3niYwQFUh8RyCtF2nQLwqDzCE8sHw7Q2YB0ilY95NMeI1JzGNbUpQ74/B82EsUPJjH2qUOojGcE9pHrGelkv0pmaU25/l0SwLybeWOkEeG9KjK90a2pmOtcTe3v7JQd7krAhTKdkkn3CT5E2Q6UJfW/qAI6XLH2M5YPGAlGh6HvKm95Ha3f750r19kgytDOB94+VOIfuPbDEXDVj1zlW+JXoAIkGpI/bw/GRZUwUpzBU59HsExEmuc3YsfvHmjo2J/6/Fx8ZOUbYv42JuJ4ESHWYIbUyxTaZMuIxwqOF9B43+vo77iWxSI63aT7hf/sy8AYJtbO3TeocfRV2qPfFDy/V1ri9U9FGmDVa1L22vReZPwoOqLTjIksKQw8Vphmp/d4zgtQNHy0IT3kv/f9DYj98ruqd9G2u9jV7sQ00JDn79yvebojgEQ7eQRH5hIj8BxH5hoh8XUT+QVh+T0R+S0S+Fb6fj/b5ORF5TUS+KSJ/7dA59i6qj6aqrS8xTHBZex5Lx2uJb0rk/qbsk84Nj43HlNSISXSoNLHZpmeQDhGlJUSkRtvobD8t4vz47ba2eKy8oX8/mntkpkmnKZKlAv6Rqv5Z4KeAnwlz9Dfz938K+J3wn978/Z8D/oWIJCy2CYg9jtgQlRHxG93kRt3EEwMNPbjBrO2YbZBSOWPqLdp2kkSZcs4UAcb2j7Cvih9RDanqm6r638LvM+Ab+CnWv4Cft5/w/TfD7y8Q5u9X1T8Bmvn7p6H/IBMBq0lzz/Vd2xgp2wcOehsH7YKEjdGRin3pGDAk6fYCcoe8tbH9E9um6nrGcCWbJbzw4S8A/5Xe/P1APH//d6Pdps/fP6Q3U6JXd5/xRg/YFUOESbUjjoE0hEk8+L2e2iTwGvWQkACdQvW+x3Yoh9NXhT3VpbFaGrO93LR7OZksInIL+HfAP1TV+2ObJpbttUJEviQiXxORr5W67rVqpAfHvTd+YIkAVXPzxwbJdzyjqZ5K342N9k2ql54hqymJ0bNlJrdpgmE6itjeOqAaJ5FFRHI8UX5ZVf99WPxWmLefh5m/X1W/rKqfVdXP5rLcNbiZyivVq/qG2Zgr2s/l9HvUVaPCvs3pV7+EfQfViXTfttpsl9o2Xr5HmJ6E26u1HTnnpEjtY/CGBPiXwDdU9ZeiVb+On7cf9ufv/zsishCRTzJ1/v4gNttPUzoQ1o1eZH9dylNpemDkNQwamanlTc/rxXX27Ip+xFcMe95NypXt/d8jUs/AT/3uoO8pHrqHE6TTlAjuXwT+LvA/ROT3wrJ/zJOYv3+gmGe3ulfPYhI9zWnyRkwqrOqHzA/Fcvr7Nb8PjXPqEy6cJ05TxO0cHdvc3LPmusfSBKn2jmDK3P3/ibQdAo91/v6R2SkHAluTZlDoi+J+0VRq31Rw7WFd3Xj/Q6o1ljgRYTrbhvvTvDuyRefawowUA7U8e5MlTwwMHl0EN6mzYT9WMSXbGiO+mdFDExu5pH1PJY7lDKmSnrdxyL3unD9eFl9nH7HxG3tWPSN/NzQ3UuVx+0LsaaRxo20/OrI0GFUPYyH8lFX/CAU/QHdI6VAUdMz+SdXgjCHuDAn7ZtRTSrm/CeN6f7+npkQh4YoSqYmEuI028t+xqojEdfKYUQlASwTdzdTQrB8bdzxk8+wNahsqrYiWtfHthEfTuTYZKexq99mp5uSozdR9eWpLFGKk3N32oY4YePF3fKwhSURPFTTGYSqe0ts22ZaIKB1S9Q1m6KivZFAsNt4PSMjm9X7t+VLvcIrbcIjI/V0PbvEkoIlg1oF6jTHdmyxBGD3/FcoGxvZ5SHXX2DmjcZMrH3RAsj5CKYM88jt+HgNE5G3gHHjnpttyBbzIR7O9P6qqL6VWHAVZAETka6r62Ztux1Q8i+09DjU046nATJYZk3FMZPnyTTfginjm2ns0NsuM48cxSZYZR46ZLDMm48bJIiKfC6MAXhORV2+6PQAi8hUR+YGI/EG07PGPZnh87X0yIzDiWtYn/cEnYf4Y+HGgAH4f+PRNtim06y8DnwH+IFr2z4BXw+9XgX8afn86tHsBfDJcj33C7f0Y8Jnw+zbwR6Fdj7XNNy1ZfgJ4TVW/rapb4FfxowNuFKr6u8B7vcXXM5rhMUCf0AiMmybLw48EePJ4/KMZrgHXOQLjpskyaSTAkeNoruFxj8Do46bJMmkkwJHgkUYzXDeuYwRGHzdNlq8CnxKRT4pIgR/2+us33KYhPN7RDI8RT24Exs17Hp/HW+9/DPz8TbcntOlXgDeBEt8Lvwi8gB/T/a3wfS/a/udD+78J/PUbaO9fwquR/w78Xvh8/nG3eQ73z5iMa1NDxxhsm/FouBbJEqbY+CPgr+LF+FeBn1bVP3zsJ5vxxHBdkuUog20zHg3XVd2fCvr8ZLyBiHwJ+BKAxf7vJ9y5pqbMuArOeP8dHajBvS6yHAz6qOqXCQU5d+Se/qQkR8LOeML4bf1/vjO07rrU0FEEqmY8XlwXWZ6mYNuMibgWNaSqlYj8PeA38WUIX1HVr1/HuWY8OVzb8FVV/Q3gN67r+DOePG46NzTjKcJMlhmTMZNlxmTMZJkxGTNZZkzGTJYZkzGTZcZkzGSZMRkzWWZMxkyWGZMxk2XGZMxkmTEZM1lmTMZMlhmTMZNlxmTMZJkxGTNZZkzGTJYZkzGTZcZkzGSZMRkzWWZMxkyWGZMxk2XGZMxkmTEZM1lmTMZMlhmTMZNlxmTMZJkxGTNZZkzGTJYZkzGT5SoQ8Z9nFNc2P8tHCiKItWDt7o3rxnTW7y1z4Y3yzdSxztFOI+sU1Pm32qvbbXPkmMkyBhEQg+QZZrGAIkeyDKz1xLAGRFBrdv8hkEHBOaR2UNX+vypaVbAt0apCyhKtHVqVfr8jJ81MliE0RDGCZBlysoLVEi1yNM/Q3IIVNDOoNWgmuMyTRZz6T+kwpYPKIXWN1AplhWy2cLH259huQS1a1/7/ERNmJksfxiJGvMppVM/JCr19iru1QhcWl1tcbnC5eJLkgssEZ706EqeYGkyp4eMwlfPk2daYi6DOrAEj6KUi4AlzxDhIFhH5CvA3gB+o6p8Py+4B/xb4MeBPgb+tqu+HdT+Hf4tGDfx9Vf3Na2n5dSDYJpJnSJFDliF5jt46wd09obpdUC8sLhfqwpOkzgWXgVr/QUFq8WSpFFOBqQx244lj1zXWCMYYjDGgitQO3W6D+jpewkyRLP8K+OfAv4mWvQr8jqr+YniJw6vAz4rIp/HTmP454OPAb4vIn1E94jvQILJPZLFAFgUsCnRR4G4vKe8s2N7NAknwJClo/6vB+5YKpgKpQSrB1IopBbtRso16QglgBFFF6hqqCuoa6hp1N3sbxnCQLKr6u+G9ezG+APyV8PtfA/8R+FmiFzUCfyIizYsa//Njau/1oCGKtUiRe6KcrNDVArfKqW4VVKeWauUlSl0QVA+eKJkniwayqA1kycDVgmklj1dVavwHp9iqRralJ0ogzLHaLQ9rs3Re1Cgi8Ysa/0u03eCLGuO5+5ecPGQzHh/ECJJnkBewWuJOlrhbBdVJTnVqKU8N1VKoF4EoeSBAIEmjikTBmZZ/iG22E1ymuCysAKTOMdsa2S4gkOY4aeLxuA3cyS9q7M/d/5jbcTWI8e5wkCy6LNBVTr3KqE4s5cmOKPXCqx/XqBN2KkiN+qtV8VNFE7YRcOIliwqIA3EGU1rsMkMuc28jbTLE2p2he2QS5mHJ8paIfCxIlaN7ueSVIOKlingXmTxDi4x6mVEvrCdHTviIlxJNlwjPUupAACO7/+rXi+t+x/ASyWAWGbLNYbHo2DDHRpqHDfcf7cslr4Qm8tpKFoPmGW6RUS+MJ0rWfHaqppEoogSvB0wJdgN27X+3nyoyeJ13q1vSCD5Wk1u0yJHlAlktkaLw7ZHjysZMcZ1/BW/MvigirwP/BPhF4NdE5IvA/wT+FoCqfl1Efg34Q6ACfuZoPaG+UbtcIMslblXgVhn10raejieKePtE8JIifHCBBEGSDBodzfbg1ZIluN4GKTJkUUOdI1UFtYu8I3M07vQUb+inB1YlXxCkqr8A/MKjNOra0cRTsgzyHFkukZMlerKkPl1QnmTUy+D5BK+nlSauq1JaAjRqJ/4vgVwS7BoHGMHlCiqY2mAWipYGLTOkrCHLwJZP7FZcBc9WBDfOGIvZI4q7taQ+yahXwaBtvR4vVSCQpQqEkch+IbJTAmFiokjQPqJeUgFUTjCVN3Rl6zyBbVCJR5jdfnbIEm6+BFvAq55AlNMV9a0F1a2C8palXAnVQnZR2Ub1BKliaoKUoGv1xZIFT5bWvVbfBAeIBK+o1tYm0syAPe4SiGeDLDFRgvqR1RI5XXminBZUtwNRTg3ViVdBnUPEdokDUUVVWnXT8XYaiWM8WZzd2TcIELylJj7jvazjJgo8K2SBECEznijBmNXVAndS7CTKqaE8EaqlVz9SBynSs0VEtWujtG6y36hRWy4QRe1Osoh4YeQUjMVHdJuo7px1vmHEXk/eI8qy8PGUpaFeGKqF7HI9QRqo66qW3XHDd8jlSHjIKsFrstEn3GVVT0A1PmbnPSxt7R5xbucJqe6Ko44EH22yNOrHiE/chSwyiwJdFbhlE0/xRHFFyPNEYXwJRmoMbY4bYiaiO2O2UT3tcTJweUwIMNuwfxW8pDbI5wumqJ0PyB0RUeCjThbYqZ9gr5Dn6CJHiwy3sLjCtMnBWKpo8GKa36266RBHkFp3Xo/dJRddRshKa0s+RBHnrWVxElIE4UiqXko1ycSm2u6I8NEnSwPnaOtdnTcg1IqXLEUc0t/lfVzwZsSFCGwvNqYWyKU1aNWE/Quol+ojvrmitvGlg9td7RjXsXmOjBx9PDtkaaC7HuuyqOSg2CUIm5KDNm4SQvrSeD0QJMmOWI10cYVSL6FeOTRXMOHjAvtg5/VERrI0Nbrhc2wqCJ4FsqgDFVTF2xjO7SrvZRdLcU1NSgYu024OqAbdCqZnu3hJFG0rXu24lYNljclcm95xpUG3wUCJCGgqMFtFKvXGrTte6fLRJkvwV9UpQu0r650LhulONbSbG08Ub+jubBGpA4mqaHtpVI56VWPU+8SFwy4riqJCghtV14bSZSjGl1w2ice1kl86sssasy6RskJdvSPzkeGjTRYIaqf2AbS69iUAjSrq2wiNaikULRQND1tUcE3muPF8DLjCQa5IUWNzh7WOPK9Z5CWrvEKBbZWxqSxVZXcSZSvYDWRrJbtwZOcl5rKEUDHnE4jHJ2E++mRp0Bi27XgeDQXVitSyi8C2XtDO3lCAQjy3jIJVxCp5UbFYVCzzikVWscpLVlnJ0pYUpmZdZ5xtl3y4WbK+LKhVfD3uGoozpThz5GcV9myDXKzRzRYtq91gtCPDs0MW8PaLC7ZB5TBbh90qtoQ6SI6dmgmkyBxiFTGKMY6i2EmOW8WG5xaX3M42rOyWW3bDwlQsTYkRx/vlKW9u7uIQPrArpBbsWsgfKIsPHYt3t2QfrJH75+jlpSdLU7w9G7hHgCDmpayQymE3it2qL1SqZadqAIxiMiXLvQ1yUpTcXax5fnnBi8U5LxQPeD475zl7wVJKTsyGQmpMCOv+qX2JUi3vbk69gd1IlQfK4v2S/L0L5OwCPTtDt6WXKg1RjlC6PHNkUdW2ONpelmhmQhGSDcE0QXPCSEP/wLLMcVKUPLe85MXlOR9bfsjHig95MbvPy9kZt80lABalRig1Y60571enfOfiHt95/3m27y9ZvWtYvqss36/JP1h7olxcoOsNWlVHP/b5mSKLumiczrZELjZYK2S5ocj9mOVdYs9QZYpqjTGOVV5yb3HBK4v7/PDifT6ev88PZR/yQ/acE1EuVDhzOVstWGvOu/UtvrN+gdfee5H737/N8o2Mk+8rp2/VLN7dYD548FQRBZ4xsqAh57LdgjWIMRgRcmNCLijDNeOWc6gXBl354EpmHKfZljvZmufsBS/YB7xkLnjFGnIszm05Ay7cgu9Xd3l9e49v3X+J99++zfLNQJTv16y+f4l5/wF69gA9v/BEOeKxQjGeLbKAt1lE/EwG1o9rNpkly8xuEFioMakXUG0sm03OWbHg7ewWC1NxYjfcNmtOZcuyviSXmrfrgu/Xd/j25hW+cfExvnX/Jb795osU389ZvaWcvONYvLPBvP8AeXCBiyXKU4JniyxN2r+qAJ+NVkCMwYYQvJcqfghIvRTqE0O5yTjLFljjyEzNaebJkkuFEYdF+X59h++V9/jmxQ/x++9+nDffeo7sjQUnbwinb9Ws3lqTvX0f3v/QE2WzObqhHofwbJEFgioKv7cGcT6OYlR99VtmfM4oDH6vF4bK5Fw6oa597N6IUjnDe9Ut3sieB+D17T2+t36Ob7z3Cm9973mW3/MSxaueNdm7DzxR7j84ao9nDM8gWXYpAMoQ/icYv6pkmQmlA6FoSoXt1lKthc3G8FaZcbZe8MbyLousIjc1pbO8/eCU87MlvL3g1huGW99zLN+rWLy7xnxwDmfn3ph9SokCzyJZIDwoh1YOrY2fzaCqoKowImTAEhCXYyqL2QrlxlCWQrUxnJ1nPChO2ti/bgzZhxnLD4TV28qt75WsXj/DfHiOXqzR9RpKP9vT00oUeFbJAlEisUZL2hoXLtcYY/yNcWGOldJit4LZCtnSei8pb9aDXQvFfVh84Fi9U7N84wx5423cxeXRB9qugmeXLEOoKk8YIFNFqgWmzMk2luzC+FqVha+Ia8oLsrVSPHDk9yuKDzaY+xe4Js/zESEKzGTpQhXdll7KOIepa2RdYi8XZOcF+Wk0BtoKtlTs2pcY2PMSe75Fzi99DGW9eWomFpyKmSywK5Cqa8T5wJ2EWQxk6+tMpKwxmxy3yHC5QY1gNzVmU/nygs0Wudyg6zV6uX7q3OIpmMkSoE2RdCiH07r2Rm9WQlkiVY3ZFpjMotaCAdmEmSc3W7QscZvtR8KQHcJMFugau01lQF2jYhCzRbZZyCct/DwuTQCvLH22eLv189nWNbjjmPHgOjCTZQiNex0F8KhdGFLYkCVMutOE7T/CRIGZLONoAnhN8lFMd/qVeEr1ZwAzWQ6hGZYa8knPMo5rHqoZR42ZLDMmYybLjMk4SBYR+YSI/AcR+YaIfF1E/kFYfk9EfktEvhW+n4/2+TkReU1Evikif+06L2DGk8MUyVIB/0hV/yzwU8DPhDn6m/n7PwX8TvhPb/7+zwH/QkRs8sgzniocJIuqvqmq/y38PgO+gZ9i/Qv4efsJ338z/P4CYf5+Vf0ToJm/f8ZTjivZLOGFD38B+K/05u8H4vn7vxvtlpy/X0S+JCJfE5GvlWweoukznjQmk0VEbgH/DviHqnp/bNPEsr0kiap+WVU/q6qfzVlMbcaMG8QksohIjifKL6vqvw+L3wrz9vPUz98/YxKmeEMC/EvgG6r6S9Gqj8b8/TMmY0q4/y8Cfxf4HyLye2HZP+ajMH//jCthytz9/4m0HQJP8/z9M66MOYI7YzJmssyYjJksMyZjJsuMyZjJMmMy5BgmuxORt4Fz4J2bbssV8CIfzfb+qKq+lFpxFGQBEJGvqepnb7odU/EstndWQzMmYybLjMk4JrJ8+aYbcEU8c+09GptlxvHjmCTLjCPHjZNFRD4XCrtfE5FXb7o9ACLyFRH5gYj8QbTsaAvUn1hRffMypJv44N8r+cfAjwMF8PvAp2+yTaFdfxn4DPAH0bJ/Brwafr8K/NPw+9Oh3Qvgk+F67BNu78eAz4Tft4E/Cu16rG2+acnyE8BrqvptVd0Cv4ov+L5RqOrvAu/1Fh9tgbo+oaL6mybLpOLuI8EjFag/KTzOovo+bposk4q7jxxHcw2Pu6i+j5smy9NU3H3UBepPoqj+psnyVeBTIvJJESnwIxl//YbbNISjLVB/YkX1R+B5fB5vvf8x8PM33Z7Qpl8B3gRKfC/8IvACfpjut8L3vWj7nw/t/ybw12+gvX8Jr0b+O/B74fP5x93mOYI7YzJuWg3NeIowk2XGZMxkmTEZM1lmTMZMlhmTMZNlxmTMZJkxGTNZZkzG/w+BVhSsgpuGUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.539, loss_val: nan, pos_over_neg: 1.0591704845428467 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.0583, loss_val: nan, pos_over_neg: 3.4122021198272705 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.6349, loss_val: nan, pos_over_neg: 6.16873025894165 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.462, loss_val: nan, pos_over_neg: 27.448928833007812 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.3978, loss_val: nan, pos_over_neg: 28.65560531616211 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.3227, loss_val: nan, pos_over_neg: 31.759992599487305 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.272, loss_val: nan, pos_over_neg: 43.032222747802734 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.2386, loss_val: nan, pos_over_neg: 64.26173400878906 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.2282, loss_val: nan, pos_over_neg: 68.04695892333984 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.2025, loss_val: nan, pos_over_neg: 84.34514617919922 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.1723, loss_val: nan, pos_over_neg: 97.43470001220703 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.1589, loss_val: nan, pos_over_neg: 104.25337982177734 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.1465, loss_val: nan, pos_over_neg: 100.4830551147461 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.1162, loss_val: nan, pos_over_neg: 188.6045684814453 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.1021, loss_val: nan, pos_over_neg: 277.2494812011719 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.0924, loss_val: nan, pos_over_neg: 227.0287628173828 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.0787, loss_val: nan, pos_over_neg: 212.0416717529297 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.0614, loss_val: nan, pos_over_neg: 220.44960021972656 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.0487, loss_val: nan, pos_over_neg: 255.9558868408203 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.0391, loss_val: nan, pos_over_neg: 301.6666564941406 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.0257, loss_val: nan, pos_over_neg: 248.2299346923828 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 446.485595703125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.0101, loss_val: nan, pos_over_neg: 515.9564819335938 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9957, loss_val: nan, pos_over_neg: 670.380126953125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9963, loss_val: nan, pos_over_neg: 492.6869812011719 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 520.086181640625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9736, loss_val: nan, pos_over_neg: 586.4039306640625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 1101.8399658203125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 2460.12646484375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 824.3259887695312 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.954, loss_val: nan, pos_over_neg: 714.6239624023438 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 811.2545776367188 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 452.0338439941406 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 612.6111450195312 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1357.3521728515625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: 463.7580261230469 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 431.4842529296875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 683.9564208984375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 880.228759765625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 309.3966369628906 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 349.4871826171875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 292.2322082519531 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 303.9170837402344 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 223.17074584960938 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8972, loss_val: nan, pos_over_neg: 614.7269287109375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8955, loss_val: nan, pos_over_neg: 455.8093566894531 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: 442.62994384765625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8894, loss_val: nan, pos_over_neg: 345.3471374511719 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8883, loss_val: nan, pos_over_neg: 352.8155212402344 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8822, loss_val: nan, pos_over_neg: 799.648193359375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 1105.177978515625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 613.6830444335938 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 422.37652587890625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8737, loss_val: nan, pos_over_neg: 630.216796875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 275.9462890625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8695, loss_val: nan, pos_over_neg: 1217.2406005859375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 858.3905639648438 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 984.2132568359375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 633.0803833007812 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 469.5979919433594 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 667.4114990234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 1070.2200927734375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 1046.255859375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 2954.041748046875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 434.0217590332031 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 693.3618774414062 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 1354.54638671875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 629.07421875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 388.3677978515625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 469.2252502441406 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 359.6401062011719 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 407.4078063964844 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 406.1896667480469 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 299.4556884765625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 392.82330322265625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 410.0077819824219 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 267.39263916015625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 515.9583129882812 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 231.5220184326172 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 362.4643859863281 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8274, loss_val: nan, pos_over_neg: 235.3614959716797 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 412.7865295410156 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8231, loss_val: nan, pos_over_neg: 283.61053466796875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8276, loss_val: nan, pos_over_neg: 505.81304931640625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 433.9123229980469 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 451.1730651855469 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8221, loss_val: nan, pos_over_neg: 737.2960205078125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 281.8869323730469 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8196, loss_val: nan, pos_over_neg: 459.01019287109375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 392.1014099121094 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 272.3049621582031 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 481.13214111328125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 280.2845458984375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 378.0141296386719 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 228.20228576660156 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 476.0284118652344 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 283.3215026855469 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8126, loss_val: nan, pos_over_neg: 252.16876220703125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 272.88494873046875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 284.25701904296875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 197.28567504882812 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 275.7304382324219 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 221.133056640625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 202.0402374267578 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 196.330322265625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 336.5147399902344 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 292.11883544921875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 245.75254821777344 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 242.7032470703125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 230.75149536132812 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 322.1445007324219 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 253.11441040039062 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 236.46673583984375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 449.6138916015625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 236.97312927246094 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 206.96820068359375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 303.0175476074219 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 302.0340270996094 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 317.5784606933594 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 566.7125854492188 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 366.2213439941406 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 292.4945373535156 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 351.0152587890625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 561.3176879882812 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 223.51243591308594 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 313.134033203125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 327.4864501953125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 247.0108184814453 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 450.4153747558594 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 214.62083435058594 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 229.69866943359375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 445.2557373046875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 274.4985046386719 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 280.2571105957031 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 387.3305358886719 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 285.9170227050781 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 353.0759582519531 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 289.26220703125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 241.57569885253906 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 333.3772277832031 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 238.17678833007812 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 349.5067443847656 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 351.0407409667969 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 276.3603210449219 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 380.2336730957031 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 278.848876953125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 354.4880676269531 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 250.54859924316406 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 252.77847290039062 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 454.88958740234375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 287.68603515625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 522.801513671875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 288.0786437988281 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 278.1844482421875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 238.36431884765625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 245.9996795654297 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 446.9671630859375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 203.57498168945312 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 319.3038024902344 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 293.9490661621094 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 280.6375427246094 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 281.28961181640625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 273.0424499511719 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 443.0677490234375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 266.17626953125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 324.44268798828125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 333.0598449707031 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 281.7242736816406 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 405.7974548339844 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 344.9261779785156 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 481.53680419921875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 261.0337219238281 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 341.19989013671875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 288.5988464355469 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 261.7371826171875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 300.3514099121094 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 446.3343505859375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 318.4781799316406 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 399.8107604980469 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 570.9931030273438 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 282.2618103027344 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 765.285888671875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 411.7437744140625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 290.22503662109375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 345.719970703125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 494.11151123046875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 360.5890197753906 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 503.9565734863281 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 574.0564575195312 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 580.3006591796875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 543.9219970703125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 453.84088134765625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 343.22247314453125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 657.8081665039062 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 699.5778198242188 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 435.004150390625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 431.5851745605469 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 585.9666748046875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 482.0037841796875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 363.23876953125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 311.08990478515625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 702.5650024414062 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 264.4759521484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 326.3577880859375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1122.3818359375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 351.5509033203125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 461.26971435546875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 527.4051513671875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 587.7734375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 457.62200927734375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 458.5024108886719 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 522.9188842773438 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 412.9024963378906 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 380.289306640625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 630.7931518554688 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 438.6168518066406 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 391.0953674316406 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 488.6028747558594 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 730.878662109375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 396.7507019042969 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 509.6579895019531 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 593.4190063476562 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 515.050537109375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 360.1600341796875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 658.0533447265625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 506.0106201171875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 455.7396545410156 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 412.2106628417969 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 595.0899658203125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 381.5625305175781 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 388.6249084472656 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 620.2337646484375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 299.41912841796875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 263.11529541015625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 441.39544677734375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 313.1983337402344 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 277.2629699707031 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 434.26617431640625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 353.3232421875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 346.2769470214844 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 347.4309387207031 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 252.1709442138672 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 458.9493408203125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 308.8638916015625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 463.05419921875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 273.8628234863281 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 306.80889892578125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 346.7381591796875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 456.7558288574219 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 506.2754821777344 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 337.66046142578125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 707.8040161132812 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 534.4109497070312 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 357.9922790527344 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 592.9376831054688 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 376.7240905761719 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 396.0650634765625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 484.5465393066406 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 550.333984375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 340.58984375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 359.25250244140625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 480.56036376953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 344.3847351074219 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 269.52001953125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 394.50537109375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 328.4510803222656 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 375.74334716796875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 309.855712890625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 479.0 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 368.1222839355469 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 413.83917236328125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 642.80322265625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 404.04638671875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 460.0336608886719 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 577.565185546875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 410.9854736328125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 754.56298828125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 399.96063232421875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 361.617919921875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 402.2992858886719 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 266.67010498046875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 264.00042724609375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 285.35736083984375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 327.36956787109375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 425.0357360839844 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 309.2012939453125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 290.6806640625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 322.08056640625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 336.3354797363281 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 363.9482421875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 364.5467224121094 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 312.830322265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 491.2886962890625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 617.9505615234375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 321.5306701660156 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 505.686279296875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 730.7528076171875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 967.4829711914062 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 509.4280090332031 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 366.3042907714844 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 456.64447021484375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 512.4690551757812 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 595.38671875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 469.9124755859375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 431.3029479980469 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 737.4884643554688 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 536.807861328125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 546.3999633789062 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 345.6302795410156 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.744, loss_val: nan, pos_over_neg: 1147.4093017578125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1477.8702392578125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 365.3201904296875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7439, loss_val: nan, pos_over_neg: 961.9419555664062 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 489.55267333984375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 613.4428100585938 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7422, loss_val: nan, pos_over_neg: 390.5010986328125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 490.5531921386719 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 725.5540771484375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 752.8965454101562 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 406.74853515625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 794.9226684570312 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 932.2721557617188 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 301.6490783691406 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 739.5684204101562 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 456.42071533203125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 407.7880554199219 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 515.2813110351562 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 345.67901611328125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 358.2624816894531 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 368.5274963378906 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 521.365966796875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 467.7925720214844 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7423, loss_val: nan, pos_over_neg: 682.3539428710938 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 771.6129760742188 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 284.3634033203125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 656.9957275390625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 434.2204284667969 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 903.4214477539062 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 443.8840026855469 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 280.33721923828125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 1204.7369384765625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7404, loss_val: nan, pos_over_neg: 698.19921875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7412, loss_val: nan, pos_over_neg: 508.0596008300781 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7405, loss_val: nan, pos_over_neg: 620.3408813476562 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 749.8292846679688 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7425, loss_val: nan, pos_over_neg: 1066.90087890625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 368.0664978027344 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 870.7378540039062 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 411.9220275878906 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.742, loss_val: nan, pos_over_neg: 556.2747192382812 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7378, loss_val: nan, pos_over_neg: 1407.787353515625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 405.31787109375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7392, loss_val: nan, pos_over_neg: 517.110107421875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 520.7637329101562 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7412, loss_val: nan, pos_over_neg: 588.3126831054688 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 486.49334716796875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7414, loss_val: nan, pos_over_neg: 581.1214599609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 700.2105712890625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7379, loss_val: nan, pos_over_neg: 705.2399291992188 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7413, loss_val: nan, pos_over_neg: 531.97314453125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7395, loss_val: nan, pos_over_neg: 813.309326171875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 821.7017822265625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7365, loss_val: nan, pos_over_neg: 1879.73291015625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7416, loss_val: nan, pos_over_neg: 516.0938110351562 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7407, loss_val: nan, pos_over_neg: 734.952880859375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7385, loss_val: nan, pos_over_neg: 719.61962890625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 821.7662353515625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.742, loss_val: nan, pos_over_neg: 662.0809936523438 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7354, loss_val: nan, pos_over_neg: 1638.9449462890625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7417, loss_val: nan, pos_over_neg: 894.9892578125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7416, loss_val: nan, pos_over_neg: 491.62921142578125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7369, loss_val: nan, pos_over_neg: 802.8861083984375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7417, loss_val: nan, pos_over_neg: 504.1235656738281 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7396, loss_val: nan, pos_over_neg: 625.4390869140625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 601.2865600585938 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7352, loss_val: nan, pos_over_neg: 734.437255859375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7415, loss_val: nan, pos_over_neg: 468.51324462890625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7367, loss_val: nan, pos_over_neg: 621.6590576171875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7395, loss_val: nan, pos_over_neg: 985.517822265625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7385, loss_val: nan, pos_over_neg: 536.2523803710938 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7388, loss_val: nan, pos_over_neg: 758.0587768554688 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 466.6829528808594 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7368, loss_val: nan, pos_over_neg: 610.5921630859375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7386, loss_val: nan, pos_over_neg: 935.64111328125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.74, loss_val: nan, pos_over_neg: 612.3444213867188 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7401, loss_val: nan, pos_over_neg: 531.3342895507812 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7421, loss_val: nan, pos_over_neg: 510.5530090332031 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7422, loss_val: nan, pos_over_neg: 738.3092651367188 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7371, loss_val: nan, pos_over_neg: 493.2787780761719 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7409, loss_val: nan, pos_over_neg: 347.7613525390625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7393, loss_val: nan, pos_over_neg: 1207.890869140625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7408, loss_val: nan, pos_over_neg: 420.1558532714844 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7417, loss_val: nan, pos_over_neg: 322.1138610839844 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7423, loss_val: nan, pos_over_neg: 513.3722534179688 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7377, loss_val: nan, pos_over_neg: 448.2732238769531 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.742, loss_val: nan, pos_over_neg: 336.7034912109375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7347, loss_val: nan, pos_over_neg: 393.0996398925781 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7398, loss_val: nan, pos_over_neg: 439.3735656738281 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7388, loss_val: nan, pos_over_neg: 459.6409606933594 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7387, loss_val: nan, pos_over_neg: 331.9013977050781 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 622.97412109375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.737, loss_val: nan, pos_over_neg: 417.8021240234375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7388, loss_val: nan, pos_over_neg: 343.50921630859375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.734, loss_val: nan, pos_over_neg: 547.3052978515625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7374, loss_val: nan, pos_over_neg: 398.31689453125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.741, loss_val: nan, pos_over_neg: 331.8992004394531 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7382, loss_val: nan, pos_over_neg: 458.6885986328125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7383, loss_val: nan, pos_over_neg: 269.2931823730469 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 321.76080322265625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7362, loss_val: nan, pos_over_neg: 400.8684387207031 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7393, loss_val: nan, pos_over_neg: 295.5303039550781 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 289.1219482421875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.735, loss_val: nan, pos_over_neg: 784.9268188476562 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7362, loss_val: nan, pos_over_neg: 632.5701904296875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7384, loss_val: nan, pos_over_neg: 482.22247314453125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.739, loss_val: nan, pos_over_neg: 276.8829345703125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7375, loss_val: nan, pos_over_neg: 556.292724609375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7354, loss_val: nan, pos_over_neg: 847.6172485351562 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7366, loss_val: nan, pos_over_neg: 519.1558837890625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7349, loss_val: nan, pos_over_neg: 847.6354370117188 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7375, loss_val: nan, pos_over_neg: 542.0857543945312 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7352, loss_val: nan, pos_over_neg: 833.90283203125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7342, loss_val: nan, pos_over_neg: 625.9154663085938 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7323, loss_val: nan, pos_over_neg: 795.3714599609375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7355, loss_val: nan, pos_over_neg: 449.9903564453125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7374, loss_val: nan, pos_over_neg: 553.9844360351562 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7342, loss_val: nan, pos_over_neg: 753.424072265625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7331, loss_val: nan, pos_over_neg: 1185.5721435546875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.735, loss_val: nan, pos_over_neg: 591.6067504882812 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7384, loss_val: nan, pos_over_neg: 516.52392578125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7375, loss_val: nan, pos_over_neg: 575.0034790039062 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7338, loss_val: nan, pos_over_neg: 691.2520751953125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7386, loss_val: nan, pos_over_neg: 657.5989379882812 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7392, loss_val: nan, pos_over_neg: 430.93310546875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7364, loss_val: nan, pos_over_neg: 495.0800476074219 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7328, loss_val: nan, pos_over_neg: 802.14404296875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7353, loss_val: nan, pos_over_neg: 410.50213623046875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7377, loss_val: nan, pos_over_neg: 488.4307861328125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7346, loss_val: nan, pos_over_neg: 527.7510986328125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7356, loss_val: nan, pos_over_neg: 734.3026733398438 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7357, loss_val: nan, pos_over_neg: 626.9853515625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7394, loss_val: nan, pos_over_neg: 612.1488037109375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.735, loss_val: nan, pos_over_neg: 588.1910400390625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7363, loss_val: nan, pos_over_neg: 572.389892578125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7305, loss_val: nan, pos_over_neg: 666.3535766601562 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.734, loss_val: nan, pos_over_neg: 901.5465087890625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7366, loss_val: nan, pos_over_neg: 459.16729736328125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7346, loss_val: nan, pos_over_neg: 565.418212890625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7324, loss_val: nan, pos_over_neg: 724.3079223632812 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7358, loss_val: nan, pos_over_neg: 852.400146484375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7363, loss_val: nan, pos_over_neg: 339.9190673828125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7359, loss_val: nan, pos_over_neg: 581.3364868164062 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7368, loss_val: nan, pos_over_neg: 672.2257690429688 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7352, loss_val: nan, pos_over_neg: 684.8591918945312 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7371, loss_val: nan, pos_over_neg: 295.47900390625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7353, loss_val: nan, pos_over_neg: 800.2689208984375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.734, loss_val: nan, pos_over_neg: 456.06439208984375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7344, loss_val: nan, pos_over_neg: 715.5205688476562 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7383, loss_val: nan, pos_over_neg: 379.54449462890625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7319, loss_val: nan, pos_over_neg: 817.0052490234375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.738, loss_val: nan, pos_over_neg: 534.65576171875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7317, loss_val: nan, pos_over_neg: 532.7260131835938 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7351, loss_val: nan, pos_over_neg: 588.37158203125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.732, loss_val: nan, pos_over_neg: 671.19775390625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7314, loss_val: nan, pos_over_neg: 761.51708984375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7362, loss_val: nan, pos_over_neg: 566.3494873046875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7333, loss_val: nan, pos_over_neg: 654.0646362304688 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7354, loss_val: nan, pos_over_neg: 543.4183959960938 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7339, loss_val: nan, pos_over_neg: 540.4757080078125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7326, loss_val: nan, pos_over_neg: 579.3526000976562 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7311, loss_val: nan, pos_over_neg: 1119.7066650390625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7327, loss_val: nan, pos_over_neg: 676.17041015625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7358, loss_val: nan, pos_over_neg: 660.483642578125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7319, loss_val: nan, pos_over_neg: 599.829345703125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7358, loss_val: nan, pos_over_neg: 477.3998107910156 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7342, loss_val: nan, pos_over_neg: 582.5753173828125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7321, loss_val: nan, pos_over_neg: 777.0112915039062 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7319, loss_val: nan, pos_over_neg: 583.9583740234375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7327, loss_val: nan, pos_over_neg: 352.8017578125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.732, loss_val: nan, pos_over_neg: 660.1007080078125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7338, loss_val: nan, pos_over_neg: 822.5420532226562 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7344, loss_val: nan, pos_over_neg: 417.790283203125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7296, loss_val: nan, pos_over_neg: 922.619384765625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7335, loss_val: nan, pos_over_neg: 331.11505126953125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7354, loss_val: nan, pos_over_neg: 548.8278198242188 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7299, loss_val: nan, pos_over_neg: 2497.592041015625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7346, loss_val: nan, pos_over_neg: 376.6065368652344 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7317, loss_val: nan, pos_over_neg: 781.731689453125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7351, loss_val: nan, pos_over_neg: 496.5908508300781 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7348, loss_val: nan, pos_over_neg: 620.7470703125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7372, loss_val: nan, pos_over_neg: 715.229248046875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7304, loss_val: nan, pos_over_neg: 456.14837646484375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7343, loss_val: nan, pos_over_neg: 403.50994873046875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7355, loss_val: nan, pos_over_neg: 421.1068420410156 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7317, loss_val: nan, pos_over_neg: 654.1019287109375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7314, loss_val: nan, pos_over_neg: 892.4197998046875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7331, loss_val: nan, pos_over_neg: 267.09619140625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7318, loss_val: nan, pos_over_neg: 506.8706359863281 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7343, loss_val: nan, pos_over_neg: 516.0744018554688 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.732, loss_val: nan, pos_over_neg: 415.524169921875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7308, loss_val: nan, pos_over_neg: 674.7249145507812 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7335, loss_val: nan, pos_over_neg: 433.6458435058594 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7344, loss_val: nan, pos_over_neg: 375.4068908691406 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7341, loss_val: nan, pos_over_neg: 410.5307922363281 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7328, loss_val: nan, pos_over_neg: 443.7552795410156 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7308, loss_val: nan, pos_over_neg: 639.9332275390625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7335, loss_val: nan, pos_over_neg: 385.2304382324219 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7302, loss_val: nan, pos_over_neg: 486.89947509765625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7364, loss_val: nan, pos_over_neg: 390.7122497558594 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7338, loss_val: nan, pos_over_neg: 675.8160400390625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7314, loss_val: nan, pos_over_neg: 597.0640869140625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7327, loss_val: nan, pos_over_neg: 533.892822265625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7284, loss_val: nan, pos_over_neg: 595.048583984375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7293, loss_val: nan, pos_over_neg: 913.5850830078125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7307, loss_val: nan, pos_over_neg: 604.1970825195312 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7322, loss_val: nan, pos_over_neg: 463.2020263671875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7338, loss_val: nan, pos_over_neg: 418.80206298828125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7334, loss_val: nan, pos_over_neg: 672.2793579101562 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7294, loss_val: nan, pos_over_neg: 657.4190063476562 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7308, loss_val: nan, pos_over_neg: 475.393310546875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7302, loss_val: nan, pos_over_neg: 556.376220703125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7298, loss_val: nan, pos_over_neg: 727.3208618164062 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.731, loss_val: nan, pos_over_neg: 920.9088134765625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7286, loss_val: nan, pos_over_neg: 735.2327880859375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7279, loss_val: nan, pos_over_neg: 527.2052612304688 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7261, loss_val: nan, pos_over_neg: 879.5336303710938 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7284, loss_val: nan, pos_over_neg: 1076.2464599609375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7256, loss_val: nan, pos_over_neg: 595.0841064453125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7309, loss_val: nan, pos_over_neg: 810.4369506835938 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7299, loss_val: nan, pos_over_neg: 1441.4207763671875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7323, loss_val: nan, pos_over_neg: 717.3233642578125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.731, loss_val: nan, pos_over_neg: 846.59716796875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7267, loss_val: nan, pos_over_neg: 1074.2528076171875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7294, loss_val: nan, pos_over_neg: 665.0122680664062 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7256, loss_val: nan, pos_over_neg: 1861.4638671875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7317, loss_val: nan, pos_over_neg: 598.5588989257812 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7288, loss_val: nan, pos_over_neg: 562.4393920898438 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7275, loss_val: nan, pos_over_neg: 796.8399047851562 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.729, loss_val: nan, pos_over_neg: 736.375244140625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7306, loss_val: nan, pos_over_neg: 1509.65185546875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7317, loss_val: nan, pos_over_neg: 829.5645141601562 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7287, loss_val: nan, pos_over_neg: 631.697998046875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7286, loss_val: nan, pos_over_neg: 751.9541015625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7322, loss_val: nan, pos_over_neg: 665.5253295898438 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.73, loss_val: nan, pos_over_neg: 591.216552734375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7312, loss_val: nan, pos_over_neg: 787.8892211914062 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7309, loss_val: nan, pos_over_neg: 1156.1607666015625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7277, loss_val: nan, pos_over_neg: 557.246337890625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7306, loss_val: nan, pos_over_neg: 545.974365234375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7277, loss_val: nan, pos_over_neg: 996.7695922851562 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7291, loss_val: nan, pos_over_neg: 681.7997436523438 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7282, loss_val: nan, pos_over_neg: 380.638427734375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7247, loss_val: nan, pos_over_neg: 798.6785888671875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7311, loss_val: nan, pos_over_neg: 1048.9501953125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7289, loss_val: nan, pos_over_neg: 707.6974487304688 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7312, loss_val: nan, pos_over_neg: 510.0243835449219 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7308, loss_val: nan, pos_over_neg: 636.1278686523438 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7282, loss_val: nan, pos_over_neg: 712.0975341796875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7276, loss_val: nan, pos_over_neg: 609.810546875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7274, loss_val: nan, pos_over_neg: 1115.1885986328125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7267, loss_val: nan, pos_over_neg: 485.426513671875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7288, loss_val: nan, pos_over_neg: 688.3009033203125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7284, loss_val: nan, pos_over_neg: 637.0734252929688 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7319, loss_val: nan, pos_over_neg: 538.97998046875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7318, loss_val: nan, pos_over_neg: 527.198486328125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7274, loss_val: nan, pos_over_neg: 772.5509033203125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7301, loss_val: nan, pos_over_neg: 578.3518676757812 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7239, loss_val: nan, pos_over_neg: 1189.6669921875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.73, loss_val: nan, pos_over_neg: 584.364990234375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7303, loss_val: nan, pos_over_neg: 503.8854675292969 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7282, loss_val: nan, pos_over_neg: 528.725830078125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7307, loss_val: nan, pos_over_neg: 549.5789794921875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7297, loss_val: nan, pos_over_neg: 661.6387329101562 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7302, loss_val: nan, pos_over_neg: 522.10791015625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7305, loss_val: nan, pos_over_neg: 348.2144775390625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7271, loss_val: nan, pos_over_neg: 459.2705383300781 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7297, loss_val: nan, pos_over_neg: 558.4307861328125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7258, loss_val: nan, pos_over_neg: 640.1914672851562 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7268, loss_val: nan, pos_over_neg: 742.21826171875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.73, loss_val: nan, pos_over_neg: 633.6007080078125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7287, loss_val: nan, pos_over_neg: 1134.4051513671875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7311, loss_val: nan, pos_over_neg: 1052.178466796875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7294, loss_val: nan, pos_over_neg: 521.4296875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7296, loss_val: nan, pos_over_neg: 737.7168579101562 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7299, loss_val: nan, pos_over_neg: 436.0391540527344 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7249, loss_val: nan, pos_over_neg: 1015.3366088867188 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7272, loss_val: nan, pos_over_neg: 735.9109497070312 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.727, loss_val: nan, pos_over_neg: 563.27099609375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7298, loss_val: nan, pos_over_neg: 781.3292236328125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7293, loss_val: nan, pos_over_neg: 691.3270263671875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7248, loss_val: nan, pos_over_neg: 757.7919921875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7276, loss_val: nan, pos_over_neg: 683.4126586914062 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7259, loss_val: nan, pos_over_neg: 1230.6024169921875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7279, loss_val: nan, pos_over_neg: 502.1765441894531 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7299, loss_val: nan, pos_over_neg: 878.4274291992188 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7251, loss_val: nan, pos_over_neg: 636.5341186523438 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7293, loss_val: nan, pos_over_neg: 650.2142333984375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7274, loss_val: nan, pos_over_neg: 494.1809997558594 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7301, loss_val: nan, pos_over_neg: 609.0757446289062 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7275, loss_val: nan, pos_over_neg: 434.2086486816406 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7253, loss_val: nan, pos_over_neg: 920.002197265625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7214, loss_val: nan, pos_over_neg: 683.6043701171875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.728, loss_val: nan, pos_over_neg: 683.123779296875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.732, loss_val: nan, pos_over_neg: 400.69757080078125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7249, loss_val: nan, pos_over_neg: 1104.2783203125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7277, loss_val: nan, pos_over_neg: 594.6380615234375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7293, loss_val: nan, pos_over_neg: 410.6400146484375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7279, loss_val: nan, pos_over_neg: 617.0905151367188 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.725, loss_val: nan, pos_over_neg: 1109.583740234375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7277, loss_val: nan, pos_over_neg: 717.6412963867188 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7264, loss_val: nan, pos_over_neg: 486.46533203125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7272, loss_val: nan, pos_over_neg: 618.8316650390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.724, loss_val: nan, pos_over_neg: 1330.2181396484375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7278, loss_val: nan, pos_over_neg: 1341.548583984375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7285, loss_val: nan, pos_over_neg: 742.7931518554688 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7246, loss_val: nan, pos_over_neg: 699.1760864257812 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7246, loss_val: nan, pos_over_neg: 594.0716552734375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7302, loss_val: nan, pos_over_neg: 683.0083618164062 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7273, loss_val: nan, pos_over_neg: 533.4546508789062 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7251, loss_val: nan, pos_over_neg: 807.3934326171875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7243, loss_val: nan, pos_over_neg: 771.5084228515625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7269, loss_val: nan, pos_over_neg: 517.92333984375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7249, loss_val: nan, pos_over_neg: 1182.9315185546875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7275, loss_val: nan, pos_over_neg: 1061.04638671875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7274, loss_val: nan, pos_over_neg: 559.7033081054688 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7241, loss_val: nan, pos_over_neg: 945.4563598632812 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7266, loss_val: nan, pos_over_neg: 559.22802734375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7289, loss_val: nan, pos_over_neg: 622.6484985351562 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7212, loss_val: nan, pos_over_neg: 935.3316040039062 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7281, loss_val: nan, pos_over_neg: 829.3524780273438 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7235, loss_val: nan, pos_over_neg: 1584.8353271484375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7257, loss_val: nan, pos_over_neg: 1322.90234375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.729, loss_val: nan, pos_over_neg: 984.1172485351562 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7234, loss_val: nan, pos_over_neg: 743.7785034179688 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7255, loss_val: nan, pos_over_neg: 923.9146728515625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7236, loss_val: nan, pos_over_neg: 901.7841796875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7265, loss_val: nan, pos_over_neg: 700.320068359375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7245, loss_val: nan, pos_over_neg: 853.8114013671875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7262, loss_val: nan, pos_over_neg: 878.4488525390625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7264, loss_val: nan, pos_over_neg: 874.0011596679688 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7266, loss_val: nan, pos_over_neg: 599.066162109375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7227, loss_val: nan, pos_over_neg: 1636.4317626953125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7223, loss_val: nan, pos_over_neg: 1150.8218994140625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7266, loss_val: nan, pos_over_neg: 600.4916381835938 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7266, loss_val: nan, pos_over_neg: 997.9525146484375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7261, loss_val: nan, pos_over_neg: 610.28271484375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7244, loss_val: nan, pos_over_neg: 1287.3665771484375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7238, loss_val: nan, pos_over_neg: 993.3485717773438 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7244, loss_val: nan, pos_over_neg: 648.752685546875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7253, loss_val: nan, pos_over_neg: 875.347900390625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7275, loss_val: nan, pos_over_neg: 690.896728515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7241, loss_val: nan, pos_over_neg: 641.4546508789062 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.723, loss_val: nan, pos_over_neg: 781.1408081054688 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7224, loss_val: nan, pos_over_neg: 857.6116943359375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7235, loss_val: nan, pos_over_neg: 1510.5062255859375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7257, loss_val: nan, pos_over_neg: 447.9764099121094 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7272, loss_val: nan, pos_over_neg: 466.90496826171875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.723, loss_val: nan, pos_over_neg: 797.11962890625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.725, loss_val: nan, pos_over_neg: 621.0033569335938 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.723, loss_val: nan, pos_over_neg: 940.7646484375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7209, loss_val: nan, pos_over_neg: 880.6488647460938 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7207, loss_val: nan, pos_over_neg: 658.9462280273438 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7243, loss_val: nan, pos_over_neg: 996.556396484375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7239, loss_val: nan, pos_over_neg: 561.369384765625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7278, loss_val: nan, pos_over_neg: 360.3359680175781 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7204, loss_val: nan, pos_over_neg: 925.653564453125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.724, loss_val: nan, pos_over_neg: 494.117919921875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.726, loss_val: nan, pos_over_neg: 680.9471435546875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7231, loss_val: nan, pos_over_neg: 555.7525024414062 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7223, loss_val: nan, pos_over_neg: 510.5920104980469 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7237, loss_val: nan, pos_over_neg: 722.6217041015625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7234, loss_val: nan, pos_over_neg: 819.5274658203125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7196, loss_val: nan, pos_over_neg: 1076.7066650390625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7259, loss_val: nan, pos_over_neg: 587.0259399414062 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7264, loss_val: nan, pos_over_neg: 659.52490234375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7244, loss_val: nan, pos_over_neg: 630.25 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7208, loss_val: nan, pos_over_neg: 948.5831298828125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7219, loss_val: nan, pos_over_neg: 874.9744262695312 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7253, loss_val: nan, pos_over_neg: 847.9725952148438 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7243, loss_val: nan, pos_over_neg: 777.5987548828125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7272, loss_val: nan, pos_over_neg: 840.8661499023438 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 685.6819458007812 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7253, loss_val: nan, pos_over_neg: 569.7026977539062 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7238, loss_val: nan, pos_over_neg: 449.96307373046875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7246, loss_val: nan, pos_over_neg: 636.9287719726562 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7209, loss_val: nan, pos_over_neg: 1175.2218017578125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7192, loss_val: nan, pos_over_neg: 747.0642700195312 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7245, loss_val: nan, pos_over_neg: 529.7506713867188 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7219, loss_val: nan, pos_over_neg: 722.8091430664062 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7192, loss_val: nan, pos_over_neg: 1560.957763671875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7223, loss_val: nan, pos_over_neg: 1281.3231201171875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7239, loss_val: nan, pos_over_neg: 467.2031555175781 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.725, loss_val: nan, pos_over_neg: 928.3290405273438 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7264, loss_val: nan, pos_over_neg: 545.3551025390625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7231, loss_val: nan, pos_over_neg: 738.8671264648438 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [20:22<101881:31:56, 1222.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.7224, loss_val: nan, pos_over_neg: 910.0470581054688 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7234, loss_val: nan, pos_over_neg: 760.316650390625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7239, loss_val: nan, pos_over_neg: 588.9041137695312 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7213, loss_val: nan, pos_over_neg: 786.6533203125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7211, loss_val: nan, pos_over_neg: 1197.9869384765625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7229, loss_val: nan, pos_over_neg: 506.4033508300781 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7223, loss_val: nan, pos_over_neg: 587.5899658203125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7259, loss_val: nan, pos_over_neg: 482.2904968261719 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7199, loss_val: nan, pos_over_neg: 789.901123046875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7244, loss_val: nan, pos_over_neg: 641.1178588867188 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7221, loss_val: nan, pos_over_neg: 1039.2593994140625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7267, loss_val: nan, pos_over_neg: 459.91815185546875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7181, loss_val: nan, pos_over_neg: 1057.1983642578125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7218, loss_val: nan, pos_over_neg: 786.6361083984375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 982.4757080078125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.724, loss_val: nan, pos_over_neg: 462.848876953125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7261, loss_val: nan, pos_over_neg: 490.6835632324219 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7214, loss_val: nan, pos_over_neg: 1137.30419921875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7251, loss_val: nan, pos_over_neg: 505.5022888183594 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.725, loss_val: nan, pos_over_neg: 656.0309448242188 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7264, loss_val: nan, pos_over_neg: 305.04217529296875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7201, loss_val: nan, pos_over_neg: 586.762451171875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7264, loss_val: nan, pos_over_neg: 426.2132873535156 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7222, loss_val: nan, pos_over_neg: 804.377197265625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.726, loss_val: nan, pos_over_neg: 372.1019287109375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.721, loss_val: nan, pos_over_neg: 578.5755004882812 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7255, loss_val: nan, pos_over_neg: 444.1268615722656 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7244, loss_val: nan, pos_over_neg: 500.0577392578125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7208, loss_val: nan, pos_over_neg: 595.7642822265625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.723, loss_val: nan, pos_over_neg: 463.1060791015625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7256, loss_val: nan, pos_over_neg: 461.52008056640625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7217, loss_val: nan, pos_over_neg: 390.980224609375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7213, loss_val: nan, pos_over_neg: 787.6574096679688 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7233, loss_val: nan, pos_over_neg: 484.6296081542969 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7226, loss_val: nan, pos_over_neg: 423.0000915527344 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7194, loss_val: nan, pos_over_neg: 694.2551879882812 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7214, loss_val: nan, pos_over_neg: 594.49609375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7273, loss_val: nan, pos_over_neg: 473.2497863769531 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7239, loss_val: nan, pos_over_neg: 658.2620239257812 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7231, loss_val: nan, pos_over_neg: 670.7003784179688 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7234, loss_val: nan, pos_over_neg: 608.1233520507812 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7241, loss_val: nan, pos_over_neg: 418.09197998046875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7183, loss_val: nan, pos_over_neg: 1203.699951171875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7242, loss_val: nan, pos_over_neg: 659.4899291992188 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.721, loss_val: nan, pos_over_neg: 469.25897216796875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7204, loss_val: nan, pos_over_neg: 677.2725830078125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7197, loss_val: nan, pos_over_neg: 452.20001220703125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.727, loss_val: nan, pos_over_neg: 391.5101623535156 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7193, loss_val: nan, pos_over_neg: 1007.5447998046875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7216, loss_val: nan, pos_over_neg: 791.5819702148438 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7213, loss_val: nan, pos_over_neg: 857.80322265625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7254, loss_val: nan, pos_over_neg: 456.61920166015625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7194, loss_val: nan, pos_over_neg: 717.4957885742188 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7207, loss_val: nan, pos_over_neg: 909.3751220703125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 1040.1834716796875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7224, loss_val: nan, pos_over_neg: 675.7108764648438 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7215, loss_val: nan, pos_over_neg: 471.3040466308594 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7214, loss_val: nan, pos_over_neg: 483.0034484863281 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7173, loss_val: nan, pos_over_neg: 776.5433959960938 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 638.2114868164062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7217, loss_val: nan, pos_over_neg: 432.30645751953125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7226, loss_val: nan, pos_over_neg: 491.112548828125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7225, loss_val: nan, pos_over_neg: 1332.092041015625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7233, loss_val: nan, pos_over_neg: 478.91802978515625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7227, loss_val: nan, pos_over_neg: 465.51080322265625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7214, loss_val: nan, pos_over_neg: 982.61328125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.716, loss_val: nan, pos_over_neg: 1160.0926513671875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7245, loss_val: nan, pos_over_neg: 623.2583618164062 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7177, loss_val: nan, pos_over_neg: 751.2324829101562 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7186, loss_val: nan, pos_over_neg: 1275.5821533203125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.717, loss_val: nan, pos_over_neg: 2276.025146484375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7189, loss_val: nan, pos_over_neg: 1172.447509765625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7194, loss_val: nan, pos_over_neg: 1051.7904052734375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7222, loss_val: nan, pos_over_neg: 943.3270874023438 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7194, loss_val: nan, pos_over_neg: 1488.7781982421875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7206, loss_val: nan, pos_over_neg: 2067.16015625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7214, loss_val: nan, pos_over_neg: 1406.091796875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7179, loss_val: nan, pos_over_neg: 1371.6378173828125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7169, loss_val: nan, pos_over_neg: 2680.896484375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7191, loss_val: nan, pos_over_neg: 1250.2646484375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7238, loss_val: nan, pos_over_neg: 354.7132873535156 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7205, loss_val: nan, pos_over_neg: 695.4063720703125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.717, loss_val: nan, pos_over_neg: 2476.572998046875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7217, loss_val: nan, pos_over_neg: 1009.2698364257812 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7253, loss_val: nan, pos_over_neg: 480.10986328125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.72, loss_val: nan, pos_over_neg: 804.0542602539062 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.717, loss_val: nan, pos_over_neg: 856.45849609375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7202, loss_val: nan, pos_over_neg: 821.912353515625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7229, loss_val: nan, pos_over_neg: 445.024169921875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7223, loss_val: nan, pos_over_neg: 669.8375854492188 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7186, loss_val: nan, pos_over_neg: 814.5357666015625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7198, loss_val: nan, pos_over_neg: 755.0322875976562 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.716, loss_val: nan, pos_over_neg: 2291.211181640625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7193, loss_val: nan, pos_over_neg: 764.79052734375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7206, loss_val: nan, pos_over_neg: 669.9075317382812 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7185, loss_val: nan, pos_over_neg: 844.9244995117188 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.719, loss_val: nan, pos_over_neg: 1076.16259765625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7217, loss_val: nan, pos_over_neg: 1034.1268310546875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7197, loss_val: nan, pos_over_neg: 1113.0850830078125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7215, loss_val: nan, pos_over_neg: 825.9788208007812 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7207, loss_val: nan, pos_over_neg: 954.874755859375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.717, loss_val: nan, pos_over_neg: 1297.8477783203125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7202, loss_val: nan, pos_over_neg: 889.207763671875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7207, loss_val: nan, pos_over_neg: 633.9117431640625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7212, loss_val: nan, pos_over_neg: 968.341796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7218, loss_val: nan, pos_over_neg: 615.6743774414062 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7196, loss_val: nan, pos_over_neg: 1063.06787109375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7174, loss_val: nan, pos_over_neg: 881.935546875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7184, loss_val: nan, pos_over_neg: 813.5584106445312 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.72, loss_val: nan, pos_over_neg: 1733.8917236328125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7206, loss_val: nan, pos_over_neg: 777.2348022460938 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7199, loss_val: nan, pos_over_neg: 496.4673767089844 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7208, loss_val: nan, pos_over_neg: 785.4602661132812 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7185, loss_val: nan, pos_over_neg: 770.2031860351562 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7188, loss_val: nan, pos_over_neg: 447.9727783203125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7212, loss_val: nan, pos_over_neg: 524.6341552734375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7206, loss_val: nan, pos_over_neg: 606.4093627929688 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7207, loss_val: nan, pos_over_neg: 572.754150390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7214, loss_val: nan, pos_over_neg: 678.708984375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.722, loss_val: nan, pos_over_neg: 526.1351318359375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7205, loss_val: nan, pos_over_neg: 641.2806396484375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7192, loss_val: nan, pos_over_neg: 580.47802734375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7208, loss_val: nan, pos_over_neg: 565.9879150390625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 1121.9505615234375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7221, loss_val: nan, pos_over_neg: 586.4942016601562 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7199, loss_val: nan, pos_over_neg: 788.1053466796875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7193, loss_val: nan, pos_over_neg: 459.34051513671875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7206, loss_val: nan, pos_over_neg: 398.5336608886719 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7232, loss_val: nan, pos_over_neg: 700.2377319335938 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.72, loss_val: nan, pos_over_neg: 594.988037109375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 562.4017944335938 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7235, loss_val: nan, pos_over_neg: 577.1458740234375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.718, loss_val: nan, pos_over_neg: 651.27197265625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7268, loss_val: nan, pos_over_neg: 596.9608764648438 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.721, loss_val: nan, pos_over_neg: 651.0581665039062 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7159, loss_val: nan, pos_over_neg: 605.3883666992188 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7236, loss_val: nan, pos_over_neg: 488.9671630859375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7201, loss_val: nan, pos_over_neg: 832.2407836914062 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7182, loss_val: nan, pos_over_neg: 1250.0555419921875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7223, loss_val: nan, pos_over_neg: 639.3797607421875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7242, loss_val: nan, pos_over_neg: 524.75439453125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7225, loss_val: nan, pos_over_neg: 657.4696655273438 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7238, loss_val: nan, pos_over_neg: 479.6464538574219 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7217, loss_val: nan, pos_over_neg: 803.9370727539062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7193, loss_val: nan, pos_over_neg: 568.0469970703125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7226, loss_val: nan, pos_over_neg: 472.8731384277344 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7242, loss_val: nan, pos_over_neg: 568.8855590820312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.718, loss_val: nan, pos_over_neg: 507.12872314453125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7229, loss_val: nan, pos_over_neg: 547.2486572265625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7218, loss_val: nan, pos_over_neg: 392.889404296875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 456.8824157714844 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7197, loss_val: nan, pos_over_neg: 625.3101806640625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7247, loss_val: nan, pos_over_neg: 446.5815734863281 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7163, loss_val: nan, pos_over_neg: 707.9343872070312 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7232, loss_val: nan, pos_over_neg: 805.9146728515625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7231, loss_val: nan, pos_over_neg: 598.7901000976562 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7173, loss_val: nan, pos_over_neg: 877.3646240234375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7188, loss_val: nan, pos_over_neg: 703.498779296875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7188, loss_val: nan, pos_over_neg: 804.2910766601562 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7165, loss_val: nan, pos_over_neg: 779.2637939453125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7204, loss_val: nan, pos_over_neg: 1367.180419921875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 1062.7896728515625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7202, loss_val: nan, pos_over_neg: 841.7180786132812 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7191, loss_val: nan, pos_over_neg: 819.1884765625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7183, loss_val: nan, pos_over_neg: 860.32958984375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7157, loss_val: nan, pos_over_neg: 1153.017333984375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7157, loss_val: nan, pos_over_neg: 1419.2498779296875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7187, loss_val: nan, pos_over_neg: 2791.263671875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7176, loss_val: nan, pos_over_neg: 825.39794921875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7199, loss_val: nan, pos_over_neg: 928.4703979492188 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 618.1041259765625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7154, loss_val: nan, pos_over_neg: 3087.47509765625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 777.0484619140625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7186, loss_val: nan, pos_over_neg: 1060.4366455078125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7171, loss_val: nan, pos_over_neg: 1319.9810791015625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 1077.0128173828125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7171, loss_val: nan, pos_over_neg: 1157.350830078125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 577.5460205078125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.718, loss_val: nan, pos_over_neg: 734.4800415039062 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7186, loss_val: nan, pos_over_neg: 2557.86181640625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.72, loss_val: nan, pos_over_neg: 710.7623291015625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7176, loss_val: nan, pos_over_neg: 1206.80908203125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7181, loss_val: nan, pos_over_neg: 995.915283203125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7168, loss_val: nan, pos_over_neg: 1811.8531494140625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7186, loss_val: nan, pos_over_neg: 1330.46923828125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 804.551025390625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7167, loss_val: nan, pos_over_neg: 998.448974609375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7191, loss_val: nan, pos_over_neg: 914.770263671875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.719, loss_val: nan, pos_over_neg: 723.4653930664062 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7207, loss_val: nan, pos_over_neg: 541.1144409179688 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7199, loss_val: nan, pos_over_neg: 555.5396118164062 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7179, loss_val: nan, pos_over_neg: 894.3678588867188 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7169, loss_val: nan, pos_over_neg: 563.2567749023438 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7191, loss_val: nan, pos_over_neg: 692.333984375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7157, loss_val: nan, pos_over_neg: 880.0309448242188 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7184, loss_val: nan, pos_over_neg: 699.592529296875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7152, loss_val: nan, pos_over_neg: 570.5567626953125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7183, loss_val: nan, pos_over_neg: 609.2029418945312 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 655.0595703125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7192, loss_val: nan, pos_over_neg: 484.9619140625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7209, loss_val: nan, pos_over_neg: 422.30548095703125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 546.9911499023438 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7171, loss_val: nan, pos_over_neg: 1154.6153564453125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7216, loss_val: nan, pos_over_neg: 651.701904296875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7173, loss_val: nan, pos_over_neg: 772.9673461914062 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7208, loss_val: nan, pos_over_neg: 999.3005981445312 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 775.4342041015625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7189, loss_val: nan, pos_over_neg: 1852.1573486328125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7199, loss_val: nan, pos_over_neg: 798.4254760742188 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.719, loss_val: nan, pos_over_neg: 664.62158203125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7179, loss_val: nan, pos_over_neg: 787.2542114257812 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7155, loss_val: nan, pos_over_neg: 999.7974853515625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7182, loss_val: nan, pos_over_neg: 1219.106689453125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7161, loss_val: nan, pos_over_neg: 615.0973510742188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7191, loss_val: nan, pos_over_neg: 805.8816528320312 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7165, loss_val: nan, pos_over_neg: 650.2110595703125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7155, loss_val: nan, pos_over_neg: 962.0265502929688 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7194, loss_val: nan, pos_over_neg: 805.9874877929688 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7175, loss_val: nan, pos_over_neg: 627.2451782226562 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7189, loss_val: nan, pos_over_neg: 683.3186645507812 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7176, loss_val: nan, pos_over_neg: 748.48779296875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7166, loss_val: nan, pos_over_neg: 533.2110595703125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7173, loss_val: nan, pos_over_neg: 819.2064819335938 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7174, loss_val: nan, pos_over_neg: 565.5133056640625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7186, loss_val: nan, pos_over_neg: 857.1070556640625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7156, loss_val: nan, pos_over_neg: 730.1956787109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.717, loss_val: nan, pos_over_neg: 787.9039916992188 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7168, loss_val: nan, pos_over_neg: 1047.0670166015625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7166, loss_val: nan, pos_over_neg: 625.5616455078125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7161, loss_val: nan, pos_over_neg: 850.3759155273438 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 721.4481201171875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7173, loss_val: nan, pos_over_neg: 953.1342163085938 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 798.886474609375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7146, loss_val: nan, pos_over_neg: 920.7434692382812 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 444.59381103515625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7191, loss_val: nan, pos_over_neg: 882.666748046875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7177, loss_val: nan, pos_over_neg: 741.6567993164062 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7169, loss_val: nan, pos_over_neg: 667.87646484375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7176, loss_val: nan, pos_over_neg: 572.8579711914062 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7216, loss_val: nan, pos_over_neg: 557.6672973632812 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7184, loss_val: nan, pos_over_neg: 553.2474975585938 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7172, loss_val: nan, pos_over_neg: 620.2957153320312 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7151, loss_val: nan, pos_over_neg: 605.6256713867188 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.717, loss_val: nan, pos_over_neg: 458.30584716796875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 765.685302734375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7187, loss_val: nan, pos_over_neg: 848.8617553710938 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7163, loss_val: nan, pos_over_neg: 620.1201171875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7167, loss_val: nan, pos_over_neg: 680.6721801757812 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7176, loss_val: nan, pos_over_neg: 478.6773681640625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 874.2620239257812 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7159, loss_val: nan, pos_over_neg: 847.3983764648438 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7206, loss_val: nan, pos_over_neg: 514.2268676757812 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7188, loss_val: nan, pos_over_neg: 673.8624877929688 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.716, loss_val: nan, pos_over_neg: 793.7957763671875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7171, loss_val: nan, pos_over_neg: 859.5841674804688 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7149, loss_val: nan, pos_over_neg: 825.6593627929688 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7179, loss_val: nan, pos_over_neg: 633.5982666015625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7146, loss_val: nan, pos_over_neg: 1045.8646240234375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7166, loss_val: nan, pos_over_neg: 899.9774169921875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7166, loss_val: nan, pos_over_neg: 480.70263671875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7151, loss_val: nan, pos_over_neg: 816.4745483398438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7184, loss_val: nan, pos_over_neg: 455.6304931640625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7175, loss_val: nan, pos_over_neg: 602.599365234375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7182, loss_val: nan, pos_over_neg: 499.9713439941406 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7168, loss_val: nan, pos_over_neg: 467.28802490234375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7187, loss_val: nan, pos_over_neg: 1041.9375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7174, loss_val: nan, pos_over_neg: 607.2183837890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 728.3156127929688 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 702.2898559570312 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7209, loss_val: nan, pos_over_neg: 508.26531982421875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7197, loss_val: nan, pos_over_neg: 717.037353515625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 953.5800170898438 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7182, loss_val: nan, pos_over_neg: 758.020263671875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7163, loss_val: nan, pos_over_neg: 576.5368041992188 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7157, loss_val: nan, pos_over_neg: 744.99560546875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7181, loss_val: nan, pos_over_neg: 479.87548828125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7134, loss_val: nan, pos_over_neg: 579.2138671875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7203, loss_val: nan, pos_over_neg: 582.162841796875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7162, loss_val: nan, pos_over_neg: 565.9987182617188 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7139, loss_val: nan, pos_over_neg: 708.0296020507812 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.718, loss_val: nan, pos_over_neg: 634.1072998046875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 882.322509765625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7185, loss_val: nan, pos_over_neg: 567.576416015625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7166, loss_val: nan, pos_over_neg: 815.3131103515625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7195, loss_val: nan, pos_over_neg: 528.5642700195312 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7155, loss_val: nan, pos_over_neg: 881.4750366210938 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 968.9425048828125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 1361.82470703125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7158, loss_val: nan, pos_over_neg: 829.8595581054688 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.716, loss_val: nan, pos_over_neg: 747.659912109375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7153, loss_val: nan, pos_over_neg: 1111.5328369140625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7156, loss_val: nan, pos_over_neg: 870.3244018554688 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7172, loss_val: nan, pos_over_neg: 469.2447509765625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.716, loss_val: nan, pos_over_neg: 1000.784912109375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7175, loss_val: nan, pos_over_neg: 603.88330078125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7168, loss_val: nan, pos_over_neg: 622.2921142578125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7172, loss_val: nan, pos_over_neg: 606.5565795898438 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.72, loss_val: nan, pos_over_neg: 735.865966796875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7171, loss_val: nan, pos_over_neg: 853.2332763671875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 2018.94091796875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7165, loss_val: nan, pos_over_neg: 496.27301025390625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7158, loss_val: nan, pos_over_neg: 628.5947265625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.718, loss_val: nan, pos_over_neg: 640.3613891601562 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 625.7013549804688 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7157, loss_val: nan, pos_over_neg: 1006.2200927734375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 848.0629272460938 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 727.5369262695312 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7167, loss_val: nan, pos_over_neg: 849.88330078125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7171, loss_val: nan, pos_over_neg: 1415.430908203125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7167, loss_val: nan, pos_over_neg: 696.2574462890625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7133, loss_val: nan, pos_over_neg: 856.0325317382812 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7135, loss_val: nan, pos_over_neg: 1361.2542724609375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.714, loss_val: nan, pos_over_neg: 1376.3077392578125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 2338.99365234375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 1365.5123291015625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 870.1096801757812 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.715, loss_val: nan, pos_over_neg: 771.5743408203125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 802.1326293945312 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7142, loss_val: nan, pos_over_neg: 3708.70458984375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 1476.6241455078125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7158, loss_val: nan, pos_over_neg: 1500.9229736328125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 1834.433349609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7158, loss_val: nan, pos_over_neg: 1241.611083984375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.718, loss_val: nan, pos_over_neg: 1052.518310546875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.717, loss_val: nan, pos_over_neg: 931.6768188476562 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 1272.65283203125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7182, loss_val: nan, pos_over_neg: 960.916748046875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7155, loss_val: nan, pos_over_neg: 1025.226318359375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7137, loss_val: nan, pos_over_neg: 2993.139404296875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7194, loss_val: nan, pos_over_neg: 1506.474853515625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7155, loss_val: nan, pos_over_neg: 1051.2105712890625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 1934.575439453125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 568.7666015625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 1094.7091064453125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7128, loss_val: nan, pos_over_neg: 1267.1053466796875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7148, loss_val: nan, pos_over_neg: 831.8464965820312 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7183, loss_val: nan, pos_over_neg: 690.29541015625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 1104.861328125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 765.7662963867188 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 1186.79833984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7126, loss_val: nan, pos_over_neg: 1146.9339599609375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 874.32861328125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7126, loss_val: nan, pos_over_neg: 2010.4317626953125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 1489.1763916015625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 644.7227783203125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 1176.2069091796875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 1424.352783203125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7154, loss_val: nan, pos_over_neg: 861.4488525390625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 1199.45166015625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7164, loss_val: nan, pos_over_neg: 604.2830810546875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7151, loss_val: nan, pos_over_neg: 892.8360595703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7133, loss_val: nan, pos_over_neg: 595.3475952148438 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 977.6948852539062 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 904.151611328125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.715, loss_val: nan, pos_over_neg: 686.9278564453125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7163, loss_val: nan, pos_over_neg: 650.5709228515625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 1441.90478515625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7154, loss_val: nan, pos_over_neg: 1487.567138671875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7174, loss_val: nan, pos_over_neg: 612.3265380859375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7148, loss_val: nan, pos_over_neg: 907.9139404296875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7141, loss_val: nan, pos_over_neg: 1689.1539306640625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7138, loss_val: nan, pos_over_neg: 660.7245483398438 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7153, loss_val: nan, pos_over_neg: 1226.8211669921875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7139, loss_val: nan, pos_over_neg: 846.3869018554688 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7152, loss_val: nan, pos_over_neg: 1365.3348388671875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7165, loss_val: nan, pos_over_neg: 1072.6063232421875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 939.2860717773438 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7147, loss_val: nan, pos_over_neg: 1160.4974365234375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7139, loss_val: nan, pos_over_neg: 929.0136108398438 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7178, loss_val: nan, pos_over_neg: 644.8366088867188 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 1317.464111328125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 1691.4901123046875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7135, loss_val: nan, pos_over_neg: 850.779052734375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7154, loss_val: nan, pos_over_neg: 881.3777465820312 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.714, loss_val: nan, pos_over_neg: 908.3267822265625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 1418.3387451171875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7154, loss_val: nan, pos_over_neg: 1146.36962890625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 978.8583374023438 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 1124.472412109375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 2358.41162109375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7142, loss_val: nan, pos_over_neg: 1213.357421875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7148, loss_val: nan, pos_over_neg: 1098.1502685546875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7124, loss_val: nan, pos_over_neg: 786.0906372070312 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7174, loss_val: nan, pos_over_neg: 1025.472900390625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 1214.8902587890625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7141, loss_val: nan, pos_over_neg: 1026.8038330078125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7123, loss_val: nan, pos_over_neg: 1025.4332275390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7151, loss_val: nan, pos_over_neg: 478.278564453125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 911.8980102539062 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7137, loss_val: nan, pos_over_neg: 920.7747192382812 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7142, loss_val: nan, pos_over_neg: 829.3825073242188 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 564.667724609375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7167, loss_val: nan, pos_over_neg: 1083.1630859375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 793.6530151367188 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7168, loss_val: nan, pos_over_neg: 897.4254150390625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 817.1785888671875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7152, loss_val: nan, pos_over_neg: 1124.9544677734375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7119, loss_val: nan, pos_over_neg: 911.992431640625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7186, loss_val: nan, pos_over_neg: 1079.8045654296875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 1379.0565185546875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7146, loss_val: nan, pos_over_neg: 751.3389282226562 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7156, loss_val: nan, pos_over_neg: 726.471923828125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7149, loss_val: nan, pos_over_neg: 854.5551147460938 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7167, loss_val: nan, pos_over_neg: 1099.6927490234375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7137, loss_val: nan, pos_over_neg: 717.430419921875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 1629.219482421875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7141, loss_val: nan, pos_over_neg: 776.5237426757812 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7128, loss_val: nan, pos_over_neg: 917.9895629882812 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.715, loss_val: nan, pos_over_neg: 2110.270263671875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7161, loss_val: nan, pos_over_neg: 487.0697937011719 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 1131.0479736328125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7163, loss_val: nan, pos_over_neg: 955.0955810546875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 1221.6317138671875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 1239.253173828125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7135, loss_val: nan, pos_over_neg: 1082.0458984375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 1184.0135498046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7139, loss_val: nan, pos_over_neg: 837.6395874023438 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 903.6385498046875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7137, loss_val: nan, pos_over_neg: 624.9810180664062 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7142, loss_val: nan, pos_over_neg: 703.492919921875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7147, loss_val: nan, pos_over_neg: 834.8377075195312 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 1257.7110595703125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7119, loss_val: nan, pos_over_neg: 1099.934326171875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 800.556884765625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.715, loss_val: nan, pos_over_neg: 1113.3538818359375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.718, loss_val: nan, pos_over_neg: 723.3716430664062 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 780.3013305664062 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7152, loss_val: nan, pos_over_neg: 846.0137939453125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7149, loss_val: nan, pos_over_neg: 623.6739501953125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 1287.31640625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7099, loss_val: nan, pos_over_neg: 898.4916381835938 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7128, loss_val: nan, pos_over_neg: 733.2327880859375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 794.87158203125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 638.9127807617188 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 510.86871337890625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 729.8963623046875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 793.9425048828125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7154, loss_val: nan, pos_over_neg: 700.6710815429688 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7123, loss_val: nan, pos_over_neg: 703.0570068359375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.714, loss_val: nan, pos_over_neg: 618.8052368164062 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 1417.12646484375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 1546.01953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7147, loss_val: nan, pos_over_neg: 1313.8326416015625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 860.2230224609375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 1126.3736572265625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7112, loss_val: nan, pos_over_neg: 1786.95458984375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 1000.7144775390625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 918.3480834960938 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 1796.546142578125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 1369.808837890625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 1839.4708251953125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7124, loss_val: nan, pos_over_neg: 1394.7442626953125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 565.5894165039062 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7166, loss_val: nan, pos_over_neg: 643.30908203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 923.1185913085938 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7138, loss_val: nan, pos_over_neg: 947.3455810546875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 777.6195678710938 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.715, loss_val: nan, pos_over_neg: 688.8344116210938 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7149, loss_val: nan, pos_over_neg: 533.8722534179688 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 1015.0076904296875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 752.8316650390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 812.86328125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 767.8255615234375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 2244.49169921875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 915.9103393554688 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 893.5072631835938 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7143, loss_val: nan, pos_over_neg: 1237.036865234375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 905.5567626953125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7153, loss_val: nan, pos_over_neg: 803.7886352539062 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 1573.2259521484375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 1939.6968994140625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7139, loss_val: nan, pos_over_neg: 828.37890625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 1028.8927001953125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7094, loss_val: nan, pos_over_neg: 1002.7783813476562 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 688.7389526367188 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 1090.7158203125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7148, loss_val: nan, pos_over_neg: 1133.5802001953125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 964.5078735351562 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 814.675048828125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7149, loss_val: nan, pos_over_neg: 953.6229248046875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 1496.8447265625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7153, loss_val: nan, pos_over_neg: 760.2486572265625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 557.2671508789062 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.712, loss_val: nan, pos_over_neg: 821.6854858398438 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.713, loss_val: nan, pos_over_neg: 814.3030395507812 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7093, loss_val: nan, pos_over_neg: 1466.63818359375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7134, loss_val: nan, pos_over_neg: 850.7380981445312 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7133, loss_val: nan, pos_over_neg: 835.000732421875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7128, loss_val: nan, pos_over_neg: 1370.33056640625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 1235.3182373046875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 566.9857177734375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 796.52734375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 1520.4219970703125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7123, loss_val: nan, pos_over_neg: 747.1314697265625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 880.7538452148438 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 886.5988159179688 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7166, loss_val: nan, pos_over_neg: 631.122314453125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 800.24267578125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7145, loss_val: nan, pos_over_neg: 792.1820068359375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 2134.152587890625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 862.02978515625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 1265.029296875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.713, loss_val: nan, pos_over_neg: 832.3786010742188 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 888.5696411132812 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.713, loss_val: nan, pos_over_neg: 1045.8643798828125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 839.94091796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 3311.609130859375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 1257.5535888671875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7142, loss_val: nan, pos_over_neg: 654.093994140625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7133, loss_val: nan, pos_over_neg: 1185.9595947265625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7113, loss_val: nan, pos_over_neg: 1356.86328125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7126, loss_val: nan, pos_over_neg: 867.1554565429688 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7133, loss_val: nan, pos_over_neg: 573.2090454101562 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7152, loss_val: nan, pos_over_neg: 968.0399169921875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 707.5830078125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7152, loss_val: nan, pos_over_neg: 1064.944580078125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7139, loss_val: nan, pos_over_neg: 841.6915283203125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 638.9423217773438 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 1451.1522216796875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 996.7366943359375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 857.26171875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7142, loss_val: nan, pos_over_neg: 557.72021484375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7108, loss_val: nan, pos_over_neg: 857.5738525390625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 711.2704467773438 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 1287.247802734375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7123, loss_val: nan, pos_over_neg: 1282.439453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 1663.4888916015625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7126, loss_val: nan, pos_over_neg: 567.5162353515625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 621.929443359375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 1173.9407958984375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 1111.69970703125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7108, loss_val: nan, pos_over_neg: 884.4176025390625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 730.1370849609375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 619.6897583007812 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 1136.6339111328125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 982.3706665039062 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 632.4293212890625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 1095.93017578125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 1269.4271240234375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 1291.05322265625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 473.7215270996094 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 910.6219482421875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7108, loss_val: nan, pos_over_neg: 1716.044921875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.709, loss_val: nan, pos_over_neg: 1197.288330078125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7137, loss_val: nan, pos_over_neg: 450.61151123046875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7133, loss_val: nan, pos_over_neg: 578.7276000976562 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7096, loss_val: nan, pos_over_neg: 824.4755249023438 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 948.53125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7151, loss_val: nan, pos_over_neg: 711.3695068359375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 1596.2943115234375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7093, loss_val: nan, pos_over_neg: 537.3421630859375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1297.5714111328125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.713, loss_val: nan, pos_over_neg: 737.8707885742188 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7128, loss_val: nan, pos_over_neg: 907.364990234375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.712, loss_val: nan, pos_over_neg: 811.1826782226562 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 797.6886596679688 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 897.4403686523438 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7133, loss_val: nan, pos_over_neg: 751.7489624023438 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 876.272216796875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7148, loss_val: nan, pos_over_neg: 586.4628295898438 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 793.3255615234375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 1036.301025390625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 786.980224609375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7147, loss_val: nan, pos_over_neg: 838.4161376953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 709.8489379882812 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 750.314697265625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7124, loss_val: nan, pos_over_neg: 895.213623046875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 701.404052734375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 781.6773681640625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7123, loss_val: nan, pos_over_neg: 806.4867553710938 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 993.8915405273438 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 476.35736083984375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7148, loss_val: nan, pos_over_neg: 507.99346923828125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 1381.4317626953125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 819.7828369140625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 915.4071655273438 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7095, loss_val: nan, pos_over_neg: 1354.9322509765625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 847.72021484375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 958.8701171875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7135, loss_val: nan, pos_over_neg: 1005.298583984375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 1059.4642333984375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7115, loss_val: nan, pos_over_neg: 1000.363525390625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 882.8455810546875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 690.9060668945312 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 827.229736328125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 657.5593872070312 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 602.4476928710938 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 802.2133178710938 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7128, loss_val: nan, pos_over_neg: 1781.85400390625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7124, loss_val: nan, pos_over_neg: 981.8010864257812 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7153, loss_val: nan, pos_over_neg: 636.3497924804688 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.714, loss_val: nan, pos_over_neg: 552.3321533203125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 823.3447875976562 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 811.369384765625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7151, loss_val: nan, pos_over_neg: 577.0676879882812 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 1502.27099609375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 576.518798828125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 802.0316772460938 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7128, loss_val: nan, pos_over_neg: 690.572509765625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 953.2337646484375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 697.0953979492188 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 703.5176391601562 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 975.4774169921875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 1068.3499755859375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 878.3484497070312 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 653.2013549804688 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 675.8939208984375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 2705.383544921875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7085, loss_val: nan, pos_over_neg: 1572.3984375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 586.952392578125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 735.7757568359375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7113, loss_val: nan, pos_over_neg: 735.4239501953125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 1683.779052734375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7151, loss_val: nan, pos_over_neg: 440.2469177246094 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7096, loss_val: nan, pos_over_neg: 1246.51806640625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 856.7133178710938 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7135, loss_val: nan, pos_over_neg: 482.7614440917969 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 986.4403686523438 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 667.038818359375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.709, loss_val: nan, pos_over_neg: 1551.8974609375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7094, loss_val: nan, pos_over_neg: 1319.7425537109375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 617.3592529296875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7137, loss_val: nan, pos_over_neg: 728.0228271484375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 833.2728881835938 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 874.4877319335938 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 1994.850341796875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 1489.2294921875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 619.8143310546875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 1411.4754638671875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 1158.388427734375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7094, loss_val: nan, pos_over_neg: 894.25634765625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 1045.7674560546875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 522.8936157226562 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 1642.55029296875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1224.1109619140625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 1126.8065185546875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 639.6080322265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 840.9818115234375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1138.7677001953125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 1148.1700439453125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 650.5845336914062 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 584.7374267578125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7134, loss_val: nan, pos_over_neg: 484.1093444824219 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 568.8124389648438 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 896.7522583007812 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7169, loss_val: nan, pos_over_neg: 559.0078125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.712, loss_val: nan, pos_over_neg: 684.4794921875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 709.6908569335938 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7136, loss_val: nan, pos_over_neg: 713.8984375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 547.3809204101562 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 720.1573486328125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7085, loss_val: nan, pos_over_neg: 779.9279174804688 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 791.0984497070312 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7113, loss_val: nan, pos_over_neg: 587.409423828125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 704.0819091796875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 757.3458862304688 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 1298.996826171875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 910.2495727539062 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 1112.0860595703125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 701.157470703125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 661.1856689453125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.712, loss_val: nan, pos_over_neg: 852.8970947265625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7113, loss_val: nan, pos_over_neg: 992.0645751953125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 714.0220947265625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 1984.2445068359375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 687.245849609375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.713, loss_val: nan, pos_over_neg: 798.0160522460938 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7113, loss_val: nan, pos_over_neg: 924.4708251953125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7108, loss_val: nan, pos_over_neg: 688.3316650390625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 580.9290771484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 825.8161010742188 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7142, loss_val: nan, pos_over_neg: 448.4935607910156 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 685.82568359375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 783.1661376953125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 1739.9910888671875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 703.815185546875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 1558.060791015625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7132, loss_val: nan, pos_over_neg: 537.52685546875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7108, loss_val: nan, pos_over_neg: 488.5296630859375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 682.9415283203125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 664.3716430664062 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 1166.2633056640625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 766.8655395507812 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 749.2306518554688 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 633.4615478515625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 472.12933349609375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1495.1641845703125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 834.737060546875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 904.2552490234375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7129, loss_val: nan, pos_over_neg: 451.2237548828125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 775.0525512695312 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 1297.3377685546875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 977.6561279296875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 637.4329223632812 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7124, loss_val: nan, pos_over_neg: 727.321533203125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [40:39<101597:39:42, 1219.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 596.5917358398438 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 749.1849975585938 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 1097.8807373046875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 1051.348876953125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 1031.9130859375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 673.9230346679688 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 1101.17431640625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 782.6641845703125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 922.27392578125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7144, loss_val: nan, pos_over_neg: 489.356201171875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 828.0240478515625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 1252.446533203125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 646.3556518554688 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 845.0199584960938 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 927.581787109375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1121.561767578125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 949.5933837890625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7099, loss_val: nan, pos_over_neg: 505.2004089355469 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 816.4784545898438 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 1155.4388427734375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7094, loss_val: nan, pos_over_neg: 1931.5194091796875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 692.4602661132812 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7096, loss_val: nan, pos_over_neg: 647.196044921875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 727.68896484375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 2004.8465576171875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 792.0315551757812 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7112, loss_val: nan, pos_over_neg: 1191.766845703125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7085, loss_val: nan, pos_over_neg: 1369.9866943359375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7095, loss_val: nan, pos_over_neg: 907.0806274414062 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 1739.926025390625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.709, loss_val: nan, pos_over_neg: 1053.539794921875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7095, loss_val: nan, pos_over_neg: 1336.68310546875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 805.7808227539062 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 718.0416870117188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 761.522216796875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7096, loss_val: nan, pos_over_neg: 2104.132568359375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 902.726318359375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 913.6054077148438 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 936.1779174804688 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7093, loss_val: nan, pos_over_neg: 762.58740234375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 1222.5408935546875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 790.7820434570312 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 660.9495849609375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 1357.5455322265625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 1437.5316162109375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 535.6690673828125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 489.4368591308594 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 998.1935424804688 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7118, loss_val: nan, pos_over_neg: 1092.00927734375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 622.5584716796875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7146, loss_val: nan, pos_over_neg: 1043.9305419921875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 1537.7098388671875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7117, loss_val: nan, pos_over_neg: 838.932373046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7131, loss_val: nan, pos_over_neg: 801.1484985351562 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 860.3329467773438 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1736.303955078125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.712, loss_val: nan, pos_over_neg: 816.90380859375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7065, loss_val: nan, pos_over_neg: 1061.090087890625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 686.1688842773438 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7127, loss_val: nan, pos_over_neg: 633.4924926757812 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 1005.7517700195312 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 528.6473999023438 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 1377.390625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 776.69482421875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 958.7656860351562 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 1160.940185546875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 520.1412353515625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 527.7688598632812 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7108, loss_val: nan, pos_over_neg: 668.5697631835938 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 1106.2928466796875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 980.095458984375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 699.91552734375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7108, loss_val: nan, pos_over_neg: 765.9263305664062 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7116, loss_val: nan, pos_over_neg: 835.131591796875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7114, loss_val: nan, pos_over_neg: 1360.7252197265625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 1012.2850952148438 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 932.34326171875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 496.29248046875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 620.1654663085938 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 1124.765869140625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 1284.9454345703125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7125, loss_val: nan, pos_over_neg: 717.5924072265625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7081, loss_val: nan, pos_over_neg: 831.5381469726562 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 615.9906005859375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 1334.21142578125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 1629.6142578125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7106, loss_val: nan, pos_over_neg: 927.1744995117188 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7135, loss_val: nan, pos_over_neg: 673.95361328125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 1097.4510498046875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 798.8763427734375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 1338.888671875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7124, loss_val: nan, pos_over_neg: 608.12890625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7095, loss_val: nan, pos_over_neg: 854.4386596679688 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 531.4779052734375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 1639.318359375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.709, loss_val: nan, pos_over_neg: 702.0794677734375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 898.0543212890625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 1606.513916015625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 705.1275024414062 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 906.371337890625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7107, loss_val: nan, pos_over_neg: 668.5762939453125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 835.80224609375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 720.3618774414062 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 899.64404296875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7099, loss_val: nan, pos_over_neg: 619.44482421875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 1139.6463623046875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7081, loss_val: nan, pos_over_neg: 777.6593627929688 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 802.2880249023438 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 844.2015991210938 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1380.129638671875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 836.4494018554688 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 1090.927001953125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 570.5625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 817.3662109375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 731.547607421875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 1169.9649658203125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 558.87255859375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 666.1212768554688 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 546.1926879882812 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 1119.7139892578125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 847.191650390625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7081, loss_val: nan, pos_over_neg: 684.9761352539062 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7079, loss_val: nan, pos_over_neg: 612.2483520507812 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 653.7844848632812 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7112, loss_val: nan, pos_over_neg: 653.2659912109375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 956.6780395507812 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 2584.283447265625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 855.5491333007812 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 984.25244140625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.711, loss_val: nan, pos_over_neg: 682.3468017578125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 999.27099609375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 1672.721435546875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1146.50341796875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 1006.5851440429688 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 1028.7486572265625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 2095.853515625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1440.9844970703125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 814.2662963867188 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 1194.4468994140625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 1136.8433837890625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 927.2514038085938 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1277.8389892578125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 598.0260620117188 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 1177.7198486328125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 847.0182495117188 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 1632.9915771484375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 980.6083984375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 1115.677001953125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 753.0805053710938 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7095, loss_val: nan, pos_over_neg: 1278.658447265625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 1381.3013916015625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.712, loss_val: nan, pos_over_neg: 1836.3712158203125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 903.9476928710938 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 985.9781494140625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 1319.3232421875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7081, loss_val: nan, pos_over_neg: 1057.5472412109375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 834.8726196289062 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 1909.446533203125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 2282.4384765625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 895.748046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 2836.277587890625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 1616.021240234375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 1443.645751953125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 798.9285888671875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1444.3468017578125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 1144.2596435546875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 876.1807861328125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 2790.619140625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 1687.9901123046875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 5472.08740234375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7093, loss_val: nan, pos_over_neg: 950.2416381835938 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1131.5732421875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 2005.1126708984375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 990.990234375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 1094.32470703125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 779.6544189453125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7099, loss_val: nan, pos_over_neg: 654.8622436523438 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 1103.97412109375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 2139.697265625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 1995.90185546875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7081, loss_val: nan, pos_over_neg: 597.5008544921875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7093, loss_val: nan, pos_over_neg: 675.9367065429688 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1321.638427734375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1451.406982421875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 1240.368896484375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1077.3201904296875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 1413.89208984375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 3702.7294921875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 1105.9500732421875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 2101.824462890625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 1197.9967041015625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 876.68310546875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 703.75390625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 1233.7459716796875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 890.0921630859375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7135, loss_val: nan, pos_over_neg: 550.7957763671875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 786.1317749023438 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7101, loss_val: nan, pos_over_neg: 727.023681640625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 564.876953125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.709, loss_val: nan, pos_over_neg: 977.5384521484375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 668.413818359375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 894.6598510742188 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7121, loss_val: nan, pos_over_neg: 519.017578125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 490.40045166015625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 859.9979248046875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7099, loss_val: nan, pos_over_neg: 664.004150390625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7109, loss_val: nan, pos_over_neg: 960.7360229492188 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.709, loss_val: nan, pos_over_neg: 772.4507446289062 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 663.7689819335938 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 815.2861938476562 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 668.276611328125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 884.5106201171875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7096, loss_val: nan, pos_over_neg: 1091.447265625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7079, loss_val: nan, pos_over_neg: 730.1029052734375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7111, loss_val: nan, pos_over_neg: 719.2302856445312 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 881.6685791015625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 936.8165893554688 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 798.5408325195312 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 1055.8681640625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7111, loss_val: nan, pos_over_neg: 2129.017578125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7103, loss_val: nan, pos_over_neg: 685.44189453125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 922.9400024414062 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 721.1275634765625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1521.9493408203125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 904.105712890625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7079, loss_val: nan, pos_over_neg: 1362.3402099609375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 621.4930419921875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7093, loss_val: nan, pos_over_neg: 555.288330078125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1501.951416015625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 665.019287109375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7085, loss_val: nan, pos_over_neg: 650.564208984375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 1146.640869140625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7105, loss_val: nan, pos_over_neg: 792.8001708984375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 702.750732421875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 705.8387451171875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 1163.6297607421875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 704.5771484375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 543.5099487304688 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 848.8514404296875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1343.6409912109375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 879.5195922851562 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 753.2459106445312 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 717.6357421875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7099, loss_val: nan, pos_over_neg: 778.9183959960938 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 1021.3573608398438 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1393.7900390625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1260.1129150390625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 969.6696166992188 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 902.3311767578125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 1630.4183349609375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 1057.4849853515625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 1449.4478759765625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 1089.34375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1483.9595947265625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 872.0125122070312 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7104, loss_val: nan, pos_over_neg: 551.6278686523438 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 2032.414794921875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7079, loss_val: nan, pos_over_neg: 968.7130126953125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 1164.606689453125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7097, loss_val: nan, pos_over_neg: 575.3568725585938 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 1081.1783447265625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 808.6788940429688 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 955.3399658203125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 1854.985595703125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 665.795166015625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 1545.4420166015625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 1330.9818115234375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 818.701416015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 853.4935913085938 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7094, loss_val: nan, pos_over_neg: 839.9154052734375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 1762.4781494140625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 1174.51318359375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 803.2550048828125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 1273.2794189453125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 2087.6640625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 3147.44482421875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1526.5262451171875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 1137.8177490234375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 1086.1058349609375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 2091.223388671875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 1204.4383544921875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 1490.49560546875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 854.497802734375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 1002.150634765625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7102, loss_val: nan, pos_over_neg: 917.1445922851562 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 810.1991577148438 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 974.8551025390625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 1696.673095703125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1809.4058837890625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1490.947509765625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 742.835693359375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7079, loss_val: nan, pos_over_neg: 1525.4443359375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 1720.5687255859375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 735.8154296875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 740.7290649414062 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 811.5096435546875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1808.0394287109375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1858.7611083984375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 823.9097290039062 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 994.3821411132812 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 1053.6929931640625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 1393.0155029296875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 1387.0780029296875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 951.7915649414062 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 776.214111328125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 1706.2978515625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 821.201416015625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7095, loss_val: nan, pos_over_neg: 777.454833984375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 883.5653686523438 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 607.9634399414062 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 841.401611328125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 1249.780517578125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 675.079345703125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 727.4449462890625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 1161.30712890625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 990.6929931640625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 1099.4990234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7085, loss_val: nan, pos_over_neg: 1442.6187744140625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 700.4071655273438 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 1363.3743896484375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 1115.4337158203125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1064.9678955078125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 815.0595092773438 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1468.5423583984375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1023.4678344726562 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 1365.607666015625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 3677.9921875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 2416.173828125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 2432.5634765625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7089, loss_val: nan, pos_over_neg: 1068.674072265625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 750.964111328125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 1385.327392578125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1281.6279296875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 2047.8543701171875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1161.1246337890625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1243.458251953125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 1196.1298828125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 1083.269287109375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.71, loss_val: nan, pos_over_neg: 1438.6904296875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 1180.7532958984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 2161.244140625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1226.111083984375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 869.4873046875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 902.7815551757812 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 2603.1962890625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 1935.3409423828125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1474.1102294921875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1159.5869140625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 1026.7833251953125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 811.2511596679688 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 1646.4306640625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1470.018310546875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 769.6103515625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 885.4452514648438 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 1194.7757568359375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1106.6171875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 684.36181640625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1489.540283203125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1092.4755859375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 1541.20458984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1363.20263671875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 823.4775390625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1063.3294677734375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 1430.6820068359375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1685.497314453125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 738.4329223632812 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1783.90087890625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 1170.8687744140625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 1547.5875244140625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 1065.9593505859375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 769.3052978515625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 1582.71044921875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1857.46923828125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1191.5869140625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 795.420654296875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 862.3073120117188 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 586.6792602539062 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 858.6049194335938 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 817.7006225585938 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 769.5978393554688 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 908.5963745117188 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 1169.72509765625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7083, loss_val: nan, pos_over_neg: 668.7379760742188 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 802.4619140625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 544.0197143554688 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 1465.2799072265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1339.0123291015625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 592.23828125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 744.542724609375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 990.6934814453125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 903.799072265625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 1083.194091796875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 873.5279541015625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1022.9308471679688 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 716.7078247070312 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 791.7723999023438 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7065, loss_val: nan, pos_over_neg: 1085.50146484375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7088, loss_val: nan, pos_over_neg: 706.63720703125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7065, loss_val: nan, pos_over_neg: 1178.4571533203125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1197.6075439453125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 1575.0421142578125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 1248.1998291015625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 860.3549194335938 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 1208.2091064453125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1897.1116943359375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 782.8346557617188 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 933.8847045898438 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 936.1902465820312 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1188.828857421875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1209.0911865234375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1405.62451171875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 734.5578002929688 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1103.42578125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1115.65283203125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 1884.1497802734375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 688.3697509765625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1596.03466796875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 1007.587158203125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 659.340087890625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 1473.58544921875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 808.1079711914062 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 985.8690795898438 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 1324.5531005859375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 1333.1351318359375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 2823.363525390625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 1459.6707763671875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 869.4323120117188 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1699.8560791015625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 2140.279296875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7069, loss_val: nan, pos_over_neg: 893.0177001953125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 1083.11474609375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.708, loss_val: nan, pos_over_neg: 1407.237060546875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 1121.08740234375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1521.313720703125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 938.9989013671875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7122, loss_val: nan, pos_over_neg: 609.0653076171875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1039.7996826171875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 839.9945068359375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 2096.846923828125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 1042.117431640625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1007.0840454101562 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1042.0615234375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1272.091796875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 1749.8145751953125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7094, loss_val: nan, pos_over_neg: 1370.51416015625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7065, loss_val: nan, pos_over_neg: 667.6150512695312 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 747.0621337890625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 680.58984375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7065, loss_val: nan, pos_over_neg: 1135.431884765625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 4298.50439453125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 800.5081787109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 529.5325317382812 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 669.6528930664062 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 1155.3592529296875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1995.8775634765625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 1275.89892578125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 885.5494995117188 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 955.8452758789062 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1701.75390625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1940.29296875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 1116.4097900390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 541.3190307617188 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 473.2097473144531 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1122.4088134765625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1031.03466796875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 993.5203857421875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 748.145263671875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.709, loss_val: nan, pos_over_neg: 546.3617553710938 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 1308.9471435546875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 885.311279296875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 653.4698486328125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 653.23095703125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7096, loss_val: nan, pos_over_neg: 810.5413208007812 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 841.0040893554688 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1349.6097412109375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 728.6146240234375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 975.9068603515625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1459.344970703125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1748.2451171875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 879.974853515625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1396.9991455078125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 772.7175903320312 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 814.7318115234375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 1632.528564453125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 1558.913330078125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 939.8006591796875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7076, loss_val: nan, pos_over_neg: 707.9426879882812 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 794.5685424804688 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1086.757080078125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 822.69970703125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1158.332275390625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 931.0258178710938 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 770.8046264648438 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1176.981689453125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1397.314697265625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1068.0855712890625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 876.2448120117188 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1837.564697265625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1622.3380126953125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1083.6588134765625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1070.4267578125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1037.3980712890625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 859.7412719726562 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 778.2101440429688 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1049.642822265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1525.4974365234375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 2105.32958984375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 4316.25927734375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 749.3087768554688 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1004.580810546875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1635.7314453125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1049.3590087890625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1404.025390625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1170.186279296875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 1692.4625244140625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 643.2181396484375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1575.7147216796875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 674.0848388671875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 906.878173828125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1005.9882202148438 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 941.8352661132812 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1129.79345703125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1260.95947265625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 982.924560546875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1972.296142578125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1111.3521728515625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1047.6217041015625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1296.483642578125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 645.3322143554688 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1012.8549194335938 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 906.2485961914062 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 828.4228515625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 1018.2127685546875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 1619.8074951171875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 652.9146118164062 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 752.09423828125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 843.6179809570312 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1653.2476806640625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 848.01416015625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 733.8109130859375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1322.9285888671875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1056.9830322265625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1000.821044921875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 988.8541259765625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 764.0076904296875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 785.6561889648438 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7077, loss_val: nan, pos_over_neg: 1014.3096313476562 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 850.2019653320312 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 746.718505859375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 849.1590576171875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 642.7809448242188 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 874.7010498046875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1408.748046875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 665.449462890625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 844.9638671875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1238.7174072265625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 522.6755981445312 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 593.294921875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 618.3743286132812 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 796.3343505859375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1229.17529296875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1055.345458984375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 938.2819213867188 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 837.275634765625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 1901.2579345703125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 599.2966918945312 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 801.0098876953125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1248.6357421875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1092.8197021484375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 822.1093139648438 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1475.846435546875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7091, loss_val: nan, pos_over_neg: 855.234619140625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1095.45947265625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 911.2034912109375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1124.5628662109375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 1624.753173828125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 550.748779296875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1099.4676513671875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 832.6326293945312 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1004.4449462890625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 852.7164306640625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1002.455810546875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 700.2894287109375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 936.972412109375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 1494.9117431640625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 644.4807739257812 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1293.408203125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1356.0986328125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7098, loss_val: nan, pos_over_neg: 529.6727905273438 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 2212.351806640625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7092, loss_val: nan, pos_over_neg: 749.107177734375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 18604.380859375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1924.3035888671875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1133.8739013671875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1606.564208984375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 614.6019287109375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 1521.7681884765625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1583.2056884765625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 1121.288330078125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 659.376220703125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 1890.5853271484375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 642.4374389648438 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 972.9492797851562 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1182.222412109375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1151.310546875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1594.97900390625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 835.05078125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 619.4322509765625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7085, loss_val: nan, pos_over_neg: 523.045166015625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1555.6083984375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1006.05517578125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 984.7991333007812 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 700.2279663085938 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1161.2469482421875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 930.1527099609375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 787.1986694335938 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 756.9960327148438 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 937.9326171875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 781.1389770507812 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1498.8226318359375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1219.7708740234375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 749.8764038085938 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 562.3413696289062 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 846.5160522460938 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1321.4344482421875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7081, loss_val: nan, pos_over_neg: 865.7256469726562 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 602.3550415039062 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 615.3099975585938 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 783.96044921875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 3567.345458984375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1761.118896484375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7087, loss_val: nan, pos_over_neg: 804.5143432617188 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 624.396728515625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 828.1222534179688 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1404.0596923828125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1050.8238525390625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 789.5386352539062 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1008.6640625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 2943.376220703125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 719.3353881835938 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1323.0494384765625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 729.8629150390625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7065, loss_val: nan, pos_over_neg: 679.6173095703125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 834.585693359375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 730.1830444335938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7074, loss_val: nan, pos_over_neg: 850.6067504882812 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1651.9227294921875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1009.9429931640625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7066, loss_val: nan, pos_over_neg: 488.3876953125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7072, loss_val: nan, pos_over_neg: 767.8733520507812 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1214.8114013671875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 1737.6865234375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 888.6203002929688 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 669.3662719726562 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1657.5009765625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 790.8016357421875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1198.3306884765625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1165.2845458984375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 667.6196899414062 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 643.889404296875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 2370.61474609375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 2045.0623779296875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1071.9591064453125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1779.504150390625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 1073.6749267578125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 4990.81005859375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 2315.192138671875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1306.14208984375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 747.2717895507812 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7085, loss_val: nan, pos_over_neg: 1560.3118896484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1335.6580810546875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1090.6568603515625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 796.0112915039062 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 929.8478393554688 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1597.16748046875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 1822.34228515625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 3372.1142578125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 854.8535766601562 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1275.0672607421875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 1953.94921875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 921.0333251953125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 2587.884033203125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1579.416259765625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1166.0306396484375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1271.1600341796875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1253.9154052734375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 3925.359130859375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1461.2591552734375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 2811.120849609375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1618.4803466796875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1519.313720703125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1023.8822021484375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 887.17919921875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1678.21826171875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 2153.132080078125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:00:56<101525:31:13, 1218.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1216.7386474609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1248.86279296875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1293.0836181640625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1608.5714111328125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 791.5162353515625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1373.355224609375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1252.102294921875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 699.5995483398438 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 1155.4427490234375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 903.2520141601562 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7075, loss_val: nan, pos_over_neg: 621.572509765625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 902.7415161132812 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 887.8201293945312 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 827.6951293945312 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 1075.6151123046875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1169.8465576171875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 1166.1368408203125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1140.0699462890625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 2339.228515625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 3724.678955078125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1179.1583251953125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7078, loss_val: nan, pos_over_neg: 734.8576049804688 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1438.7484130859375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 928.4097900390625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 873.5755004882812 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 876.0936889648438 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1102.1611328125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1556.2520751953125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1341.791259765625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 977.2308349609375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1136.6651611328125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 10452.939453125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 2648.587646484375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 2155.281982421875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 820.7731323242188 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1088.52392578125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1375.030029296875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1803.803955078125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1147.69677734375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 1005.873291015625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1063.0533447265625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 721.6592407226562 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1208.5577392578125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1107.3719482421875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1540.3951416015625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1071.93896484375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1583.1895751953125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 877.7852172851562 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1419.549072265625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 839.2547607421875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1053.091552734375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 802.3837280273438 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 995.6190185546875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 908.4887084960938 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1411.8428955078125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 3892.0068359375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1139.4412841796875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1392.2183837890625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1288.559326171875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1012.6026611328125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1023.3289184570312 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 767.8637084960938 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 973.865234375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 953.8046875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 1233.3648681640625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1151.5943603515625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 889.97021484375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 577.4459228515625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1145.4984130859375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 640.5794067382812 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1848.781494140625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1059.6346435546875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 944.6529541015625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 979.5408325195312 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 2041.14208984375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1403.461181640625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1118.3106689453125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1329.152587890625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1032.783203125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 767.5209350585938 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1037.6585693359375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 978.1127319335938 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1562.99072265625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1191.731689453125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 810.43310546875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 687.6922607421875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1797.145751953125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1388.471435546875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1028.6689453125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 859.1168212890625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1414.287109375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 864.851806640625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7094, loss_val: nan, pos_over_neg: 809.4099731445312 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 1520.3984375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1125.439697265625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1440.5546875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1624.698486328125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 2826.135009765625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1158.6162109375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1314.255126953125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7067, loss_val: nan, pos_over_neg: 1460.41845703125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 23126.333984375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7082, loss_val: nan, pos_over_neg: 1197.712646484375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 3469.539794921875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 3636.629638671875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 2979.527587890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1705.6337890625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1508.5755615234375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 2564.369140625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1080.601806640625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1284.060546875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1527.8740234375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1720.7015380859375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1184.8314208984375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 925.2708740234375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 6080.03271484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1194.884521484375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1212.3232421875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1419.226318359375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1892.3128662109375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1221.9072265625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1037.0938720703125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 861.3970947265625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1863.9757080078125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1656.50927734375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 842.8095092773438 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 693.256103515625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 3074.8388671875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1259.4451904296875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 936.1034545898438 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 1096.271728515625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 568.5660400390625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1072.61328125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1560.530517578125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 972.4644775390625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7086, loss_val: nan, pos_over_neg: 1136.16748046875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 879.0419311523438 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1500.7607421875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1310.9111328125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 819.7770385742188 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 927.8618774414062 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 2283.25390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1678.5128173828125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1262.3497314453125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1674.5390625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1468.07470703125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 900.7266235351562 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1413.6689453125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 2364.23779296875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 1015.2114868164062 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 943.885986328125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1607.478515625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7054, loss_val: nan, pos_over_neg: 510.1249084472656 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 721.96923828125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 2623.933349609375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1180.227783203125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 803.6761474609375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1031.8704833984375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1156.379638671875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 901.0643920898438 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 2241.491455078125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 1346.067138671875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 7870.35009765625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1209.8597412109375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 829.4833984375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1150.8184814453125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1715.551025390625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1018.4676513671875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 495.9579772949219 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1198.2176513671875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1310.325439453125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1497.8316650390625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1654.59619140625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1315.9254150390625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1423.2061767578125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 702.9444580078125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 760.9514770507812 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 3393.9326171875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 2612.52734375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 714.0859985351562 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 852.4857177734375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 854.2993774414062 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1474.2197265625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1481.4403076171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 938.9686889648438 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1223.9190673828125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1103.4261474609375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7084, loss_val: nan, pos_over_neg: 1005.03076171875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1322.5003662109375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1206.9207763671875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 825.7449340820312 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1147.5186767578125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 4779.38916015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1198.3404541015625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 673.2864379882812 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 800.7540893554688 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 804.8651733398438 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 779.562744140625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 910.0870971679688 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1154.9344482421875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1025.1868896484375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 922.7103271484375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 3438.536376953125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 973.029052734375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 1266.6195068359375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 738.7264404296875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 654.9597778320312 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 848.5651245117188 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1280.686767578125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 540.3800659179688 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 616.7562255859375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7071, loss_val: nan, pos_over_neg: 530.6998901367188 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 844.0418701171875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 2782.25634765625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 937.7635498046875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1011.2093505859375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 2907.515380859375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1390.501220703125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 2344.3662109375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 906.4346923828125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 973.4155883789062 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 984.2807006835938 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 885.4307861328125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 688.0291137695312 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 662.0551147460938 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 910.1251220703125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 1025.374755859375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1230.5263671875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1015.30712890625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 803.4353637695312 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1920.20703125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 962.3142700195312 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 885.9393920898438 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 28514.0390625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1191.613525390625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 890.1990356445312 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7068, loss_val: nan, pos_over_neg: 1429.3603515625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1066.3719482421875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 2733.507080078125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1194.6031494140625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1322.5821533203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 805.0592041015625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1799.8887939453125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1667.6043701171875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1696.4517822265625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1112.2474365234375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1433.0614013671875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1177.8046875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1704.7364501953125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 1120.8848876953125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 2004.078857421875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1350.681396484375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 747.0523681640625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 858.4243774414062 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.707, loss_val: nan, pos_over_neg: 1277.7071533203125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 2710.94873046875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 562.32080078125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 780.70556640625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1228.752685546875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 2046.8558349609375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1590.189697265625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 2438.907958984375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1472.3477783203125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1195.428466796875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 1152.627685546875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 2362.2978515625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 2412.146484375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 859.4102172851562 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 2134.267333984375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1242.1824951171875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 7817.69287109375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1921.951171875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 1042.6534423828125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1161.9151611328125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 728.0498046875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1046.3359375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 1078.7969970703125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 2447.530517578125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 1456.3341064453125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1518.5888671875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1273.224609375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1180.289306640625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 2140.4736328125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 804.5253295898438 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1065.682373046875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7064, loss_val: nan, pos_over_neg: 988.1835327148438 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 661.2169799804688 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1415.756103515625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1121.1400146484375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 852.2019653320312 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7053, loss_val: nan, pos_over_neg: 712.3053588867188 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1055.3145751953125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 2035.0052490234375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1203.5867919921875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 874.4415283203125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 782.2097778320312 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1995.629150390625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 972.9759521484375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 986.9927368164062 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1228.217529296875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 2270.31494140625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7059, loss_val: nan, pos_over_neg: 2460.07763671875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1196.06884765625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 811.7247924804688 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1458.987060546875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1138.0491943359375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1607.778076171875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 968.190673828125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1004.5831909179688 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1109.1558837890625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 952.3356323242188 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1029.2462158203125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 744.1431274414062 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1302.1634521484375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1250.4527587890625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1843.1845703125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1152.388671875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1310.2818603515625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1467.9171142578125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1900.0201416015625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1449.0318603515625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 812.062744140625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 875.6671752929688 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 859.2577514648438 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1265.4490966796875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 927.357666015625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 855.8307495117188 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1275.855224609375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7062, loss_val: nan, pos_over_neg: 749.803466796875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 960.9705200195312 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1313.2095947265625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 988.7447509765625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 658.1904907226562 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 771.7538452148438 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 945.0037231445312 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 887.0016479492188 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1041.6395263671875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 744.1748657226562 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 613.604248046875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1705.590576171875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1090.0675048828125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 681.6402587890625 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1019.2980346679688 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 3043.8505859375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1343.2806396484375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 781.3299560546875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 803.6015625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 2270.857666015625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1583.5479736328125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1739.9283447265625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 716.3430786132812 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 2023.9986572265625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 2487.644287109375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1748.429443359375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1370.80908203125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 1683.29150390625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1537.7935791015625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 649.5958251953125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1115.8406982421875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 3144.556884765625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1639.750732421875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1153.377685546875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1285.1373291015625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1451.7255859375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 2576.44189453125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1386.0159912109375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1148.705078125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1168.0013427734375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 2114.47021484375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1441.498046875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1332.82666015625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 691.3773803710938 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1936.6915283203125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1356.8438720703125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1104.63623046875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1847.8778076171875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1087.55859375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1728.557373046875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 630.9349975585938 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1054.7998046875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 803.8193359375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 913.1686401367188 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1104.423583984375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 780.5673828125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1194.7723388671875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 672.513671875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1460.379150390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1118.5526123046875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 998.322509765625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1015.1450805664062 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7042, loss_val: nan, pos_over_neg: 699.8860473632812 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 1114.6798095703125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1829.271484375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 2196.59765625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 2146.760009765625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1254.9183349609375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 713.8425903320312 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1849.614013671875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 903.4545288085938 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1084.1944580078125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 880.8136596679688 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7055, loss_val: nan, pos_over_neg: 632.6796264648438 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1356.024169921875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 765.6887817382812 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1075.740966796875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 753.4410400390625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 1190.6055908203125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1712.6602783203125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1199.9947509765625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 913.7982177734375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 1017.2267456054688 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1274.9654541015625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1479.198974609375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1010.1128540039062 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1076.92236328125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1251.9764404296875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 2336.892822265625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1595.9991455078125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1049.984375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 723.1724853515625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 932.3294067382812 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1383.703125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1415.96435546875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1475.148681640625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1672.01708984375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1730.17431640625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 2080.95947265625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1666.6746826171875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 2920.159912109375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 4353.208984375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1569.9434814453125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: -57664.16796875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 991.940673828125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 791.5186157226562 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1878.5479736328125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1234.87744140625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1344.640869140625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1087.00146484375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 771.67138671875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1615.2471923828125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1117.92529296875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1257.697265625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 952.3096923828125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1262.8624267578125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7056, loss_val: nan, pos_over_neg: 966.6295776367188 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 895.576904296875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 2315.7626953125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1022.4959106445312 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7057, loss_val: nan, pos_over_neg: 709.9044189453125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 830.6472778320312 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1350.0650634765625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1105.32373046875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 745.806396484375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 695.83203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 895.0779418945312 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 10204.2451171875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1886.13623046875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1116.0743408203125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 941.4493408203125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 2002.8756103515625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1532.4150390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1342.2608642578125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1265.8128662109375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1078.2977294921875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1091.697998046875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 986.5150756835938 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1345.1240234375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1528.1180419921875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1204.90478515625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 765.1224975585938 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 823.136962890625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1012.3219604492188 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1735.775634765625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1646.1910400390625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7046, loss_val: nan, pos_over_neg: 782.0852661132812 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1720.1597900390625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7058, loss_val: nan, pos_over_neg: 1776.1173095703125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 836.9471435546875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 4433.00732421875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1134.556396484375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1508.35107421875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1043.3350830078125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 592.4642333984375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 1275.9652099609375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1102.982666015625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1255.189697265625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 957.5075073242188 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1357.6341552734375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 907.610595703125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1671.904052734375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1036.1602783203125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 837.1270141601562 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1601.2769775390625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 2541.187255859375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 877.475341796875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 912.88232421875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7073, loss_val: nan, pos_over_neg: 764.505859375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 711.2171630859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 799.1546020507812 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 956.7129516601562 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 790.6526489257812 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 595.6060791015625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1309.89111328125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 675.568115234375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1217.2252197265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1020.2464599609375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 908.7950439453125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1412.044677734375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 877.196533203125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1588.9013671875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 2273.684326171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 773.4221801757812 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 985.85302734375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 2837.52880859375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1200.9490966796875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1077.196533203125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7038, loss_val: nan, pos_over_neg: 1071.2330322265625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 925.1204223632812 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 2010.8363037109375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1143.178466796875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 973.7110595703125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1151.860595703125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1397.0076904296875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 10735.5947265625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1261.7542724609375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1034.7484130859375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1321.92919921875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 1186.527099609375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1036.969482421875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 7377.31494140625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1017.8760375976562 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1177.1053466796875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 774.586181640625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 869.0755615234375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1027.535400390625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1976.6085205078125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1013.2615356445312 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 2110.69091796875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 836.6331787109375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1377.28369140625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1133.728515625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1131.677734375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1325.7830810546875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1038.333740234375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1118.627685546875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 796.4326171875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 2378.962158203125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 782.1729736328125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 973.9797973632812 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 731.4177856445312 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 973.03857421875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1626.3311767578125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1783.611572265625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1672.0792236328125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1103.3123779296875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7061, loss_val: nan, pos_over_neg: 1052.4305419921875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1245.1219482421875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 894.40869140625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1256.473388671875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 868.508056640625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1510.2867431640625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 931.5433959960938 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1332.7989501953125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 2086.86962890625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1532.6307373046875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1747.9979248046875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1947.399169921875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1600.9974365234375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1644.8778076171875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7051, loss_val: nan, pos_over_neg: 1830.671142578125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 946.9385375976562 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 714.926513671875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 2608.760498046875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1072.57568359375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1359.73974609375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 2763.44580078125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 939.75732421875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1151.8524169921875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 2501.5830078125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1031.161865234375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1522.2255859375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 753.8230590820312 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1139.3958740234375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1310.4442138671875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2351.009033203125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1370.843994140625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1571.930908203125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1344.8280029296875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1089.4166259765625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 934.4860229492188 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 791.4343872070312 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1116.148193359375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 3967.73779296875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1175.55029296875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1383.9002685546875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1598.5299072265625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 969.65966796875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 693.9368896484375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1922.9840087890625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 767.0831298828125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1325.3399658203125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1579.1851806640625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 983.2369384765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1225.254150390625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1015.8204345703125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1308.838134765625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1652.76513671875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 1847.7667236328125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1555.8626708984375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1353.5010986328125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 919.8553466796875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1359.5018310546875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 1693.9229736328125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 946.7076416015625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1241.2266845703125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1438.3233642578125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1316.67919921875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1851.449951171875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 3352.66943359375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1917.2716064453125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 2031.646728515625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1565.75048828125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1161.6898193359375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 993.7491455078125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 3405.883544921875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 2040.6051025390625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 2445.699462890625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1393.72412109375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1302.0089111328125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 999.6220092773438 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 2010.03466796875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1754.79150390625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1560.807861328125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1104.06298828125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 955.3717041015625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 988.7054443359375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1499.2274169921875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1780.5845947265625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 987.4049072265625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1239.0218505859375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1068.2130126953125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1670.496337890625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1544.40380859375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 2134.290283203125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 913.3794555664062 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1267.1810302734375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2101.788330078125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1008.6427001953125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 982.0283203125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1061.93603515625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7063, loss_val: nan, pos_over_neg: 789.9653930664062 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1060.3184814453125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1175.314208984375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1387.4786376953125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.706, loss_val: nan, pos_over_neg: 900.4613037109375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 794.8316040039062 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 2245.79833984375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 2200.506591796875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1046.4429931640625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1155.3819580078125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1652.8980712890625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 610.2318725585938 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1402.8470458984375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1204.55908203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 3360.308349609375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1071.248291015625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1290.64794921875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2251.155029296875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1815.5667724609375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 797.260498046875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 685.5366821289062 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 895.8861083984375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1153.8746337890625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1200.654052734375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 627.10205078125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1113.15234375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1890.9990234375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 861.00927734375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 680.01708984375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 781.450927734375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 807.5241088867188 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 972.0863647460938 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1353.636962890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 792.3676147460938 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1014.5196533203125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 739.9453735351562 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1197.9859619140625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1224.616455078125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 692.2127075195312 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1270.053466796875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 3610.11767578125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 888.3012084960938 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 788.2687377929688 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1612.98974609375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1850.848388671875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:21:15<101540:50:45, 1218.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1250.5992431640625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1761.4781494140625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 713.4094848632812 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1284.0623779296875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 3361.99462890625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 2943.10205078125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 735.3368530273438 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 778.0721435546875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 1008.6463623046875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 2416.265380859375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 804.6087036132812 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1021.2711181640625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1904.921630859375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 3024.02099609375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1092.632080078125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 828.7857666015625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 836.1724853515625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1089.486572265625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1533.886474609375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1856.014404296875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2569.00244140625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1060.597412109375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1085.6724853515625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 1007.7828979492188 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7041, loss_val: nan, pos_over_neg: 2888.35888671875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 759.4686889648438 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1062.29248046875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 945.53076171875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1467.5233154296875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 3586.184326171875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1581.326904296875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 689.2476806640625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1289.389892578125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1728.3812255859375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1056.67578125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 4575.32958984375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1733.8116455078125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7037, loss_val: nan, pos_over_neg: 1133.6990966796875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1119.721923828125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 3072.47705078125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2434.22998046875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 2458.431884765625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 3131.841796875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1362.135498046875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1904.3028564453125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2833.2451171875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1599.4490966796875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1123.0802001953125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1658.49560546875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1943.0013427734375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 2736.59326171875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2105.531005859375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2591.85205078125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1167.81982421875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1923.6082763671875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1685.72265625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1437.0401611328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1319.5946044921875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7034, loss_val: nan, pos_over_neg: 725.79248046875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 740.1192626953125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 852.33740234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1392.0567626953125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 2294.012939453125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 662.5732421875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 754.60791015625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 571.5272827148438 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 2442.23828125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 3205.66259765625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1661.825439453125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1079.8760986328125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 777.1056518554688 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 614.7864379882812 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1852.1649169921875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1179.450927734375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1282.1202392578125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1217.732666015625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.704, loss_val: nan, pos_over_neg: 1237.2149658203125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1152.6607666015625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 969.0693969726562 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 2068.105712890625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1514.386962890625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 552.5244140625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 834.8471069335938 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1921.7470703125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7049, loss_val: nan, pos_over_neg: 693.4208984375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 906.4271240234375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7044, loss_val: nan, pos_over_neg: 629.1396484375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1372.61962890625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1488.8692626953125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1237.6944580078125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2350.956787109375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1774.3975830078125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1362.221435546875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1768.94775390625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 8813.744140625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1025.994384765625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1786.5479736328125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1750.95361328125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2551.70751953125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 2545.22607421875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1447.04931640625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1905.6029052734375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1185.954345703125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2057.6728515625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 11204.525390625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1298.96484375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 939.1848754882812 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1327.1923828125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 4089.4111328125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1987.6529541015625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1366.877685546875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 852.9673461914062 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1269.32421875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2193.38818359375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1535.16796875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1273.5201416015625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 10307.27734375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1020.9119873046875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 2416.1220703125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1038.8072509765625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1467.82666015625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7045, loss_val: nan, pos_over_neg: 882.0748901367188 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1302.05859375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 819.5454711914062 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1142.6781005859375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2728.882080078125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1656.6942138671875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 738.7754516601562 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 840.6326904296875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 3425.812255859375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1640.089111328125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1176.1749267578125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 940.9053344726562 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1016.722900390625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 2478.862060546875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 3059.68505859375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 990.1534423828125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1778.505126953125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 3239.7392578125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 835.5379028320312 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7043, loss_val: nan, pos_over_neg: 977.6946411132812 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1534.5404052734375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1163.5753173828125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1812.744384765625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 3832.765869140625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 2342.070556640625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 776.3103637695312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 846.5159912109375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: -20395.5859375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1662.691162109375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1472.1099853515625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 821.8391723632812 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1822.665771484375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 2948.755126953125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 3086.560791015625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1027.9708251953125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7052, loss_val: nan, pos_over_neg: 892.898681640625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1824.3338623046875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1499.1871337890625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1582.72802734375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1524.465087890625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1111.1072998046875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1744.76806640625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1383.048828125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1132.3133544921875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 1661.290283203125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 929.7550048828125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2212.997314453125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1334.40283203125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 951.824951171875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 737.6062622070312 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2416.757080078125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1243.3199462890625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1012.833740234375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1100.5411376953125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1568.6136474609375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1069.0362548828125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1409.2078857421875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 796.8528442382812 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1107.3336181640625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1229.85986328125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1463.75830078125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1385.7725830078125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1003.830078125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2631.756103515625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1358.87353515625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.705, loss_val: nan, pos_over_neg: 851.4231567382812 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1528.6578369140625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1343.0018310546875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 743.2804565429688 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1836.8446044921875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1167.49755859375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1051.9901123046875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1386.87939453125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 893.5021362304688 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1777.4141845703125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 2689.70166015625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 983.6351928710938 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1879.252197265625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1345.9134521484375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 5455.25732421875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2453.544921875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 6008.861328125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 949.5325927734375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 3487.9619140625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 3914.618408203125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1494.51220703125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1673.556396484375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1198.620849609375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1087.5858154296875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1780.380859375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1135.3726806640625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 2083.535888671875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1009.9306640625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1264.7320556640625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 2939.8603515625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1347.093017578125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1398.9364013671875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1264.0245361328125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 2003.984130859375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 729.7112426757812 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1242.730712890625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 2552.01806640625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1328.5823974609375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 930.2683715820312 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 746.74951171875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 2163.813720703125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1235.135498046875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1450.9462890625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7026, loss_val: nan, pos_over_neg: 1471.3055419921875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 970.7135620117188 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1151.07861328125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7048, loss_val: nan, pos_over_neg: 1151.0443115234375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 2744.623046875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 2043.3597412109375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1094.0103759765625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1507.72998046875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1339.8756103515625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 2261.534912109375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 2348.404296875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 992.7486572265625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1619.114501953125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1089.871826171875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1513.794189453125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 3744.973388671875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 1584.2742919921875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 4429.72900390625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 912.7260131835938 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1758.6351318359375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1541.1502685546875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1189.7484130859375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1504.556640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1825.8486328125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2026.5914306640625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1576.8775634765625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1793.652099609375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1229.4468994140625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 2757.888427734375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 3115.696044921875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 767.23095703125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1452.168212890625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1344.3524169921875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1437.407958984375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1504.202880859375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1663.432373046875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 2284.75390625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1628.2369384765625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1805.311279296875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 2034.8880615234375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1756.224365234375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1103.3935546875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1461.03466796875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1004.1943969726562 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1569.2222900390625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1150.2933349609375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 955.0171508789062 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1610.0928955078125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1488.499755859375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 2117.217041015625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1006.9425048828125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2878.5673828125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 3799.906982421875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2407.95361328125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1364.9317626953125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 4277.35400390625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 3773.4033203125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2500.718505859375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1330.0211181640625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1844.981689453125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 5054.7373046875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1151.0382080078125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 93540.828125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1210.1923828125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 3651.653076171875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2057.168212890625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 3463.54248046875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 2663.916015625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2724.4423828125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1141.8037109375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 821.0526733398438 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: -2077657.625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1779.3328857421875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1536.3548583984375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 787.7614135742188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 840.66455078125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1371.3497314453125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2638.015869140625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1551.6033935546875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 2419.76513671875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 730.1053466796875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 1217.7364501953125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1381.1048583984375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2145.266845703125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2907.695556640625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1558.4830322265625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1602.9136962890625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1489.2359619140625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1883.1602783203125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2143.5302734375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2069.05419921875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1719.79443359375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1618.7177734375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1875.0748291015625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2031.38720703125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1734.278076171875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 2827.4970703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 3903.181396484375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1463.765625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 761.263916015625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1137.9244384765625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1209.9693603515625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 2138.717041015625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1893.5301513671875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1853.431396484375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1175.608154296875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 746.1936645507812 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 4007.514404296875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1058.9117431640625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 923.9360961914062 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1702.21875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 4403.4560546875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1982.986083984375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 6852.0771484375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 3661.08544921875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1259.3587646484375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1238.2587890625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 3295.248779296875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1797.7479248046875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 2943.659912109375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 3249.735595703125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 968.978271484375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 3144.433837890625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1334.872314453125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1597.7430419921875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1371.0606689453125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1515.553466796875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2524.65625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 2120.008056640625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1629.822265625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 3169.289794921875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1463.451416015625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 1966.57470703125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1603.0494384765625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2004.465087890625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1326.279541015625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1666.9849853515625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 797.1912231445312 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1215.7601318359375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2589.262939453125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1114.7032470703125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 901.9671630859375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1822.4259033203125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7035, loss_val: nan, pos_over_neg: 961.878662109375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 686.154296875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1649.1451416015625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1832.0882568359375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1038.7801513671875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 704.3739013671875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1642.7486572265625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 2049.932861328125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1349.71142578125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1144.49560546875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 778.0532836914062 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 881.079345703125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 921.5860595703125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1320.67236328125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 845.42626953125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1397.3778076171875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1089.6893310546875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1789.2110595703125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1656.2733154296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2267.16650390625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 2089.132080078125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1200.6571044921875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1630.460205078125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 3807.4697265625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1474.6668701171875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 946.5175170898438 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1344.064697265625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1029.030517578125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 3945.16650390625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1136.5068359375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 2876.183349609375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1341.6732177734375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1174.77734375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 966.1019287109375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 5558.115234375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1009.0573120117188 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1262.99267578125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1982.156005859375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1288.228271484375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 2081.453369140625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7028, loss_val: nan, pos_over_neg: 1143.8255615234375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1701.8128662109375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1395.6239013671875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1099.6136474609375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 7928.6123046875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2223.035888671875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2088.140869140625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1454.681640625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 978.484375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1157.8218994140625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 3298.100341796875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 2905.012939453125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1472.3240966796875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2792.054443359375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1063.1258544921875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 4279.5498046875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1419.00244140625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1462.185302734375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2190.286865234375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1026.0438232421875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1597.7120361328125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 933.1008911132812 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1117.14306640625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 2149.84912109375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 836.1614990234375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1630.1456298828125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1165.9959716796875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 988.6520385742188 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1261.8818359375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 4967.28662109375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 959.5552978515625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1062.339599609375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1499.8795166015625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1458.627685546875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1567.82080078125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1506.78955078125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1520.9044189453125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1357.40869140625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7022, loss_val: nan, pos_over_neg: 821.3261108398438 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 797.3070678710938 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 820.080322265625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1999.4207763671875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 6979.96142578125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1270.17724609375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1664.68115234375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1983.328125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1311.287109375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 7772.64599609375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2176.03076171875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1534.5152587890625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2425.114990234375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1806.11279296875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1866.646484375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1850.9725341796875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1845.92529296875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1455.8177490234375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 3435.3603515625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1261.58056640625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 2727.696044921875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1686.5093994140625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1502.9666748046875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 853.5552978515625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1566.5780029296875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1988.1287841796875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 8186.9365234375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 2418.0576171875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7031, loss_val: nan, pos_over_neg: 1525.8157958984375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 3055.70751953125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2171.016357421875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1265.947021484375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 2227.0458984375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 3309.677978515625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1058.953369140625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1317.28564453125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1636.8009033203125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1819.8385009765625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1646.85546875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1255.5439453125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1481.6092529296875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1220.7801513671875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 5438.2099609375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 5750.53857421875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 911.3296508789062 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1271.9033203125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1250.87109375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 3481.92626953125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 2069.491943359375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 929.3262939453125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1381.7579345703125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 994.4439697265625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 4469.05908203125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1832.64111328125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1158.012451171875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1807.0281982421875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2553.509033203125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 731.1437377929688 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1935.8604736328125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 6591.7724609375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1239.3951416015625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1140.0341796875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2283.61572265625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1877.943359375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1487.6341552734375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1186.279052734375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1783.26123046875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 786.5811157226562 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1797.40966796875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1189.7042236328125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1093.3006591796875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1288.687255859375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 976.7664184570312 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2106.759033203125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2290.680908203125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 828.4044189453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1400.823974609375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1083.7779541015625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1346.5703125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1160.0665283203125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1210.9554443359375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1114.1312255859375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1056.080810546875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 986.75537109375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1115.88525390625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1988.233642578125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7027, loss_val: nan, pos_over_neg: 1511.268310546875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 2346.306884765625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1428.783447265625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1127.3104248046875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 722.3778076171875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1426.579833984375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1012.5075073242188 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1790.3233642578125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1835.638671875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 641.46923828125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 719.68359375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1329.4862060546875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1055.9581298828125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 964.5020751953125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1186.490478515625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 703.6182250976562 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1025.5469970703125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 923.7987670898438 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 2143.656982421875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1308.70751953125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 918.2246704101562 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1698.5394287109375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1104.431884765625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1056.5576171875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1470.8607177734375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2664.219970703125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1525.6798095703125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1497.8536376953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 2152.9248046875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1544.14697265625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1928.62646484375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 2595.1796875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 6355.79052734375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1800.82470703125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1466.4158935546875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 3060.0341796875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 851.5563354492188 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1520.0137939453125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2075.176025390625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 1264.9754638671875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1131.1632080078125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7032, loss_val: nan, pos_over_neg: 1194.4398193359375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1386.2547607421875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1526.4444580078125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 981.4615478515625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1306.30615234375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 849.2942504882812 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2074.45947265625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1620.2413330078125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 980.0863647460938 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1017.5872802734375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1539.73779296875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 982.8958740234375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1727.3311767578125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 919.9456787109375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1081.3857421875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1088.599853515625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1518.3975830078125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1050.5174560546875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 964.181396484375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 2647.587646484375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 857.8303833007812 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 600.9998779296875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2464.89111328125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 4397.5869140625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1484.7276611328125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2392.638427734375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1040.0400390625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2749.711181640625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 852.9126586914062 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1751.321533203125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7024, loss_val: nan, pos_over_neg: 919.5913696289062 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1167.463623046875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1223.2890625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 722.6275024414062 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1390.0352783203125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1265.7615966796875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1315.91796875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 665.3855590820312 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2957.916748046875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 853.182861328125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1278.693359375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2764.149169921875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 655.6978759765625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1400.3525390625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 4183.044921875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1705.3134765625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1190.9715576171875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1125.0419921875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1636.858154296875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1217.1533203125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1267.505859375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1959.1595458984375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2858.988037109375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1937.6353759765625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1979.6187744140625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1232.285400390625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1535.208740234375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 4378.34228515625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 16591.986328125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 2552.5693359375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2005.6287841796875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1573.4239501953125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1158.4210205078125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1323.862548828125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1284.7425537109375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 3649.7734375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1554.207275390625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1521.8980712890625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1173.1302490234375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 2527.71728515625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1009.3170776367188 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1246.78369140625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 4594.712890625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 6529.91259765625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1929.131103515625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1120.0311279296875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 1430.4781494140625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1162.923828125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1115.33740234375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1821.8135986328125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2584.311279296875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 962.2327270507812 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1193.0146484375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1926.1708984375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1294.9044189453125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 2017.02392578125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 5283.10693359375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1322.026123046875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 850.6361083984375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2564.409912109375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 3286.215087890625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 807.3023071289062 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 757.4526977539062 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1401.4937744140625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1256.1895751953125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 2651.279296875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1161.8486328125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1251.131591796875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 2709.549560546875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 7215.6376953125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1145.1220703125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1441.9893798828125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2148.0234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 962.123291015625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1119.488525390625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1479.30322265625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1945.65673828125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 987.9127807617188 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 990.8665771484375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 4068.741943359375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1951.8922119140625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1572.6146240234375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1103.0379638671875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1070.993408203125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1221.95263671875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1141.6678466796875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7033, loss_val: nan, pos_over_neg: 1555.3492431640625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1200.588623046875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:41:31<101458:51:30, 1217.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 948.5049438476562 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1125.4866943359375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1739.0802001953125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2107.21044921875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 957.59521484375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1103.5155029296875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1360.6513671875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 882.6831665039062 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1408.296875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 793.5508422851562 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 974.2828369140625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1790.78564453125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1753.1053466796875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1169.3331298828125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7036, loss_val: nan, pos_over_neg: 965.5284423828125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 2157.317626953125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1259.24951171875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1963.6583251953125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1125.7652587890625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1567.4981689453125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 4230.74853515625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1161.108642578125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 820.7852172851562 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 556.2471923828125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2639.35400390625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1750.561279296875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 3845.405517578125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2267.98046875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 990.8222045898438 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2398.255859375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1635.2069091796875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1041.3360595703125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 919.1785888671875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 707.9661254882812 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1668.748291015625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2023.6712646484375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 859.4447631835938 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1688.3482666015625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1648.730224609375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 821.2672729492188 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1701.9471435546875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1379.6861572265625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2076.221435546875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1103.7286376953125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1490.7147216796875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 2193.56640625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 973.0989379882812 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1096.616455078125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1484.27099609375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 1048.99169921875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1025.9906005859375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1403.4329833984375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 913.5438232421875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1860.606689453125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1732.2000732421875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1349.095703125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 807.7711791992188 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1000.0445556640625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1375.2166748046875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1694.5911865234375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2900.265869140625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2736.99365234375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1489.27197265625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1308.5347900390625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1954.8653564453125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1035.7318115234375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1589.5076904296875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 5801.97412109375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1673.7872314453125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1291.8433837890625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1556.0126953125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1696.0501708984375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1617.0401611328125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1393.1375732421875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1175.6981201171875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 2133.5380859375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1677.2655029296875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1428.396240234375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1642.6053466796875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1194.4127197265625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 7976.25634765625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7018, loss_val: nan, pos_over_neg: 947.6583251953125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1088.5933837890625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 4990.1982421875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7019, loss_val: nan, pos_over_neg: 1158.763671875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1564.6177978515625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1101.27001953125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 2269.269775390625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1220.983154296875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2486.58984375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1645.1192626953125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1833.6234130859375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2913.693115234375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1484.6209716796875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1296.7012939453125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2463.12451171875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 5815.55712890625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1185.132568359375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1741.233154296875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1011.5430908203125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 4209.6123046875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1529.324462890625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2474.748046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1193.6427001953125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1236.065185546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 2726.712646484375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2882.394287109375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1648.1114501953125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1390.1307373046875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1668.553466796875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1128.7021484375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2080.798095703125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1374.699462890625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1988.688232421875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 772.7191772460938 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 968.2367553710938 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1262.7506103515625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1421.5645751953125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 771.7379150390625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 884.4859619140625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 915.740234375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 985.3906860351562 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2607.155029296875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1322.4908447265625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 859.486083984375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1726.71044921875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 2074.2490234375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1056.715576171875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 639.8411254882812 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 732.1244506835938 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1668.2205810546875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1173.7098388671875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 841.4036254882812 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1580.7164306640625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2040.4857177734375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1460.490478515625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1962.9544677734375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1483.394775390625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1238.734619140625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1418.476806640625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 10815.9990234375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1267.6649169921875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1246.274658203125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1155.6507568359375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1438.767578125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 2152.4892578125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 5472.05859375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1397.4642333984375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 709.4962158203125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1214.18603515625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1577.5257568359375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1808.226318359375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 756.9754028320312 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1190.064208984375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 841.9357299804688 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 2136.09765625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1623.200927734375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1640.9749755859375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7039, loss_val: nan, pos_over_neg: 810.5061645507812 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 814.3351440429688 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1542.5335693359375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1752.3255615234375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1102.295654296875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 8068.458984375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1891.530029296875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1633.9520263671875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 2393.159423828125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1225.12890625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1363.1986083984375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1219.621826171875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 806.47607421875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1238.1767578125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1224.5987548828125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1066.54541015625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1759.4207763671875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2303.168212890625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1495.8026123046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 4758.48486328125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1477.830078125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1569.8062744140625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1632.3958740234375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1947.528076171875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2162.64404296875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1766.9525146484375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1951.5513916015625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 2037.48486328125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1370.343994140625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1559.8150634765625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1956.32421875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 1228.03662109375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 2882.693359375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.702, loss_val: nan, pos_over_neg: 9466.71875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 29124.59375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 843.019775390625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7025, loss_val: nan, pos_over_neg: 1158.767333984375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1088.8056640625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1917.450439453125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 2597.0693359375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1316.3157958984375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1778.5721435546875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 2333.94921875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.703, loss_val: nan, pos_over_neg: 935.5195922851562 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1860.97705078125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 2741.7705078125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 723.6273193359375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 2048.16943359375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1245.903564453125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1311.1136474609375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 10024.7880859375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 3696.791015625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 2080.9033203125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 141244.640625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1501.1370849609375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1501.5206298828125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 2072.92138671875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1778.385498046875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1885.8023681640625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2321.809326171875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1234.1226806640625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1257.358642578125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1399.83837890625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1316.056640625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1899.987548828125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 941.7249145507812 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1118.946044921875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 736.97900390625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 755.2059326171875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1019.2156372070312 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1171.2684326171875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 962.8072509765625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1013.8490600585938 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 963.6951904296875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 958.8027954101562 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 2146.6455078125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1760.103515625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 3516.69287109375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1609.4058837890625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2458.462158203125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1640.805419921875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1208.0489501953125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2004.8192138671875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 2843.719970703125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1339.530029296875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1240.5252685546875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 5110.50927734375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1672.8839111328125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1414.5592041015625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 818.0432739257812 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2026.344482421875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7029, loss_val: nan, pos_over_neg: 1357.2900390625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1573.3011474609375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1077.4039306640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1137.184814453125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1166.15966796875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1021.9363403320312 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1413.1763916015625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1024.7652587890625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1004.7926635742188 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2319.562744140625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1674.0550537109375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2002.4814453125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1876.57763671875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 6086.8779296875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1711.4476318359375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 2049.694091796875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 956.4172973632812 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1097.841796875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2155.20068359375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2390.322021484375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1308.3780517578125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1852.6463623046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 1526.729736328125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 3560.796630859375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1446.25341796875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1292.5035400390625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1260.2913818359375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 2849.70458984375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 2233.1806640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 3163.482421875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 3181.4951171875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1707.678955078125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 3350.509521484375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 4344.64404296875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2529.198974609375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2847.341064453125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 2091.041748046875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 3670.41064453125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 3193.647216796875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1100.71923828125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 5257.94140625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2357.0654296875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1365.5274658203125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1403.5142822265625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1701.617919921875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1315.7205810546875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 2324.78759765625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 947.5322265625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2112.392578125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 988.1465454101562 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 856.9434814453125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1121.1329345703125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 884.532470703125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 2033.2332763671875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1067.807861328125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1665.901611328125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1022.072998046875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 908.3836059570312 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 882.4720458984375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1701.01123046875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 4133.72119140625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 938.7411499023438 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 941.979248046875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 815.1088256835938 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1448.751953125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2200.944091796875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1881.955810546875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 11135.6171875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1801.960693359375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1190.2750244140625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1170.2430419921875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1753.172607421875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 963.8641357421875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1266.23681640625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1243.528564453125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 3457.065673828125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1215.428955078125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 3789.006103515625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1626.1241455078125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1111.915771484375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 979.913330078125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2218.54296875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1644.1402587890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 15207.126953125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1477.9052734375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2171.9677734375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 2370.783203125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 2565.626220703125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2287.1767578125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2930.740234375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1935.8470458984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1214.9969482421875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 3026.875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1036.3572998046875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1022.5684814453125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1537.411376953125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2948.16845703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2176.655517578125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1382.9078369140625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1992.3416748046875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 2065.4375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 2477.971923828125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 893.5416870117188 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 12545.7861328125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7009, loss_val: nan, pos_over_neg: 778.1060791015625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1731.3143310546875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1269.1953125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1826.9462890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1050.1590576171875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1383.3929443359375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2000.24365234375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 2386.516357421875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1240.3509521484375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1797.5948486328125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 4749.46630859375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2840.77978515625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2142.25634765625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 4646.57568359375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1490.169921875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 998.8054809570312 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2402.93017578125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1601.9189453125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1398.499755859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1211.9984130859375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 3842.080078125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 6382.28515625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1062.90380859375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1044.1473388671875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1665.56201171875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1777.8826904296875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2123.6123046875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1616.310546875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 796.9889526367188 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1276.5086669921875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1569.3292236328125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 3195.661376953125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 6923.05224609375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1192.0291748046875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1627.2412109375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 912.9943237304688 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 1105.899658203125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2386.123779296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2062.8251953125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1254.022216796875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1651.7440185546875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1645.8006591796875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1751.4443359375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1252.4718017578125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1262.091064453125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2540.53076171875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 858.0095825195312 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 810.8004150390625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1173.51904296875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2338.5625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1273.62255859375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1464.8203125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 5416.02880859375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 793.11962890625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1207.6175537109375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1938.0546875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1067.3939208984375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 3174.024169921875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1461.2381591796875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 848.43212890625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1088.735595703125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 929.757568359375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1447.0924072265625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2848.151123046875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 5176.1474609375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 826.5458984375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1677.6136474609375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1713.5867919921875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1000.947021484375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1226.0140380859375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 5306.810546875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 791.8560791015625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 3032.811279296875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1165.176513671875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1250.6612548828125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1068.6082763671875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1841.871826171875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 4696.2412109375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2614.72216796875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 898.552490234375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2125.384033203125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 5226.86962890625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1555.9737548828125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 2378.718505859375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1374.6949462890625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2332.575927734375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1473.8642578125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1242.42041015625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 3983.933837890625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 912.181396484375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2399.10986328125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1526.6475830078125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1843.4730224609375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1440.5330810546875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1782.558837890625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 3338.583740234375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1627.008544921875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2682.829833984375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 4911.1953125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2043.7637939453125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1664.1337890625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1784.5091552734375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 3828.777099609375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1338.8760986328125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1254.529296875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2073.3095703125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1443.0498046875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2972.618896484375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1183.525634765625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1949.0716552734375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1352.2423095703125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1493.258056640625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1000.4359741210938 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1852.7275390625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 3539.84375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1307.864990234375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 2716.35400390625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 671.776611328125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1804.046142578125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1391.432373046875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1590.759521484375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 936.94677734375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 2653.5703125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 3477.65478515625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 3651.072509765625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1382.412353515625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1578.610595703125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 966.8950805664062 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 3429.134033203125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1999.6585693359375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1100.890625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2424.81640625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1665.5299072265625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1024.4658203125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1840.395751953125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1413.7159423828125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1462.3077392578125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 843.5865478515625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1502.14697265625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1554.1724853515625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1984.684814453125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1302.680419921875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1064.4110107421875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 657.3882446289062 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1532.53662109375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 994.1768188476562 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1637.9642333984375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1657.7889404296875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 4120.6591796875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1033.24560546875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1082.1363525390625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1386.780029296875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1986.994873046875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1703.1881103515625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1392.5145263671875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 735.8599243164062 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1555.446533203125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 3601.846923828125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2561.408935546875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 920.5619506835938 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 581.2342529296875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 812.1276245117188 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1551.373291015625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1199.1661376953125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 3264.48193359375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2205.54345703125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1144.1746826171875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 885.5380249023438 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1502.0545654296875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1285.3408203125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2060.84619140625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1258.7264404296875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1488.001708984375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 3422.0166015625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1464.12158203125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1529.7733154296875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 17217.4765625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2186.370849609375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 2365.560546875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2502.173583984375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2725.182861328125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 4024.8369140625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 5845.52392578125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1105.982666015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1716.5302734375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2108.99072265625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 8237.0791015625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2444.941650390625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 1924.9913330078125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 3475.254150390625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 1765.534423828125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 2670.0693359375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1937.419677734375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1663.74853515625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 2376.2216796875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1667.803955078125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7014, loss_val: nan, pos_over_neg: 1187.9197998046875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 2386.069091796875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1554.7891845703125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1666.787841796875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2083.510498046875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1704.0955810546875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1081.3406982421875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1113.020751953125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 886.8159790039062 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 2269.281005859375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1215.5877685546875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1562.73974609375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1314.7725830078125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1825.4400634765625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1233.27099609375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2645.806640625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1453.29296875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 968.189697265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1281.65380859375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1255.7999267578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 3430.85400390625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1902.3360595703125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1401.1065673828125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1054.01220703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 1019.5987548828125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2283.2978515625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1582.6973876953125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1430.8658447265625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 678.9813842773438 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1698.4053955078125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1052.239013671875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1792.7005615234375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1370.5391845703125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 5518.3955078125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1589.825927734375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1164.957763671875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1043.926513671875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1250.755126953125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2558.552001953125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1512.44384765625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1766.10888671875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1295.8336181640625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 893.41455078125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1065.05859375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1729.599609375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 5026.5908203125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1674.0504150390625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1095.673095703125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 961.7598876953125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1032.7205810546875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1478.511962890625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 2610.13623046875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2617.67138671875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1615.98388671875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2003.921630859375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1298.38671875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 3235.802734375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1911.9381103515625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1750.4456787109375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1874.0894775390625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7016, loss_val: nan, pos_over_neg: 1229.868896484375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1174.68115234375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7047, loss_val: nan, pos_over_neg: 799.3450927734375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 15195.14453125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 4957.52392578125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2090.808349609375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1451.0531005859375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2623.948486328125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1717.1763916015625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2153.046142578125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2911.872314453125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2155.240234375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1256.8377685546875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 820.387939453125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 2958.896728515625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1203.802490234375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2541.365234375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2250.052001953125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1127.3653564453125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1623.3336181640625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 819.3224487304688 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 855.7372436523438 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2207.15576171875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 987.7542114257812 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1752.764404296875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1984.430419921875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1396.6240234375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2287.57568359375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1264.372314453125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1354.984375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1242.0013427734375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2087.1767578125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2085.631103515625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 3519.77587890625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 2280.44873046875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2007.7841796875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1849.062255859375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1565.9683837890625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 3069.63623046875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1804.74072265625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 3031.603271484375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1186.918701171875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1570.2918701171875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2460.065185546875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1462.41064453125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2677.601318359375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1710.9171142578125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 824.2815551757812 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1068.173583984375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 937.395263671875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2541.827392578125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 2671.8154296875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1785.164306640625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 839.2399291992188 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1703.939697265625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2357.978759765625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 5699.2294921875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1305.5933837890625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1443.3935546875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1258.566650390625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1265.78271484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1576.3052978515625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1831.284423828125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1850.6429443359375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1542.1087646484375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 4091.21533203125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2716.3359375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1537.649169921875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 3115.55126953125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 3454.013671875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1520.91015625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1920.608154296875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2715.8955078125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 12858.3056640625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1345.55810546875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1871.1754150390625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1252.691650390625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1623.929443359375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1896.5159912109375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7017, loss_val: nan, pos_over_neg: 996.758544921875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7003, loss_val: nan, pos_over_neg: 3549.553955078125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 2986.348388671875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1720.966796875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1356.6461181640625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1869.76953125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1974.759765625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:01:51<101533:05:27, 1218.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2862.262451171875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1292.2564697265625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 3329.11865234375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1674.24560546875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1389.4676513671875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 839.7831420898438 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1239.19189453125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2349.85205078125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2753.0751953125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1390.4801025390625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 847.5797729492188 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2206.005615234375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2898.156494140625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 10736.208984375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2828.79736328125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2337.598388671875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1415.884033203125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1435.4556884765625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1135.2205810546875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2031.5772705078125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 3303.340087890625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1376.7646484375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2166.71484375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1074.156494140625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1748.4403076171875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1388.488037109375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2089.0791015625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 770.93017578125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1377.39599609375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1625.0921630859375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 2470.447998046875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1680.4168701171875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 839.4918212890625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1569.0428466796875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 3447.266357421875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1472.9130859375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 3992.248046875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 890.821533203125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1518.464599609375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 5801.087890625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2207.47802734375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1633.171630859375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1181.0753173828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 3943.433837890625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 989.1270141601562 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 830.7294311523438 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 2262.6953125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1176.43310546875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1400.4315185546875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 962.36181640625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1312.8375244140625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2713.28125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2412.8427734375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1695.8359375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1961.4063720703125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1393.295166015625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1225.7696533203125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1520.8951416015625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1289.607666015625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1558.0926513671875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1885.7518310546875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2764.200927734375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.701, loss_val: nan, pos_over_neg: 1276.3631591796875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2061.22802734375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1655.2235107421875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2088.10986328125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 755.66015625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 3239.05419921875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2709.146484375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 977.9491577148438 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1549.59765625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1104.923583984375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1099.406005859375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1068.32421875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1125.2078857421875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1967.2001953125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 2625.9873046875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2163.23779296875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1089.73779296875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 965.6351928710938 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1554.4798583984375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2529.39013671875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1491.3377685546875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1147.5010986328125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1045.3831787109375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1255.2802734375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1139.737060546875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1342.826904296875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1969.137939453125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 1215.09130859375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1947.610595703125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 886.8580322265625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: -29698.255859375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2355.418212890625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 856.7619018554688 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1169.836181640625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1002.4020385742188 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2150.19677734375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1591.4256591796875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2718.290283203125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 2092.6357421875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1116.446044921875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1104.5057373046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1062.0762939453125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1666.0499267578125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2890.33837890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2131.051513671875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1009.8106689453125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 749.9761352539062 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1473.7020263671875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1979.1734619140625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2573.92236328125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 2262.786376953125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 1316.87353515625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1441.2435302734375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 3699.941650390625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 4921.65185546875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 3922.76904296875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2277.148193359375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1835.218994140625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 2987.977294921875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 3846.555908203125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 3399.52392578125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1942.549072265625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 4769.62158203125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 6533.06982421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1345.792724609375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 2268.831787109375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1050.8692626953125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 926.5509033203125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2167.6728515625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1274.565185546875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1338.7276611328125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 876.3544311523438 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1079.8919677734375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 724.3643798828125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1082.571533203125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1564.5506591796875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1328.83984375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1124.73486328125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 732.7306518554688 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1038.23583984375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1577.8974609375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1773.7838134765625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 957.836181640625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1270.43017578125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1397.4613037109375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 696.9216918945312 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 845.2636108398438 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1046.89013671875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1226.4517822265625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1015.493408203125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 1325.5435791015625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1122.744873046875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 897.4169311523438 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1190.43994140625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1655.27490234375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1162.0706787109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 742.4033813476562 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2807.681884765625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2379.619384765625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 3441.66845703125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1444.4534912109375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1962.5732421875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1491.302734375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2376.75634765625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 4714.1103515625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2797.638671875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1514.968017578125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1834.79638671875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 747.7465209960938 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1963.107177734375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 8081.3115234375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 2285.1279296875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 4273.06591796875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2015.2181396484375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1998.762451171875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 3578.807861328125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 8974.474609375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 9136.8115234375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1117.665283203125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1660.4921875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2282.071044921875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1366.3529052734375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1287.1993408203125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1104.978515625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 4234.33740234375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1516.907958984375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1750.7486572265625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 2454.99658203125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1402.854248046875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1563.5218505859375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1973.759765625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2023.7030029296875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1524.7093505859375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1185.1953125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1732.31787109375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1812.906494140625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7023, loss_val: nan, pos_over_neg: 1006.4635620117188 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1281.0068359375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2159.65478515625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 917.4813232421875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 818.6766967773438 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1306.4468994140625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 2752.539306640625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1594.2532958984375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1058.0125732421875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 820.9760131835938 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1562.267578125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 2365.203369140625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1247.58251953125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1219.637451171875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2601.09033203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1825.23486328125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1194.32861328125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1917.3485107421875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: -38978.296875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1822.73779296875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 4799.005859375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2299.86279296875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 618.4356689453125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1906.8463134765625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 7721.07275390625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 933.1946411132812 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1484.593505859375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1498.4140625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1565.8280029296875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1597.637939453125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1032.1400146484375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1834.0511474609375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1398.0750732421875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 2077.069580078125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 3074.59814453125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1307.12646484375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1215.333251953125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1257.884033203125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1447.0150146484375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 1064.90869140625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2213.2490234375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2293.74365234375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 9107.6220703125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2886.39013671875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1076.8021240234375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1015.8445434570312 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1419.420654296875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 3929.064208984375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1384.57177734375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1984.099609375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 3666.67529296875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1208.0264892578125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2057.59228515625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2089.930908203125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1132.704833984375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1635.718017578125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1802.374755859375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1005.5789794921875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1908.4466552734375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2393.525634765625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1063.6539306640625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1579.4697265625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 2666.71630859375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2061.130859375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1344.4814453125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 779.3829345703125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1840.938232421875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1022.7713623046875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 3476.9580078125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1137.2506103515625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1300.3665771484375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1772.1015625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1468.826416015625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1418.0096435546875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 3504.012451171875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1451.0697021484375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1668.1739501953125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 18593.513671875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2269.423828125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 952.6631469726562 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 991.3829956054688 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1276.6611328125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1754.1134033203125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1628.81494140625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 3871.2333984375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2968.18310546875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2980.400146484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1101.3951416015625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1633.021240234375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1386.760498046875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1326.4063720703125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1061.49853515625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2368.666015625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2419.364990234375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1806.3726806640625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1184.197021484375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1510.447021484375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1498.8504638671875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 764.895263671875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 827.8438720703125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 4842.4755859375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2178.580322265625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1800.66650390625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2372.342529296875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2339.378173828125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1779.1158447265625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1626.7449951171875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1387.48046875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2385.35302734375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1055.0472412109375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2628.671142578125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1150.0174560546875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7011, loss_val: nan, pos_over_neg: 1248.0625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1127.343994140625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1695.7080078125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1212.4864501953125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 922.7269287109375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 748.4955444335938 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1620.4085693359375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1301.9786376953125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1635.1026611328125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1040.0679931640625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 791.8021850585938 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1887.3870849609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1152.513427734375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1248.8717041015625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1050.0322265625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 3173.55810546875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1832.8865966796875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 905.7536010742188 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1753.662353515625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2127.940185546875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1490.9722900390625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1626.6708984375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1786.1776123046875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1160.3172607421875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 3210.7041015625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1646.7462158203125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1761.8709716796875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1524.893310546875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1239.0869140625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1486.6759033203125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 694.6752319335938 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2690.184814453125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1225.8131103515625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 906.6416625976562 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2013.4970703125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 3598.97119140625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 4207.34033203125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 2229.85302734375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 933.4751586914062 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1187.1661376953125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1671.52880859375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1201.6927490234375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1761.2685546875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 3241.858154296875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 672.4142456054688 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1455.304931640625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1513.7978515625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: -6807.8095703125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2454.568115234375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2356.074951171875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2306.484130859375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1239.410888671875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 19451.4609375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 2490.57763671875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1259.5863037109375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2308.9384765625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1638.72265625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1324.796875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1519.9119873046875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1392.58984375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1269.5927734375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 855.817138671875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 3698.697021484375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 11195.361328125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1114.525634765625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 964.6547241210938 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1494.1270751953125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1195.3856201171875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1819.8746337890625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1629.11328125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2132.7685546875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1361.4022216796875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 906.392333984375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1047.337890625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2230.21435546875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1042.7705078125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 901.5350341796875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1085.219482421875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1061.3232421875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 824.1470947265625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1220.6099853515625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1568.891845703125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1026.312744140625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 818.6865234375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 606.2022705078125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 2210.83837890625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1332.513427734375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1085.71044921875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1287.326171875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 3556.6767578125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1630.0758056640625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 924.1084594726562 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1678.7239990234375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 171909.125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2084.028564453125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 3794.208740234375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1126.0665283203125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1174.9251708984375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 931.1735229492188 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1775.1453857421875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2569.028076171875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2987.11962890625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 961.3277587890625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1185.8311767578125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2381.448974609375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1273.46923828125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2699.4453125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 6804.1826171875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: -15803.2646484375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 1047.759765625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1162.687744140625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1330.7388916015625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2094.0634765625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1255.4130859375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 7153.0908203125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2287.855712890625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 2400.68701171875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1500.5517578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1488.471923828125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1779.3565673828125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2429.05810546875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1610.31884765625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 2873.109130859375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1830.218017578125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1706.1943359375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 576.1351928710938 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1288.5582275390625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1681.501953125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 4785.73828125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 3023.79541015625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 896.7869262695312 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 3055.051513671875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2099.76611328125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 851.2821044921875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1582.86962890625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1927.146484375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1675.0889892578125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1166.5601806640625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 3154.386962890625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 927.315185546875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2914.861328125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 986.2313232421875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1183.4417724609375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1895.7452392578125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1274.646484375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 3336.93896484375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 4658.08251953125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1658.8514404296875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1136.6646728515625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1217.801513671875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1293.1409912109375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 2511.5478515625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 3315.426025390625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1836.1102294921875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 885.3203125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 851.8101806640625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7002, loss_val: nan, pos_over_neg: 1376.2601318359375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 919.49169921875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1178.71533203125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1505.6630859375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1249.696533203125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 709.2232666015625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1112.787841796875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1625.880859375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1424.867919921875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1350.2666015625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1343.858154296875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1256.9227294921875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 2025.8271484375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1106.937744140625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1462.786376953125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 3676.3017578125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 3233.371337890625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 2977.000244140625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1367.0640869140625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1014.0814208984375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1897.5074462890625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7001, loss_val: nan, pos_over_neg: 1319.515380859375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2217.004150390625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1584.9488525390625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1207.2587890625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1140.40234375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 7941.91064453125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 5743.61181640625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1557.3419189453125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1676.0592041015625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 4295.560546875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1259.7874755859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2898.221923828125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2079.457763671875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1512.763671875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.6931, loss_val: nan, pos_over_neg: 5589.8828125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7015, loss_val: nan, pos_over_neg: 927.1359252929688 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1279.219970703125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2490.450927734375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2123.737548828125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7006, loss_val: nan, pos_over_neg: 710.3663940429688 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1540.2176513671875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1963.1385498046875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2023.798095703125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 3893.126708984375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 871.1213989257812 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1158.4576416015625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1593.96826171875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 2400.111572265625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 4395.49951171875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 7135.58984375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1557.8751220703125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 4030.2529296875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2526.705810546875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2841.50927734375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1404.9630126953125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1454.85400390625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 2054.85595703125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1879.472412109375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2175.01953125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1080.9794921875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 2120.469970703125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7021, loss_val: nan, pos_over_neg: 1165.149169921875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 910.445068359375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1097.6578369140625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1745.6888427734375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 868.8402709960938 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1088.377685546875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1463.6697998046875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 962.0068969726562 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1321.6312255859375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1515.4228515625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 984.923828125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1389.7442626953125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 579.5194091796875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 874.9623413085938 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2037.21484375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 798.4614868164062 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2020.8038330078125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1802.1533203125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1116.3026123046875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 732.9300537109375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1410.2606201171875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1759.8424072265625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1311.9033203125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2795.218994140625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 3284.304931640625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1097.389892578125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1321.6585693359375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1836.165283203125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1433.501708984375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1371.0390625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 3624.10009765625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 2240.22412109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7013, loss_val: nan, pos_over_neg: 899.104248046875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1696.0408935546875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 3150.1611328125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 4528.736328125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1645.883544921875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1495.620849609375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1534.7178955078125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1426.32666015625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2321.564453125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 905.988525390625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 6214.77197265625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1429.6688232421875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1277.985107421875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 2294.287841796875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1439.112548828125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 909.6763916015625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 2978.4462890625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 4047.28955078125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1293.4398193359375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1836.123779296875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1570.91015625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 3623.443359375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1218.7025146484375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1167.4273681640625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1839.7325439453125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 4213.5712890625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1277.8072509765625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1004.6454467773438 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 978.3860473632812 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7012, loss_val: nan, pos_over_neg: 1083.2232666015625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1451.48583984375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1427.0726318359375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 881.0973510742188 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 887.8287353515625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 3727.5732421875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1790.393310546875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1769.73681640625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1245.533935546875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1695.0389404296875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 917.7813110351562 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1738.6409912109375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.6996, loss_val: nan, pos_over_neg: 1209.58935546875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1136.0133056640625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1218.5758056640625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 1358.156982421875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1219.82568359375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1226.955810546875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 785.955078125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 874.7452392578125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 2023.19482421875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 3217.5634765625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 974.4564819335938 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1125.8245849609375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 834.9561767578125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1588.913818359375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2135.37548828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2069.016845703125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2740.476318359375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 720.9988403320312 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1435.058349609375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1127.396728515625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1541.603759765625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2304.18798828125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1471.298583984375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 966.8472290039062 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 1137.57421875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 972.6168823242188 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 4134.73193359375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1759.913330078125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1190.694580078125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 862.6492309570312 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1017.841064453125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 3004.9736328125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 3041.386962890625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1604.041015625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 920.1067504882812 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 672.8178100585938 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1098.4696044921875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1274.8116455078125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1792.4071044921875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1761.558349609375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1164.9705810546875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 869.9764404296875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2156.756103515625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1708.0457763671875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1440.0311279296875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1273.1722412109375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2013.137451171875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 3136.672607421875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1319.04931640625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 992.6837768554688 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1682.2725830078125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2119.691162109375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1205.8306884765625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1491.388671875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1978.5865478515625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2438.149169921875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 825.2892456054688 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1990.559326171875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1254.241455078125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1637.823974609375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 958.2951049804688 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1742.4305419921875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1233.2275390625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 814.030517578125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 1134.49169921875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1158.4783935546875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1796.924560546875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 962.9688720703125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 1709.10302734375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1944.2369384765625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1600.5208740234375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1145.6629638671875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 5145.78466796875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 2210.199462890625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 2052.78955078125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 4978.78759765625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 941.9934692382812 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 951.2413330078125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1367.3331298828125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 864.7525634765625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: -6202.6982421875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 4714.8759765625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 6013.72412109375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1237.8577880859375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1496.9261474609375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1500.2572021484375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1047.7392578125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1737.603271484375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2962.8779296875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1702.4256591796875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1049.45703125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1120.27392578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2026.49658203125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2311.1337890625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:22:10<101556:28:14, 1218.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 803.3729248046875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1446.2982177734375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1047.9871826171875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2088.4404296875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2478.0029296875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1251.5682373046875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1434.1904296875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 977.9278564453125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 781.8062133789062 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2361.873779296875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1941.3499755859375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1356.923095703125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1666.8359375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 897.4364624023438 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 45795.53515625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1612.1580810546875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1678.857666015625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2326.896728515625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1021.2984619140625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1332.16162109375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 2183.05810546875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 2156.40234375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 3139.60791015625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1322.6998291015625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1107.6031494140625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1825.70654296875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 4391.75244140625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1612.1090087890625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 804.4468383789062 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 890.4625244140625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 3690.506103515625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.6998, loss_val: nan, pos_over_neg: 888.718017578125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1366.2003173828125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 662.6223754882812 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1094.19384765625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 3313.18310546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 3496.142822265625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1393.377685546875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 919.581787109375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 940.5121459960938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.6991, loss_val: nan, pos_over_neg: 1460.8453369140625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2542.2998046875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 3699.055908203125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1300.1939697265625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 857.2188110351562 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1105.5877685546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1027.2288818359375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1341.023681640625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 3883.252685546875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2167.164306640625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 896.613037109375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1818.522216796875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1609.1700439453125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1481.0108642578125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1021.0370483398438 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 5263.75634765625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 6808.44482421875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1578.042236328125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2962.625244140625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 2773.849609375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2173.672119140625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1430.3768310546875 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 7428.86865234375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2297.165283203125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1755.388671875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1720.8291015625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2158.0068359375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 903.825439453125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1284.33056640625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1511.7081298828125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 1633.322021484375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 4306.55224609375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1578.7230224609375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 808.1678466796875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1962.1837158203125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2484.75341796875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 997.6626586914062 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1483.201416015625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 3119.616943359375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 643.7008666992188 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1788.544189453125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 4639.56884765625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1428.7857666015625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 968.9744262695312 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1866.558837890625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1159.349853515625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2957.515380859375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 1798.0765380859375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1196.470703125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1589.2747802734375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1309.49658203125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1455.563720703125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 12838.0712890625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1099.8358154296875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1479.8372802734375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1283.3553466796875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1239.6583251953125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1316.1943359375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 3046.88427734375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2145.67138671875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1186.659423828125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 7035.65185546875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 950.4025268554688 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1274.2481689453125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1561.5582275390625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1499.4952392578125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 2287.37060546875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 3869.448486328125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1271.7828369140625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 842.0997314453125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 795.366455078125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2161.333984375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 2773.726318359375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2543.41015625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 865.9911499023438 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1931.32080078125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1485.3223876953125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1752.0643310546875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 13048.2626953125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2157.300537109375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1553.099853515625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2254.5029296875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 3048.173095703125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2176.3037109375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 881.1658325195312 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1751.17138671875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1430.50830078125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1519.16162109375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1096.016357421875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2678.57568359375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1492.184814453125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 685.22509765625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1137.6600341796875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1114.04638671875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1850.7364501953125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 13879.615234375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1090.7442626953125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 928.3295288085938 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 971.3296508789062 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1432.3636474609375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1501.7159423828125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 4541.97998046875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2303.359375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2082.3486328125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 8503.4912109375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 3337.268798828125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.694, loss_val: nan, pos_over_neg: -50885.78515625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.6925, loss_val: nan, pos_over_neg: 8455.2646484375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2506.437255859375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 3972.74951171875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2603.083251953125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 1283.776123046875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1527.786865234375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1475.0362548828125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 1835.7384033203125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 3382.748291015625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1693.6539306640625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 3311.923828125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1572.562744140625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2303.9970703125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1580.645751953125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 1787.4752197265625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 2986.542236328125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.6932, loss_val: nan, pos_over_neg: 44336.75 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 2028.7315673828125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2801.57421875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2024.2247314453125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 967.6940307617188 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 2566.055908203125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 3422.046875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 4014.870361328125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1924.219482421875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 1767.567138671875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1017.8932495117188 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 13423.3935546875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1500.723876953125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2208.48681640625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.6923, loss_val: nan, pos_over_neg: -15510.5712890625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 2648.38916015625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1361.873291015625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1051.0087890625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1466.3104248046875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2781.331787109375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1209.153076171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1193.339111328125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1606.54541015625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 2052.96142578125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1449.8226318359375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1504.3203125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1406.31298828125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 3739.88671875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1294.09375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1042.3140869140625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7008, loss_val: nan, pos_over_neg: 821.2376098632812 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 757.0191650390625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 819.1458740234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1090.3656005859375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1345.952392578125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2116.600341796875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 946.2613525390625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 813.2537841796875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.6987, loss_val: nan, pos_over_neg: 1390.41015625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2296.1572265625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1609.361328125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1433.220703125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 3764.663818359375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1915.9599609375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1426.6375732421875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2345.9423828125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1443.957763671875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2465.709228515625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1749.1181640625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2744.102294921875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.6995, loss_val: nan, pos_over_neg: 938.8483276367188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 3287.239013671875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 2180.471923828125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 3563.79833984375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 870.9401245117188 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 685.129638671875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1834.7357177734375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 2138.8505859375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 3278.299072265625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1437.961669921875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1263.04736328125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1016.3358154296875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1245.0345458984375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 5341.0048828125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1187.0567626953125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 2497.26025390625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2256.118896484375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2082.569580078125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.6926, loss_val: nan, pos_over_neg: 3258.708740234375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1608.3377685546875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.694, loss_val: nan, pos_over_neg: 1237.214599609375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 37729.56640625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 1942.847900390625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 1924.078125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1950.4998779296875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1397.286376953125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 2392.658935546875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2563.7509765625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 2504.589111328125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1834.8040771484375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 2729.3583984375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1346.721435546875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1568.0294189453125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1549.3271484375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 3025.357421875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7004, loss_val: nan, pos_over_neg: 2017.5875244140625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2721.17138671875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1311.206787109375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 4361.77978515625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1707.6527099609375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1376.04638671875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1997.848388671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 1822.3128662109375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1726.1856689453125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2535.527099609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2600.978759765625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1214.465576171875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 5634.962890625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 1633.5421142578125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1738.201416015625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 3866.706787109375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 2101.90869140625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 956.6407470703125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 809.71337890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 3695.450439453125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1573.9609375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2642.89501953125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1433.6275634765625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1375.13525390625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: -5740.25927734375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1395.3939208984375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1338.3956298828125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 4256.67724609375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 2504.853271484375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 9488.1005859375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2585.556396484375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 2145.020751953125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1624.58935546875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1588.470703125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 15007.2490234375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1339.853515625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2112.3359375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 8435.046875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1810.5322265625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1566.5263671875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: -11055.126953125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 824.7587280273438 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 3059.1875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 3057.887451171875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 968.2662353515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 15104.181640625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1064.609619140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 1147.64892578125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1936.3460693359375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2814.94873046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: -40326.4765625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1497.0008544921875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 2729.55712890625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1783.66259765625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1914.935546875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1523.0313720703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 13536.6728515625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1313.08642578125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1263.7320556640625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 875.3085327148438 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1735.3707275390625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 3925.619140625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 39846.25390625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1493.5438232421875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: -1279683.875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2431.4912109375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1627.50537109375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 2927.663330078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 8957.736328125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2213.830322265625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2259.59375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1577.2818603515625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 3386.051513671875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1085.74609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 7356.39892578125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 4033.3388671875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 3222.75146484375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2311.0615234375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 180641.46875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1586.1324462890625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2567.90576171875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2485.349853515625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1828.2032470703125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.6934, loss_val: nan, pos_over_neg: 8572.5576171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1046.6883544921875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 1839.766357421875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2039.812744140625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2846.87548828125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1912.736083984375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 700.1458129882812 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2164.29248046875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 4019.656982421875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1811.1409912109375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1864.9901123046875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1611.2615966796875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2677.7578125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1839.919189453125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 1271.036865234375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.6994, loss_val: nan, pos_over_neg: 1133.9263916015625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1019.649169921875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1912.2861328125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1419.8690185546875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 982.164794921875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 967.7962036132812 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.6927, loss_val: nan, pos_over_neg: 2726.232421875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1103.4591064453125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1501.7357177734375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1458.337158203125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1894.1802978515625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1158.2720947265625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1000.0759887695312 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1641.72998046875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1106.9969482421875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1820.0048828125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 5301.70166015625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1610.5867919921875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 3374.451904296875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 1736.9490966796875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1367.395751953125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1931.3572998046875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2834.66259765625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 3309.6044921875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2900.34326171875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2144.2509765625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 6146.884765625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1478.3924560546875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2619.830322265625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 812.71142578125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 2189.566162109375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 2336.865234375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.6993, loss_val: nan, pos_over_neg: 1357.4901123046875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 2902.568603515625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1503.6915283203125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 2195.9794921875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 991.89013671875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7, loss_val: nan, pos_over_neg: 644.6279296875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1812.317626953125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 10231.529296875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 3583.76025390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 3767.900390625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1475.9404296875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1003.5004272460938 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 3180.054931640625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 3518.138916015625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 3169.2041015625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 39650.890625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1916.4425048828125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.6936, loss_val: nan, pos_over_neg: 3086.669189453125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 974.7407836914062 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1813.4906005859375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.6932, loss_val: nan, pos_over_neg: 1582.7581787109375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1457.16845703125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 2382.77294921875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 2247.50537109375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 3573.743408203125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1063.3179931640625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 3244.885986328125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1080.533447265625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2303.26806640625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 5931.380859375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 8123.19580078125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1227.1319580078125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 4689.3310546875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1244.9544677734375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 1675.5633544921875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1883.1610107421875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 3692.54150390625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1768.2677001953125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.6931, loss_val: nan, pos_over_neg: 3528.985595703125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 953.8190307617188 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1338.1009521484375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2036.7962646484375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 1459.8609619140625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.694, loss_val: nan, pos_over_neg: 2403.210205078125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1031.820068359375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1093.1187744140625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 3146.32275390625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1847.7733154296875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1945.49267578125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.6973, loss_val: nan, pos_over_neg: 1377.54931640625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.699, loss_val: nan, pos_over_neg: 1091.6922607421875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2231.58544921875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1742.7735595703125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1702.7041015625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2306.2265625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 8772.9423828125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1179.053466796875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 1807.1405029296875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 3140.6728515625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 6731.9736328125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 3011.995849609375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 880.0873413085938 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 938.9287109375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1259.0933837890625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 6885.86474609375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 9387.380859375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 6691.87353515625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1927.849853515625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1161.859619140625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 2680.85302734375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1343.833740234375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1353.9715576171875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 1471.36572265625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.6989, loss_val: nan, pos_over_neg: 1179.1484375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1601.0286865234375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2899.9228515625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 3343.597900390625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1905.138427734375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1797.0941162109375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 3503.151123046875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7005, loss_val: nan, pos_over_neg: 897.4645385742188 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1920.5767822265625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 2442.15087890625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: -11170.0146484375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 16218.7470703125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1584.9561767578125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2553.6513671875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2048.514404296875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1864.82958984375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 6277.4482421875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 945.2333984375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.6933, loss_val: nan, pos_over_neg: 4246.7138671875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.6936, loss_val: nan, pos_over_neg: 4002.6904296875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 4178.67431640625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.696, loss_val: nan, pos_over_neg: -31104.453125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1366.5545654296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 2001.3543701171875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2277.37353515625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.6928, loss_val: nan, pos_over_neg: -48351.515625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2925.13623046875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 1469.6298828125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1099.7215576171875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 3023.666259765625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1922.3878173828125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1585.568115234375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1131.945556640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1137.8931884765625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.6925, loss_val: nan, pos_over_neg: 3454.3466796875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1453.3775634765625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1306.125732421875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 2259.055419921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1382.8355712890625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 2721.239990234375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 3168.69384765625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: -102156.1875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1489.6031494140625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 3078.4921875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2024.4329833984375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 1462.9376220703125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 2257.178955078125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 3984.95703125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 6149.41064453125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1851.5770263671875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2145.306640625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1163.2215576171875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 2292.94140625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1029.9979248046875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 2579.193115234375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1182.4400634765625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 5463.84326171875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1307.459228515625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1626.126220703125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 3394.27734375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 2731.541015625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2906.765380859375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.6922, loss_val: nan, pos_over_neg: 6856.548828125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1757.294189453125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 3503.5048828125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 2025.8150634765625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2691.410888671875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 3781.498046875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.6933, loss_val: nan, pos_over_neg: 43783.01953125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.6936, loss_val: nan, pos_over_neg: 1688.02978515625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1356.2904052734375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 2464.606689453125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 2045.867919921875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 2582.2197265625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 1484.5262451171875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 3766.173095703125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1931.830322265625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 4329.7607421875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1611.6661376953125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2446.110595703125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.6936, loss_val: nan, pos_over_neg: 193860.75 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 2106.793212890625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2035.1885986328125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 2165.307373046875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1395.2779541015625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 3076.984619140625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 1384.9090576171875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 19208.263671875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 2364.355712890625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2713.466552734375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.6931, loss_val: nan, pos_over_neg: 2452.046875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 2621.9501953125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 2252.224853515625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 4212.8623046875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1611.635009765625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1191.46728515625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 4105.931640625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1190.7152099609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 17833.837890625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 921.712890625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2266.97802734375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 2634.232177734375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1036.1776123046875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1343.9859619140625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1770.9420166015625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 733.548583984375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1302.0770263671875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 2036.2711181640625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 2279.1171875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1484.76123046875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1389.392822265625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 784.1943969726562 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1113.6279296875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2498.169677734375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1943.5443115234375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 1411.39306640625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 1594.5302734375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1968.8734130859375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 3533.939208984375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 5553.9775390625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 2495.908935546875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1036.7034912109375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 1962.187744140625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1253.04931640625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 1613.88671875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 2479.151611328125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.6926, loss_val: nan, pos_over_neg: 5515.76220703125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1770.861328125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1702.6024169921875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1832.7225341796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2092.76318359375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2038.5830078125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1161.1373291015625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.6925, loss_val: nan, pos_over_neg: 2324.10009765625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 3086.133056640625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 3108.396728515625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 3160.869873046875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1683.5693359375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1581.4398193359375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 787.419189453125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2652.235595703125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 1631.064208984375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1928.4090576171875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1270.7142333984375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1255.597900390625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 1415.690673828125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 1644.1446533203125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.6988, loss_val: nan, pos_over_neg: 1860.7022705078125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: -9929.87109375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1091.4437255859375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 1555.8082275390625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 3156.619384765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.6992, loss_val: nan, pos_over_neg: 1114.0704345703125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2002.20703125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1144.980224609375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.6999, loss_val: nan, pos_over_neg: 3228.95263671875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.6933, loss_val: nan, pos_over_neg: 3805.115478515625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 1138.0216064453125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 1169.5538330078125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2432.650390625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.694, loss_val: nan, pos_over_neg: 1749.62158203125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2514.338623046875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1706.2103271484375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.694, loss_val: nan, pos_over_neg: 3423.52392578125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1239.214111328125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1080.8701171875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 2420.3818359375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 4769.34326171875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1387.9024658203125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1464.3583984375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1804.085205078125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 1489.351806640625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2230.67138671875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1405.2242431640625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 2053.728759765625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1665.52783203125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1885.662841796875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 2488.689453125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1750.830322265625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 4071.6064453125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1091.706787109375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 3601.047119140625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 4884.267578125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2395.783447265625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1112.84765625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.6977, loss_val: nan, pos_over_neg: 2621.6455078125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 3324.960205078125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2943.555419921875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.6932, loss_val: nan, pos_over_neg: 6363.46630859375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1458.951171875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.693, loss_val: nan, pos_over_neg: 5645.14990234375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 2667.41552734375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 6038.52587890625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 3407.505615234375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 3626.056884765625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 2083.714111328125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 2149.781005859375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 5794.35107421875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2058.92138671875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: -107907.859375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 3373.402587890625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 3764.373046875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1754.766357421875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1714.1966552734375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.6935, loss_val: nan, pos_over_neg: 3307.146728515625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1077.0198974609375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 892.3670043945312 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1033.7763671875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 2803.80712890625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2544.307861328125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.6983, loss_val: nan, pos_over_neg: 815.6600341796875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1481.056640625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1371.062744140625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 2234.71435546875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1312.139892578125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2608.176025390625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1330.489501953125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 1173.101806640625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 4880.51318359375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 3885.10498046875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.6971, loss_val: nan, pos_over_neg: 1866.1993408203125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1580.052490234375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 2864.209716796875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 2417.54345703125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 2404.944091796875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 2574.6826171875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1449.054443359375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1403.1856689453125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 931.2113037109375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.6951, loss_val: nan, pos_over_neg: 1094.8291015625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1610.4429931640625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1284.273681640625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2434.5654296875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 905.9727172851562 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1265.8870849609375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7007, loss_val: nan, pos_over_neg: 862.0979614257812 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1526.529541015625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1424.0616455078125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 2098.031494140625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1461.3819580078125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1177.9693603515625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 1861.85498046875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 3788.331787109375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 2116.96435546875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [2:42:26<101480:10:45, 1217.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 3340.52197265625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: -8112.169921875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1227.8505859375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.6942, loss_val: nan, pos_over_neg: 1268.9921875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2945.553466796875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 2972.888916015625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2300.343017578125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 1347.25390625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1671.2364501953125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.6923, loss_val: nan, pos_over_neg: 8167.576171875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 1254.9443359375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 1619.13916015625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.694, loss_val: nan, pos_over_neg: 3676.87548828125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.6934, loss_val: nan, pos_over_neg: 2639.572998046875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.6939, loss_val: nan, pos_over_neg: 2623.50830078125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 1816.6705322265625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1109.6705322265625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1452.7728271484375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 2563.231689453125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2811.912109375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1104.1337890625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 2018.063232421875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 3428.840087890625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1135.236572265625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2748.76025390625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1050.0819091796875 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1125.59423828125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1491.374267578125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.6928, loss_val: nan, pos_over_neg: 2347.5732421875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1409.6357421875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 894.4263916015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 1174.117919921875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1200.85400390625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.6985, loss_val: nan, pos_over_neg: 1043.2608642578125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1183.3541259765625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 2456.1494140625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2867.753662109375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1191.531982421875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1034.2540283203125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 1117.188232421875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.6974, loss_val: nan, pos_over_neg: 1027.91015625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1388.4739990234375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 1101.333740234375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1534.398193359375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1575.8880615234375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1155.8729248046875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.6946, loss_val: nan, pos_over_neg: 2343.455810546875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.698, loss_val: nan, pos_over_neg: 971.1682739257812 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.6934, loss_val: nan, pos_over_neg: 2492.349609375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1930.1546630859375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 1271.523193359375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 3750.822509765625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2025.643310546875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2449.6611328125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 1724.3563232421875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 767.1361694335938 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2941.64306640625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 2771.345703125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 1252.866455078125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.6997, loss_val: nan, pos_over_neg: 895.1224975585938 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.6972, loss_val: nan, pos_over_neg: 1206.693115234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.6969, loss_val: nan, pos_over_neg: 1533.72802734375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1367.919921875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 2576.62060546875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 1280.5400390625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 1336.236328125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.6941, loss_val: nan, pos_over_neg: 3216.433837890625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1213.8409423828125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.6986, loss_val: nan, pos_over_neg: 946.7637939453125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2070.366455078125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 1007.4472045898438 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 1053.10546875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.6949, loss_val: nan, pos_over_neg: 2314.40185546875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.6962, loss_val: nan, pos_over_neg: 2016.0455322265625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2291.309814453125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.6957, loss_val: nan, pos_over_neg: 1362.5850830078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.6944, loss_val: nan, pos_over_neg: 1422.3929443359375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.6961, loss_val: nan, pos_over_neg: 2333.3046875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2170.71875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 3628.296142578125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.6979, loss_val: nan, pos_over_neg: 2166.00244140625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 19649.07421875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.695, loss_val: nan, pos_over_neg: 2768.595458984375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.6954, loss_val: nan, pos_over_neg: 1521.6304931640625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1247.0091552734375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 2216.25634765625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 1539.5264892578125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 1869.4749755859375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.6978, loss_val: nan, pos_over_neg: 1177.8243408203125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1723.2452392578125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.6966, loss_val: nan, pos_over_neg: 2501.076416015625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 2753.3359375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.6963, loss_val: nan, pos_over_neg: 869.1339721679688 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.6931, loss_val: nan, pos_over_neg: 2413.721435546875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 2985.2197265625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2982.520751953125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.6938, loss_val: nan, pos_over_neg: 3090.700927734375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.6967, loss_val: nan, pos_over_neg: 4561.25341796875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 2006.265380859375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.6955, loss_val: nan, pos_over_neg: 1532.2933349609375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 2756.5009765625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.6976, loss_val: nan, pos_over_neg: 1307.698486328125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.6982, loss_val: nan, pos_over_neg: 2873.484130859375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.6947, loss_val: nan, pos_over_neg: 3496.88720703125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.6968, loss_val: nan, pos_over_neg: 82326.46875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 6619.763671875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: 1473.9217529296875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 3302.04541015625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 5158.05908203125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.6937, loss_val: nan, pos_over_neg: 2791.361572265625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.6945, loss_val: nan, pos_over_neg: 4548.75341796875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.6952, loss_val: nan, pos_over_neg: -51819.65234375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.6943, loss_val: nan, pos_over_neg: 4314.83642578125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 1880.4046630859375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1731.0516357421875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.6975, loss_val: nan, pos_over_neg: 2867.78564453125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.6965, loss_val: nan, pos_over_neg: 3613.0537109375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.6953, loss_val: nan, pos_over_neg: 1800.22216796875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.6964, loss_val: nan, pos_over_neg: 2920.7900390625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.6936, loss_val: nan, pos_over_neg: 2034.629150390625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.6956, loss_val: nan, pos_over_neg: 1939.59423828125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.694, loss_val: nan, pos_over_neg: 3878.39208984375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.6984, loss_val: nan, pos_over_neg: 1245.9453125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.6948, loss_val: nan, pos_over_neg: 13955.7236328125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.6981, loss_val: nan, pos_over_neg: 1226.145751953125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.6959, loss_val: nan, pos_over_neg: 1731.3487548828125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.6958, loss_val: nan, pos_over_neg: 2331.70751953125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.694, loss_val: nan, pos_over_neg: 5483.2880859375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.697, loss_val: nan, pos_over_neg: 7673.6962890625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.696, loss_val: nan, pos_over_neg: 3590.700927734375 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=scl1'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
