{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    # augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "    augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "                             stds=[0.229, 0.224, 0.225]),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABmRElEQVR4nO29XahtW3bX+2u99zHmXGvtvc9HnVNJVSWa0hvhRl/MFUUUEUTUINR9uIoR5AoBXxQVfEjFPPgUiD4ELvgUsFBBEwMKNw+BEIMSBPVGQ9R8EFNJ1FSsVOXU+dhrr7XmHGP03u5D672PPsac6+NU7V17HrMbrL3XmnPMMcdHG+3j3/6tdVFVXskreYi4l30Ar+SjI6+U5ZU8WF4pyyt5sLxSllfyYHmlLK/kwfJKWV7Jg+WFKYuI/GkR+SUR+byIfPZFfc8r+fqJvAicRUQ88F+BPwl8Afhp4DtV9Ree+5e9kq+bvCjL8geBz6vqr6rqAPww8JkX9F2v5Osk4QXt91PArzd/fwH4Q7dt3LszPfOPQcorMr+pCprQpIgIOMnvq70nAs6Dd6iz9zW/LcVqFuOpIClBsn3a63r4ndxmbVfbaD4+sOOQss3q8+vdyeqPIx9ZfAeat8s/5dyV+Tvl8KN3SnusdVvl6fTOO6r69rGPvChlWR+6HUm7gchfAf4KwNY94g+/+X+BE8Q5cLPB02mCmx1pv0dCQLYbCAFihGkC75HHj0hPzknnPXEbiBuHRMVNikwJKfdzjLjrEXd1A8No+44xv9kYWU35fzXFcuVGrbaZJnSc7ONdsOMSt/w82D5acVIugh2/SN5c5201zd8fI3gPfYd0HcSIjmM9f0Ko+6iHdyy8KA+JOMQ7+2xKaEr2Har8+Ds/+N+P3DvgxSnLF4Bvbv7+JuB/thuo6g8CPwjwWvfxWbdTmi0C1JtZL0a5eGAn6zyoIrsR58y6qLMnTZ2gva/7EydITOjYIar2XJV9lRtsBzdfWIDkzGELB4og3s/HIs4UIc7vm0LIvM9yDq65ufncBFCXzzmm+btcc+7TZIrQXqOkqONAYRaK1xwzGqtRUrV9Mk7HFayRF6UsPw18q4h8GvgN4C8Af/HeT+WTU5obFZOdRLE2mkDzkx6CWaKYkP0AIrjg0M6RvENDdknZIohPSAzIZE+txLS0BLBUlKSoKiIKeJDVRRcHWVeoLvKIiANtlLJYKHHZtea/U7Ymag+Efbfkh0LMHUfm4xWXr1kE/HGFaS1U+zlJqIop5jihw/BylEVVJxH5a8CPY5fzc6r68/d/cGX+8w0TEbQx18B8oSE/iRFxDhkC0nlkIyiC+tnSiHOkyePGgIwRQr4JqVGY5nuBJk5qRLKlaN1TcaGq5m+r+8rWpgZSt+QURVHWkuOw+p1F6dauTZOdhz+isK2i1O21WjBVXbrAW+RFWRZU9ceAH3vg1ssnAJZPlhNE/MHNKeZ4dlHJ4pLdVN2QOEEdpCD2IAYhBYcEB8FbDDBN8xNblMel+UbPJzV/f3NTpASe3s8usty8ohviwLXBsCvXydzA2mV0AWnjpfsvuJ1zez3K93o4SHyLBdNk24esCsPtX/HClOVDSyxWZTb/xJgtSg7ifDHfYkFZ67ZyPMI44ZzdTPWO1NnFVo9ZmZAVzXskeLMu1fTnY3GCuZ2sAOXJjAm8zMEhmEUA+7sc3zhmy5LyzQh2jOUmtkqYA8vFU+2dbRPkUInK360Sr2OgVsq5MLuocm2rkjpB3P2qcBrK0lq//CSIJgvCGjdw8NTAHAPkwFemiMqIBId0Hj+YKzI3kD8i5IC1TUdX0mY/JWhtjyO7jeLn6x7yzbdtqNvPX645C5bjMUK56U5smzZ+E5mPu90f2PmLWTNdGZH1NRMRVLLCyx2KtpLTUBawC+L9fJM0Ib6N+CNoPsnmgpLadFZhinZxhgmXrYjzgvfmjljfuHVm0T79rYgDsqucJtPvGGfr1oWcmZl1I+XzaeOvYj1LcAmz5XIsA1CykmkTyxRMqc2OSNSnTfPfaXXs5bOtcntn2FQ5vhQPP7OS01GWIjWQzRcsJjuRGBu/n12Ak8WFU1UkRrsw44TsPc45fDD3ox4k6tKaqM6upEirKMeCvnGylLv9bIw1jS6xlFmX/MRrYyFUZvNfUmuYFSg1CrbAetzCFSOa4920jJE0HuJC7fmUOCgEgxWmCV0HwEfkdJSlWJWvRtobmm+gRAOadPIQi/XIytW6IADnEFW0xiFH8In22Jwg+NmNiJii5M9LibW8mzMi0vLmtdlLBscWVuhYdlJSZycZgM3n493sZtfxS8ZgqnVy8zmbZcYeUFbu8oichrIU85rlAMmEBptYBXbHpLnwoloRXLBrqi7HMW3Mki8YMc5xQgkEy/eXC+19hl10DmpDaFyKZAvnGrfhy8k1/ycOs5Tsrsr3OpmD6ZhAJ/uuEJZKfsyVVKyqXGc3W7P6oOR4rpz/HXIaygIrP8xxzGVtVltFWgdpR+IRFTHXnVFe9c2FKzhG656amARXLIGbv689roKziM7xekn7C/7S3tQcp2hGktfnpcVCtlmKJjQmRBLqtVoXgWw9ZKl7BXCsFrA5rnKtnSyt7B1yGsqiCsO4uhFtjaVcpObmNNvWG+7cwlLYNnYRRKmw/2xdPNoFu4aTxUWaiv/P2x7LwKAx9/7w/daSlPODObvJ50QkwwA6K1/OUsxq5RJCUeSUY6RyfCUjW5QG/OJQqlVaX+9Sb4qtpbtbTkZZdBisGNf1dlEdWWFiTYsVqlIIAXpvT22Jd9ysNBq8/e+bp6j4dZeDyuCgC1aPGqdFQa1VmCpOZvdRSghuZe3abevpzcpS44Sy72maH5DisrRxPa3FLCl8tmiaZGmp/BwAL76zXJ/8MGhMtl1s3NZHRVlUFZ2muSh398b2f80ksgVxDs3/4122GvknlBglm2uE5A3FleCQMT+9SZeBpRPDSmqQSuOmEnNW5uan/DbYfF04LJZkJaW0UV1ILU6WGGaNKjfxDW5pCZPaQyElCDZoYREof4ik4iSUZSG6CvpEDPr2fja15fVyEbI7IXhTCufQzpM6R+o9qXekzqEByJksKrhBUOfmrKU81Q6qIoQjuMtB4THNiHJJ89e1oVzoq/sqdIYjN6ym0+LbF+f4RswFm1tjVqyYFbZ8JilKQKTUk1ITw8wAXn3o7pHTUxaabKj1w+WmFcpCyT5E0C6gm2BWxJuF0d6RgiP2Vn1OnZAyYCkJJGXovzx5ziHOzWAbLHGN9rjsj/nJbYGtGNFhRAvu4r0dKwYqzgqSjz9bQmKym1lwojWUkLIyFCV0rRIXlxlr5tPW1dAcYLfpOTpjViFb5HvkJJRFROyCFuCpjVnWhKjyv7e6jnbBfvpA6n2uB7lsTYS4EZIXUoBCDBMFFYdMahXoyVBXKXgD1JuywB7W1dtizucTaQLUVM9t3r6ky/c8xUWB1oFt+z2L7aUCfQdYSbIAWNrP+kbRnBy6tlvkJJQFEWPANXUgUxCWGQ5Y4a8Esd5B36Ebn91NdjlBiFvHtBFinxUl41aSHyp14KLDTR6JAT925s9FkGlCy9O6Bshalhss0/cca0gBysrrMFuEUiAVZzcwJazK2ShEm87HZAhrSjOOko9nXQMysO5Y5hYNoS2KvAIk6wN4T/xyGsrixKgCRUoavNb4rDg1kA2e1Af72XjixpuibIRpK0xn9rv6HNgWs6F2fyQ5ZAKJatSGMcxQxDShKS4AMlVdVpxbKN5LVXBNMteJWkWrCgeFrYaTJTNQ1sqoc5ZWuTYuB15+qayra1q/s7jVEBDXLV3cWvnvkNNQlizaXPwCOFGzETsZzb9bnOJJG1OUaetJG2HauKwoMJ0LccNSWQAU4pnFMdNG2GyFbuPoNgF3M+H2IwwjMk7G1R1HY+O1qWaJFY6RjWCJAFeFWsUhuqwSi8yWp2Z9C2Aw4zcFcS7VY9IBEAw0ylUYfwn12SVV5uFHVFlmzADUZfjZ++zmV6lncKRNqATtuHXVmkznwnQO0xnEraIeUlBaC+0GIW4FfyFMZ57N1lxX9yzgrwP+2sMwWSa2D+hu3zzFeUfrDEJnHk6xSKXgKCFYkN7yY1h/vChIAh1nRNl7wC9T+gpEygznr69lyw0qmdA0oc4D8Qj0f3eQe1rKAvMFkVSj+IVpzhdIO2+u58yZsvTmesYLYXwE8UyZziBtE9ophAR+BubSKMQzh79xpF5Ive0j9kIf7Dvc3iO7XBxMyboBipTYqnki5yxOZ8i+MP5yqmwZV1re4Lb4Vx6Y3DVAF+bv8c11WPN216w6x4ES1UC3xk9N6u2Eg+r7Sk5PWdaBYcFSWtDNzWlx3ORAdpMtygVMF0rcKmmj6CYhm0jYTISQSEnsJ3pi75j6jMd0YkrTWYCsQQhXjqCKjE2K25KNcrZ0QGQqbiKp4TjlJhyrX62r0cXVFC5PAQubFL9KjmV0GG3TvstpelbcHCCLpgbhvsV63MO/hVNRFs0nt6YANhZlgc4G+0ndbA3ihhynKNOFkrZmSWQT6c9GLs72bLuJMXrGyTMWZdl4pj4wlEzKW6pdgmE3BOP0lloUc72qpPYH6ao4+7zXBeJcJTWvtx+rWZCirjtSbsjXoPBlUnYrgxFnK/ZUFFQElZxFyeo4Wr5LW2y8Q05DWdCG3CQHilKfqIzQzuisEDtTlLgRUgepB+0VuoT0idBPnG8H3jy/4SIM7GJgHwNjNKWMKjzbbbjZ9oxdhzoDZEQFiR6/D7ihw03R6kctuXott73ewu+lGNpmRtJsl0sHQg7K24JmVoAaBJfgtwXqSgZZ2mVqULxSiAUP+GF36TSUpVadcxtqQU5LfOKzonTeLMom1Dgl9pgL6SzrAezhF/AhcbYdebLd88bmmoswMCbPkDxOlN5NeFG+sr/gt64veC+cs5ctpIAkwY3gh4BEJUTFxWguKVeo58Nvkd1VFtTGFAXvaKvEizR43v4AxY5Ydbptv43JHq7thgVDP19TXbi/Q85LJXC3lNE75DSUJVkhEVVEc60HjH0P2Q0ZPJ/6jKdszKqkjvpTlEWiBW5dP/Ha2Y7XNze80V/zyO8BiDjO3cBr4YZHfscXh9f5H/0b/Hp4gy8C+3SGix43Cn5whr/ETNncOcRNMMri5gLzDWyyoBI0aowG1uU6jlEuZAZzS1wCc0a1vnlxmr+nKKf3yCbMZYVSoCyWp2lVNauYv8uxxFpi8/ctchrKAlSCcyOakvE6wBSmkJZC+R1KJVkF1FsRzdj7inOKdwkniaQOJ0onkY2beOx3vBme8bq/ZisjWzdy5kecKL8B7PUMkkeiGHinAYkbfHaLoloJ2Bbe5BiGOQuqUmKNpr31oAywvk9tF+PyouR9ZsXLiiAF6W7joQWXOM1KVILzqdnnRyZmKdlDQUcL+hnJJ5my1Vl9LEHlErn8ExSCVtc+Rs8udlxOGzoXeT1cc+73PPI7zt2erYx8Y/c+r/trPt49ZeMmnChfENhxhqRgypLAjR0o+MJikwhJIMqsMOu2lXwTC5t+voG3Bwq1NlapmEVBmtcy9+Ww9kSFGySz9ys5KqZMgM/XUxoo4AFyIsrCTHguBKBINemSdMawRGbQURVJUvdhjWRmYcQpqsIQPTdTR3CJ3k08DjvO3cATd2MWJf+ch/f5Fn2HXkxZHMp/S8IQz5DsksLOW/FxSsiU+5qa6y2q6NqUS0PmLqhree+25jApQX6JXQq3pZyrMxfdYi3t/8WS1d7pNDeUldfGcT6OYpXukdNQFhoofEXsqRwOR45b7Cd2JWZpaj9gliaBTo5hCFw7zcFsZAqeKTlG9cQGH9/KxGOZ8A6+ufsKl9szrh5teDb2/OboGfcbws6xv3FI9EjqCFPCFY6JarYaXSZ956fXuQVTbWEF2sxpfaMK/bMNctsg2jOfcOHS1F3JTHNY7c8q+GGF6uqDFAVORVmEpYle4RC1HuRnIK5gK3FDrSqDZaYSBR0ckcBOBRFlGyYmNUXZp45d6tjKSERwolw4YSOOt/0V39x/hQ/Ozvhg3HIzdLy38ww3HeFa8KPDjR5XxnZM3pRDJFfJV3XfSoh2Mx6jTTbU0BVa8vai+Wtd41E3j8wofVWQra6bv7dcyyYYLhxdaYPxtuJ9h5yGsrRyB1ZRGPkplCxoTpm1NJ0nMVB0AsUREwwhsNsEdrHjJvZcxi3nbmDrRmJ+Qh2wlcDrbs/b/inf1L/Le2fnXD/pudn37K89w5W5I4nemtWSEsY4z3iBuZ4VAouOBHfkSb9FblWU0hoLiNdlA14heBfdi3Vni2tY3VeMBtjlthWFjwjcr2TfujKfLXVQS83Ffpdk/FiJQMgZUHHpETQzO1SEafRcDx3v+zOCRDoX8WJZUicTF2nPpVwDA7scEG1l5GPdFZ86f5+nT7Z84aZjuHFI7ZkO1iY8RlzGXSQmw2Dai57yOaWGTN2ip8ekVZR8HazJPgN2bRExp8IHLSWtC2vYfWVY0lczePI0lAW1wtmCs8qilcMUpPyYQrhJcVHsIWrJTdFyExX7PY2O3a7jUiyVDi7hsCC2k8iF27OVicTAdeoYNdBJ5K3uEoCrxxsudxve33sg1CDWTR5/HXA3edTGFC1wLEFlS2EsYy9KDNPerIZBV29iS0sQQ2RzBWImWeUHaVFVljArSOtinKutvdoqakWXP2puqPBISytEZZ81NaKMr1RsRQAHyee0uRHJLklHx+QDN04J3pQlSCK4yLnfc+Eu2IoV48aM7G3dwOtc4Ul8cHbGO48v2A0du7RFNOCi4PeOcN3h9xGnOleK12QnoPb/lC7Cktvn86qtJdL0FrWfJeVsan6Y2nYPLTjOivpwQMauitn+zZw93SEnoiwZ1i+sNBG7KEGairPVg+LWMZ054tbqQXFjnJW0VaMilOusVAyEJOjkmCbPzdARfKRzZ5z5kQ/8OVuZ6HIO3Emkk4nHeV7GqIHX/A0fP7vk+knPb0ZhjIKMHrcX/OCRtKH3gs8UABlGdEzLOKXUbRanPafCBxyVdbtHKU665rXSQ1RaVmDObrxbwv/lM8Wdt6QqjijVETkNZRHsYqSpBovShRnU8g78zDmZtjBthbi1wmHsqVXmKpMgo2VGTIKKkJxnEOXK9XQu8UHYchH2bN1IJxMe5XV/xdaNbJkYxbOVkdfCNZ/YPiWpY0yOL0+eaXCMg7AfxObURUX2G6sfldilbUY7wF/crASr4qlZCn9gJQp53WiWNCM83GHZoMD/jRu3QQHTDM41gxxlARgel9NQFiR30zW+XJp4JYtm4M34JpYyp14ruUm6+eIqPluV+cMahThZ/PIB0PlI72IuAYw81h1RHT2JjURwN3iUiJDU4UlcTx3X+56ng2ccOtyYC45jwO/7Sr2UGOeOwWOyUoSDtHlxeRqFAhbAXpG2aa3GPWkG3FbX0iiZ2S22vOI75F5lEZHPAX8W+LKq/r782pvAPwO+BfhvwJ9X1ffye98DfBem+39dVX/83qMQ7GB75q7EPCUAyHiAWrqat1eXFafEKl5xQSk1gZhMOebBwoBCGj0aHTcqvOfshgUXeRJ2xCB4SXSSuHCJXgfOZWIrI71EnCSeTmdcPt4yjIHd4HCDx++t4Oj3uX4ESKE0DKNVm5mty4y35PghYjhMHne2qDavG9FKJlOsSU2zm4AYcquqgpuzp/q9pUe7TMG8LzvL8hDL8g+Bvw/84+a1zwI/qarfnxdx+Czw3SLybdgY098LfBL4lyLye1SPVcSWUnuDSpBanobsYyWl2sYBs7JoUDQo4hXnEuJsFKkmIU3OrrlgSpSsjKAIowaeyQYR5aI753rbkyiYi9IBWyc4lK3c4HJQ8Gy75em04Wbs+M0hMO0c414sftl7ZOosY9tPdVAOk1JposfQ2qTU4YTauI0cv9R+6jblLbSDGO2irKdMxAS5VqUlHmmtXKlQfwi5V1lU9adE5FtWL38G+OP5938E/Gvgu/PrP6yqe+DXROTz2Bz/f3v3l2D5f0t0aslPx0SsBrSA+QHnTGmiU5LLZkXJPqzZNhq1cjd0XI09749nvNs94tzt6SQysqMj0UtiyF9yLnveCFd8YvvUAL4x8O7gGMYemQQ/ilEbps7il5Rsxt20Ktite6rLa6WvKLUHyjxl4S7JVqSOCCvpc8ks14MOC4emHs+L47N8g6p+EUBVvygiH8+vfwr4d812X8iv3S2qsN+jIVj/0NrHVhNcvUwem0HNDjQJSQUPeJ8Ql2aQTvNYi7o/IAkpCtPkuR47PhjPeGd8hJPEqIFLf1WLjGAcGC+J1/0V39S/B8D11LMfO65GxzgF/CC4Ug7Yd8gYkWFcmvljN6Qdolj+bn8v6XK7n3W2BDP0QJNWJ+yiFdZdOySofKYliN8hzzvAPab+R9V1MbtfLvIYCL1Xu0uTWPm2CvPX/SpejMtihSIOcYsiSYjRsR8DT4ctX+kuAMNartKGC7fPgJ1lS51EnrgddO/jJHF1seFm6viNybGPZ7lmZArjdwG/79B9B7tl/Wd+6u/GNYwXY3HNPDGCW+OLZatt3ncZKJAr17MS6XwMLVnrDvlqleVLIvKJbFU+AXw5v37vzP4ii9n94S1t54pIjE35XezEo6G3rizgoDLjDiHhuoTLAeuUHKpi9ZMu2T5gDnYFS7OdZSAxOW4msy5gyrIPHa/5G0bved1f08lET8S7G/tdItdnPftkl/ALKuzZggZkcoSdx+87un2P2/VzC+r6phztJPTUMWT5Rqpzufhnb8/uRms5YFHCbFlwdT4LQK6IxzgH00lfKCj3o8D/DXx//v//bV7/pyLyA1iA+63A/3fv3poyvo2JaBZ7kDwMORmHRKJB/XXGrFckKKGLhBBRNWuhCs4rqTdlacHSYnFcxmVS5r08HbYATMmTmrm5Wxk5lz2di2xRLmTkQgZi7yrzzonyBfc6+3iO33m6q6wwux656ZEbNexlHb/AYTxSOD3FFcVcbxKxuCRiAfGqjrboG6qBs8z7a7IwVbWOy7b/6WsF5UTkh7Bg9i0R+QLwdzAl+RER+S7gfwB/zo5Bf15EfgT4BWAC/upDMqF85+Y/k1bXIV4r70KS5noQs9JEObCeqpLj5HyBVZYWv72mQIyOm6GrVmboPGXuQCeRq7Thsb8hImwl0kninAl4Dy8Jb+qNQ/nV0TNcndE9M7KUv+lw170df4xoWnYWLuOZmTJZ59MlI0/NkzEbGkdrlA5S4DTzbJv2jzosqLnWD82KHpINfectb/2JW7b/PuD7HvTtRYRDfMF2BtHZ5ISYarxiFEdwe8HdOGLwTBmQ8z5ZgCtCxJkyiRpZmsXDlWMfYRwCqsKUHEM09n9SR5DEmR85dwOX8YytjHh3w1YiG4HRDQw8Y+g819segGdjz/+87BguO/xeCHuP328t1EhqShCjIagwdzGoMfsOgDXnoWuCwQInlGB1bVEOb8gc1KrOawJ8FXIiCG6BuKGu1VP6ZWKeaau25IskA+fcCH4v1ifUOVLniZmk7ZxN2VYVVJNlQ/M3ZabhnFLHpKRkVIa4ccQc42zDyJPQce17nqYztqlkRwNbEZKLwE29iudu4P3hjK+8ccFw6fE7R3ft8DcB0iaj/4KME1KoDG5WFknJeLw0waoTxHfL12IEjTkpgMW8l3XQnJRK/C7kp7B6MB9IVzgNZVFWZjbNqR7MNZNiVSK4qfxIJiIJ2sQmpihUkEskP7n5+4qiaJL62CZgmjyDU27GxNNhy5m/wOV83ZP5LzLSyciYv6vHsqQxPOPj20sen+9559EZ04VjPBf6rcPvPW6wWTAlzl6Pkq8BcJHVyNZFn09bgT6GwDbEqWrFxGWWRDs18+FW5jSUBbXArzLWA6WdorLic3YkU8KNLrdo2Ec1B632gAopOVIqlsVolbBWGKmuyEa0GckbICVhPwae+Q3BJaJKDXi9JDoiA9e2rTqG/GR7Eudu4NFmz1fOJ+KZt8p4b+y+akWcs5GqDVlKp8muwWJIs51cu16BtJ0Cx0TKiikcUiYznVNVZhDuQ7ik01CWwgXp51lrZpI5IHLX1DkXdVugruyqZEOaCstuVhjbZml1AIuxRevnB+Bq6EhZUbyokaYk4UiMeDqMcQdkLm9i4yYuuoF+OzJu+9xWWxrubYqCBCA3wklKMKXaFKbjlIneCnjqLP48j04dy3GubUxS2k5U0YaGWd1dy7NpPwfPrTb0dZA5bas4S5HSNzRFQ0RHjxu1suQkgkwCkyMVv9D05AjZ46SsQFHQ6BbQv2beS+HjRmDymi2Uq62uvTNgbivW9rqVkQtnXY671HOdNpU85ZwaYChUgpaW2btDgQLyHJcVO24xsrWVdnbNMVZ+Wsc7GTspgw5r+ryKa45NKD8ip6EsItB3FSBSjbOmi+ETAtAF3BRMUSZyD4/Y73lMaaq7tIKiuIRGT4q5Cp3yTwXo1BRmypMqFXvfKbqx8RwiyiZs2fqJjZvYuhEnicfuxri8JK5Sz1XacBM7ptUSLjbwUOr0B9lnjCMvNXOwQKaTuhjXgrydsyiRydLfBkM5toLrYn3Fdda04PfekU01cjLKUofc1NW17MnRnB1JSrZU7RhNSWK2LMksixuFmNtakxOcp7oezcpg/cmt68kYQwl4sxJJsnQ7YWDRPgSux47r0HPlN1z6kU4inlSpC6MG9qljn7pFnLQ8TyrwyLrAWFxt7vUuHJOaIWVidp00XkenKoQwT1xo21vbpXaO9RG1M3A+Om7oiFTQKWVaAc1Jsuh1RnTWgVxdtgcsA3JZUWRys2II1kivVEQXl7Oq7D7AXFSMjmEK7GLgJnbcxI6d6+hkAxhwF3Fs3MhF2NP5mGtTVFxIkiJjQoZphtth5prkwxevM0DZNt0XvKV1F8UqlDJAK6WPqfzeXr9jQNxHZ72hpc+uUpjxGbCzhZ6kVpy1ELWr0ix3WbAUnUxRZJQ5Vsk3Un3evxRrRuOiABVSdEzRsY+BIXn2KXCdDIQb1bNxY41fHvsd29xg35KuJCpuiMiQMZYi2VVUPs8B67+4k7yK7Hohh4MFrBorISyxlAXo6edtH6AocCrKolBX44Dlwa/NYyytIHPv0KISDYu0uLqXzAVq5+Aa/A2a45aFi2okNeju9dRzHgbO/IjPEZL1H8UaILciahmcHxJuPyH70WKVI0phHyhorjYvSYXqLTu6Y42AEsBWhWmUwcmM7axdT/woWZY8F+1AUfLTJHmVVDdMhOuR7sxY/tMWmwB1ZspjaxRTG+YrSOezQpQMqihNiVHyYSwsj8x5+TR5rvdG9L4OPY/CwCZnRx61eIWO98ZzdtFSboulwO8Vt4/I9R5udgvl0CmCjvP5lmvQup+71moWI7O3QTCwaFW1fWflKGPtW8LTiul/m5yGspRhPlnqzPmiKC2heBhx10I4C4RzR9g64pndGLtgOU4paF1xKY46XEkyE7GKYl0A+Xc0b1tmywExCruhowuR665nHwMXXmpn4z51XMYt74/n3EwdMbqc1mOp/n5Cbvbozc1y1bNxmLGVwrAvfJfS463LpvpFE5swD2seYh2pIXYhl9fZyTygsNA94/yZ++Q0lEVmKHvuRvRLRWn5uFPCDYnuOhlCujWUFHHEs5RZ67JE60RnCmyNe9QsDhg/prUqUF2YJiHhEEnEzMgbc5P9qB4SXKe+Br9j9MSM5VSqRWKOG2I6jBVSWZr4lif8GCzfVpSb/dwpdTzHMsXWj8pCD4jUea/1aVjPmG8JKSLIGPE3nq53pGAdfpIEUccE1h7iWRKhSxaUg+My+KfyndtMqATAY+bGdGlRr0vqGFLgg+nMOLtZcRJCLDUnLGtLIYNxwc8WsullbikIi9U8Vj1Hizil7Uysy9cU19USo2S+doXxbx+aYxiPEavukdNQFmTRPbcgbhdpgSu1NNTvJjpHnjBZmtatlhJJdVxYsRi1qb287hUKk26iMBbmVDp3A4CzviTRDIKaddlHO+YgkaSOhAXCMTP10NxqGyAFhy+obNvgJfPNqu6gtu66qjCLWS1Ne8hiorcm6kyYY5zfwviHOXOq8dBHxbKgMzcD6tKy0jxtUDAtC0ZdLgtI9e2geZk7zeBWbIPdCKjMnsnZj3hTjEpZKIpSlEbLU22xQ0zCkCwrKhKdNaFN6tjFjjFaIXNFD54VvgXMiiRndIM16srKoix2VxRotjYSmpu+xlMaElQdTNhc3/vkNJRFWa4VWG/Q8iQ05f6hKULwyN7ZuK4ag+QnWfIcW4z5tghgXdM+kivNmn8/iFmKNEjwlBy7KRCcKUuhL4zZLe1jYIo+dyMuw6Yas7QBe7EWvkTh5TvbtLYtf7jl51qL0w4RqFnOisZQWHgNriOl2PiCCNvPV1Tn1VXJsGrb3tC0Lajmsv5oZlhixOcimVkVXzEUbZ+qBAiZKqCGrbSS0dv5mPL/q0WhSzeAF8VLoveWxQ3JM8TAfgrmhvJQwtkF6uJmlMaxecolS9dSpC7wGVlwc8vkJphdk1i2o6pN2STRwv61/3m9+NUD5DSURZiXl1sHWm2LZRGdSwAiggwjbucJZQZ/cHkQoX0uFVfjsazIzzGJTs0T7HXuj26zI0AnR3Q2Fsz7lC1Mh8vUhSF6xuTZTSFnQjKn4cfuxbGnOM1Wb6E09WZmqsI6fqnFwKULsqnckJupLJYpdIUWhPtIuaFjpGWoQV6dSV8xgTmTULFGLnft8N7R5ZilLHWXPEgQUqdoyPGQJyuLZTs4nV1SwjbK2ZGVBGz7JKYwViuypzWpzaSbkiOqkaZinBVFyg3LpYqawcQ0W4kipcZTgDbIViVnO4Xf0jaLeeZAuG0cK6uyZkuyyCzXLqfCFncHuaehLCUbSnEuiDUDgUu6fAAyaVac/YCI4LyvCpNyBqJOiGfkoNdm6c6lgMZ61KY0+w5poH/R/JTnlDjljGeMWpUlJuPuxsyBWZOyjtIA7jD/R4POtmB40D7SxHltR2M9idYyN+sitY33cV2NXMppKEt2Q5a6yowPlGxgPQet1EiSsb80JWQYEe/wdf3mHLOIfYE6SAFcwSy8WK90cUmagZWcVtdLrVAR4ErdNIuS8kTtSMmUnLH0CmeGjLN0UleFrRD8Ip5a3dh1FbnMlFv8vVK+lsFf4rwyf/cY/aBkUZmFp2W/d8hpKEtmygmWotZpRuXGxkRtPyqjw2KD2U82BKhcOq/ajBPzNpqjF1zIFiLl+S4dFaSb00+d3VSxQA7Ig5jJPF5VA99KcFrolykjvlVZ6jwZmdlqaynu6LYlXUqqXdxLIUetcZby2XJVM1dXCoe5RXfXLbTt8ji3yIkoC0sYWnNU1qKPRUrGsJ6TpnnV+b1dcO9tBREbherrUr3JCy5A7M2tJMFQ28ZorcG8irlAk9abYpfLWxWlcWtl4rfRKJp4Yi3rXqljTfGtlBjEe8t6tLnpFXeaQb0FVbVYo6IcDyA9FTkNZVHN1dcmHRRXb5DRC5cXTESg65b+N1nKKIDrBnxnytJ3klcUw9Z67slPvrk79TM8v/wSZswldzWWqvZtIgLOKzEoqShKqyx3seoXtSI9boXWizespe1tLsXKwvIvtbYSaN/WIXCLnIyy2BPSBGY55ih4RFmZVPNPZbhDbgu1p0snrOq684uAF8BFYdoYQCeWd1uv9CRoZyZFV9akYlzZtRxVKqiuyY5VkZByTJQnbArzU912JJYblw5n00qCxcwV11qFRtp6EVRFsflzcS4vdNS2GlFdWudyLHfIaSjLbVKa5JvYpTyZGvIazAfsshwcjhMyjPibpgSAqwGrOnNHfsiz/0OuvOaAV5wiPllxWLHWjbi8SVYrMmJ4ynUpEcV5qyAvVnx1Yqhz1+UWkHF+3XtTzLL45roCv/zS5lzzeXvHAXErrdOxfD2PdBPU+PAjoSzCXAF1je8Ve6pqbpKHzhjYlBXG65wuwnwhos10Y+/wwXALlTDXj7xmBbEERb2QnDOEuDNFcV7xIZKSEDMZRnK9qFgR5xJOsBZY0dy6Y2j0kB9cW1hTSH3Ab/ocyOsS+o/mhs2ayByrLMA3WbTMVCkWoqK9CiR0OkLeLryhtk5U4sN75DSUBeZ0uE0ZC0WwXJgxjz7NDHdxheG2UpSSTo+T4S8t2ORCXTTT5SKwcVupdSWyRXA+1RVbNRkgtxiBQslgi4UBsH5rkcTeaR7snDOzzqF9N3ciSprdBeR4bd3svnJNa2S3cdVa+ppb7nK5tl7mynTS2RI9YEplkRNRloKppDkoqwHhKoWMzEjjGkRqs4iyjEqOCYq6lFHqor6yJgtZyrIXZzwXJQNwmanmEs6bmyltrpq/0q0qj5ozI6DiO+rlsAIc6wfmc9b2iW93mgC3tCgtn6Vuc0QKbtXwVw6uWwNI3ianoSySM54ymeiuIL3FC4755Tq3PqFMc0KTFJcSARDtMoZiPIUym87SbCVuLD5JDmLMzWoCzsdMs1HmniQhN9rm10xRYo5vDOPJBUz7wJz+5zly2hZOj6XKVZGUo1nSugJ9TIrClHUn24Uk2m3ukNNQFrIPruzzW2oXzcLa8wdvwQo0WWyRU2lUs9soFx3QkInbjorydkLaWHaUnOCcjRvzPlUlKXPrgk8EH3GijHlZGRtR5mamnKdZQOtIEHlkivYi7XWNyyjbp0ahClF7feMX/UVzFjVzWBLVxWmyB/VYQN3IaShLwUnK70VagMlJnYNW59SvK60lkGuf4pJqF87GMGbilHF5/T7gJo+oBXnJiy2A1SllvLv3iT5EuhDzAp1K5xLbMPKos17nZ+OG67HnZuxsGZoy/NBrjZFaZVkE5fX4m1ZUsX3Uq1FdbI7j2hGma1mRnMrfy/WycwF11cx3l5yIsnB334qTOQhUhXRkuRmY8YUSBNZ1AUuGlGfVTZZVhf1oY7yidRaqeFshbUu1LmDKsukmLvqBzsW63uJr/Y43uyucKF/aP+YdeURUYYqOIeM0lpabhaGNW1qGG8yKUldqlWoNVVeuuShMvR5NxtReizXZqbyumvEb5mM5yqNYymkoS0kr4O5qaZFWUY6VBNpiZE27k/0d7SbVjGSKeC90wZGC2OSnjS0qMfU2SaH0LhfuSu8mtn7izI88CmZZHoWBXbdnTN7m0k2eIQ91ts5JIXlb3UTaVd3h+E3NlvCA01JLAKtrc2w+bpFiVQ+ue86SHshnuRfvFZFvFpF/JSK/KCI/LyJ/I7/+poj8hIj8cv7/jeYz3yMinxeRXxKRP3X/YUjlXdSTLXC0uIrePkhy2twCTsCMP5QLV8Z4DCPuZsRfT3SXkf6Z0l9CuLJ5dWmSyl8Zoy3IWcjZUYVd6ojqCBLZ+pGLMHDWjWz7EenTcu3pzPLX4CtHR0JAgl8qSl69Q8cxD/hJNcOrwCTM2Et7zaR5gMrkhWlCp2g/42ilFcjfHfJo9lVT2hF5iGWZgL+lqj8jIo+B/ygiPwH8ZZ7X/H5hdjPTxCLIa2UNa6+lmOf2mwoVEZZPVyEARZsG6YM93WWZmrgR4pkwDp5p4xiDZ0yOLisK2AjUfQqk/ISf+dFWeFXjuzwNieTnCnjygna+rlO4cDlFSh2nnQKVz70O5xGZ019xhq20PNvFxO3ykK0omH1XOypshbP7l8J7yLTKLwJl9PqliPwiNmL9MzzH+f3VGrRWJAe4i14amyY8b9NyXFppzPIBlK0l7bUUW8YJ2U+4biRce7ptDnI3Quo9oxg5u/ORziU2mXebEJvHIvMoVJfdlR1CQju1RT+bFWLLMWjpHGzWYF5YjvKwSAHUtCqT1MCUZXwiGV1uKRy3ufhGOR7C8P9QMUte8OH3A/+e5zm/v82G7Ivm3xtcoU4TKBejXNh6UZunCpoK62GldtGHM02Vx+tvJrpnFr+k3khLewmMXhk2E2NniuJQi09UuAG6nKKVJvpYqAxBiRsldg2BvBT2ChJdlKQUU9tuzErWSrm3KE+5zPPhWg5usVSSkqG5bvUg+SZOSnMcs3Dbd8iDlUVEHgH/HPibqvr0jgrlsTcOjmQxu989sgu4NqXHMJTM5dBcS6nAXGgUBuZ93cYrbYAsLe0l+wF/7elyp0DqCpfXMWwC+23HLkzsu8DglxzcEvgWFxSTs3vk1BDcnBHNc2R0thTiIJVVxiLSY+6zoZTKNC0Xwiz7yMnBwqU5t8x2ynXRtHgQ9YBDdLc8SFlEpMMU5Z+o6r/IL39N8/sXs/u7t3XBrTgyy7Uu3NS6n8JgL9utMgEtgSzMlWttTHMBAKHyYHCOkOOC1Amxc6SNZUlj1/OBCmP0PNv0bHyk8zaE0Bbp7Gx+S7R2kJRZ/gVkNmvl8V2YiUu1YLg8X1JEh+acYlxZ31WsUwL34tIWRPAm1S5/q82nqw/nc1rJTIB/APyiqv5A89aP8tzm90tTeW0g7RKdFyi7RTKPrHR+MGmgWVRBbYOl5WnoiprMvEsuQAZVuo1j2nbErdD1ggZPjMKzwbPbdmy3I+f9yDZMFf6PeY7LFHPKnUd8qGRl2XirPnfhYI5/XXq3UkTn92eCVzLSV6E2NO8LzIH7mrS9NrAtNpOnaN4nD7EsfwT4S8B/EZGfza/9bZ7n/P5yj9uByfeVzOu2d7kZXXbzFaSyMefzZO8Ik40ElRhxInTnPWlj8Uvhw0yTMI3CODpS9KTkGDu3RN+TY8xDD62lxN6zoqLcDVisLSAss7jSy3xEisIsraceWNyFlIdmnZUdkYdkQ/+G43EIPK/5/W0BbL0IAdD20bQleiEveO3ksL6xYt0dS8UXC1QW1l3JnFLCPRvopdxwj6gNayY5JhWiwk2Csfd1BbVK5o7WwGbDDMt5YjN8p2TTn4plachOgj+wGouYIukcy9xZDypV/CPAHdSSSOXyfmT4LIoBSAUkEjHGfjNzvgWtBEAElSbNBBYtFG3tRZq2TV3hCY3CALOlmiLu6saypPEMUVuLyOIcAYGonpSEYfRISHVJGvtqseWClfqo2Th5UxYdRjS3ryyyuU7m462nohwNQm8Zp1at5QEXoXkYar/zEaW7RU5DWcoUBdfgCrfJAyYUVblnuF7182tFKTJFJCXctSc8C2xK+usEENwgxJ0BeRqMpN0OAnKDw+1sOzeR5+CtakIwu4oS5IurHN2FtHWvFqA7OG9ZuqO1FEWpSUWDat8hp6EsyvzkHDvgnB7XUWItfwPmC7Zu4UzLAHHh60v7q8a5D/iYZGvkhki4jvTBYhc32kLkaSOGoQRmzm05nDpTLs+Vm6gZiARvKGr7PeX4C3rbPvE+b19m4xZW3LHjXV+7tjywcEuSyx7T3F1xh5yGssBMI1gX0Mr7DVn70FdDqZHMc2XTnB436fYycGxuSDhyKUqsk0xZ/M1kXiJ5/OgIN9hq9R21kSyVBjWhBrd+sB83ZQX12c2UEafrAT1JzU01s3Kl74Futn4FR2msi7S4UptmV75vuVgsHipNCcbhXnd0MsoCzLjJMYLQASDVPC1fixSSVIvXHN1uuUajm0pFWSizNRJifHODV+axq4nsfpgVt7iz5gYtLV+qmVwdbjRv+OHO7yFypBVlLfLQqT8vUkTkt4Ar4J2XfSwfQt7if83j/Z2q+vaxN05CWQBE5D+o6h942cfxUPnteLxfow1/Jb+d5JWyvJIHyykpyw++7AP4kPLb7nhPJmZ5Jacvp2RZXsmJyytleSUPlpeuLCLyp3MXwOcz8fuli4h8TkS+LCI/17z2HLsZnvvxfh06MDCY/WX9YAD0rwC/C+iB/wR828s8pnxcfwz4duDnmtf+HvDZ/Ptngb+bf/+2fNwb4NP5fPzX+Xg/AXx7/v0x8F/zcT3XY37ZluUPAp9X1V9V1QH4Yaw74KWKqv4U8O7q5c9gXQzk///P5vUfVtW9qv4aULoZvm6iql9U1Z/Jv18CbQfGczvml60snwJ+vfn7/k6AlyeLbgag7WY4mXO4qwODr/GYX7ayHKuIfdRy+ZM5h3UHxl2bHnnt3mN+2cryoE6AE5Ev5S4Gvppuhhctd3Vg5Pe/5mN+2cry08C3isinRaTH2l5/9CUf021SuhngsJvhL4jIRkQ+zYO6GZ6vPKADA57HMZ9A5vEdWPT+K8D3vuzjycf0Q1jL7og9hd8FfAz4SeCX8/9vNtt/bz7+XwL+zEs43j+KuZH/DPxs/vmO533Mr+D+V/JgeWFu6BTBtlfytckLsSwi4jHX8icxM/7TwHeq6i889y97JV83eVGW5STBtlfytcmLImwfA33+ULtBO0XBE/6PC/968+b6l9b6ySFK0BKhFy/eJ/e1yEpDjs6Ea3IHQm0eO9a3c+x1Xfx35+HU72k/vmqME7n18tz92m3Hbh94Gr/yjt7CwX1RynIv6KPtFIXwtv7hJ5/hYFzGev2dZsDgYl/tKC14cIfdwbTLusN5OlLtDGyX3CuTvl0eFdo2s63WY17sU4+0sRw7ptpDlTOX9TJ1vowZa9pc1j3Nt606smjpdQev//gHn/vvtx3ai1KWDwf6CMveXrntJmodgHPwenPCqqt2knY1jVtaPpfH07R15v6b9dqMpT+nrhp7zwzZel7lK4/1KaeVMpUxrs2K8kBebKpcozIlSpaKU+Rgzclbju0BbawvSlkq2Ab8Bga2/cXbN5d7h98ByxlpR2a4APVJ1PW0gaoAKwVZj9Baz3gpbaRFURZrAuXJSWAN7WWl2HJ8i1GjVUsOXeYxRSvbl47e9fbt2LR6k1f7XlvadvHNdm2nu46jkReiLKo6ichfA348H+LnVPXnv6adtqM9W8WAA7e0kPUY1GPGpH2iWnOelgMAjy5UqY01e8h5lIGBrduq82ge2DxWLO+xYzlQ6NU27UPjy2cad3SHvLCORFX9MeDHHv6BlU9vpwtElieSTeYxV1PMdn1tvfhk+f+u+KG0fq6tTbOwpSRsLHzbJhoj4M0leL9U8Na1FUlKHfugcjj6Ym2BFu+trMh6LEc57sWxiz0Amq9LOcY60PFrH236dRCdb165+KVvmdmCLJ769sK7Q6U5CJbb12xn+XtXbbCacnzSjvJwc5yQbpnMUFxfXr2jjr04GpuwDF7Jbiw0SnbbEnitC1lZukVcU+a3HJv1UqaGOGettknuXaYXTkZZsujcEC9Jl2Mn1svVtnLfdKOSSd3X9+vk6Px6KWa/uLHkqBsuRrGm+vfRkRnlWPN3Vet0bHLE2vKVm6xz7HTsO+Y5M6sHpI5HzTNsRKjL0yc9/L4jclrKIg7xzE9WGaAM89Jut8UOB09wvpBlBfb1TNjye7vCxvpwSsCaR42aMkXQOD/Fq5utemSl0/UxitjUBi3zWrKbytle/X8tcd5HtUrtGpJrKKBd8KEcX7aaMl+ROmr1PjT/dJSlfWpdUYbY+PKSJh6e0PokqwmuMru2g8UpWze1FpdvaoPt1PGiRUGcQ0pqmwfiKGYZbRHuJvuoCtZkTTGfYz3U2b1UCCC73NZtLaxZXcShHdDTXpDy/aliNhoTkgccapnLe4+cjrK0kfxd5rBag/lptvjsAZnROqBcDCa847jKrFo4wFrK+j3VpdSPqcUvKlQspFqWtEzhW1d2m6zd1q3Dh+44GXFLS5jma7ZeifWYnI6yFFk/US3KWKTgC0Uk35h79mm/lyfT1XGkC8VptykDmVFsDeBmXwXHqdOm4kJpF58vgFk5xsg94JjFRAfXwKWjIN3RfbTH0Eg7tbtuKw55QNp+WsqihxOf5vea19dPVXFft61q5pItHbNa6EHBbnrK26ywizb+WLiA/LmamtZJmI3CpHT3IOIyUbycz+Kc9Diiuk7dH8oYKC6wrn2wsuAtTHGHnJaySPO0txjJMWnT6duWjivbpRw4V9ST+tnKAisKA8ug8ZgU91MCX+BgvttCmXK8lYPaepPWKXER07tl7FbOqznuRbzSvr+WY4jxsYU175HTUhY4BJHukmZIn6rOWEhbOhCAOF+cdh3laaoWR4uF4VBJFk9wWzxsla8JUuvnF9usVlirAaU/PNfkwKnF9yUzkmZGLlDHiNFkbUBdQ7Ee/CoGrJZQZkjhgaPWTk9ZjoFp6yfjCCZwZ2BbMq18YWpAFxPkKd0LfKIcwzqdXG937NiPYUFJa6wgYEsDt/Wa1TnoMR5DsYBtUcG118rV7VjP6YfZRS/qZY2C3Z8MnaCyFKlIo8za35YEiv89ZkrbBa6KrLEGsAyjnVJ5LDBsU+TWkpTjW4wfXWFB5SY4Rdt4qgXTCtyOX1bLq2JrXYt6cVzFVRYrt4hxGld+rH60frBaescdchrKoizrKLA0jW3xqxbhZqCsPpFlzZ6CXazrQmX/urrB6+8DIB4+bW61n+TsRpc1f4AFFlTijTImvi0prGkU+QFYWjIBNfdTXi+fWyhJm3q3N709/rtWf8tL490np6EssFQSt3q6ylrEBZd7iI+tc19ZKsfaf6+/vxxDsSK6CnZbN1n3nZYKneONo64qf8/CSpV9rOkT9fviUrFasG9t6e6yEO11a4qOB7DCLXIaylIQRe8OLkKdNY/FqupyqlBqPclRZ/xrY37XT916rR6g8mNuIwh5f5x2sGbHrW92TaWZ/y7uc8VTqcfhYBFr1GvTvFYC2bLKWfmOlOzzt63s0SpD6RuKVOxHRMzV3ZM+n4ayQMYl3GxJFsBRA7Fn0zx/7ggpqVUUcRCnuSbimzhnTV0EpAvZhTX4Sbv/AmJ5W09osWbQHWsr29oEHC96FobcXQazpOEhWLW4WBeY61vOoeXaNQVZO7Z8jjHNq6LF5tif03pDX18pS8EUcWJj2lu/vK61tLJOQ4+Z5dvAu/V+Wmh/ZcI1QlkKF5i5KAteTnaFZSR8LWes3Ec5nqJIt5U71lyXpBA82nv73/t5Nr8qpBo92TUcJ2TMCqIjt1XZb5PTURaXL1ZZCq6+LuZ61hf3mBwoynyDjF/SuB83W4p1XWd2cbpUhlr8Swb/J52J2zDP/y/kbkC0CX6r22pcVvtaiyDfxspLaoPf8wOk3qGbHt0GNGTLks9RmqyPpLh9Tt8zMq0VYylkr49CzCJNAJkziFoYrFXUO/zpmmrQxgjt1xzJPsrnCyC3tjj1ONr9tgSpCu03OEdRMrFl7I4Suu8KRI8CaY1yJZvnL6pWJ9oG4jagnSNly+KiIlkhVcSWr1HywljZjU2Ro3HSLXIayoJU7T9aPa5YBI0rkhVS27zfBrolqGzjlPvkWBpaK8hQeTdrQniRzIE5LATK0nKUcyvv1f9z/WYtxbWVNQ27YO4nODSYoqTOIQqJZCGQMq+2cvCztK4fnQC3oJ+xxUAaLu0t1EpxLkf45UakGfRycoijHAuIy7b3SaUpOmo02sZNTUwl4QgSDTPMfhs9s2xblh5uM67s3iwT6tEuoJ3PK9JLdkPF+onhb9FiF4l5ub3YWMP6nQ+rE52GsiyA1vYJy9Ka5OLv/QzGVRjdAVMTMB79LmFRpT7G422+b+Ee3fzdSxCOo25vkZG1x3+XtAhrBSJXnxFnytTNcYpm8BgBKwoYC68oiRsjTLmeNEVbY0iPnPsdchrKUkP2rOFt0Hd0+0S7RtCC0N1uk1yz83S4jxaQiiVG4tClLKrEM4RfZe1a6uuuyU6aLOk2WmjZd7kO7XqP+fpI75G+m61KtibqhcKVlKi4UXFDQsaIjPn/YbRsqKDctRDrH6Qwp6EsReoB++MXdX1CLYMNDre/rX8m7+eAplgC0qIwDeloDmgzjUJXx9JyY7OIJ2M2zMBgq6A0SG5DUF/QNMq+i/vzlibXWMVn11OAy8zrdWPC7aesLGZNZBhhGA3an6YmY7sFPV7JaSkLLPGU1kWIO7z5DtA4m+2Hssfyvmo6vea1HhxTLuiVltXCf1nJARmpMu2WVIoFwfsYNcJJYxlXx5EBOfXeFCWILbnnoaya5kY1RdllXCXGbFGirYUYVw9VWxq5Q05EWXQu/vk5sKvFtkJEPlbRhUN8ZR2vtLFDdXFiSUdOcxc3sbiExX59fgIzYargK02NZ07IZsWQFSG7KmicsQ1tyVRt2t/GXuV7nLOl9rzkDGhWGDfmZfmGiBsisttnZbFYxeKUJvhvk4mPUtW5cjWOweGugd3TdCSQTNz6aLS1lNwrozHNQbWzbWScanRTLUebHpegmDjjK2X/7Xetjm1m4RXoPy0pnsWyeVeP01zetNyvFM5vVpjsglInpLKE8KjIZMoi+xHZDbO7SR8Cqr1FTkNZpOGVlIvvPdJaiGmqYNetWMxtoraS++KJrd9dCnu5aLimOa4pA3gjNx/jx4JZq/UxtlhLbWlpzt03TfX5OJQwW5bCuwneAtuznrTtiBuzKipigW0CF1O2lmmmHqwLqfddr1vkZJTF+KmrmxiklgB0mjhKmF67nGOKUC7WsViINKfTLszHA/WCV3fk/JIGsL4JZV6KNoiwX2UaB+QlS4Fx3uIRaQA9/FxJDh7tO3Tbkc46pjNPPPMWqwjz4uJRZ6posYAr+uny0svx63hETkRZmKu4+eZUHx4jOuabtlYWOERFj+EmcDjspyCl9Rgai1L2WXgp2ri59nNtA30ns9Uo9ZZSKV5U0KmUgKIo0nVZqXJs1tAytHy+C6SzjrQJTOeB6cITN1nhkrm6kkTJkUD9KGflIWTvRk5DWVQPmFrm63NriLgld6NI5ZQeBqmyplwWgAvm4LWVgvr6/M9amUppoVXMOAe80lqYo8hsA+J1wagQzjATA9fMFWkXIDjDUHx2MU5IvWM680xnjthbnJJ8cT3gBwtwfQ58pQvIdgP7rJhkN35fz9EdciLKwlxtLq4hOVv1PSl4hxDmADTGOb7IT7OuXEKdl9Km1RW3cPMTX6gIiz6a7Aa8YRsL0K+ZYLCYtiDCXL09Qq4uUvadXa9usrJ0Hu0DqfOkjSdufU2J1QlxI4xnwnRmLrMkWG5S3CiW3A1C8g4XHNoHiL1RMPeGH5Xj/lA9R42ciLLonK4yp84LroV3iDo0E5haBl2VFqMp+1iDZ3DoGoBCN6jBa0FeSymhFN7aqvPiFJqsh1XwXYLUQqYK5nK079CNKUzceFLviVvHdOaYzoTYF2WBuBHiFuIm43wKEsFNgtuDOsWPDrdXXLTMsdgyaWGBaP8cdEMcy0JXchrK0h5jyxhrT6AGfm6JP2iyG9F3MzWglWODeYpLW8P6dQRFM7euKE9LdMJiLNIM1C04teuMCix4bTOavqvUgri1YHU6c2Y9zmG8EFKPKYuHFJTUQep0hvUT+L3gdwb3SwQ3OkQDPh+PU0WmiDiHlkJre4yt3JMhnYayrGURF5CfTGpWUN+rWEdLTHIHkwgqLiK5UUsay1Fomm03ZMkeynttxbfNHtzKpB8DA4tki6Lb3tzNJhDPO6Zzz3jhGM8d0zlM58J4AdOFEs8SGvLD4xR8VhRRRCAlId140pVDRXCTMA5GUShFRYmK7idk8ByQm9oH7r6BBJyistRZJenQsrSIannIM9dFJMzBsFPKpIXF5IG2pbR8V1pZF1je9OIib7mQ85yWw8xH8rgOiiXpO/SsI54HxvPAdOEYLhzjhTBdwHQB44USLyJyMdFtJ0KIeJ8OJtaqCjE6dqFnkgDJ40bBDSAqqJjSuCFZsBu84T+FstEqcjP35S65V1lE5HPAnwW+rKq/L7/2JvDPgG8B/hvw51X1vfze92CraETgr6vqj997FNr4fOCgq+82ukLze40htMFNcsvqwuAeY6y1lmEdz5T3vF9Oajj2JDYMeXEONn2mPHZmTXrPdOYZnwSGR47hkTA+hvGRMl0o6VHEPxp5dDbw2tmOR/2erZ/YhhGAKTmGFBiiZx8DN2OHqrCLQpyEaXS4UergBzcJfu9wnUeDR2JAVFFpYIQmxnsew3z+IfD3gX/cvPZZ4CdV9fvzIg6fBb5bRL4NG2P6e4FPAv9SRH6P6rpr/LjMI8LyCwfIJ0dPsrZitFKymfr3EU7JmoNSFMU3OMoCt5mr4fPI02yxcnpvc9rsRzc96dGGeN6TekfsHdOFY/eaY3hNGJ/A+DgRH0e6xwOvP7rhrfNr3the81Z/xevdNeduYOtMWa5Tz3XseX865+m45d39OUmFGIVxcsRRmCZBksUvfrTv9F2pVtuDIUwzWexhtwZ4gLKo6k/ldfda+Qzwx/Pv/wj418B30yzUCPyaiJSFGv/tnV+SQbnlaytz+RA6pB3w7L7WxKOyn9uISkVKILuYiLREYms2UaD6vjNwLdhTTBdIFxumxz3jRWA6E6atY7yA4XVheE2ZHid4NHH2aM8bj675hvNLPnn2lI/1z3grPOM1f8XWjfQSieq4Sj2X6YxHfs97/pyNm0gqTNHxNDliFGSyHzcKfgA/OOLQITHhbub6kkyTQRO1eBuPp/qNfLUxy2KhRhFpF2r8d812ty7U2M7u37pHx/mfLUZSGGTH2kRbKRlRHS/WvNdgJPN3rJVJl9ajTjCAmtaXgmBxWyGjsH2HbroawE6PesYn3uKSR8J4IYyPYHwtEZ9EwsXI+fmeN85v+MaLp3zz2Xv8js27vBme8TH/jHO3x5PwKBHhKm14ojsu3J5zv2fjJkZ1RHVMyfEsWxc3OabJlMVNDpmClQOcK4MqWVArS7nlHnneAe4x1TzqCBez+7u39Wgn3ZrDUtzNXX27mjJot0JwG4ykgmYih2huKVbmWpLGWGkFiyPMdAIr8AV028OmJ20D8awjbpvY5LEwPIHxsTI9SfBk5NHjHa+d7Xh9e8Pb22d8YvMB37J9h0917/K6u+Z1t2crs4sY1LGViQvd08lEl9+7iR272HEzdgxDYL/3xL3gNgbguTErTAz2fE0JN0UkNc1mRV4QzvIlEflEtirPb3HJ1j0UJPfYNhoPA13HnLW0/UGyik9Krw9kBZBlVpR5NCUmkkwJWI7kyKl2MPejfYdebIkXPdNFYLzwjOdi1uRRtiZPzJp0j/e89mjH2xfPeGv7jLf7Z3y8v+Qbug/4xvABb/tLzmViK4lO5isQUXw+Jo/SycRWRjZuoneRzkdCSOxDMhS4U1InxB5iB7435r8PzuCHKZpFlPxQPADR/WqV5UexBRq/n8OFGv+piPwAFuA+fHHJtt8GsBiBZWbSUi3XvTTjfLILdLKM4yquZV0prkpgt6W6nhK4lgJfK6XQWRTlfMP0eMP4JLB/4mcluYDpkTJdJHht5MmTGz7++Blvb5/x8e0ln+g/4K1wyTd271dr8thFHNDn44qqCyA7Nj2uThQnWhXGu4QLidQlNHhSIPNdqLyXFHLnQfBIwxCUNZh5RB6SOv8QFsy+JSJfAP4OpiQ/IiLfBfwP4M/Zd+rPi8iPAL8ATMBffVAmpCxvJuQ09Ugccw/esZxO0EDct7iug9EXrZTKcAXhcnwSgmEnmx4965keb9i/2bF/zTM8EYbHMD4xJdGLSHcx8OaTaz756AO+6fx9Pt5f8la45O1wycf8M153Nzx2I+cC23ycEWVUZcSeg516rrRjlzqu0obrtOE69YzqSQiSFYcMxqnLPx5TmiDGffFmKat18crBgg+3yEOyoe+85a0/ccv23wd834O+/YjIus/nKAeF2+OWVkpsUvZZCE7SL5WkUYaMpC+bxPLsOPEZO+kCbHpzO496htcCN2869m+ULEdJjye6i4HHFzveOr/mkxcf8Knt+3yif98UxF9z7vZcyECXXc5WHBsJ7HViVOVa4UoDl6nnKm3Yacd12vB+POeDeM670wXvDhdcjhv2MTAlh0ZBosw9amIKEwujrrOKNlMuk5RpCjwfnOXrI+ten9uAuHVKDcuCY0PubuexVYpknqJQJ1uXfTYYy6L+VGiZbXxy1pPOe8YnPcNjz+51x+4tYf+mMr0+4R+PvPHohrcvrviG86d80/Z9PtF/wDd07/Nxf5mVZKJrWN8O6MTRiWckMmZFeTee834652ncstOefep4d7rgvemcD8YzLscN11PPED3T5NGYCeClYuKwTsWgpGBxSwoO33QEQFaUjxTc3yrMqvOwnkjxrS25u63rFElNathKqUy3DVwHPUYsFEi8q5O2S3oct4HxkWf/mmP/ZlaUt0cevXHNW4+u+MT5Uz519j6f2rzHJ7v3+Jh/xsfcNeduokNr8JobBgEYNQEj1ylymTzvpy3vp3O+PD3hg3jGdTTXczlteX8449m04WayTOhm6IiTm3cmzFSG6prEeqFblPpD0CtPR1mkuXFFWt6Jl0qv1BiREMwC+FxkbOObBY7SKBwscJT5e5TSjL9wPbWzwM/V4s6TOkfcOMZzYXgtK8pbI2+8dcnveO19fsfFu/yus9+y7CY8rUryWJROhIQQs8kvRnGvkLDzN0Uxd/OV6RHvjI95bzrn6bTlctxyPfXsYmA3deymwM3QMYyBGLN2lPqnhwqstIC3K4So9jJJLbrfJqehLO1RrzGQdvRnVhQdBvtYF8g8xaaHWOdq81rWGVfjyjR36Kn3iGsuS1ZW9W5u7OocqTPOyfAYxtcjj9685n978x3+98e/ye/efInf3X+ZN92Oxy7x2Hk6OqMNADud2JMahRGiClfqGNVxmbY8TVvejY94Z3rMV8YL3h0veG9/bvHJZPHJGB3DFBiGwDR60ugoi2Wa+1GjZUKuF2m1OIvzq9jci8FZnr84OR57wAIHkRpX3HFibjUV4C6wKUPd9bFqMRpfUm7r8psJ0PlvwSgEfeLRds8nzz7g05vf4lPde7ztb3gsyrnzdNnqjRoZNbHTxE5hRBizguw0MOBrAPuV+Ij3pgu+PDzm3eGC94cznu63XA0dY/Sk5EhJmEZPnBxpcjA5SHbzJYEkoTSeVY5uKaM55ozIKYv2llvkdJSlQvlxbiwrLkjVRlplwK2W/uEwWwLunAqwhvrXQe1i23xlS1/w5JApWbN5nneiAhIST/q9xSfhPXM72eWAuZedRnaq7HMaPKpjwDGqZ9TATi0lvko970xP+NL4hHeHC94bzvhgOOPZ0HO977O7ETQri0ZBJwdRGkWRyqSTWIL5okAlq3Q2KaoLmUnnjsd4jZyOshyTNggrNzIX7qpClfeLrK3ImmrZSrFegl3JdcdjDnpVEhITMkUYIy4rjO0DxCsX3T7jJlc8diO9CGV84aiJa1Uuk+cy9Qx4RvVEdYyYslylDZdxy2Xa8uXhCV/aP+b94Zyn+y2X+5792DEMnjh5U5CUlaMoSbYakhWGbEnmn6ww2ipMvj7HKKpH5HSUpdR1CrgGTZqsdVTogr6wpjHC8qTbto5SRITDz5TvbxBii5tK1hTRcbQL3oX6BJYg0vtEcAmXnf9OPaSIQ/FiGc8HqeMr6ZyrtCHhGLJFGdWzSx2XacuzuOWD6Yx3h3OeDmdcDhuuhpWiTIalFCsyDxjKp+HUeMTN6amQR4LpfK5TQva5F7q4oHuGEJ6GshQEd53u1gymsSJ+9d6ahtBYICmN7KTj8VCtaqdlpbkoQ21aj7WmJH1XrUpBSUNIBEl4UUYt8YnLX6UkFb6Szvmt6QlP0xlJi8UJXKeeXep4Om15Op1xOW14Nm64Gnuuhp7dGBj2gTg5NDp0KooiNZitUlJkb1O5K4rrGkXJ17uM4GC8f1hykdNQFjiMF+AQa1lLe7OPfVa02UYPtysxia4U5WgmlersWM3juFInpD4RQsSJGhSvPV4VVwuYMKrnK/ERX4mPeBa3OU7xjMmzT4F96ng6bXg6nHE19VZBjt4UZQhzAJtKTrw6Nqe082JUHRo0k72tQyD5OYaX3JPFlBe7yMXS++R0lKUQoNvItCwrC8ubXqT8vma0ZR5unZffjiitn80xSVutBmj4urV1o4gTtO+IZx3jI0c8g7RN9MGezg9yFtNLZCvGbosIowYu41l1M8/ihpvYs0+eKXkmdVxPPVdjz24KDJNnmALj6InRWXwCs1LUkWhYzOTKgwFGURVSr8TJKAoaco3IyzxSrExumEpceH996DSURZgR1XZQYJ2C5GbQbh2wJhuCo8Mwp9Xec7Rf6JisATpYKkrL4BMHXciEa8d0BmwifYgkFT6IZ5xPjzh3e6JzRISkjp1aTHKdep6WmGTcWgqc/ch+CuymwH4MTNExTaYoacrKomJK4Ur+Ww5J7ac5/AhoFFJU4gi+E1KY3WbFpYpFeUDaDKeiLEVqx6Cu4hBlwRVtrYg2MYaqEZJb8vdd39WWF9rguT0WmIuMIRD7QOwdcQNxo/g+sc2WpQSqEceodmlH9ey0473pgvfGc94fDaa/Gq2eE5MjJmcKkhwx2k8qFqUWBM16SLYglsAo4hLOKc7lIYzYR+KgqDc3XuIWdTrHOIWz045ov0dOS1mKrOOT9ulvFaX2LtsKqHPXXUZiW7rCXTWQkjX5ZrtKb0g2SjRYpTmdBeImV3CD4kNk4yeCM+uyTx1JHXvpZsuSOt4bzyu4to+BMZqrGSbPOHnDTNRSYtUyP5dFLFJutBmEhPMJn3+qDgAxCjF/TqWpD5UNkuFEdRDhA+U0lKVkQ0VasrUm0DgHn6UDMFEJS3WxqeKiyuqkXWgs1B20mvb7pBC63ZwllU7CbW99yH0mFAWlD5HeR3zOenapYxSPdWpLDWLfH894d3/O9dgzNtZkPwSmMSy8wALyEHM12rpVMdfjfSL4RPARn7dJCqMPjG71cDTKImqKUsFGWFryW+Q0lKVImw4Di54emGOJteVxYvTIMjUpb1tFlwt11nWfqxvLFef1Ba4N7tj0geKC+rm1FKyfZ0yW4YRKe0jsUsdN7LjKmc6zYcPN2OUKgjBGw07S6CofHUCzMrTSVtUFqusxtlxO0QGXiVDSWhPy6UVweXLlQYzykRkT1sq6wStbnINqcNum0QzMkfU+YMZv8h3RonR1wM2MNdRFn+pnczzkXa04p8xrxSkpOfYxsIuBST1JJxDMsqhwNW0yZG91nf3Y1V2PoydFqSCbtlMqdc5wFvSLEqtkZlx5Lx0L6Muzp9QShR8Tsm9oo9V6692umpNUlrZ42MQqDWOtXszI/OS3o9lbl3PLE7Ncf3m2YArzdOxWnJGGUi/GOusN/EpJrArcGW6SGqs3qjfLMppF2Q0d4+irLpdsh6kJLERz+ktNizVbixLYImZVvEtVWYoLUpWFyzJ4P1uVCWRIyDgtF4Eo7av3WJfTUZY15/YYiFbY+k1/crU4NZBttm2bwvxqDUJYuL3SQC+yypLKdj7Pxw+ScQvN7Ij8ZCNMaoVBl5RRPEMKTOqMv5JKlmON+QIzbF+ANqHBUObMRtosRhTvTVGCL1mgKcqUs6tp9OjocFNpOgOZsGmWU479yuqtMa6a6W6XE1GWbAKPVYrXc0OOtYKQg9x1BXoF1NW9rFLmuuL8MbK3s9mzKeQRonkEhuEVc9yQdA5mkzqcJPYxkFSsxTQJKTpSFIvDBTRatVhiVgwxBVwrDNmQ1hjFpRpYRxWmTFmYojMgbzRrJaPgJuqP5FlzEq1Opvn/g9Gut8iJKEuWYjna/ua6EDbU2fmwdE+s3MoxaekOtbMw3wyf714ttM0XrYBz6v0M8XeGiuJ1EU8kFbMmGRy5ij272FUsxfqLhQKhaVsxhlVAmo9VQZPM35OD1xKzWIqtuefZWbFx8MjgTElGcIMS9oofUl7sIVuUlpn4kUFwYUl7LABcwVLyOPSaLq/TvJb9dmyCkThqI9qx/mf74My2WxcdvbHjLBPCfjaKhDwnRYUpOSZ1EIO5pGQQ/k2mPtYAVNTAtph/WkVpXE2VqjSCJkXF1oSMSarVqmeQMrdlEtwgedAPhBsINwm/i8iQhygvJkesyiy3yIkoS2P66/AesaBVkrUttKu6u6VVORgptp64XakHzhTCydJtAXVCQmmPzROlbF+WCcWN5HFdim6soUvy061ZYZIIu9ixj2EB4cfoMuKq+Ya6zDkRczOltlNS5oLqC3PAq4JiGZiqUTHbYFaTVaVlzG2re/B7JewUfxNx+8mqzNPEsVk298mJKMtK1sGuZIQWGoZcWsUtsQlYixvLfxsBj3bG3K2tD4snTqyPue/yyAwhbiD1wMaG7ADE5BiSZ6MelDo7ZYiGzk7RVYTW9g2lIq5oVZiFtIgrYoF7LiKqsnBtVgcVUhSYHG7v8vRKcAP4QfH7vDJIHRWi3Lr20i1ymsoCFMY9kK3MeqCx/bPoRKw3PwKeui6QxsN93sugy03vXUfadkxbX8G4UhPywS7yGB29dwzRo2qpdFGUypeNrqa14tRingTEbOGqAq2ORdsf21bVsqtxmh+oafKk0SN7V5XEjeBHmlilhQsijNODakJFTkRZ9EjWc6TqvMqKdJojeF3xMaz9tTDlykd0JjMd49zC/D3eQ9cZxH/WkTZmVWIPukmEbqqWRVUYM4qb8u/LwqDFFvXrxOIdKxRmnsrBcRR3JFVRQJuKhjBlC6p5oA/jHKu40SyKG8tPnHub84NVp5bfth70Sk5EWeR4UFqkEqebvw9m0a9ufCV6ryZMLpTOz9aoToNqMq7g0U1P3AamrSNuhbRRCIkQEr7x864Bx4qLSEnM/aS5n0dYWg/1S3R2HpPNImaxAXGWNrsc19j+TRmnISBjzoAG5sA2xyuym2AY52VkcrW+gppwLwHqRJTlFimz8JNSFqxu3U5Na5tSe5E6WLkdY3psGtS6cAg1HtJgK5tOZz4PLDbLIp0pSlEWv+KTQI4hkqtVZDtmVm5GwevxATawVBgHzivOt8riaiuI7jyuxCmTKUl3rXRXEX8z2nIy+9F6rvJKIQsX1I4VuUVOR1nWEypr31CpOi+LX3X2vZOVL57BtMUk7lJTanqOtCgJaQ6CYcGaK4OM4xbi1phxLmit9AKGprqEE83MS1OQlBr3IyxISobMtkv3wYIy2f7fGp+KMJjrKYoiuxzUDoLfZ8tynfDXE+5mRPYjjOO85F0ZW1/Ghh0r0K7kNJRFWGAqwDIYZYbrF+v0lOKhNCuSrVYmOxiLnpTFs1yKaO3sWydI8KQuZHylSZk747B0PluXDI55l3B5vynHFIXhduepixoaXDCXlrWv1EYxTUURZw+rUdC9BbX+xuGvJWMqMwhX4f3yoDlvYGJ7DO1YkTvkNJQF8iPTwPQtWpubxiokv/jc8j2t4FyjOGWKAszK0Uqc+TKCz+sl24ILKTjjruQsiE2k6yLBJZu25BK+odgV91OtipoVKAisiGUzQsFWJMcxMoN1KStJlNwYhpHEFdLUpNkJZMiKssuKcqWEG0uV3ZiWy/NChQNwYi0j7XW8R05HWeDQuty6zREvX0oDt9XD2lFi667ElqztvV3MzubWxo1jyplQOkv4TaQPE5swZYZcqkSnKblKGzCrrqTcryMrKEWhwXyKFcIUoMxYifn3BDqCRGd1qQLXxHkce7iG7lLpL5X+KhGuIm5viK1M6+V3ssUrq8Pd436KnI6ytJq/hp5XzV9FahrcymINnSNlgYNFGtIiWKbvkL5HzzbEi47xwjGdC9OFImeR7XZk0010LrL1I1s/4USZ1FVO7abL1V9nxOvUorc1NpFcmpFaUCSJAXRZaUjZskw5SYolYDElchNNfJIV5Wmku55w+2ixys56g2oGtAjuBe43KFVOQ1mUWcOPNY21tEqojnuNrcBc+KvSlgSKsrU+vJCh8gwWKdhKzoKmszxT/0zpzwfONwPnnSnJ1k9chIHORW6ikZomnThXA+CGybgrsWG9WXKnNabRmPksbdtpUZJoiuKmPKEp5Qy6FAhH8DvF76G7TvQfTPQfDMjNaDPi8vK8JQOy6zGf9+L/jwxhuwS4MGMqRZpUebGsLiyD3bxNlftMa7NtVTAnFq/0htraci7Ysi2bRN9PnHcjZ2HkPAw87nY88nujDaBNgGtYix2+5ENdZkilmlwtShSjFeRlYSSBjFTYXhqqgS3wbaBb2Ct+p4TrSHc54D64tk7Dcu2mhs7R1tA+xEjTIg8ZQPjN2Cj2b8Tu4g+q6v/z3Of3F6rkuuZTovQcxCqs/O8RRYGlL25aX9sgebFIQ96HBp+Lhj7HKhmI6xKbENmGkSfdjtf7Gy78nkd+j5dEJ5Hg7E5MyTEFV5XGCn6Z/JTnvtXG9hyjlEqxm2ZrUms7+4LGUpHZUu/x+4TbR/xuwl3ukJu9pchZFucYWVrofP0OqKS3yEMsywT8LVX9GRF5DPxHEfkJ4C/z3Ob3zzdaW9aWGBwuIUP+KjYXZf3ph7Z8tI1smuaUuvQ956arEtiWwmEB4vowu57XwzXnfuDcDcuL5Y0ANaTAmGKF/q2OmUdkxNzcXn+ogJrfG7PNxQaJ3alVj7MVcWNWksEqybKPyH5AbvbobjePc21duvMVX2mXGBTvDfPxfO2pcx67XkavX4rIL2Ij1j/Dc53fb+Vy8e7Qetwm7aKaFaG9o4p6LBOaT7T20Gjm2qbMXdHOgtOYjCKZVIjYGPS5Ed6UZFR3QJ6uxKToSKOHUZDJIeNMJzBAzSxHmavixoyZ7PJP5qTURb6HyZj642TobAHdklLnCLcTyeWIFRZZLkB+h3yomCUv+PD7gX/P1zi//2B2fz5wQkC8ztgHUKkFC/fUnPhi2mQDhLW+em1x2ob7Qp4q7snZCuyxNxeknYFYw+TZT4F9Z2O6rumrstzELvcwd0aCgoyzZJJSdMZjGRwyCH7nTDmGPGO/VIonrdMm3QjdjRJ2ydDYG3M3MtgPY06LY2qQ2RXcu7IWxZK0fxe44LnB/SLyCPjnwN9U1ad37PjYGwe+4WB2P8Cqsiwwn/yC6d8oyrHRG4Jt61hcvKOE7QIGJp3nt5RxFYVvKwpJGKbA1dhz0XU8Cxu6lHCSSOq4idYjNJRm98w1GUvf8uTQwSF7R7gR/I2x2HwmKJWajkTq4B2r8RhmEm4ibjfirgdTknHKwWtevznGmdR0y00osMRiEfMy47csfXOHPEhZRKTDFOWfqOq/yC8/3/n9B4z6QnZqtllMUV/VMo5lUeWCtChv3YGbFax8R26Cl6QZw1D8teB7T3Sw88rTXAOa1BMk1mpzWTRKVaqruh47bvY9+5sOvQ74Z45wbYoSri0WqeSkESRqtSySNINuMWMmE2435AB2mi1v2/NT+qOcm61x64baRczXlfvnQdgWexT/AfCLqvoDzVs/yvOc319udhlnWghPdWyXVpS2rnJ6bPmXIsfaYWmq2G2cU4Leuna01VXCjaO7Au0s8I0ucM3WMJToa20IIDZ82PLabgzsd1lRLj3dpdA9MwCtu4KwN66JH1KOUZp5dTGZwozR5tjtrHKsu30Tl3AcPyolDiJ1GZ1MJbXXvYF09bolS/HvwVoeYln+CPCXgP8iIj+bX/vbPO/5/c2BF4Wps1lTw3aD47jAbVlQmxVU8zvHJ/U7yu5jxA0T/ibQXSVrAXGmaCoQk3AVhWEIuYU0HZDuiqe7ue5JzzrC06woT6G/VLorpbvKweqkdahhKfrJlGs6KVVrIeNkirLfzwt7lkt25CZXolfpMmOuugN52kQDTsb4XLKhf8MtLpDnNr9fZrfTDP+rSfKxAHVNu2yXqgMWDLuyTZHyXW26nYttsre4oHMOBCQFJLpcq3FMgzDtHcMm2DJyvulLLjWeyVJid+Por4VwZRalv1T6y0T3LNI9m3C7cVYMnY+BZFZlgVgPY+WhVPF+BQGsSejleuaHRHQuW5drROPGji3Z08jJILhSVmZvR6TDnaZRYzRQaa3LpWGtLPXCEpw6eCrVAkSdFNwecYJLagHsvsONPZI8Lma+yE6MNZcr0hS2m5KXm7Nt3J5KGeiulP5ZoruMhMsBf7lDdoMpxLQyvGsiV4kv2qVtahfmMqibl1dq6Bm5H6tUu6vCtK2rH5mORGVm27emcU15XEuJ3kuwWuKaY6XntjuxfG1l4aXZ509m7nOEY118InligseNMO2zsnRC6mzGXJkz6wajCvgbKpAWdkq4TnRXE/7ZgHu2R653VrNps7zbHoyCbi/S4iZAL9eoOff6QBS6R7EuNQA+AiV8JBrji5k92uuzClRhzm4OXhdq+2fbqbgIBt08o76w71pljNHcyH62QD44umcWu9jyt2RkV0i9xTwFprc+HWYQbT/jI+56gP2A7AZ0P0CaY6eSnRQ3s1xsvLlW7dgRaWK7EqDG2YouYIJiXZLU6Vj1/Nsg/w45GWXRaVrWKG6jJLRg2gKJbDi0i96iZn+tJJ3Tx4bPoqoWTGIXWwAXPCFPVvCDm0sBfS4yCnml09zQtUsZF8n815vBinu7PTqOy5JGmVuXFB1tIYtaNPU+k6bmGGtxDaBmOBktyueWlphJie+y9dDK62kexPXqt0fkQ7AZvg6yMoMP6mmRVRFsXXqH293YLVKskTapqOR5/ZbiGh5imUypBueWi0FxQ8o/GW0dp5lT0jaiL750dr9zg3+DYK/P+eA6yPEA99aT/HDXBEA+TJPRixIR+S3gCnjnZR/Lh5C3+F/zeH+nqr597I2TUBYAEfkPqvoHXvZxPFR+Ox7vabmhV3LS8kpZXsmD5ZSU5Qdf9gF8SPltd7wnE7O8ktOXU7Isr+TE5aUri4j8aRH5JRH5fObyvnQRkc+JyJdF5Oea194UkZ8QkV/O/7/RvPc9+fh/SUT+1Es43m8WkX8lIr8oIj8vIn/jhRxzKTi9jB8MmvwV4HcBPfCfgG97mceUj+uPAd8O/Fzz2t8DPpt//yzwd/Pv35aPewN8Op+P/zof7yeAb8+/Pwb+az6u53rML9uy/EHg86r6q6o6AD+MEb5fqqjqTwHvrl7+DEZMJ///fzav/7Cq7lX114BCUP+6iap+UVV/Jv9+CbSk+ud2zC9bWT4F/Hrz91Fy94nIgqAOtAT1kzmHu0j1fI3H/LKV5UHk7hOXkzmHNan+rk2PvHbvMb9sZfnqyN0vR76Uiek8F4L6c5a7SPX5/a/5mF+2svw08K0i8mkR6bFOxh99ycd0mxSCOhwS1P+CiGxE5NM8lKD+HOUBpHp4Hsd8ApnHd2DR+68A3/uyjycf0w9hXZgj9hR+F/Ax4CeBX87/v9ls/735+H8J+DMv4Xj/KOZG/jPws/nnO573Mb9CcF/Jg+Vlu6FX8hGSV8rySh4sr5TllTxYXinLK3mwvFKWV/JgeaUsr+TB8kpZXsmD5ZWyvJIHy/8PP3a8mJ6v688AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAphElEQVR4nO2dT6z82FXnP+de21Xv/X7966T/JBNCFBpNFoTFiEwESCCENEKEbDIbJLJALCKxCRJILKaHLFhFAhYsWbRExCxQMtGANFlEQgNihJBmmEQoQP5MQicZoEknnU6nu3+/916V7XvPLO51laue7bpVr957ft3+SqWqsq/ta/vrc84959xjUVUmTEiBue0OTLg7mMgyIRkTWSYkYyLLhGRMZJmQjIksE5JxbWQRkQ+IyFdF5HkRefa6jjPh5iDX4WcREQt8Dfg54AXgc8CHVfXLRz/YhBvDdUmWHweeV9VvqGoJfAr40DUda8INIbum/b4T+JfW/xeAn+hrXMhM53Jvc2Fb4Akdf7S77aX2Xftq7UPb7WW9rKvt9v6G+rXab7vRFaT40KYdXdxY3rWfnmv0kO+/rKpPdx3musgy1M3QQORXgV8FmHPKT2Y/v9nYr5uL6dpdd9u+9k0bMQJimoWo13V7MatlXW0797V54FW71X7FgBHwurGPfbF9jpuHlc42G/3bOre+7f7c/dd/6jvOdZHlBeBdrf8/CHyr3UBVnwOeA3ggT+jQxTg6Drxp233cINoWNpZf8dxSiDK4XMzG8mZ/+17z67JZPge8R0SeEZEC+CXgM8fYsXpdfRpsX7CuiyBGOp+kjW23SKSNNNiHXLJ1SZvtWxJn6Ca1z6/rPLc/e/Un/k7argPXIllUtRaRXwP+DLDAJ1T1S8Mb+csX+lIT3avt0EXZtX7w2A26iHGX0PR3x7VscF1qCFX9LPDZQ7ZtS4D2DV2J0MST67UtttZf2n9fmw6i7Eu6XW27+tCFXTbdql9DD1bidWwwOg/uNjm61jeftqjeJUX6jjHUbmvl4P53qZchdPUvSc1s9alXHTftWqrwEIyOLPugfXEOMQL3PFj374H+7CMhkm/gntKgU30eiGtTQ3ujJdb7m3RLmlTJ0aDvGH37Wi0buFHHIGSXhBzc71Z/Umy6XUPnIYyHLG30GJH72gfdux4mVpsczbpdEmzbT7Pr2JfOoe/mJkiRzn1t7XOwz6STZpxkGbhIuwhzDNsl1ci8tI89bYKdEitBdTTX49IxD1VXAxgHWWS/G9Q1Shpq23nIHdJgn/4Mod3XXU/wIZ7qlW3UIY1TJcadVEMHjVIGcGnYu+2Uap6+HeqjD12EO6SvySTp84v0kL39e5/wSR9GRZZjY+PJ6hLLOwhyiHTpepqPFsroI/WQHyW2P4YBfifIknLTLhmPG/GQYVLsO5pKQYoKaKuR5GPuMPg7bZeEh2IF199u1H6Wtq8iJf7Tiz2Nvb1xBRWW4nxLattSsUPxrqtgHJJFh2/+4JPX9+TscZG6hstdbY6Fnb6THTZIGxtu/csrL7dLOFYfxitZtqK1l9ZtITkKe3nD3u07CdzhCNtoJybdebfdLlEC7nueg17iPaTuOCRLFxrW7zqZnifjWtz/7dFT37E7kqQG97W97ZFV5s4cnD0ky3jJAjujpUl2S8dwc+OCDVysFMN6Z6R6q21yf28CqcSOGAdZOpxyV7YRti/6MZ7ann10jnyG8l62JNMue2kIhzgkD8U4yNJh4G6Ly5QnfHP7LVXRYWscGnBsHXS3JEhUN53GZ9O2c7cdts+OHNukMMnA0HkcZNmBQ+yP9YUajjPt2s/B2EOKDRq9RzjOPpHrIdwJsvSh8yK0T179XtKpa/8rCdTs1wgikYiq4Pu9pIPS8iq2yXX4jRIeltGSZSi2sb1+YCcbN2Vv6WEtJhMwBiR8i20Njesaqgqtr2aUbqiJlLzYLtvpUPI12w1I4AbjIIv0GIktXZycqLN9sQ8MEiIGsRasDQSxFkz8HSULpaDeg9tMWxzu3rDzcX34xD4PjGj2834PGCsR4yALsrqpvbkZVwmI9T2F28sbt7pIIElRQJYhmYUsA2vAGNQI4jXMpHMOnLukkq6KtsrqvOk9yU19gczeh22PUeJIyLJGJ+v3vQEt+6IbdnPil5HLUiTLkDyHPEPzLH7b2B+F2iOq4D3iFeoaxQ3aMH3n2HUjd0Wv2yprw7bqwTbhUlJFtzE6sjQIJ98/ZbRzJNPlA4m2xga8R5tFXhFrkSyDIg/fWSRHkaN5hs4zfGHR3IAHcR5TOowq4uMU2Ga/e85nHkri6n1wxFwix6DETbCD3jBD52S0L0ozarF2JTXakibc3JhhlhfIrLgkRTQz+NyihcHNLD4X8GAqj83DhTeqzXR6UEWoUEevNNxnRLbLg5yc2rBDeqSq9nGRRUyvYTc0BO4SzyJZIEqeBTK07Y7GQI21abTI8fMcLbJAjsygVlAjIOAKg58JrjCIV0wt+NKgImQiGJFAPvWxeEI3YQ4Z0V067z2Djcne4YT9josssGHM7uUGX21jVu2lsT2KHGZFkBqZRa2sCQP4WYY/yYL0yCQQxTb7BZ8Jbia4QhAvmFoxuQbxJOEiGueR2oHz6xHSls9n8zQPc+33Spo9UxsOwfjIknhROyWNmLUDzVrIo5Fa5Oi8QIssSI3MoNagmeCtwc3WksNn4K2wUcLFEpZnYBxB4gBuFm0Yr+BnGECMgUWG2CXqogEQDeHwrb0q6qbQae8ljIrGRZYDiNL8X0kUCY4zyaOhWuTBUC0y/DzDZwbNgw3SSAtXCD4XfA7eElUQSJMnLQSD2IBWoKKggnhADaLxMhqDyWxQedYEp11DkDjExrmVijpkHtRVPNK71OCK3D0YCVk2n7a9c1Ibp11TOMcEySJZMFg1D6MZn9sgRQpDfWKoT4R6HsiiUXKojZ+uSILG5Rr8LE4FFEQFlQw1gpVQNkIgqCUNRNHaIXWN1nWwaWq/8xwPUcO79rW+bPuTbiRk2cReT9u2NJKWPdL8zgwaieLmNpLE4OZQnwSJojZKlUaCrD6KKIgXJD54ojSsid+GTD3OG8RnwQgGaMjiPVLVUFbB60sgk7pW3/e0N9rZb9s3fsOVsOWL6ULqLIZxkEX3fIpIGFm0CKNW8HkjUSRKFajngp9FkjRSxWhQO5E4mgVCmFqRmqiTiGNliSMqiWQyOKdwkgcvr/PRpvFQ1ogxwaYB1PtAmEZN7RGW6LuxnQTagzDAG9jP0jvpSlfDYlqlW9WE4XAwVoNEcTloHknSECQIC3wWlolX1Eu47hbUgc9BfPitLu6vANGgCtUKUvsglZxHjAlD7GaYDWhZBSI14YK2Z3kPI3hf382huHNk6Qw0rleijo2YjTiP1B5ThY+tBFcJUgSJENTPmihE1aOGELLyIC6qHg8rHdIaLaFhW5cLKgZrFG8F4wziFHEGYwSsBOliTVCXWRYj140h3CLO7rheL7oI8YabkXiVzLW1GguEwfkYtwlkkcphS4PPDTZX3Cw6rRqDtpEakTiIIk4iWYK9YpysjNxGAolGw9cIPtcovcDUgtYayRKXW4PJDMY2fiADpUGNRXwcKRmDVBUqZmN00pmycYWE9g3c1UDiNg4ZHooJTyhlFUZGeYYUGVJriBb79dBXfGuUYzRMjjGN36a1X4lthM26rU33BFRk1aYhn7hg76hRNDr8IM7ByUIoQqo6BCJrh0gdQwcl0jUka2OrCmXHxejYpKddAmF2thCRT4jISyLyxdayJ0Tkf4jIP8bvt7bW/edYr/+rIvLz3XvdPshu0dlauLmp6a7gqM6hZYkulkhVI85FkgASJISpwZQgtawGNu37s7JjrMabH8nU2I5RJalsksnbll2UEXw40enn5hZ3kuFOC/xpgb8/R0/ncDJH5rNVMFOs3c+1nzBfKSnYOICU3vwR8IGtZc8Cf6Gq7wH+Iv5HRN5LKGP6o3GbP4h1/HdALp1oZ97FPi5z59CyQpdLWJZI5UKEudmtBlvEVIE0jTqR1UgnNms75EyLTC1yNfZNW6L4THC5rL5dHkMGc6FuCHMSYlL+dBY8zLM8eJzb8auUSWs9Jcw6J78N7W8HYXaqIVX9KxH5oa3FHwJ+Nv7+L8D/BP5TXP4pVV0C3xSR5wl1/P/XjqP0EqEzEWpoTxvtXRiZlCVyscSuIsiCzU10xAWHnF3IylDFyIoUjWG7MnQbIxfCiCkPhov42E7Z2FZtS905UCexoVm1EefXPiEZeAD6T7r3ugxOb90Th9osb1fVFwFU9UUReVtc/k7gf7favRCXHYyVMdc1daJn/s0GnEMXSxDBADa35JmJKkIwBWhJlDSyVjttsqxc+6xuPESPrwTCBQmlK4mDtARU3LdxoJUCJoYMQrrDCttD/oQ0h/Vvt9Mre9V5RMc2cLsei84ebtfuH3Lxp3gYBx1VdQ3nFyAGMyuwRUY2M8GWKEK3V09+ExuKI52uM1jFjKJqkpV/XzaG1isVRVB1WmlMe1BUgiPP5wZjQ7qDbKVOJJ3nRpjEdF7DzslvB8wQOJQs3xGRd0Sp8g7gpbh8Z83+Btu1+zdWDoT2N3fSEudDhHIu3MOqRJYl9qIgm1l8Ibjcxqc4XGQPq5ttXLOAlT0CbAQYIfpYZuGzOoWWhEFB60jCauXsxzvBzQ3i8xhS8HEEF5O17KaTLnWi/uUmLZV+aAI7h1dR+AzwK/H3rwD/vbX8l0RkJiLPAO8B/k/KDodSI3vbwsaJD+a8RoOXZYmcL7HnFdm5J7/w2CXYSqMvhfV3DbYMn5Uh3BBINyWMm0F1X6keU+p7Sn0K7gTcPH5mzbfgirUB7OaG+tTi5jHH1xgks6sRkdjD5whtj4D6rvH2SLIPOyWLiHySYMw+JSIvAL8N/A7waRH5CPDPwC8CqOqXROTTwJeBGvioql7BF9nRnx0n1Cdh1oQpkYsMm8WMONOoDoMaxUdxodIyaJv/no0g40rVZODmipvHofWW11e0yYURTEY0YoMqUgPexgBkXSCVWyeCO4c2aQ5HvYqHIWU09OGeVf+hp/3HgY/v25Ehm2VX4CzZfe01GLsERZBFG2OtBoR6DvgQN4IQZNywQZrodMvz64tAFH/ig0OvMYqdIJWEAGQODkXqYFhnNozCQgBToiGdg4A1BqMaPLgxxaE5z0OM1GOVHxmVB3c7o78XW3o3OdNdffC91DXGhSc4qx3iTgiWbtYyZmPqQsaWoy4s87muUxsKxc89clJjssb/L7jSoEuLVLJ26CloTB4PxBPUhBFS2L9QKEhZIcsyGOcia0dxImEuuR5SJuPvwKjIkoyr+AzUgyPYL4sFYg0mz8hiojaYtWNOBGeBVjTa5+BnGuyOQtHcQ+GxJ45iVpFnDhONmWWVscxzfGUQo4hVtDbBsK0NUgumDoRxuSLzYEVnC4u2p6WYasNNkJRqkHQp9svUGzVZ9i2zMbT9RluJHlJVqB1SVphlRpYJkIXcFG8oCSrCZUH1+CISZab4mYeZx84DSeZFxf1ZSW7XZLmocs6LnLLOogtFqMqMOrOrrDwghCGIRm8BPjeQmeDJbeY0QVBLHbbL0HW45KfaGjrvQ5jRkmXXrL19I9Qb1RCabb2GNMeqRhY1xgiZB6kNKLjcrobDmhFIMo8qZ+7ITyrunSx5MF9yv1hyP19SmBobyXJWF5zXBRd1zrLOKJ3lTJS6yNA8GNdtf44aVs5CtRJGRsaEWQnNhLYDJvt3SqKt6a8p+xkdWY5dragLa+dXjE7HkYdUFsl8GLXUGj4O8CHUrJniCw+zQJQH9xY8eXrGU/MznijOeGt+zkzq1XEeuRmv1qe8Wp7wqJ5xVhU4LyznOa40+IWg5zEKXYfhu10q2YWLsazWjAB/ORxypQlrCf6ZbYyOLMfA3oRzPqgj5xBvaRKaNuJBRLWRKXbmmM8qnjg55133XuUH59/nqfwhT9pHnJolVjxODa+6U75TP87L2WO8Ut7j++YE5w3LKue8tLjCrFIWbKXkZ578oSN7WCKLCuo4dI6jopQXtg+lmzazILbXrXJjduDOkCWVAHsRxWskSo3UGVThiZbMIN6uZh9Ky4tLpuS54/58yZPzM9598j2emX2Xp+3rPGnPOJW1GnrFzblnSuZR2tRqOK8LimzGIvchbTOOkEylZBee/GGJOS+ROGrD+WCr+O6bOVghgX4Vc0i23J0hSxtdIng1C3FAPDdDc1UJDi+i472qgivF+zCAFUEzg50LdfToSg1SSZjrbDynecUTxTlPZQ952r7OW+w5p1Jzz3hywIoACxb6iIXmvOZOeEXu4byhchZXGmwpK69wcOC1Yzi6npDWSJXWZLpmVNQ+t6tdz7s4fbWFIVIAG5Z9+wkaetKA4MkVs86udy7kwdY1QvB42MxgyiBdVmmVNTgvGKPcy0oeZBc8nT1cSZS5BKKcGstcMqBkYc9ZaM53zOMAlN5SlhksLHYpmDKGFjyrRG9Eop3i0JVkOZwM+/hl7mR2/yVJ0VNqAuh0OPVhQ2y78NSKyNpbGqsuyLLAlB5T22DoViENgVpwzlCroVLLQnMqtVTiKfA4YKkerxULXYcPKrWc1QUXVU5dWszSYJbr1IZAypYxG5O3aYiyI1rc92ANBRb3xWjJMoQuq75PLw9KJtNKC2hquMQYklQOUyu2VGwp2BJcKZilYbnIefn8Ht+0T2JEOZvPeGf+Cu+0r7EQz5lmLDTn3M848zO+5+7zr8u38N3FfV4/n+MXGbYKKigQEUypmNojlYeqDp7mqo4kXkechzzcx7JN+nC3yNJVL46taGpqxt3QMVwYSpvSYUsbbIsKTCXYpVCdZbxanABQe8uFy6lOLfNZhUX5dv04360fUKnFITxyc7518TjfOztlcV4gSxOcslG1NcN0KT2mDH4frcrgZW6I0iR4uY7z3jzhzWuViLszdJaEmzogfofW72qrUQ0Ba7+Gc1BHslx4ssJERxkxMSpjqSd8d5nxaDHjlcUpLy/v8+K9t5CL4/V6zqvVKUufUXrLeV3wrdcf8Oqr9+D1HPvIkF0IdkFIjyg1qLwqHJf6QBvlwJKnd2yu87YvYPcLmzrtltXPRD3tNRiV0Tu6SjZSRaoac1GRGYndy6KLNUST7dJSnxrO7uWcn8z5zslj/N+TtzHL65W7f1FlXCwLymWOe5RhzizZuYTPBeRnSraIRCkdEqVKM0wWa4IkwXVKzUvosd32kqoDGAlZ6LwYSbPoOiLQO4/TRjM/2BDySNSjTWGeZUmcbxZ8IbXFVoZsIdRnQn0a5k27ucVlBa8Wp+gsBBZN4fBLi5xZsnNDtpQgRZrPQsnPlezcky0cZlkHr61zqPfrahCqoUrDVh7yUGS97xpeWp+akRgxHrIcin1tlO2nr/3fR6JIHSa2x9QAkZB/IqqIs9jSUC+E+lxCFYZC4hQQwWcGNwspDMVSyM4huwCJoQNTEY3m4ITLzhz2rMJcVMiiDGmVVxwqJ6Hv5RMDGAlZNt3NXaOYfYvtNaOGvtIUl+CDN7XtrKOZxC4SBktekcpjF5asMGEO0CxUi2o0qRpZle6wpZItPLZsSUgXCxiWHrN02PMKuQgeW8oKraowO7Hx2h6bNB3S6Y7ZLGnYecPb9W/92svZS5iGoK2S5IoL0d1Vm1i2VBXrFLM0qAnVo3xhyWMdOnEap8K2iFF57NJB7cFKzJdhNfdayhpZbpKEZri8qqzQnNrmeQ9F3/fNjLtb9VmOjR7n1OpC9eln9YEwhjA5HRANHlSJN1KMQbOQWG1mGVpk0eMaJuDj4k1sqiLUoZKDxurcGFkto2pq/8e5zm7tjNO2u38AXcV89mm/fY2GMFqy7HSoHbC/Llw+RsykEwNaBiLYGl2WIQmplZAkCxtyTSAYxFW9eaObY8Clt4lodOevJEkk1zZJLkmQHQOBXeeeMtLsw2jJsk+U+VASDUZpm/RLWL3IQU25LsDcKtsOoHXwujb5MSv/TaxvB2zOVVvlqvh1ubB9RnVHOt82xMjdiA2lXJyNbLeu+vhtD+8VLnynbdMaMYUbbEBqxIb0g0t2RmMwN3GeruMcUup0oG2fOtrp6Ew8/jjI0j6XvqDg9vKh4spdJ9+1rMdvMSSqQ/CxFe02MTNu+6aLCYTpmr27pys+NXXyujEOsvRge9rlrrnQh6qk7W1693fpaXUbBXdSjOhj2mF9uK7U1PGRpSNI1nlBr1refCgYt3GY7mhvV5xp15ynJI90IrYHAAcRZE9VPQ6ybAUSe4OD236Rg46VRpLtY3ZWKOjzgh7pBQ/JDsUr4m455XR9QXYSZdeu+p7elLhKxz46+9K3LGFIm3rTd3mwdxHwmElPDcZBlojeJ2gj+efyBeuLQPfVKxk6/iWkhP0Txfkub+uhN3M1QtxY1lOiI6zcGBxsHPcuDJ03sOG232NI3bNuc9e7n8DuLnVIvD10fgoRDrE7NojSHKP1EHU69Q483mjI0ptv0tWmld1+ycg70J4ZCqr1qsaBHJMhA3RfCTJE8GbGQjNU37XtVey9cZBl+9oN1W9tp1Tqjlf4JiRydw2bD0HXU3wpZDGkHjvyTC6d864qWJc71bvuECl2eFmho+Kyzk3bLNHwvEGkJiUdipRA4a71h0q5cUiWjtKm/U0TxP6ufN3UXm1f1ITpGL3YFYLoSYfcOO6RsdMVsIWRkGUTu/wKQ2H2Zn3fuqF9pqy/brf6VZ1thxr6KRJ5dGRJcUTtumFXkSDt7XemZm4/8buSqjsqL13FNZ90nkd80cPOViLyLhH5SxH5ioh8SUR+PS4/bv3+AWzr2WNi6IlrnITtT2vDzu27CDEkBbfX7z2UT8DgPvew8VJa1sBvquqPAD8JfDTW6D9y/f7L6CXJUH36Pfe/udsO0dxT877dt768mPZ+dsW6hm7+NqE2RklD57xLqux5vXa2VtUXVfVv4++HwFcIJdY/RKjbT/z+j/H3h4j1+1X1m0BTvz8JfU8itG5A31OdkILY9Xtgg8F+DmHnk7/rRnfspy98cTCal2e0PwPYy2aJL3z4MeBvOGb9/lZs6PI63y9J+na3ZYy2SdZXWuKQIe++wcienaRJgK5kr52H7b5Wh9pKyWQRkfvAnwC/oaqvS//bK7pWXOrZdu3+TlxxyNjlEOtyfvVte/DIZyBPtiuO07+bLUmSaqymEnzP65vUaxHJCUT5Y1X907j4O7FuP4fU71fV51T1/ar6/lxm3TemJar3uXGdRumRnHWNrTJodG/5Uwaf4oEbtqF2t1VvX7rGQHHCJNU4gJTRkAB/CHxFVX+/teozHK1+v/SqmvYN7xyVbLXvMkY71yWEAbaJMXjztpYPokPq9BGqy866RJgd53HpoVmFAZTVG0iO5Gf5KeCXgX8QkS/EZb/FNdfv73UatXR37xO7Fa2+qj9jA8f0ph6Qo9Ne1hU7GlSfKXGyq6QoqOpf022HwNHq9+ug+NyFS+1b0edUkrQvcpJfYk+Dsz/npsOuah/nAKTYW2IE1TXZUs5jdB7cK6NLt/fYPV3ZaNvoTVhKvMCdw13tSExa2SBKqP69O2J+ZRzbz3Ij0Kv7PTabdev0Q2IufQQ6eoxolXpxvbGn7kOnHXNUkmVQfB4yaeyAqZ59/Vrvco8ck0vb7oiY9z3pXTGonvVDDs1VktSBGBVZoMcfkWJ/9PkX9rk4u25K4j5TRja7hrX7pGkORcQ7s+oOxDjU0IRLuO5UiEMgKfXgr70TIt8FzoCXb7sve+Ap3pj9fbeqPt21YhRkARCRz6vq+2+7H6l4M/Z3UkMTkjGRZUIyxkSW5267A3viTdff0dgsE8aPMUmWCSPHRJYJybh1sojIB+IsgOdF5Nnb7g+AiHxCRF4SkS+2lt3YbIYD+nszMzA0vlLtNj6ABb4O/DBQAH8HvPc2+xT79TPA+4Avtpb9HvBs/P0s8Lvx93tjv2fAM/F87A339x3A++Lvx4CvxX4dtc+3LVl+HHheVb+hqiXwKcLsgFuFqv4V8MrW4muZzXAM6A3NwLhtsrwT+JfW/90zAW4PG7MZgPZshtGcw9AMDK7Y59smS9JMgJFjNOewPQNjqGnHsp19vm2yJM0EGAmuNJvhunEdMzC2cdtk+RzwHhF5RkQKwrTXz9xyn/pwxNkMx8XNzMDgdkdD0TL/IMF6/zrwsdvuT+zTJ4EXgYrwFH4EeJIwp/sf4/cTrfYfi/3/KvALt9Dfnyaokb8HvhA/Hzx2nyd3/4RkXJsaGqOzbcLVcC2SJZbY+BrwcwQx/jngw6r65aMfbMKN4bokyyidbROuhuvK7u9y+vxEu0G7ioLF/vtTHlxTVybsg4d8/2XtycG9LrLsdPqo6nPEhJwH8oT+hHTOhJ1ww/hz/W//1LfuutTQKBxVE46L6yLLXXK2TUjEtaghVa1F5NeAPyOkIXxCVb90HceacHO4tumrqvpZ4LPXtf8JN4/bjg1NuEOYyDIhGRNZJiRjIsuEZExkmZCMiSwTkjGRZUIyJrJMSMZElgnJmMgyIRkTWSYkYyLLhGRMZJmQjIks1w2RWymxfh0YXYXtNwy2CdJFmDs2Z2siyzEgsn5LSLtUe/NSLREwZk0YVdQ58PG7/UaQEWMiy1XQ3HwxiLVInq2XGRNJEglkDZKFy611jdQ1WtVQCzgXSCMyasJMZLkqxCB5hhQFUuRgLGKjFLE2fhs0z9AsvN5alhWUFSxLZLnEL5cIBMKMGBNZDoUIkuWINch8hpycoPdOYFbgc4svLJpbfG5QK+FjguTIzh32osKcLZFHF5hHZ+hyCWWJ1vVtn1kvJrIcgsZGaRPlwX38gxPq+zn1iaU+MbiZ4HLB54CACpga8vOM/FFO8VpOZi3GhxdxqnMwkeWNBzECeQ55ASdz/GNzyscLyrdkLB8zVI8J9Sm4GbhZfB+QgimF/KFQvCb4Qpirklc14j3ifLBj1I/SdpnIsi82jFqDFDk6L3D3csrHMy6eMCyeFMq3KPUDh7lfUcxrVMF7YXGRU72S4+YGtQZxBaY8wTqH1A4py7XBOzLCTGQ5BM3w2FrILFpkVKcZ5X3D8q3C4mmPvH3J29/6kB987FV+4OQ1arVcuJwXzx/wzdMnuchPQC22tGRngTBmWSGLAi3L+AbZcRm8E1muAq/r90cb8FlQO/6e46nHz/jhx7/Hv3vwAv929h0qtSw055snT6MqfMM9xbKcky0M+VmGrWbk5SlSlogI3is6MnU0keUQqA/WajRIpXLgg02CALlyf7bkbfOHvLt4mfcUL2FRKjW8xZ7z2ltOOKsK/nVpWZ4X5I8MtsqR8oSsrBARpFFFjIcwE1muAHUOqgpqh3Ea6kQISOa5l5e8LX/ID+Tf592ZkhN8LI+Zb/PS6QNeefyU1y7mnD3MWD602KXFLgrM4hTrPLJYIKUNmmgk6mgiyyHYfvuq94hTjAOpQBeW7y9O+Hb5gG9Vb+Xf2DNyHFbge35GpZZcPEVW82jmqe9ZyvtCdmGwixlSOcxiiVQ1lIxGHU1kORRiQryn+esVUyu2FOTC8srDe/y/kyd5Mj/j1CwpJEiHV90pL1UPuHB5CB0VnvpEqe8L5cKQLTLMcoY5nyOL5TqONALpMpHlAIgJMR+RdeBQvCIOTAnZubB4OONf5w+4n70dK55cHEY8527Gt5cPeFTP8ApiFT9T6rlQnwjVPUN2lpHNCyjyYBMZ2fWu8RvBRJYDoF6RYIIg1qDGoCLB6VaDXQj1I8ur+T2+CrxanjC3FYV11N7wWnnCw+WMs4sZWhpMFBpqwNvwjTRklNHUp5/Icii8hoiytWCbN9srplLsQsgeGWoKXiktDx+dkOWOPHMYUcraUlWWepFDaRAniCeEBEyMIzVkGREmsuwLVcJwtnUjRVZV9MSDLSG7EMDgK6G+sFS55yJXxCjqBZxAJdgLgylBXNh2hRHmME5kuQq8gl+PVNRE0ihIDXYZVJPUUVpEFWM0tDGVYJZBbWULMGUwksWFzxhGQG1MZDkEqqhf1bMP/0VQE8ggPtguAOIEYwm2hxA9d3FdLZgK7BLsQjEVGEf02YyLKDCR5UqQmOCk1uJzCZ9sLUEgqpeNUe9afZk6jJ5sqeG7CpIlOPeiPbSdqnmL2NmLu/ZyyRuFtZBn6MziC4MrwBeE76xFGF1LG1MFm8Yum4+G7zKskxhr0sxAnoVjjAQplP0j4ANby54F/kJV30N4NcmzACLyXkIZ0x+N2/xBrOP/hoOYmC6ZWXyR4QqJH/A5aA7aDK99kC4NWUy5Vj0rwjT2Sg2iipro9DN2lfh929hJFr1jL5e8ccSkbLWCZkGi+EJx8+Bs87OwrLnSa+JoIE/82EojaTxSeUxZh0Cld+vI9i3jUGV45Rc1isivisjnReTzFcsDu3HLaNkSKkHtaBbUkJvr+jOLzjZpVJKubBnjNHwqxVQeu3SYixqWZfg0mXMjwLEN3OQXNW7X7j9yP24Oq1FO+KhVNFN84RHfzBMCUwtaxU3i0LktZaQOUsUsXUiCWlboMiRw6x2XLKN+ueSNopmeGn11wRsrK6KojaopV3y+bfjqOqZUeUzpkaWDskLLMnxGJFkOJctoXy55o4gBRYjJ2C4448QRPLQKaqKkyYOK8jmoXUsc8TFi7TSkJkSpwnKJltWo5hLtVEMi8kngZ4GnROQF4LeB3wE+LSIfAf4Z+EUAVf2SiHwa+DJQAx9VHUFs/boQ0yqb9ARTKrYSfCmYLI6GokcXWDntQqAwLBNPsFeWDrOokEWUKGWF1lFvjcRBt5MsqvrhnlWdLwhS1Y8DH79Kp+4MvEOqGlM67NKTLQz+vFEzYXSkhpVrPwyLw6ZqojSqFHteY88r5HwJ5xdBqlTRBTwSosDkwT0Y6jUMbZ1Dyhq78GQXHpeHdAWUdTxImpEP4FmlX0KwVeyiRs4WyNkF/uwcqmqaCvJGgzqPLEvkfEl2lpMXYY5z43zTbB0vQteeXInOt9BOkbJGWkYt7coKI8JElkOhPkiA8wvEWmyRUxjBuJzswlDP23Gi1vCallEbfStSe3B+JanGMlTexkSWQ6EafCCqGCOIMWSAKR1+lpEXBm8FzUwcPjeT45vtwS48pnJQhyklWtWjVD8NJrJcAepcGOyUFSyWiDWYqsZkNgQCTaiggDGhmkJu8NasJsnbhcOcl0h0vjGiYXIXJrJcEWHuUA2LBahfFezZqPYkgpnlIUJtLVhBjcGUNXK+QBfL4IgbqURpMJHlKmgy5JyDxTLYG80672MTDTMMiwJmM0xmIctC3m7t0GUZHXCNYTtewkxkOQbUX57W0xrNrG6/aiBJLBuG80H9VBXq/GgN2wYTWY6BmMS9QZitoa96DXXkxGzmp3gNZIvfY8ZElmNBd5TIUDd2LuzEOJI7J9wJTGSZkIyJLBOSMZFlQjImskxIxkSWCcmYyDIhGRNZJiRjIsuEZExkmZCMiSwTkjGRZUIyJrJMSMZElgnJmMgyIRkTWSYkYyLLhGRMZJmQjIksE5IxkWVCMiayTEjGRJYJyZjIMiEZE1kmJGMiy4RkpNTuf5eI/KWIfEVEviQivx6XT/X732RIkSw18Juq+iPATwIfjTX63/T1+99sSKnd/6Kq/m38/RD4CqHE+lS//02GvWwWEfkh4MeAv+GK9fvfELX732RIJouI3Af+BPgNVX19qGnHskuFR1T1OVV9v6q+P2eW2o0Jt4gksohITiDKH6vqn8bFU/3+NxlSRkMC/CHwFVX9/daqqX7/mwwpxXx+Cvhl4B9E5Atx2W8x1e9/0yGldv9f022HwFS//02FyYM7IRkTWSYkYyLLhGRMZJmQjIksE5IhY6gXLyLfBc6Al2+7L3vgKd6Y/X23qj7dtWIUZAEQkc+r6vtvux+peDP2d1JDE5IxkWVCMsZEluduuwN74k3X39HYLBPGjzFJlgkjx62TRUQ+EBO7nxeRZ2+7PwAi8gkReUlEvthaNtoE9RtLqlfVW/sAFvg68MNAAfwd8N7b7FPs188A7wO+2Fr2e8Cz8fezwO/G3++N/Z4Bz8TzsTfc33cA74u/HwO+Fvt11D7ftmT5ceB5Vf2GqpbApwgJ37cKVf0r4JWtxaNNUNcbSqq/bbIkJXePBFdKUL8pHDOpfhu3TZak5O6RYzTncOyk+m3cNlnuUnL3qBPUbyKp/rbJ8jngPSLyjIgUhJmMn7nlPvVhtAnqN5ZUP4KRxwcJ1vvXgY/ddn9inz4JvAhUhKfwI8CThGm6/xi/n2i1/1js/1eBX7iF/v40QY38PfCF+Pngsfs8eXAnJOO21dCEO4SJLBOSMZFlQjImskxIxkSWCcmYyDIhGRNZJiRjIsuEZPx/WSk+yS4mpjIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5587, loss_val: nan, pos_over_neg: 1.0463637113571167 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.0802, loss_val: nan, pos_over_neg: 3.766254186630249 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.8559, loss_val: nan, pos_over_neg: 3.8215863704681396 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.6793, loss_val: nan, pos_over_neg: 18.27652931213379 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.5719, loss_val: nan, pos_over_neg: 19.421201705932617 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.5198, loss_val: nan, pos_over_neg: 28.400211334228516 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.4645, loss_val: nan, pos_over_neg: 50.231056213378906 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.4335, loss_val: nan, pos_over_neg: 53.602088928222656 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.3797, loss_val: nan, pos_over_neg: 51.45674133300781 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.3531, loss_val: nan, pos_over_neg: 60.74815368652344 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.3504, loss_val: nan, pos_over_neg: 88.90898132324219 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.3222, loss_val: nan, pos_over_neg: 141.582763671875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.3064, loss_val: nan, pos_over_neg: 184.57456970214844 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.2865, loss_val: nan, pos_over_neg: 128.48211669921875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.2843, loss_val: nan, pos_over_neg: 148.0657958984375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.2782, loss_val: nan, pos_over_neg: 137.0606689453125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.2577, loss_val: nan, pos_over_neg: 167.84774780273438 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.243, loss_val: nan, pos_over_neg: 263.75634765625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.2362, loss_val: nan, pos_over_neg: 301.8110046386719 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.2352, loss_val: nan, pos_over_neg: 280.9101867675781 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.2299, loss_val: nan, pos_over_neg: 228.8472900390625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.2184, loss_val: nan, pos_over_neg: 232.0416259765625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.2033, loss_val: nan, pos_over_neg: 379.7417297363281 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.2043, loss_val: nan, pos_over_neg: 601.3388671875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.1713, loss_val: nan, pos_over_neg: 718.37451171875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.1771, loss_val: nan, pos_over_neg: 567.7335815429688 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.1668, loss_val: nan, pos_over_neg: 240.3358154296875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.1658, loss_val: nan, pos_over_neg: 470.6559753417969 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.1599, loss_val: nan, pos_over_neg: 414.66558837890625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.1441, loss_val: nan, pos_over_neg: 590.4993286132812 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.1496, loss_val: nan, pos_over_neg: 830.8899536132812 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.1387, loss_val: nan, pos_over_neg: 703.2197875976562 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.1271, loss_val: nan, pos_over_neg: 2162.826416015625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.1213, loss_val: nan, pos_over_neg: 316.80987548828125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.1246, loss_val: nan, pos_over_neg: 468.9641418457031 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.1173, loss_val: nan, pos_over_neg: 450.40484619140625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.1252, loss_val: nan, pos_over_neg: 685.3872680664062 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.1123, loss_val: nan, pos_over_neg: 1902.573486328125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.1187, loss_val: nan, pos_over_neg: 507.97723388671875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.0922, loss_val: nan, pos_over_neg: 599.371826171875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.0907, loss_val: nan, pos_over_neg: 458.7165832519531 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.0915, loss_val: nan, pos_over_neg: 917.9236450195312 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.0764, loss_val: nan, pos_over_neg: 672.4711303710938 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.0793, loss_val: nan, pos_over_neg: 511.6710510253906 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.0734, loss_val: nan, pos_over_neg: 513.6574096679688 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.056, loss_val: nan, pos_over_neg: 319.19903564453125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.0516, loss_val: nan, pos_over_neg: 798.717529296875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.0505, loss_val: nan, pos_over_neg: 1529.83447265625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.0395, loss_val: nan, pos_over_neg: 654.0528564453125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 577.9224243164062 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.0421, loss_val: nan, pos_over_neg: 573.9695434570312 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.0436, loss_val: nan, pos_over_neg: 196.1876678466797 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.0382, loss_val: nan, pos_over_neg: 783.4732666015625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.0345, loss_val: nan, pos_over_neg: 132.97593688964844 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.0284, loss_val: nan, pos_over_neg: 388.3067321777344 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.0365, loss_val: nan, pos_over_neg: 155.18527221679688 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.0259, loss_val: nan, pos_over_neg: 442.8148498535156 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.0304, loss_val: nan, pos_over_neg: 146.77459716796875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.0308, loss_val: nan, pos_over_neg: 272.4349670410156 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.0216, loss_val: nan, pos_over_neg: 170.69888305664062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.0134, loss_val: nan, pos_over_neg: 158.09056091308594 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.0248, loss_val: nan, pos_over_neg: 201.91346740722656 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.0267, loss_val: nan, pos_over_neg: 230.28097534179688 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 557.8272705078125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.0223, loss_val: nan, pos_over_neg: 109.51348876953125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.0044, loss_val: nan, pos_over_neg: 284.98974609375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 244.60745239257812 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.011, loss_val: nan, pos_over_neg: 207.0958709716797 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 474.7755126953125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9992, loss_val: nan, pos_over_neg: 215.7072296142578 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.0019, loss_val: nan, pos_over_neg: 384.2242126464844 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.0065, loss_val: nan, pos_over_neg: 159.2694854736328 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.0076, loss_val: nan, pos_over_neg: 251.764404296875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 219.37060546875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9988, loss_val: nan, pos_over_neg: 330.0223083496094 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 328.7473449707031 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9871, loss_val: nan, pos_over_neg: 204.400146484375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9796, loss_val: nan, pos_over_neg: 185.1700897216797 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9942, loss_val: nan, pos_over_neg: 336.32489013671875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.0015, loss_val: nan, pos_over_neg: 198.37008666992188 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.985, loss_val: nan, pos_over_neg: 414.7425537109375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9817, loss_val: nan, pos_over_neg: 205.12574768066406 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 309.9580078125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 532.4124755859375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 411.0006103515625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9741, loss_val: nan, pos_over_neg: 268.92431640625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9792, loss_val: nan, pos_over_neg: 185.21609497070312 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9668, loss_val: nan, pos_over_neg: 288.6422424316406 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9633, loss_val: nan, pos_over_neg: 573.853271484375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9603, loss_val: nan, pos_over_neg: 2062.26806640625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9658, loss_val: nan, pos_over_neg: 367.1512451171875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 927.89208984375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 403.6012268066406 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9702, loss_val: nan, pos_over_neg: 416.2385559082031 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9635, loss_val: nan, pos_over_neg: 381.346435546875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 897.2767944335938 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9608, loss_val: nan, pos_over_neg: 258.8701171875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 1136.50927734375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 593.9765014648438 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 427.6163024902344 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9448, loss_val: nan, pos_over_neg: 628.40283203125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 592.9531860351562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 580.338134765625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 707.0685424804688 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 309.79248046875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 520.0994262695312 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 1321.1307373046875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 359.4255065917969 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 771.293212890625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 536.6649169921875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 554.0269775390625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 364.5650939941406 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 374.2427062988281 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 534.358154296875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 910.2036743164062 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 243.93710327148438 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 337.9479064941406 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 200.83497619628906 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 408.031982421875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 252.2806396484375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 607.8696899414062 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 220.00033569335938 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 255.3684539794922 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 429.1139831542969 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 224.468017578125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 529.5594482421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 224.78851318359375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 301.001220703125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 520.7054443359375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 355.5070495605469 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 891.8355102539062 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 385.8611145019531 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 210.62547302246094 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 393.4499206542969 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 387.8304138183594 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 900.2819213867188 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 720.6959228515625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 299.4462890625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 439.13812255859375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 188.36679077148438 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 267.7032165527344 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 251.90908813476562 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 230.99342346191406 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 318.05364990234375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 229.5149383544922 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 285.38763427734375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 374.97613525390625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 188.9615936279297 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 1106.4700927734375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 286.3263854980469 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 616.089599609375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 447.30133056640625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 541.9341430664062 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 529.971923828125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 493.1505432128906 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 321.2541198730469 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 460.03424072265625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 266.0008239746094 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 510.66119384765625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 258.0645446777344 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 594.8255004882812 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: 695.0076293945312 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 365.0043640136719 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8923, loss_val: nan, pos_over_neg: 1427.3646240234375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 423.2065124511719 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 219.2989501953125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 981.21044921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 380.7654724121094 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8919, loss_val: nan, pos_over_neg: 568.5880737304688 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 319.4693908691406 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 299.0747985839844 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 274.6278381347656 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 275.566162109375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 293.4501037597656 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 296.9486389160156 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 1419.8712158203125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 302.3809814453125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 320.62255859375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8898, loss_val: nan, pos_over_neg: 493.8181457519531 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8913, loss_val: nan, pos_over_neg: 227.92269897460938 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 507.92559814453125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8887, loss_val: nan, pos_over_neg: 325.7248229980469 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8906, loss_val: nan, pos_over_neg: 357.38287353515625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 681.069091796875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 371.794921875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 960.6661376953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 330.8414306640625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 666.32763671875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 475.0352478027344 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8908, loss_val: nan, pos_over_neg: 351.2890930175781 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 315.6412658691406 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8783, loss_val: nan, pos_over_neg: 793.1367797851562 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8855, loss_val: nan, pos_over_neg: 418.2935791015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 434.1011657714844 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8815, loss_val: nan, pos_over_neg: 516.7066040039062 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 564.9000854492188 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: 302.2835693359375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8799, loss_val: nan, pos_over_neg: 422.9893493652344 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 351.2707824707031 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8981, loss_val: nan, pos_over_neg: 202.97132873535156 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8838, loss_val: nan, pos_over_neg: 627.498046875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8855, loss_val: nan, pos_over_neg: 352.3435363769531 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 407.2021179199219 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 426.79010009765625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8928, loss_val: nan, pos_over_neg: 180.3863525390625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 941.682861328125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8799, loss_val: nan, pos_over_neg: 345.8048095703125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8797, loss_val: nan, pos_over_neg: 545.1105346679688 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8791, loss_val: nan, pos_over_neg: 351.453857421875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 456.2667236328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8848, loss_val: nan, pos_over_neg: 394.2069396972656 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8758, loss_val: nan, pos_over_neg: 497.9176025390625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8738, loss_val: nan, pos_over_neg: 315.7369079589844 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.873, loss_val: nan, pos_over_neg: 488.23968505859375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 342.97027587890625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8767, loss_val: nan, pos_over_neg: 345.46905517578125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 285.0691833496094 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8863, loss_val: nan, pos_over_neg: 363.2892150878906 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8748, loss_val: nan, pos_over_neg: 274.9122314453125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 458.87890625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 420.79931640625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 453.5384826660156 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8688, loss_val: nan, pos_over_neg: 330.1257019042969 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8772, loss_val: nan, pos_over_neg: 427.61505126953125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.878, loss_val: nan, pos_over_neg: 415.0859680175781 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 475.59393310546875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 624.3652954101562 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 429.2597961425781 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 329.6488342285156 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8869, loss_val: nan, pos_over_neg: 238.2509307861328 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8739, loss_val: nan, pos_over_neg: 548.131591796875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 231.61514282226562 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8811, loss_val: nan, pos_over_neg: 295.8414306640625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 279.816162109375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8825, loss_val: nan, pos_over_neg: 278.2301940917969 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8802, loss_val: nan, pos_over_neg: 289.8297119140625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8823, loss_val: nan, pos_over_neg: 396.01080322265625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 495.1772155761719 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 204.40882873535156 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 490.0487060546875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 485.6562194824219 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 314.8815002441406 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 450.3526916503906 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 340.7039489746094 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8812, loss_val: nan, pos_over_neg: 192.91583251953125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 296.31610107421875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 367.8131408691406 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 297.58990478515625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8753, loss_val: nan, pos_over_neg: 280.2462463378906 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 285.903564453125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 233.55728149414062 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 360.46087646484375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 318.4286804199219 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8707, loss_val: nan, pos_over_neg: 257.1835021972656 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 427.1397705078125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 463.3018798828125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 497.46649169921875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 263.7445068359375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 323.6800842285156 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8682, loss_val: nan, pos_over_neg: 337.4404296875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 319.6629333496094 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 335.96661376953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8877, loss_val: nan, pos_over_neg: 231.92994689941406 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 270.4831848144531 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 352.75732421875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8728, loss_val: nan, pos_over_neg: 295.5950012207031 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 170.36351013183594 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 535.2041015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 288.4883117675781 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 182.3743896484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 642.6149291992188 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.868, loss_val: nan, pos_over_neg: 185.99501037597656 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 175.13070678710938 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 428.4310302734375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: 319.0596923828125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 213.3763885498047 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 260.0855407714844 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 245.4774627685547 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 265.6065979003906 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8645, loss_val: nan, pos_over_neg: 784.564453125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 324.1209411621094 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 400.55889892578125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 1762.755126953125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 297.5491943359375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 409.0596923828125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 686.1463623046875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 503.3856201171875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 311.5482482910156 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 527.108154296875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 330.3941345214844 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 410.7255554199219 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 367.3937683105469 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 570.0186157226562 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 503.461669921875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 355.94781494140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 354.681884765625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 595.1351318359375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.873, loss_val: nan, pos_over_neg: 199.05442810058594 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 262.82855224609375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 725.8319091796875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 332.44091796875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 379.22760009765625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 591.5203247070312 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 604.9830932617188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1212.1837158203125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 362.3002014160156 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 480.5890197753906 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 414.1405334472656 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 423.5791931152344 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 454.2806091308594 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 687.822998046875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 400.9823913574219 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 460.0264892578125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 327.8464050292969 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 400.49456787109375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 449.6253967285156 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 552.1898803710938 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 692.9371337890625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1101.2042236328125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 451.90814208984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 399.3565368652344 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 440.57244873046875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 455.2120361328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 586.0719604492188 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 786.8392944335938 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 408.2918701171875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 577.222412109375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 484.48614501953125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 302.2103271484375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 432.61822509765625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 1146.10888671875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 377.51910400390625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 560.0536499023438 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 1393.7645263671875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 350.58868408203125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 334.5410461425781 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 595.2517700195312 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 351.64117431640625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 479.59332275390625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 508.0940246582031 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 546.7769165039062 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 685.9225463867188 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 775.3059692382812 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 369.6273498535156 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1475.7508544921875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 452.9361267089844 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 441.6918640136719 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 483.4727478027344 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 375.044189453125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 578.0592651367188 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 492.5455627441406 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 386.68670654296875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 779.8329467773438 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 860.7576293945312 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 995.6942749023438 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 519.6853637695312 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 917.5982666015625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 354.70074462890625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 541.5726318359375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 572.2471923828125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 1278.202392578125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 354.9791564941406 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 749.1587524414062 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 763.5342407226562 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 700.523681640625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 207.39999389648438 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 737.5433349609375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 385.1487731933594 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 468.4450988769531 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 545.5149536132812 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 474.03466796875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 258.57708740234375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 343.4541931152344 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 433.6487121582031 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 409.09344482421875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 376.6317443847656 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 412.5563659667969 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 462.0089416503906 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 245.95028686523438 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 668.6800537109375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 291.08905029296875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 411.1493835449219 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 475.1204528808594 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 466.3460998535156 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 362.31878662109375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 354.4947509765625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 459.26580810546875 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 528.0426635742188 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 907.6571044921875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 598.1431884765625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 581.715087890625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 612.9237060546875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 478.8102111816406 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 589.9039306640625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 760.7168579101562 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 805.1981811523438 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 669.6482543945312 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 557.2269897460938 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 466.0688781738281 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 1279.0684814453125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 284.9652404785156 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 363.2684326171875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 458.84619140625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 332.2691345214844 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 261.1371154785156 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 543.2796020507812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 560.46923828125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 410.9308166503906 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 439.9167175292969 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 429.1470642089844 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8467, loss_val: nan, pos_over_neg: 726.2797241210938 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 554.9170532226562 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 527.9559936523438 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 590.66748046875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 643.6299438476562 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 488.77703857421875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 270.50531005859375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 519.5454711914062 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 217.72714233398438 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 347.51275634765625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 750.0604858398438 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 423.6301574707031 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 272.2481689453125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 965.3362426757812 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 215.5577850341797 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 271.2730407714844 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 1289.0401611328125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 774.3412475585938 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 229.8717041015625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 479.3948974609375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 414.2652587890625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 824.3021850585938 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 275.65771484375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 464.0544128417969 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 685.6320190429688 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 364.7319641113281 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 412.0262145996094 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 606.3860473632812 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 741.123046875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 319.65484619140625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 378.00128173828125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 356.5713195800781 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 477.1324157714844 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 594.4136352539062 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 335.91790771484375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8273, loss_val: nan, pos_over_neg: 469.435302734375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 526.4893188476562 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 386.5045166015625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 519.4515991210938 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 260.17816162109375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 403.4833679199219 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 404.8020324707031 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 298.83050537109375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 218.27069091796875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 508.4967956542969 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 442.03131103515625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 318.2485656738281 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 641.5552368164062 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 403.9620056152344 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 635.455078125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 306.8438720703125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 463.8038635253906 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 582.7650756835938 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 412.0228576660156 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 791.0733032226562 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 463.3399353027344 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 280.4111633300781 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 517.6639404296875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 441.952880859375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 310.62396240234375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 307.505126953125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 596.3780517578125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 315.4479675292969 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 427.81170654296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 721.0731811523438 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8211, loss_val: nan, pos_over_neg: 629.1632080078125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 291.3645324707031 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 397.23931884765625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 581.0455322265625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 433.16998291015625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 438.0317687988281 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 502.1142883300781 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 788.3214111328125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 657.2462768554688 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 588.4503173828125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 670.8612060546875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 446.9910888671875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 830.8610229492188 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 1103.7906494140625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1028.9052734375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 895.4664916992188 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 817.0609130859375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 1012.0709838867188 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 752.0635986328125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 760.9461059570312 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 853.9740600585938 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 594.8385009765625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 686.7787475585938 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 445.7933654785156 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1011.6453857421875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 2564.872802734375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 689.3756713867188 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 677.391357421875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1622.1322021484375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 763.5117797851562 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 326.59814453125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8221, loss_val: nan, pos_over_neg: 1557.121826171875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 1203.39794921875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 527.8980102539062 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 486.19219970703125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 566.1845703125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1307.1092529296875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 400.8914489746094 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8188, loss_val: nan, pos_over_neg: 664.4779663085938 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 539.2205810546875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8222, loss_val: nan, pos_over_neg: 629.8131713867188 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8248, loss_val: nan, pos_over_neg: 689.234619140625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 372.8727722167969 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8306, loss_val: nan, pos_over_neg: 523.424560546875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 379.9832763671875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 610.31787109375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 389.7930908203125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 423.3454284667969 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 396.8931884765625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 498.8281555175781 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 736.9615478515625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 864.4725341796875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 379.0234375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 662.6111450195312 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 408.01763916015625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 241.17196655273438 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 461.343505859375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 405.9095458984375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 567.2240600585938 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 374.5350646972656 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 724.7662353515625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 1000.7446899414062 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 428.331787109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 826.1251220703125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8256, loss_val: nan, pos_over_neg: 1435.029541015625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.82, loss_val: nan, pos_over_neg: 772.9208374023438 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 419.9820861816406 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 512.2479248046875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 685.5014038085938 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 405.7402038574219 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 957.3154296875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 1564.1080322265625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 522.9310913085938 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 512.7092895507812 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 1323.224365234375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 783.0018310546875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8241, loss_val: nan, pos_over_neg: 656.4979858398438 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 432.6850891113281 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 501.5152893066406 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 607.3228759765625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 609.7243041992188 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 553.445556640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1720.2119140625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 551.6347045898438 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 422.83453369140625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8194, loss_val: nan, pos_over_neg: 921.6117553710938 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 443.7527160644531 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 851.2319946289062 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 355.140625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 421.8660583496094 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 583.7886352539062 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 514.9639892578125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 608.9036254882812 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1518.79443359375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 550.4590454101562 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8217, loss_val: nan, pos_over_neg: 412.74066162109375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 766.8250122070312 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 544.5199584960938 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 294.1707763671875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.826, loss_val: nan, pos_over_neg: 769.6375732421875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 500.17877197265625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 360.9644775390625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1083.346435546875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8159, loss_val: nan, pos_over_neg: 767.8530883789062 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 795.8289794921875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 329.6371765136719 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 653.72509765625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 457.7151184082031 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8205, loss_val: nan, pos_over_neg: 338.3625793457031 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 721.4608764648438 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 445.2054748535156 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8231, loss_val: nan, pos_over_neg: 299.68359375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 403.680419921875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 428.34930419921875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8252, loss_val: nan, pos_over_neg: 310.9274597167969 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 393.5570983886719 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 299.6945495605469 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 278.5340576171875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 413.827392578125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8118, loss_val: nan, pos_over_neg: 426.9234313964844 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 749.1969604492188 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8199, loss_val: nan, pos_over_neg: 246.59854125976562 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 507.73626708984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1029.3585205078125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 462.5622253417969 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 492.7926940917969 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 529.7352905273438 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 701.8826904296875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 420.0191345214844 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 732.1431884765625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 380.6136474609375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 698.95703125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 593.4246215820312 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 654.4664916992188 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 625.2017211914062 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 539.3986206054688 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 624.0978393554688 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8171, loss_val: nan, pos_over_neg: 554.0811157226562 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 529.7869262695312 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.819, loss_val: nan, pos_over_neg: 506.1485595703125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 556.2318115234375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 488.1476135253906 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 594.5242919921875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 663.83642578125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 790.2598876953125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 458.42999267578125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 844.9437866210938 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 668.62255859375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 561.488525390625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 517.4143676757812 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 656.6162109375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1193.3607177734375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 926.0494995117188 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 574.7750854492188 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 570.8011474609375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1010.6051025390625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8233, loss_val: nan, pos_over_neg: 1140.2890625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 561.174072265625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8127, loss_val: nan, pos_over_neg: 1320.9908447265625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 861.7894287109375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 668.742919921875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1082.064697265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 571.1317138671875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 660.3724365234375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 412.2113952636719 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8236, loss_val: nan, pos_over_neg: 628.3108520507812 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 481.5339660644531 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 447.5116271972656 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 710.0650024414062 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 738.68359375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8248, loss_val: nan, pos_over_neg: 530.9866943359375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8161, loss_val: nan, pos_over_neg: 561.9724731445312 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 1074.3192138671875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 942.4482421875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 543.7356567382812 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 593.4419555664062 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 607.5390014648438 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 791.7787475585938 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 463.0740966796875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 876.0960083007812 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 595.8547973632812 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 366.8993835449219 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 516.6937255859375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 822.0288696289062 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 430.744140625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 590.7935180664062 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 419.6275634765625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1561.085205078125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 526.8374633789062 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8233, loss_val: nan, pos_over_neg: 329.1357727050781 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 769.378662109375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 358.9131164550781 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 309.7991943359375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8212, loss_val: nan, pos_over_neg: 496.2594909667969 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 588.8369750976562 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 581.5484619140625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 621.6235961914062 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 662.4765014648438 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 737.6992797851562 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 364.9369812011719 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 737.138427734375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 919.7787475585938 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 884.111328125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1807.579833984375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 824.24951171875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 1459.3251953125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 1588.5047607421875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1165.0386962890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 1156.145263671875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 717.0997314453125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1161.654052734375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1346.2603759765625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 616.0308837890625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 900.9297485351562 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 1017.314453125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8221, loss_val: nan, pos_over_neg: 521.3380737304688 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 555.2377319335938 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 838.7802734375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 942.4846801757812 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 1518.5374755859375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [21:13<106119:04:49, 1273.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 609.788330078125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 509.2325439453125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8182, loss_val: nan, pos_over_neg: 571.3419799804688 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 713.7899780273438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8198, loss_val: nan, pos_over_neg: 610.425537109375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 707.8052978515625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1390.362548828125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 631.4072265625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 747.34375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 819.7352294921875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 604.325927734375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 691.0167846679688 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 600.0771484375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 653.1404418945312 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 1014.4102172851562 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 535.5944213867188 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 745.2576293945312 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8181, loss_val: nan, pos_over_neg: 1722.126220703125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 969.6682739257812 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8153, loss_val: nan, pos_over_neg: 463.71923828125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 742.2012329101562 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 666.6190795898438 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8112, loss_val: nan, pos_over_neg: 481.2845153808594 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 644.1983032226562 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 511.98095703125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 685.322265625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 576.4588623046875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 694.2780151367188 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 638.6641235351562 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 568.1671752929688 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8156, loss_val: nan, pos_over_neg: 1084.57080078125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 498.6284484863281 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: 827.5286254882812 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 623.6656494140625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 422.8110046386719 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 666.0903930664062 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8171, loss_val: nan, pos_over_neg: 562.9207153320312 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8187, loss_val: nan, pos_over_neg: 670.9736328125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 943.8040161132812 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 467.1346435546875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8214, loss_val: nan, pos_over_neg: 494.99200439453125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 907.58837890625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 562.5975952148438 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 531.5568237304688 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 542.21142578125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 769.6659545898438 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 432.8582763671875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 690.6578979492188 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 878.135009765625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 940.4818725585938 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8126, loss_val: nan, pos_over_neg: 774.1168212890625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8114, loss_val: nan, pos_over_neg: 826.3255615234375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 680.29150390625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 885.7904052734375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 825.802001953125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8077, loss_val: nan, pos_over_neg: 708.6417236328125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 479.94775390625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1026.6038818359375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 990.5628051757812 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1010.064697265625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 576.6776733398438 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 851.3173217773438 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 1029.97216796875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1971.18896484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 863.3262329101562 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1443.4677734375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8145, loss_val: nan, pos_over_neg: 1957.4205322265625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 885.399658203125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1134.18994140625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 1234.9578857421875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 546.58935546875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 675.9906005859375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 762.2147827148438 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1147.9114990234375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 907.6568603515625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 820.5736694335938 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 618.7177124023438 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 476.9209289550781 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 413.63153076171875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 582.0996704101562 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 401.52685546875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 584.7890625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8133, loss_val: nan, pos_over_neg: 423.71697998046875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 431.01409912109375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1144.64306640625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 506.0931396484375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 439.46258544921875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 785.8652954101562 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 1242.9561767578125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 463.10003662109375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 454.2681579589844 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8157, loss_val: nan, pos_over_neg: 940.432373046875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 627.5760498046875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.815, loss_val: nan, pos_over_neg: 308.8321838378906 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1195.6778564453125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 4581.9716796875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 596.410400390625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 584.9486694335938 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8114, loss_val: nan, pos_over_neg: 707.2216186523438 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 661.6134643554688 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 820.24560546875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 434.7679443359375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.812, loss_val: nan, pos_over_neg: 527.6776123046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 485.6877136230469 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 747.509521484375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 728.3947143554688 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 509.7876281738281 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 735.481689453125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 749.9902954101562 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 898.0697631835938 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 641.7564086914062 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 891.26123046875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 552.63330078125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 484.2884826660156 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 809.3671875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8059, loss_val: nan, pos_over_neg: 573.3519287109375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 554.6560668945312 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 321.7773742675781 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 702.1954956054688 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 785.2557983398438 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 496.39031982421875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 451.9810791015625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1602.530517578125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8116, loss_val: nan, pos_over_neg: 729.9273681640625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 431.6014404296875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 513.8038330078125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 714.591796875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 771.0302734375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 759.6730346679688 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2407.098876953125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 446.28466796875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 552.1209106445312 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1319.862060546875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 597.5422973632812 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 966.4207153320312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 514.8787841796875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 854.1151123046875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 504.4775390625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 736.4938354492188 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 584.9454956054688 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 626.4812622070312 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 1488.58203125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 572.2113647460938 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 744.0607299804688 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 882.5347290039062 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 725.0050659179688 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 377.5912170410156 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8074, loss_val: nan, pos_over_neg: 1496.46337890625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1271.29931640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 1908.870849609375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 398.51385498046875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 949.2787475585938 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 599.6538696289062 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 529.1272583007812 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 925.6937255859375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 706.4840698242188 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 915.3634643554688 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 931.9791870117188 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1119.42236328125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 894.66259765625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 661.1975708007812 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1121.930908203125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 551.2096557617188 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 443.03887939453125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1088.3707275390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 661.6842651367188 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 411.745361328125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 746.3009033203125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 554.677734375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8104, loss_val: nan, pos_over_neg: 487.26507568359375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 540.498779296875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 659.4892578125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 488.52984619140625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 688.9338989257812 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 723.1837768554688 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 481.41888427734375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1123.5809326171875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 629.9379272460938 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8035, loss_val: nan, pos_over_neg: 778.5667114257812 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 529.8375244140625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 572.974365234375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 511.6713562011719 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 806.3685302734375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 394.4627380371094 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 619.4620361328125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 642.3604736328125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1745.692138671875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 657.024169921875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 467.34368896484375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 555.5103149414062 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 1220.5621337890625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 495.8842468261719 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1107.5015869140625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 511.59814453125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 953.2023315429688 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 605.1292724609375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 453.5714111328125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 724.1434936523438 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 471.9178466796875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 810.8889770507812 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 475.1126403808594 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1113.6470947265625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 454.8326110839844 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 729.7368774414062 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 689.522705078125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 526.4653930664062 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 964.810302734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 733.65478515625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 544.0858764648438 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 538.47900390625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1028.1812744140625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 528.2865600585938 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 500.15118408203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1357.3035888671875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1285.616455078125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 563.271728515625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 953.0883178710938 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1706.328857421875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 555.3450317382812 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 482.1121826171875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 662.2210083007812 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 1269.681640625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 834.6310424804688 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 583.8138427734375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 766.8526611328125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1329.427734375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 379.1914978027344 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 407.4989929199219 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 775.1843872070312 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 564.5889892578125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 437.3827209472656 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 694.1936645507812 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 589.5076293945312 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 350.09210205078125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 650.8584594726562 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 809.1246337890625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 364.93157958984375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 476.8314208984375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 1191.5677490234375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 462.663330078125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 438.79052734375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 422.326171875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 976.7871704101562 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 630.05419921875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 422.9144592285156 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 1107.265625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 685.553466796875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 2037.153564453125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1369.7939453125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8033, loss_val: nan, pos_over_neg: 441.77630615234375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 621.0774536132812 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1027.6153564453125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1377.176513671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 716.849365234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 709.7009887695312 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 991.427490234375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1140.4207763671875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1030.417724609375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 708.7573852539062 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 388.0227966308594 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 536.25927734375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1380.181396484375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 345.29119873046875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 589.754150390625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1148.2320556640625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1240.2593994140625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1203.75 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 608.3988647460938 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 464.4737548828125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 414.74261474609375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 539.1277465820312 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 633.2932739257812 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 517.162353515625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1399.8665771484375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 915.3469848632812 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 643.8460083007812 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1072.1856689453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 352.9810485839844 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 543.9840698242188 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 669.2637329101562 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 515.2654418945312 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1003.751708984375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 819.2607421875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 581.6222534179688 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 754.38916015625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 626.8073120117188 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 848.4281616210938 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 408.3953857421875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 459.1434631347656 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 353.16937255859375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 706.7361450195312 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 441.9649658203125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8109, loss_val: nan, pos_over_neg: 399.5171813964844 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 702.255859375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 428.00677490234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 624.9161987304688 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 2035.51513671875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 858.6246337890625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 814.7723999023438 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1299.0052490234375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 1411.606689453125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 791.1026000976562 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1112.9937744140625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 964.1653442382812 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 983.2815551757812 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1271.3326416015625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1232.736572265625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 2522.7373046875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 839.71484375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1240.4862060546875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 850.1572265625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 3219.88427734375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 820.1143798828125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 663.8670654296875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2460.914794921875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1438.55419921875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 797.7960815429688 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1900.38720703125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 847.8467407226562 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 1281.2822265625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1304.142822265625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 586.3984985351562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 525.8903198242188 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 5131.72265625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 725.3682861328125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 757.4830322265625 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 406.6026916503906 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 692.778564453125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 752.1546630859375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 831.1854858398438 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1360.3663330078125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1838.1871337890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 827.9458618164062 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 600.8353271484375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 580.6343383789062 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 883.4246826171875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 552.4899291992188 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 586.94677734375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 791.4200439453125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 1339.3870849609375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 453.0027160644531 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 7469.38818359375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 778.3904418945312 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 810.344970703125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 476.1302490234375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 589.365234375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 1041.2965087890625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 754.71826171875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 571.1768798828125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 682.581787109375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 957.7449951171875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 552.4754028320312 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 494.1650390625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 653.9799194335938 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 1162.9896240234375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 684.8333129882812 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 936.2760620117188 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 517.8302612304688 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 953.0446166992188 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 837.2146606445312 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1688.33740234375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 1053.4822998046875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1392.42041015625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1006.1851806640625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 1310.73486328125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1173.567138671875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 502.34979248046875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 918.6875610351562 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 702.9430541992188 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 507.5233154296875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 647.7637329101562 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 937.2047729492188 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1223.985107421875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 461.7417297363281 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 537.5547485351562 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 2691.161376953125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 791.2208862304688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 854.632568359375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 861.8467407226562 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 768.6116333007812 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 2291.241943359375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 908.302490234375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 965.912353515625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 946.7808227539062 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1576.6702880859375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 774.6422729492188 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 393.9298095703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 717.65576171875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 832.7353515625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 695.0101928710938 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1697.939453125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 449.69012451171875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 849.6024780273438 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 531.2716064453125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8011, loss_val: nan, pos_over_neg: 513.5401611328125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 704.326171875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 835.9163818359375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 784.92724609375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 885.9083251953125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1322.1885986328125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1303.4716796875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 562.3860473632812 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 350.2685241699219 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 2611.54638671875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 836.3932495117188 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 675.9807739257812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1222.7318115234375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7916, loss_val: nan, pos_over_neg: 576.6085815429688 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 789.4427490234375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 895.451416015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 432.7040710449219 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 565.373291015625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 628.244873046875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 482.5628662109375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 711.507080078125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1030.456298828125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1761.7659912109375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 376.78656005859375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 419.1022033691406 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 496.47088623046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1935.79443359375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 650.1573486328125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 490.7609558105469 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 975.90380859375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 763.24658203125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 780.9609375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 459.1907653808594 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 468.73114013671875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1064.760986328125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 647.1942749023438 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 591.1923217773438 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 751.3092651367188 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 974.28173828125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7939, loss_val: nan, pos_over_neg: 688.9762573242188 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 751.4356079101562 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 711.6565551757812 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 844.9815673828125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 600.30419921875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 2479.969970703125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1119.589111328125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 842.3906860351562 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 945.3848266601562 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 823.5054931640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 439.2279052734375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 709.7883911132812 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 566.8765869140625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 364.74359130859375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 481.0936279296875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 781.9390869140625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 483.9427185058594 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 458.0205993652344 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 855.1494750976562 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 412.72564697265625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 588.1674194335938 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 508.3714294433594 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 915.8094482421875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 450.5770568847656 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 485.8924255371094 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 565.0718383789062 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1076.81982421875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 795.916259765625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 538.5718383789062 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 562.3417358398438 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 918.419677734375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 922.339599609375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 663.3353881835938 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 567.4327392578125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 855.4146118164062 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 761.3186645507812 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 628.0177612304688 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 524.0032958984375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 737.82861328125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 727.2979736328125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 514.7799682617188 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 616.5540771484375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 475.2095031738281 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 574.3087768554688 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 384.5555725097656 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1054.5135498046875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 617.9312744140625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1116.3341064453125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 908.8895263671875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 399.04876708984375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 575.3433227539062 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 547.5440673828125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 642.891357421875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 381.09844970703125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 534.5838012695312 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 641.5181884765625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1046.93701171875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 659.5950317382812 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 652.0220336914062 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 651.2857666015625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 736.7905883789062 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 556.5177001953125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 525.8138427734375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1615.0989990234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 355.8753356933594 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 799.5130615234375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 538.9649047851562 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 853.4640502929688 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 904.007080078125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 452.8295593261719 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 797.6915283203125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 715.9428100585938 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 950.9528198242188 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 542.99365234375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 607.363525390625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 411.1937255859375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 644.01611328125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1132.204833984375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1238.2603759765625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 673.9238891601562 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 476.177978515625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 988.4356689453125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 714.416748046875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 545.6596069335938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 233.50277709960938 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 919.6904296875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 721.7051391601562 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1191.03125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 832.886474609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 826.900146484375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 747.0029296875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 1464.94921875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 916.5631713867188 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 769.144287109375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 2048.2392578125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 850.9757080078125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 472.21405029296875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 5045.017578125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 608.2400512695312 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1123.7274169921875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 918.072998046875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 952.9575805664062 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 755.6011352539062 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 867.091064453125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1464.201904296875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 814.6896362304688 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1119.98974609375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 498.4050598144531 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 918.123779296875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1824.794921875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 419.4953308105469 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 468.1632080078125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 930.8477783203125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 554.0404052734375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1125.630859375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 669.3864135742188 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 524.4058837890625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 981.7482299804688 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 1140.761962890625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 509.9118957519531 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 567.1012573242188 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 967.0340576171875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1221.97705078125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1382.273681640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 489.5352478027344 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 864.6249389648438 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 5173.6708984375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 1424.85400390625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1628.54736328125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 662.173095703125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 966.2130126953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 568.497314453125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 4394.38427734375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1579.305908203125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 979.0402221679688 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1042.0577392578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 634.2376098632812 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1314.979736328125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 680.8152465820312 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 687.4452514648438 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 636.9102172851562 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 591.02783203125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 2295.084228515625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1565.9630126953125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 633.1351318359375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1795.688720703125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 2536.34521484375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 729.6556396484375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 485.41571044921875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1011.8912353515625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1977.854248046875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 713.9539794921875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 769.4133911132812 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 713.9878540039062 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 2963.69580078125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2065.366943359375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1191.22998046875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1526.9940185546875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 2038.7755126953125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 2456.77294921875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1023.7263793945312 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 783.47216796875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 787.4248046875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 828.8035278320312 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 446.13055419921875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1036.0113525390625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 447.2882080078125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 464.4985046386719 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 399.6627197265625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 846.5247802734375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1553.1514892578125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1043.149658203125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 924.8990478515625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 801.9793701171875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 993.3822021484375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 2790.947021484375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1209.208251953125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 940.8840942382812 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1567.3616943359375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1248.099853515625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 839.3660278320312 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 2580.269775390625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1370.547119140625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1107.3336181640625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1725.7354736328125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 758.4631958007812 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 522.9207153320312 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 541.68505859375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 607.6920166015625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 538.9068603515625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 474.7742004394531 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1287.1876220703125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 475.5709533691406 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1319.4661865234375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 821.7785034179688 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1006.2268676757812 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 567.4542846679688 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 728.931640625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1196.9398193359375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 683.0709838867188 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 468.2001953125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 544.9006958007812 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 1415.428466796875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1068.4754638671875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 577.3253173828125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1298.4959716796875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 596.0494995117188 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 805.581787109375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 949.6702880859375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 489.67626953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 633.4331665039062 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 655.5736694335938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 794.2447509765625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 873.3569946289062 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 691.7037963867188 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 934.403564453125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 475.15869140625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 2724.187744140625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1442.178955078125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1197.5582275390625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1074.17138671875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1508.63671875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 835.5635375976562 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 591.2799072265625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 791.0457153320312 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 868.0714111328125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 949.0062866210938 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1208.9310302734375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 718.233154296875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 653.6909790039062 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 671.6427001953125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 780.1256713867188 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 668.460693359375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1510.4005126953125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 714.033203125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1105.24462890625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 3927.43408203125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 746.927001953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 937.7252807617188 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1695.0382080078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1270.69287109375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 569.8968505859375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1372.7442626953125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 791.3941040039062 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1509.4503173828125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 829.218017578125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1506.9580078125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1965.43115234375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1013.5216064453125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1843.3212890625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 1186.5989990234375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 863.2474365234375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 804.469482421875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1836.9237060546875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 582.4686279296875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 924.47900390625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 783.1080322265625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 794.6357421875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1150.5272216796875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1365.043701171875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1038.77783203125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 934.3685302734375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [42:09<105272:56:31, 1263.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 811.5289916992188 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1319.1134033203125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1285.5338134765625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1342.169677734375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 590.3429565429688 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1870.6231689453125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 2697.01806640625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1080.3848876953125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1383.94140625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 787.5067749023438 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 4712.591796875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1092.5882568359375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1211.0230712890625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 506.8359069824219 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1085.86474609375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1895.6241455078125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1662.430908203125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 866.0725708007812 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 941.968994140625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1251.0587158203125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1523.4246826171875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1608.8037109375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 661.2158813476562 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1006.6263427734375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 701.7608642578125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 432.5838317871094 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 470.2866516113281 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1315.8472900390625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1175.60302734375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 619.05224609375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1256.8115234375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1780.4666748046875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 647.605224609375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 617.0983276367188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 563.6981811523438 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 689.55615234375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 809.8182373046875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 493.247314453125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 1206.285888671875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 740.568115234375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1347.3675537109375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1269.5350341796875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 654.7665405273438 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 645.41943359375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1310.4649658203125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 552.1231689453125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2775.614990234375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 469.70977783203125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1230.452880859375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1208.31982421875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 861.2926025390625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 544.105224609375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 560.1873779296875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1179.2291259765625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 580.5245971679688 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2220.66552734375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 598.4481811523438 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1368.1766357421875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1198.6201171875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 675.9517211914062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 756.2680053710938 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1129.9287109375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 443.25250244140625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 551.9642333984375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 811.1227416992188 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 431.22393798828125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 521.6239624023438 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 508.14862060546875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1541.8717041015625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1126.813720703125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 552.2098388671875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 534.930908203125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 800.2608642578125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 724.613037109375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 687.4182739257812 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 597.4465942382812 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 533.8441162109375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 794.6596069335938 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 598.93212890625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 542.1268310546875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1760.49169921875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1698.8759765625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1306.9205322265625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 647.3949584960938 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 639.3165893554688 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 654.6743774414062 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 2102.08349609375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 763.9010620117188 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 690.3474731445312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 573.317138671875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 926.2449340820312 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 494.3186950683594 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1569.3162841796875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 662.79345703125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 803.0112915039062 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 4352.79150390625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1055.9896240234375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 613.782470703125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 514.775634765625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 994.7977294921875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 739.5504760742188 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 551.4151000976562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 791.031005859375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 897.0352783203125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1129.2357177734375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1010.0 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 956.6363525390625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 564.5003662109375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 500.13134765625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 762.6665649414062 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 682.622802734375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 456.1044616699219 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1079.652099609375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 551.5320434570312 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1189.60791015625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 22821.869140625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 816.8944702148438 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1367.728271484375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 907.3109130859375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1143.4127197265625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 563.90087890625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 5240.79443359375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 680.2759399414062 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 939.8267822265625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2534.4853515625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: -5357.8203125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1147.2578125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 857.8399047851562 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 817.7559204101562 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 966.7450561523438 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 646.5316772460938 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 655.0104370117188 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 394.4474182128906 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 835.2550659179688 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 864.421142578125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 913.7135009765625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 4155.345703125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 951.167236328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 704.9521484375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 892.8331298828125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 965.2677001953125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 657.1910400390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1116.6806640625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1008.0513305664062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 497.802978515625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 565.9790649414062 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 872.2449340820312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 695.1626586914062 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 978.773193359375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 541.96728515625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 801.1641845703125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1255.69873046875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1557.6248779296875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 523.3513793945312 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 516.354736328125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 738.3433837890625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 931.9771118164062 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1067.238037109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 902.0706176757812 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 648.2310180664062 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 997.1251831054688 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1602.2322998046875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 683.5198364257812 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1049.7509765625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1004.8988037109375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 667.6837158203125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 509.6638488769531 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 904.2144775390625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1330.4989013671875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 586.113525390625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 973.5732421875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 481.71234130859375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 790.5338745117188 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1947.63671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 774.9302978515625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1182.5731201171875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 721.0890502929688 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 6367.490234375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 629.0234375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1038.5675048828125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1060.2059326171875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1179.5362548828125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 497.626708984375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 532.6080322265625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 538.45166015625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 613.1852416992188 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 493.6982421875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 847.0479736328125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 900.7723999023438 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 720.294189453125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 489.608154296875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 790.576416015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 524.88427734375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 676.9751586914062 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1295.5517578125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 648.3065185546875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 926.8937377929688 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 720.6679077148438 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1132.1502685546875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1268.9202880859375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 507.5629577636719 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 751.5645751953125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 545.3063354492188 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2016.0457763671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1493.79296875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 585.0651245117188 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 512.1046752929688 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 651.8179931640625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1273.2501220703125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1870.615234375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 820.592529296875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 432.7087097167969 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 846.0430297851562 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 667.9352416992188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1230.3138427734375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 680.8118286132812 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 744.5721435546875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 727.456298828125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1417.8201904296875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1259.663818359375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 676.3740234375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 626.810546875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1077.0343017578125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 509.69781494140625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1514.5247802734375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1092.2203369140625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1002.02880859375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1875.2606201171875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 1116.17578125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1277.6519775390625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2420.080078125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1019.64990234375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 730.1832275390625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1314.8836669921875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 962.7462158203125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 819.47314453125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1133.552490234375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1043.2672119140625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1055.765625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 834.51611328125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1148.5093994140625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 564.0223999023438 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 816.5730590820312 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 503.17913818359375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 603.5535278320312 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 557.4090576171875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 648.9515380859375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1795.637451171875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1173.027587890625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1344.8118896484375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 849.9795532226562 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 763.5664672851562 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 607.9702758789062 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 550.6439819335938 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 536.9638061523438 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 759.2957153320312 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1145.3463134765625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1142.0255126953125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 675.8765869140625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 888.8177490234375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 808.103271484375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 911.0999145507812 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 852.4762573242188 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 663.416259765625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 404.98077392578125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 736.6746215820312 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1018.3807983398438 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1228.84423828125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 654.2127075195312 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 543.1530151367188 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 613.054931640625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 904.3578491210938 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1010.4837036132812 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 849.9281005859375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1195.333251953125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1148.4874267578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 566.8056640625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1688.1646728515625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1375.2349853515625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 711.6692504882812 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 725.5162963867188 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 653.1211547851562 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1195.8662109375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1203.5809326171875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 702.734375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 600.42236328125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1139.59033203125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1017.1299438476562 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 427.1745910644531 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 774.4566040039062 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1036.779052734375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 753.01416015625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 724.7022705078125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 523.6077880859375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1283.5201416015625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 838.8558349609375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1265.079833984375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1581.742919921875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1527.370361328125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 571.0125732421875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1795.186279296875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 786.2492065429688 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 721.20361328125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 562.314453125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1122.281982421875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1189.778076171875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 695.6051635742188 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 704.2625122070312 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 828.4472045898438 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 591.3353271484375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 829.8283081054688 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1044.738037109375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 601.1853637695312 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 611.4470825195312 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 579.0167236328125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1118.459716796875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 862.0068359375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 592.2293090820312 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 743.0355834960938 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 559.7942504882812 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 621.375244140625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 523.5669555664062 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 552.2941284179688 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 574.7185668945312 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 471.9261169433594 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 849.7578735351562 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1013.2236938476562 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 918.5974731445312 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 518.5700073242188 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 572.5247192382812 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1086.845703125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 914.308837890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 531.730712890625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 677.0490112304688 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 821.7542724609375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 745.277099609375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 973.478271484375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 548.2317504882812 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 560.5201416015625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1767.767333984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 583.4246826171875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 851.9226684570312 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 396.4991760253906 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 571.5182495117188 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 955.52490234375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 770.4091186523438 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 567.2261352539062 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 788.5796508789062 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1355.216796875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 684.1162109375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 936.6747436523438 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 721.0297241210938 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 805.4459838867188 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 611.3428955078125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 920.058837890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1330.7489013671875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 687.191162109375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 784.1804809570312 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 902.1431274414062 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 379.2627258300781 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1839.4296875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1215.5367431640625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 459.1734619140625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 737.475830078125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 553.65283203125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1194.33447265625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 776.1730346679688 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 755.6879272460938 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1672.404541015625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1548.4111328125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1308.909423828125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 893.255859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 639.783447265625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1136.7210693359375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 647.0089721679688 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1340.2301025390625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 830.7685546875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 628.2515258789062 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1602.4356689453125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 3236.17529296875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1092.962158203125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1107.7911376953125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 652.4555053710938 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 726.0314331054688 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1258.1976318359375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 667.7543334960938 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 552.2780151367188 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 774.8259887695312 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1208.0313720703125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 2315.0810546875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 2360.331298828125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1576.2947998046875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1005.912109375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1194.46240234375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2299.115966796875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1263.3189697265625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 721.798583984375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 764.9136352539062 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1210.7235107421875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 866.011474609375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1081.819580078125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 841.126953125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1044.9757080078125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 2775.81005859375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 921.604248046875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 612.1952514648438 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 688.7406005859375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1098.52099609375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 519.8298950195312 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1670.468017578125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 809.538330078125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1031.7698974609375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 977.446044921875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 674.2670288085938 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 710.6322021484375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 505.0550231933594 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 705.3370361328125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1413.1495361328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1326.931884765625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1085.4085693359375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1364.75 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 640.1588134765625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1593.16162109375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 773.4702758789062 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1067.29052734375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 700.2268676757812 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 906.9965209960938 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1133.5498046875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 933.9119262695312 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 503.75079345703125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1064.2098388671875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1234.4068603515625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1424.7979736328125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 912.7285766601562 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 759.6300659179688 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 761.9395141601562 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 531.29931640625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1714.6240234375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 833.0526123046875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 718.5899047851562 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 641.8343505859375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 618.098876953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 612.6192626953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 426.59033203125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 523.4669799804688 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1059.1622314453125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 754.8872680664062 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 507.3439636230469 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 487.99560546875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 986.7271118164062 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1748.6910400390625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 717.3541870117188 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1320.48095703125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 771.6140747070312 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 874.2329711914062 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 528.41845703125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 821.9847412109375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 674.664306640625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 518.6862182617188 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1592.0367431640625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1076.26318359375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 854.2095947265625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 453.3673400878906 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 935.71435546875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 956.6838989257812 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1148.08251953125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 965.1085815429688 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1773.2020263671875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 833.417724609375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 834.1328735351562 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 2268.813232421875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 797.016845703125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 516.9354248046875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 506.3650817871094 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 640.7673950195312 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 842.2896728515625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1020.0264282226562 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 633.130126953125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 566.1823120117188 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1074.1810302734375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 849.501953125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 797.0001831054688 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 752.3306274414062 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 400.04608154296875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 556.3753051757812 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 418.1819763183594 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 781.6906127929688 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1463.6900634765625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 915.2453002929688 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 593.3728637695312 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 629.5087890625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1645.015380859375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 936.0004272460938 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 852.4406127929688 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1075.6724853515625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 981.3570556640625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2080.522216796875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1487.3594970703125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1223.5352783203125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 559.2947387695312 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1045.810791015625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 630.7171630859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1089.697998046875 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1048.7440185546875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 747.8216552734375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1030.7064208984375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1121.104736328125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 822.3804321289062 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1715.178466796875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 809.8180541992188 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 602.94384765625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1009.0614013671875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 469.3238830566406 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 687.3101806640625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 639.9110107421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 961.9983520507812 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1268.5595703125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1334.0478515625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 544.9404296875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1388.4168701171875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 1118.3382568359375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 676.9843139648438 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 699.4033203125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 3247.06640625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 828.5148315429688 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 742.4534912109375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1242.222900390625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 5477.650390625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1342.670654296875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1045.5123291015625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1339.521484375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 2962.775634765625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 792.907958984375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 546.786376953125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1502.67431640625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1404.0142822265625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1278.6580810546875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 599.16943359375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 828.7637939453125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1424.1268310546875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1342.7376708984375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 2262.539794921875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1391.0948486328125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 954.5677490234375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: -8072.74365234375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 638.1166381835938 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2837.23388671875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1068.719482421875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 2013.33447265625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1836.317138671875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1260.5770263671875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 690.76025390625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 2460.011474609375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 732.3938598632812 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1019.7047729492188 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1265.986083984375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1550.46533203125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1850.4532470703125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 892.1719360351562 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 677.2322387695312 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 2508.126708984375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1088.6805419921875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 752.0841064453125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1410.8577880859375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 723.06884765625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1845.125244140625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 628.0984497070312 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 950.1923217773438 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1491.59033203125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1809.120849609375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1333.300048828125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1305.474609375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 890.5625610351562 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 617.2571411132812 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1060.1488037109375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 507.1881408691406 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 815.1095581054688 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1869.413330078125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 712.7205200195312 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1068.90380859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1313.4490966796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1156.4083251953125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 919.6776123046875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1251.3480224609375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1045.1947021484375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 910.010986328125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1939.5821533203125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1317.4232177734375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 876.8552856445312 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1699.3590087890625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1047.6307373046875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 2832.053955078125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 765.1513671875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 942.474853515625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1190.5091552734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 678.5650634765625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1099.3818359375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 803.06005859375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1999.197021484375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 787.9405517578125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 2039.007568359375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 830.5236206054688 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 668.3228759765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 823.9531860351562 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 487.9786376953125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 608.4600830078125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 896.698486328125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 873.3352661132812 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 3748.11083984375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 843.3680419921875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1746.7777099609375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 5939.1396484375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2373.116943359375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 597.5460815429688 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 476.99847412109375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 621.0660400390625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1259.8013916015625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 551.1543579101562 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 538.862060546875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 872.1337280273438 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 965.5403442382812 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 546.5408935546875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 587.0694580078125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1048.5621337890625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1458.0208740234375 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 913.6638793945312 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 773.7337036132812 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2332.80126953125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 904.7278442382812 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 3901.305419921875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 801.1605224609375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 441.4413757324219 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1221.2781982421875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 10553.66796875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1433.5953369140625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 763.8890991210938 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1862.6978759765625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 2847.5009765625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 2739.54931640625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 982.8090209960938 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 446.4375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 960.0236206054688 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1118.9669189453125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 870.3377685546875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 940.76171875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 851.2229614257812 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1016.5455932617188 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1613.9071044921875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1207.5826416015625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 633.5304565429688 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 496.748046875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 913.1279907226562 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1657.9134521484375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 781.427978515625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1333.699951171875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1407.5196533203125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 904.0305786132812 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1488.3359375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1182.527099609375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1028.4659423828125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 733.3531494140625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 2438.6552734375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 981.9487915039062 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 743.1066284179688 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1022.3301391601562 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 13400.541015625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 642.0006713867188 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 2206.434326171875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1756.2147216796875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1214.226806640625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1213.5948486328125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 658.8509521484375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 762.5130615234375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1145.8924560546875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1441.4114990234375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1350.1402587890625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 863.8484497070312 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 761.3350219726562 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1615.3057861328125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 559.0734252929688 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1185.3199462890625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 552.7955322265625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 586.0205078125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1106.15380859375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 590.6190185546875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 752.07421875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 626.1209106445312 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1254.66259765625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 730.9546508789062 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 624.0726318359375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 889.3692626953125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 564.5620727539062 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 361.1278076171875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1083.3765869140625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 871.2781372070312 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:03:20<105580:15:23, 1266.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 875.8759765625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1111.1373291015625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 902.0773315429688 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 892.1907958984375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 769.300048828125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 616.8064575195312 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 865.061279296875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1197.7471923828125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 991.906494140625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 434.4156799316406 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 875.6856689453125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 864.9790649414062 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 554.1574096679688 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 684.4320678710938 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 897.5399169921875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1312.587646484375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 709.6805419921875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 377.3351745605469 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 755.4008178710938 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 810.8510131835938 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1087.3525390625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 659.7057495117188 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 667.4532470703125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1166.27783203125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1270.7291259765625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 801.8967895507812 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 907.20556640625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 955.8851928710938 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1228.214111328125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 798.53125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 911.3174438476562 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1505.8055419921875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 909.4244384765625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1721.395751953125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1036.2125244140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1824.3837890625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 901.4558715820312 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1318.9368896484375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 960.0773315429688 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 692.4721069335938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 2976.954345703125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1122.09912109375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1986.068359375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1412.865966796875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1313.912353515625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 606.4940185546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1873.2701416015625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 861.7344970703125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1432.0985107421875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 734.3942260742188 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1293.200927734375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1251.0159912109375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1447.2105712890625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 846.3413696289062 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 958.4971923828125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1256.224609375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1020.8329467773438 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1189.8330078125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 914.8994750976562 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 667.6170654296875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 948.0462036132812 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1373.2113037109375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1136.6158447265625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1742.009765625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 4011.604248046875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1435.0985107421875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 926.1871948242188 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 802.0321655273438 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1086.2459716796875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 768.1984252929688 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 767.5415649414062 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 631.8369750976562 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 899.755859375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 745.8598022460938 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 740.8951416015625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 680.1044311523438 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 751.8527221679688 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1005.7022705078125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 809.362548828125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 874.856201171875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 696.5739135742188 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 397.1595153808594 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1701.5198974609375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1182.97900390625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 2361.085205078125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1212.0567626953125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 831.6763305664062 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 901.0339965820312 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 857.4807739257812 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1399.5789794921875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 896.9077758789062 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 644.1498413085938 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 500.7955627441406 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1214.924072265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 562.5505981445312 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 537.3587036132812 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 473.78173828125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 586.0119018554688 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 596.286865234375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 775.0023803710938 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 692.0517578125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 627.5361328125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 588.015869140625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 529.9180908203125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1000.48681640625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 854.0386962890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 669.1203002929688 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 679.3797607421875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1348.990966796875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1320.4873046875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1366.089111328125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 867.2508544921875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 856.6878051757812 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 9618.333984375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1365.3172607421875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 2443.1181640625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 798.0803833007812 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 835.9549560546875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 844.3269653320312 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 851.9821166992188 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 5080.96728515625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 815.787109375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 812.9288940429688 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 822.966552734375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1294.2513427734375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 3032.39404296875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 622.8580932617188 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 859.1392211914062 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 990.0372924804688 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 2694.66650390625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1330.3453369140625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1114.308349609375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 726.2156372070312 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 756.540771484375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 673.79736328125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7867, loss_val: nan, pos_over_neg: 843.95751953125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1440.3240966796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1708.418212890625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1278.8941650390625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 646.2225341796875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 709.296875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1393.926025390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1230.9085693359375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 794.6607055664062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 763.21630859375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 901.3426513671875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 617.2284545898438 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1223.3685302734375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1041.7288818359375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 881.1823120117188 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1294.0916748046875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1435.3035888671875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 963.361572265625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 6248.8271484375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1458.654052734375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1383.6434326171875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 968.478271484375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1189.7332763671875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 885.1382446289062 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 713.5057983398438 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1022.7745971679688 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1528.282470703125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 602.4728393554688 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 979.4019775390625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1110.934326171875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 417.6190490722656 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 545.57275390625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1504.67236328125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 533.4021606445312 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 472.5212097167969 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2104.514892578125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1305.677490234375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 951.23681640625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1015.4517211914062 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 845.4816284179688 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 451.0108947753906 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1211.907958984375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 633.0623779296875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 618.7477416992188 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 932.0673217773438 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 877.218994140625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 972.064208984375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 578.2598266601562 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 882.402587890625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1005.7171630859375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 4701.8359375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 759.5706176757812 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2206.488525390625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1214.552978515625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1930.7135009765625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1299.619140625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 973.5474853515625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1775.1661376953125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1229.95947265625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1028.3814697265625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1082.8427734375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 814.0960083007812 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1289.3671875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 662.4937133789062 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 664.2606811523438 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1081.567138671875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 3010.958984375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 885.1428833007812 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 863.6531982421875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1357.6407470703125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1070.0576171875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 957.294677734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1269.957275390625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 2678.96240234375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1818.931640625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1062.1141357421875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1994.8873291015625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1206.48681640625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 557.8161010742188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1533.9595947265625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 3443.65283203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 825.5445556640625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1301.082275390625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1749.87548828125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 853.9752807617188 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1713.0245361328125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1330.8880615234375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 573.3327026367188 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1191.6070556640625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1301.7176513671875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 724.5172729492188 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 796.7622680664062 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1423.8233642578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1754.295166015625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1429.6580810546875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1098.3956298828125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1309.4697265625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 719.4903564453125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 983.0704345703125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1562.9864501953125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 674.3723754882812 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 885.3108520507812 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1006.8851318359375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 732.40673828125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 955.3851928710938 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 921.116455078125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 467.6567687988281 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 623.3914184570312 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1330.6007080078125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 727.173828125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 448.7243347167969 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 554.7251586914062 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 809.8515625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 808.654541015625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 789.9290161132812 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 851.7061767578125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1281.722412109375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1063.0443115234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1161.73291015625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 988.8668212890625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 997.8893432617188 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 2203.22265625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 520.710693359375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 914.189453125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 2349.18896484375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 2874.303955078125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1024.520751953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1937.638916015625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1740.2716064453125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1530.8612060546875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1216.909912109375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 4350.650390625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1869.833251953125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 614.249267578125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 908.1127319335938 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 792.0078735351562 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 2572.135009765625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 808.9523315429688 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 813.5228881835938 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2518.474853515625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1555.8321533203125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 595.4767456054688 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 861.6939086914062 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1417.1715087890625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 991.9840087890625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 777.4060668945312 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 854.7581176757812 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 713.017578125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 921.3768310546875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1626.318359375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 835.770751953125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1035.0863037109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1100.765625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1269.166748046875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 762.7055053710938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 937.3164672851562 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1073.9840087890625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1265.9228515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 816.8826904296875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1696.093505859375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 894.8734741210938 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 588.4359130859375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 881.7174682617188 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1797.7366943359375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1055.7781982421875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1241.545654296875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 977.299560546875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1200.7191162109375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 697.9472045898438 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1179.334228515625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 3758.952880859375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1067.1575927734375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1602.175048828125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2232.82177734375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1579.6929931640625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1117.677734375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 5148.494140625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 530.5411987304688 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1717.5947265625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1070.0438232421875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 465.0778503417969 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1273.3822021484375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1549.2589111328125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1659.4261474609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 3137.048583984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 4007.960693359375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1356.3662109375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 810.97509765625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1935.844482421875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 559.4116821289062 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1338.0694580078125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 442.9588317871094 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 475.6374206542969 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 633.2365112304688 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1026.38818359375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1545.1864013671875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1728.5787353515625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1076.9261474609375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1288.862548828125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1194.4732666015625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 681.0050048828125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1690.404296875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1492.7325439453125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1405.3426513671875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1171.676025390625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1781.4427490234375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1554.867431640625 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1721.77392578125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1733.2823486328125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1317.2093505859375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2843.045166015625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 911.5860595703125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 907.613525390625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1857.1458740234375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1783.9686279296875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1199.5626220703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1077.983154296875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 2015.5604248046875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 7685.70068359375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1645.591552734375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1055.2783203125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 2064.252685546875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1823.064453125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1192.30029296875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 920.2473754882812 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 4808.46826171875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 612.0671997070312 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1124.22900390625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 893.90283203125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 7118.23583984375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 5589.36328125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 2036.697265625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 2251.45703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1629.8074951171875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 877.4320068359375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1601.2222900390625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1985.2945556640625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 7590.55517578125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 520.1220092773438 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1515.7447509765625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 730.49951171875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 673.7111206054688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 626.8427734375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1023.0239868164062 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 525.7044677734375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 859.5079956054688 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1800.27099609375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2161.731201171875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 715.215576171875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 559.5409545898438 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 842.2689819335938 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 3723.687255859375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1142.2794189453125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 851.28466796875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 940.32666015625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 624.4379272460938 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 825.4423217773438 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 900.0306396484375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1123.3563232421875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 901.0264892578125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 756.37353515625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 827.228759765625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1076.2535400390625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 966.0105590820312 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 687.8034057617188 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1045.355224609375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2380.58056640625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1009.742919921875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 690.7675170898438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 901.0463256835938 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 2138.927734375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 950.5836791992188 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 914.9635009765625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 636.2492065429688 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1050.364501953125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 929.5498046875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1668.7130126953125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 562.9613037109375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1255.1573486328125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1562.7723388671875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 763.2340087890625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 754.70703125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1424.4796142578125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 970.7003173828125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 837.1991577148438 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 990.8659057617188 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1038.1610107421875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 827.44677734375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1455.46337890625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1115.199951171875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 689.108154296875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 919.7216796875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1215.7347412109375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 883.7864379882812 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1590.387451171875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 975.713623046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 754.0386962890625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 777.0106811523438 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1220.891357421875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1564.1385498046875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 768.8386840820312 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 431.454833984375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 626.4464111328125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1661.0010986328125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1750.6612548828125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 552.6290283203125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 343.2318115234375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1259.991455078125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 695.9993286132812 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1617.263671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 721.25146484375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 928.2537231445312 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1258.494873046875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1685.7799072265625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1298.618408203125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1470.797119140625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1571.80859375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 694.5272827148438 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1554.283203125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1980.149169921875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1140.45361328125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1428.3717041015625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2150.208251953125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1085.177978515625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 2498.016845703125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1242.7850341796875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 3144.623291015625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 2289.38916015625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 828.5309448242188 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 902.0775756835938 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 2464.255126953125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1095.5419921875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 783.1513671875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 844.5599365234375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1114.8262939453125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 6609.35791015625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 796.1605834960938 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 860.71630859375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1943.6036376953125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1211.8448486328125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 774.0402221679688 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 2543.692626953125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 916.4344482421875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 741.546875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1197.876220703125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1312.33837890625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 905.5189819335938 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 881.3546752929688 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1005.7398681640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1301.5816650390625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 3483.591552734375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1061.4825439453125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 899.2941284179688 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1013.0599365234375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1370.4110107421875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 3444.219970703125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1051.471923828125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1095.3389892578125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1427.8580322265625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 747.4966430664062 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1349.7808837890625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1591.20703125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1043.03173828125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1120.932373046875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1290.9281005859375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 635.8790893554688 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1159.1859130859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 430.00665283203125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1514.5518798828125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 816.35595703125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 574.7169189453125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 2196.4619140625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 2778.7568359375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 951.1835327148438 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 805.09033203125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 911.15576171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 907.81787109375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1691.2303466796875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1233.587890625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1576.2449951171875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1553.950927734375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1280.9256591796875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 740.8853149414062 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 953.240966796875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1243.443359375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1276.9071044921875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1278.51953125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1369.4439697265625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 932.0330810546875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1496.093505859375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1218.72412109375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1679.8292236328125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1989.92333984375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1828.785400390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 986.4232177734375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 956.227783203125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1556.9244384765625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 959.5869140625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1313.398681640625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 769.08740234375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 535.19580078125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1032.0655517578125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 662.1483764648438 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1005.1268920898438 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1139.4881591796875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 648.7373657226562 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1894.9237060546875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1256.396728515625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1549.24169921875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1475.5550537109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1448.227294921875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1712.5003662109375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 972.8926391601562 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1856.673095703125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1054.7332763671875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 521.1294555664062 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1842.7606201171875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 2170.380126953125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 2549.54833984375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 912.0626220703125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 985.9403076171875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1515.171142578125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 4202.8720703125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 925.779296875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 932.7745361328125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 2016.4093017578125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 2415.704345703125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1210.8756103515625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1058.5595703125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 775.7683715820312 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1400.670654296875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1847.5479736328125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1566.4599609375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1014.164794921875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1553.82275390625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 2966.599853515625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1099.724853515625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 981.9800415039062 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1336.7078857421875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 822.0546264648438 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 929.2881469726562 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 2089.36474609375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1283.435791015625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 518.1825561523438 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 747.437255859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 723.72900390625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 635.4810791015625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 906.3977661132812 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1093.9417724609375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 588.265380859375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 922.9605712890625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1384.38671875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 936.451171875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1778.0013427734375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1805.8707275390625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1380.218505859375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1227.9373779296875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1821.7044677734375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1166.136474609375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1111.0631103515625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1362.67236328125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 616.346923828125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 2020.4036865234375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1597.522216796875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 910.1906127929688 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 983.8414916992188 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2192.689697265625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1025.2093505859375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: -8676.3056640625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1042.9779052734375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 528.7291259765625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1303.5509033203125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1744.4395751953125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 745.1964111328125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 781.037353515625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1827.6307373046875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2331.42578125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1530.1484375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1416.302978515625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 561.80126953125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 778.1784057617188 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1766.7274169921875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1138.923828125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 711.3275756835938 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1198.428466796875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 784.0174560546875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 819.2708129882812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1290.7857666015625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 767.237060546875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 502.32733154296875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 664.6464233398438 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 793.7097778320312 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1193.614501953125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 702.2447509765625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 815.159912109375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 441.1123046875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1136.4105224609375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 773.7708129882812 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 615.3951416015625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 670.9225463867188 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 663.3413696289062 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1723.8970947265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 2636.470458984375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1007.6746215820312 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 639.674072265625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 935.1238403320312 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1600.39404296875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 760.9802856445312 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1219.56982421875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 543.462890625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 700.5641479492188 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1526.3153076171875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 909.4017333984375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 735.857666015625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 718.9783935546875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1152.921875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1033.366455078125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1546.8560791015625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 687.9132690429688 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1117.4097900390625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1317.4271240234375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1155.2496337890625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 938.5608520507812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 699.078857421875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 577.845458984375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 717.967529296875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1263.4620361328125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1177.8321533203125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 818.8583374023438 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1148.181396484375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 952.5118408203125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 893.885498046875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 2244.386962890625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1042.37060546875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1427.74267578125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1241.659912109375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 733.7024536132812 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1704.5234375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1124.8717041015625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1312.31494140625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 632.4257202148438 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1650.470947265625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1505.228271484375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1253.889892578125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1599.795166015625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 656.176025390625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 659.6989135742188 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1395.5789794921875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 8321.5458984375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1070.4085693359375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 888.4688720703125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 759.6229858398438 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 554.8099365234375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 669.4611206054688 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 988.0426635742188 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 740.20263671875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1226.7237548828125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 411.89703369140625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 953.9907836914062 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 831.529541015625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:24:23<105419:26:41, 1265.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 739.2103271484375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 653.6482543945312 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 733.9569091796875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1222.999755859375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 432.6592102050781 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 455.9900817871094 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 669.7667846679688 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 984.7442016601562 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 581.0574340820312 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 576.3468627929688 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1042.283935546875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 786.6644287109375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1320.3170166015625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1723.017333984375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 784.761474609375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 811.0428466796875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1252.3028564453125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 896.1829833984375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 2758.9892578125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1205.2742919921875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 689.637451171875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1554.3089599609375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 635.3490600585938 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1174.7509765625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1628.9847412109375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 2303.9462890625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 867.7361450195312 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 3368.2099609375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1400.375732421875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1008.9710693359375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1194.185791015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1874.4163818359375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1381.147216796875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 627.224609375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1114.4503173828125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1017.1575927734375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 867.6575927734375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 548.1049194335938 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1114.7030029296875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 789.1068725585938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 846.96728515625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1197.50830078125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 880.1564331054688 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1265.478515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 846.9321899414062 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 913.6541748046875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 878.03662109375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1310.6572265625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1196.52001953125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1422.870361328125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 10616.1904296875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1952.5526123046875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1264.98291015625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 2815.353271484375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1097.8704833984375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 808.993408203125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 698.3487548828125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1018.2943725585938 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1062.5806884765625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1313.48193359375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1606.23876953125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 704.6647338867188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 2199.84423828125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1106.68359375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 941.8491821289062 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 780.7640380859375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1171.479736328125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1645.19189453125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1183.266845703125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 943.1302490234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 804.0697021484375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 2876.525634765625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1920.850830078125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 823.2750244140625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 706.7429809570312 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1043.6187744140625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 3482.614990234375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 891.8259887695312 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 575.9196166992188 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1444.9248046875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 746.1796875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1139.045654296875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1418.8990478515625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 669.8822631835938 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 575.4752197265625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1259.111328125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1070.71337890625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1280.7899169921875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 736.8523559570312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 865.42529296875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2274.309814453125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1080.69140625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 675.1807250976562 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 914.5625610351562 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1443.1878662109375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 6811.57861328125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1135.6810302734375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 732.6176147460938 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 2058.4775390625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 5334.38916015625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 845.1780395507812 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 914.893798828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 790.604248046875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 779.0288696289062 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1228.158935546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1447.19287109375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 861.0712890625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1129.6392822265625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1314.797607421875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1210.2305908203125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 945.8917846679688 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1988.8515625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1028.991455078125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 2703.793212890625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1396.2467041015625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1436.9306640625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1832.05322265625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1301.548583984375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 3490.174560546875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 6722.736328125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 6259.77685546875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1357.3834228515625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1742.9097900390625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1436.1090087890625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 2306.300048828125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 731.7178955078125 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1000.1068725585938 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1498.6375732421875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1189.1119384765625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1107.82568359375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 749.536865234375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 2247.5693359375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 850.7742309570312 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1421.857666015625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 659.8025512695312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 805.69921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 919.9776611328125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 919.0015258789062 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 779.8140869140625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1079.22607421875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 783.9048461914062 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 767.280517578125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1568.3094482421875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1151.6322021484375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 610.5445556640625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 631.1323852539062 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1337.724365234375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 840.8463134765625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1079.7125244140625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 657.0681762695312 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1127.2899169921875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1602.81201171875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1388.7435302734375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 454.00665283203125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 859.6278076171875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1164.5848388671875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 931.2548828125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 928.0655517578125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 853.1580810546875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1051.3658447265625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1578.426513671875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1546.0616455078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1103.570556640625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 973.1841430664062 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 759.6442260742188 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1073.7904052734375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1194.73095703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 819.7293090820312 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 873.3331298828125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 793.0276489257812 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 701.0062255859375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 908.7509765625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1149.5699462890625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 759.2002563476562 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 891.234375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 583.7171020507812 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1518.0404052734375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1220.7864990234375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 805.5827026367188 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 849.4434204101562 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1606.712890625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 996.7468872070312 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 907.15625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1131.9949951171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 705.5772094726562 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 426.2516174316406 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 753.3343505859375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 864.2223510742188 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1802.305908203125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1305.5836181640625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 560.0990600585938 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 884.8578491210938 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1028.1070556640625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 988.4002685546875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 767.1783447265625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 803.7115478515625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1722.1256103515625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 716.485595703125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1820.809326171875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 856.3565673828125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1207.708984375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1077.1590576171875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 891.3316650390625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1414.480712890625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 927.1212158203125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1452.702392578125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1284.62646484375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1498.038330078125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 916.2696533203125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1032.762939453125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1307.286376953125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1607.620849609375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 712.2434692382812 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 916.8198852539062 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1663.914306640625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1328.5823974609375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1343.9991455078125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2012.4219970703125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1265.752197265625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 950.4304809570312 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2618.241943359375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 3761.025634765625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 903.8775634765625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1914.0413818359375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1478.46337890625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1238.59521484375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1119.7789306640625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1069.9739990234375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1188.5943603515625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 792.6185913085938 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1086.6641845703125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1189.6644287109375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 758.0115966796875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 931.9368896484375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 806.570068359375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1630.4412841796875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1453.4493408203125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1077.2850341796875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1008.5042724609375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1552.2742919921875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 880.1238403320312 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1459.435791015625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1000.39794921875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1108.09765625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1099.6673583984375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 675.4115600585938 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 956.3351440429688 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1364.142578125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1480.01171875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 891.85107421875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1166.543212890625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 974.474853515625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1567.70849609375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1518.217529296875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 724.0492553710938 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1820.5185546875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1244.7347412109375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1308.6385498046875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 979.8072509765625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1271.0980224609375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 535.5138549804688 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 608.6935424804688 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 2065.177734375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1270.8489990234375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 703.5148315429688 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1125.65283203125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 690.4059448242188 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 875.88916015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1448.873291015625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 992.8057861328125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 728.6087036132812 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1819.6597900390625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 699.2628173828125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1014.5679931640625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 498.8116455078125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1493.8392333984375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 3360.6162109375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 968.9417724609375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1121.07421875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 890.239501953125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1145.5 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 2211.0439453125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 659.1508178710938 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1099.2066650390625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1472.8155517578125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1293.4576416015625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 921.982177734375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1072.10888671875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 938.2683715820312 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1096.239990234375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1313.0594482421875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 953.408447265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 608.63818359375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 747.2216796875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 7371.9248046875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1125.8995361328125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 852.8277587890625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 650.8336791992188 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 708.89697265625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 707.4921264648438 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 791.5115356445312 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 714.2711791992188 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 871.9446411132812 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 941.3820190429688 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 988.1822509765625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1280.0845947265625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1584.9029541015625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 672.966796875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 917.1923828125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1049.6346435546875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 621.7926635742188 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 891.5676879882812 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1604.360595703125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 631.1134643554688 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 758.5957641601562 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 955.646728515625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 962.768798828125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1015.7821044921875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 829.9217529296875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 860.1004638671875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 853.983154296875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 946.4639282226562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1197.370361328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 745.060302734375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 586.83056640625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 658.0252075195312 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1247.9381103515625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 608.5654907226562 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 860.0842895507812 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1058.7723388671875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1278.250244140625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 3815.579833984375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 741.8178100585938 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 682.6390991210938 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 843.1524047851562 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1162.681884765625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1136.188232421875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1386.6436767578125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 994.078369140625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 936.1908569335938 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1710.7276611328125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 3111.546630859375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 2003.6492919921875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 2014.8935546875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1477.1710205078125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1159.6065673828125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1540.131103515625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1918.408935546875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 993.7976684570312 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1974.2149658203125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 741.557861328125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 2058.127685546875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2020.4188232421875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1313.7928466796875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1166.9840087890625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 843.3223266601562 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2012.9405517578125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 614.0288696289062 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2676.602294921875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1493.516357421875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1195.3377685546875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 780.1767578125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 788.6640625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 3212.33056640625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 2802.32177734375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 955.9087524414062 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1505.3721923828125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1022.5676879882812 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 856.09814453125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1796.71240234375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 539.5779418945312 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 468.5759582519531 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 634.0430908203125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 650.7655029296875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 643.0665283203125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1656.3172607421875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 781.5767822265625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1541.2044677734375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 870.1532592773438 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 858.8665771484375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 717.2130126953125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 531.5191650390625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 978.4916381835938 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1708.71728515625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1006.3705444335938 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1207.6478271484375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1709.0313720703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1056.2755126953125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 942.15673828125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1365.05419921875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 869.0162353515625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1437.714599609375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1387.0908203125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 618.7041625976562 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 2623.593017578125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1738.8253173828125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1649.060546875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 901.07666015625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1045.639892578125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1447.0679931640625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1300.626708984375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1892.1484375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 850.1821899414062 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1211.96826171875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1561.3028564453125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1423.86669921875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 4989.26904296875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1183.73828125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2594.06298828125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1283.216064453125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 2933.91796875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 4950.6572265625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1682.080078125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1011.6818237304688 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 2158.74853515625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1481.3638916015625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1942.9503173828125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1751.4718017578125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1527.96533203125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 74049.2578125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: -30735.22265625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1935.8797607421875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 2853.629638671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1174.2603759765625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 3866.09423828125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 3026.928466796875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 945.5773315429688 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 653.4384155273438 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 2379.09130859375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2218.83349609375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1621.7152099609375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 919.9471435546875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1452.0875244140625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: -23558.970703125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1727.4608154296875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1587.6995849609375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1353.3857421875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1914.8690185546875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1249.9488525390625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1321.8477783203125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1104.56005859375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1608.606689453125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2420.327392578125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1545.8450927734375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 922.305419921875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1853.689208984375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 825.6982421875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 2031.2635498046875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 765.3320922851562 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 739.3297119140625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1354.6636962890625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 715.3701171875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1170.7083740234375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1087.9832763671875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1514.37353515625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1345.09033203125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1684.185791015625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1129.62548828125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1251.9970703125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1747.7412109375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 991.1546630859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1262.3126220703125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1165.067138671875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 746.9939575195312 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1540.021728515625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1292.284912109375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1268.33203125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 555.88623046875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 897.8881225585938 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1414.303955078125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1486.7799072265625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 748.7276000976562 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 912.1939697265625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 615.0098266601562 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 851.25634765625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 718.5049438476562 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1260.805419921875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1096.4366455078125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1206.245849609375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 819.9215087890625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1155.5540771484375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 754.5986938476562 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 630.2002563476562 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1129.080078125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1110.25 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1407.5032958984375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 756.5089721679688 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 834.3546752929688 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1037.283935546875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 698.0961303710938 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1962.5494384765625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 755.3184204101562 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1355.8331298828125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1190.2303466796875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1415.8900146484375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 771.78271484375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1634.882080078125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1025.2724609375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 820.7655029296875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1281.8250732421875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1514.9969482421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 935.281005859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1187.2724609375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1079.10302734375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 3018.2265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2093.834228515625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1056.4012451171875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1262.0657958984375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 864.482666015625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1328.653564453125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1568.9775390625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1535.8829345703125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 2717.487548828125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1204.8184814453125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 4554.56640625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1049.678466796875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 965.4154663085938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1296.9412841796875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1091.2830810546875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1114.626953125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 777.1013793945312 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1393.0430908203125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1342.586181640625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 567.9963989257812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 814.9647827148438 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 531.593017578125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 827.45458984375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 656.803955078125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 601.7432250976562 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 741.6565551757812 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 772.0709838867188 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 910.3421020507812 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 491.7245788574219 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 852.6278686523438 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 887.1227416992188 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 598.160888671875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 951.3737182617188 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 680.4718627929688 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 681.5557250976562 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 712.1205444335938 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 727.959228515625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1324.4674072265625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1071.1605224609375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 435.0902099609375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 426.70709228515625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 871.0026245117188 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 722.1591796875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 614.1203002929688 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 463.62713623046875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 535.6912231445312 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 739.6409301757812 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1682.865966796875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1420.40869140625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1577.4278564453125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1882.407470703125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 992.1644287109375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1438.3798828125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2650.578857421875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 885.5425415039062 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 599.0596313476562 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 657.8471069335938 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 651.7818603515625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1964.2486572265625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 855.4102172851562 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1812.61865234375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1120.330322265625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1627.637451171875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1035.9246826171875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 5247.046875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 3526.2421875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1281.372802734375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 753.3464965820312 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 5100.29345703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 3198.451416015625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 781.0512084960938 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1674.2406005859375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 2324.29541015625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1463.1796875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1134.119384765625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 844.3712768554688 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 912.5237426757812 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1349.271728515625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1122.42041015625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1020.7159423828125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 4249.2109375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1431.7969970703125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1416.1163330078125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 3429.809326171875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 581.9138793945312 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1839.2667236328125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 789.8084106445312 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 2633.849853515625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1646.54345703125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1099.0244140625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 907.4080810546875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1689.110107421875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 2007.3577880859375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 940.6649780273438 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1127.1246337890625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1364.8353271484375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1218.6563720703125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 4358.4365234375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 2369.646240234375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1570.061279296875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1454.000732421875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1067.974853515625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1521.958740234375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2195.04833984375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1492.2413330078125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1214.0128173828125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 2900.89404296875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1484.3179931640625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 998.9642944335938 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1274.989501953125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1403.7457275390625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 2616.1279296875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1198.981201171875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1523.01318359375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 800.0447387695312 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1140.11376953125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1869.3436279296875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 891.6506958007812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2136.30322265625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2086.5439453125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1132.8448486328125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1142.482421875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 878.2811889648438 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1148.297119140625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 912.7667236328125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1556.3016357421875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 583.3359985351562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 652.0126342773438 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 849.6691284179688 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 639.3930053710938 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1379.629638671875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 899.3931884765625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1079.63232421875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 811.8025512695312 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 971.4014282226562 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1372.4324951171875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1881.8101806640625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1950.8272705078125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1515.1494140625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1430.2515869140625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1248.1253662109375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1901.7574462890625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1121.403564453125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 894.7743530273438 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 774.159912109375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 899.2329711914062 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 980.1412353515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 846.2885131835938 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1430.4073486328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1224.49755859375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1017.5725708007812 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 855.8482055664062 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 10270.5341796875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 828.6943969726562 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1034.2344970703125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 548.4096069335938 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1348.6351318359375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 2193.608642578125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1632.573974609375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 796.6903076171875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 705.431884765625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 999.8800659179688 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1046.736328125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1246.8619384765625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 867.6129760742188 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 790.46142578125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1337.7952880859375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1049.7054443359375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1450.9478759765625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 817.31298828125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 670.2935180664062 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 925.9559326171875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1152.85791015625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1019.3055419921875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 913.9572143554688 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 2299.677490234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1281.1690673828125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 677.148193359375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1390.3702392578125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1993.959716796875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 721.916259765625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1511.3232421875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 716.2125854492188 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1376.6380615234375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 2532.205810546875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1168.5555419921875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1463.791748046875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 979.1475830078125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 2056.40771484375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 991.5755615234375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1154.7198486328125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:45:32<105558:54:24, 1266.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1431.099853515625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 765.1487426757812 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 3187.507568359375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 907.0603637695312 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 944.46142578125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1765.876953125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2467.568359375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1053.24658203125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 806.7343139648438 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 792.5269165039062 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1122.659423828125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 3522.732177734375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1847.4876708984375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 478.34844970703125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 701.996337890625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1215.0362548828125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1103.294189453125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 868.8233642578125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1414.211669921875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1236.8541259765625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 886.9662475585938 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1230.3868408203125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1006.0126342773438 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1958.4591064453125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 912.8534545898438 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 850.9699096679688 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1257.1368408203125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 682.154296875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 2747.373291015625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 930.5517578125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1808.5731201171875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 2081.96875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1240.284423828125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1554.430419921875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 766.888916015625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 653.0223999023438 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 709.73095703125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 989.4148559570312 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 888.6320190429688 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1295.22705078125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 531.7694091796875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 847.0502319335938 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 639.1192016601562 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1390.1256103515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1241.4495849609375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1567.1978759765625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 854.712890625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1154.994140625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1243.9111328125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1554.0333251953125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 592.1624145507812 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 6100.73046875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1749.614501953125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1247.5963134765625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1590.52880859375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1284.5599365234375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 618.089111328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1841.4659423828125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 977.2457885742188 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2511.455810546875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1983.1141357421875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1341.1717529296875 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 870.1400756835938 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1249.43896484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1251.9033203125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 699.159912109375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 831.39697265625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1194.3720703125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 866.8963012695312 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1200.003173828125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 627.0542602539062 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1291.2821044921875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 949.9718627929688 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1424.46240234375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 4097.80126953125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 948.1602783203125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1247.0540771484375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 2164.97607421875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1525.2882080078125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1853.7923583984375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1004.551025390625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1376.271484375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 944.6898193359375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1343.113037109375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 2082.805419921875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1522.2642822265625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1410.6336669921875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2768.43701171875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 2954.612060546875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1455.4259033203125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1471.346923828125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1040.084228515625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 681.3568725585938 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 992.626708984375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 3872.1455078125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1556.8232421875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1292.692626953125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1215.6026611328125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1443.3497314453125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1646.0599365234375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 2831.805419921875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1647.173828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 729.3735961914062 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1563.41943359375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 2246.575439453125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 3234.509765625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1722.4976806640625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 2290.24072265625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 3031.09326171875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1128.67578125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 918.8659057617188 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 947.411376953125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1440.597900390625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1054.0 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1063.569091796875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1864.896484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 822.0985107421875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 947.472900390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1041.0108642578125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1048.7010498046875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1751.2894287109375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 800.5678100585938 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1071.943603515625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 856.8858032226562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 757.875244140625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 575.1177368164062 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 757.2597045898438 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 798.05615234375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1047.1588134765625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 634.6265869140625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1009.4898071289062 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1105.81591796875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1005.1727905273438 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 500.24688720703125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 603.1914672851562 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 886.2742919921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1093.11279296875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1194.0499267578125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1102.5794677734375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 756.6301879882812 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 718.56689453125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 864.982666015625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1114.227783203125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1091.6715087890625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 898.3137817382812 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 868.2401123046875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1415.3756103515625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1287.972412109375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 964.529296875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 4564.06396484375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 917.7081298828125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 29401.1328125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 5525.685546875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1918.8465576171875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1413.8525390625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 823.626708984375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1479.4561767578125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1163.6893310546875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 933.7523193359375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 3435.38671875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1466.93017578125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1787.4263916015625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1111.1741943359375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1490.935302734375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 772.623291015625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2541.4794921875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 729.6707763671875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 890.4600830078125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1243.40234375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 988.8317260742188 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 711.93310546875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1065.54052734375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1620.4298095703125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1799.8641357421875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1009.1035766601562 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2607.866943359375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1795.7000732421875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1600.6827392578125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 4632.0126953125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1364.395263671875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 3188.705322265625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1104.1199951171875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1488.186767578125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 847.5072021484375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 3744.999755859375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 2086.077392578125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1175.8033447265625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 666.831787109375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 652.781982421875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1587.8521728515625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 583.6616821289062 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 871.7614135742188 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 758.7548217773438 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1244.6209716796875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 716.7825927734375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 706.7671508789062 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 841.3895874023438 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 711.0267333984375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1506.77490234375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1157.297119140625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 722.6903686523438 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1257.419677734375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1579.3665771484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 949.6996459960938 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 2001.2464599609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 561.608154296875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1258.936279296875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 567.2807006835938 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 991.9616088867188 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1074.029296875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1140.7132568359375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 900.1754760742188 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 931.77783203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 980.6072998046875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1640.2589111328125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 2700.02001953125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1406.403076171875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1110.4228515625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 943.4525146484375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1483.60986328125 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1019.3118286132812 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1099.986328125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 2085.022705078125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 702.06103515625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1469.646728515625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 2755.001220703125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1112.2911376953125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1578.2686767578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1510.8001708984375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1253.9091796875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1488.9490966796875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1091.5625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1631.5728759765625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 791.4423828125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 907.1473388671875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 948.5595092773438 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 685.4542846679688 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 991.7203979492188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1313.9962158203125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 603.8212280273438 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 961.5651245117188 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1018.9124755859375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1715.5301513671875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 579.1441040039062 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 833.940185546875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 2468.11279296875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1904.436279296875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 876.5847778320312 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2150.907958984375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 899.334716796875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 4337.3779296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 2621.16748046875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1585.50146484375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 940.616943359375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 919.2127685546875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 871.7415771484375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1376.335693359375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 3288.96875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 664.3607788085938 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 441.3763427734375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 784.8989868164062 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 379.34716796875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1095.2789306640625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1100.2216796875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 649.8869018554688 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 974.2296142578125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 469.6352844238281 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 765.5015258789062 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1490.7891845703125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1491.1927490234375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 710.3046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1361.9991455078125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 949.5472412109375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 795.8778076171875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 970.8223266601562 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1185.8477783203125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 885.7318725585938 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1391.8414306640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1701.3299560546875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 933.830322265625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1923.1612548828125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1289.2918701171875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 983.22314453125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2163.7158203125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 926.5184326171875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 943.460205078125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 323.6719665527344 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1156.56494140625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1590.1370849609375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1838.137939453125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1458.9464111328125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1049.0054931640625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 2019.991943359375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1180.1822509765625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1094.549072265625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2217.189208984375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 681.77099609375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1996.3651123046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1576.2860107421875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 824.2678833007812 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1461.5509033203125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1975.0966796875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 920.1268920898438 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1737.2977294921875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1385.9451904296875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 773.6060791015625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 877.1624145507812 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1500.883544921875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1428.268310546875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 859.9152221679688 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1605.3033447265625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1104.93359375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1216.981689453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1277.0755615234375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1471.9791259765625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1351.615478515625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1288.958740234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1884.60302734375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1004.9933471679688 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1434.67138671875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1499.82421875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1214.16015625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1639.2608642578125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1622.469482421875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 915.3995361328125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 793.849365234375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1117.2332763671875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 935.203857421875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 649.0477294921875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1227.1866455078125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1140.367431640625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1435.876708984375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 689.88330078125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 904.2672729492188 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1040.5126953125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1037.1759033203125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 982.22216796875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1511.79638671875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 739.4881591796875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 621.5208740234375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1695.6497802734375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1082.38671875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 955.0508422851562 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2226.813720703125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 986.51025390625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1202.6854248046875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1449.66015625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 937.7058715820312 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 639.0354614257812 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 992.1649780273438 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1046.792236328125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1275.640625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 833.9255981445312 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 715.9558715820312 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 637.0595092773438 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 860.480712890625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1069.0054931640625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1351.970703125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 2020.052734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1164.0906982421875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1246.2193603515625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1090.1837158203125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 2221.34228515625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1689.79296875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 913.876220703125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1538.944580078125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1322.2081298828125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2543.686767578125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1367.1253662109375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1908.436279296875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1662.6846923828125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1176.4483642578125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2833.861572265625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1533.633544921875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 782.244873046875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1785.204833984375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1474.4915771484375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2013.4783935546875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2819.70849609375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 926.1250610351562 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1124.79443359375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1160.5262451171875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 996.5917358398438 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 2927.098388671875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1250.7183837890625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1571.0185546875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 769.7867431640625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 830.7095336914062 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1307.7535400390625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 850.01123046875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 888.8060302734375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1212.949951171875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 997.2500610351562 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 2029.1510009765625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2206.55029296875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 903.7720336914062 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 43649.78125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1087.356201171875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1131.5003662109375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 985.1676635742188 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2047.006103515625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 3016.5615234375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1025.4923095703125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 2338.64013671875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1016.0208129882812 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 2037.0450439453125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1835.655029296875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1371.6951904296875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1128.349853515625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1960.28076171875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 847.9480590820312 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 11063.9296875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1542.09228515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3733.642822265625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 815.8862915039062 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 695.2493286132812 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1420.1522216796875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1649.3529052734375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3832.251953125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 161075.84375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1324.25537109375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1253.549072265625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1596.6015625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 976.8724365234375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1094.55224609375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 2218.699951171875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 2111.575439453125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 856.875732421875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1373.04052734375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1111.864990234375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1520.9384765625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 861.659912109375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 893.225341796875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 3084.43017578125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1191.2198486328125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.758, loss_val: nan, pos_over_neg: -16088.646484375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1284.332275390625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 3711.01416015625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1040.8206787109375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1357.0677490234375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1417.52197265625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1284.1705322265625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 682.2977294921875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 570.9345092773438 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 940.895263671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 3222.39404296875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 5081.81298828125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1302.0816650390625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 507.9853515625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 934.8577880859375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1017.9141235351562 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 2021.0487060546875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 535.77587890625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 702.432861328125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1238.1597900390625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 957.2254028320312 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 2078.6748046875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1146.58251953125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 4565.84765625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1217.8321533203125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1471.8577880859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 773.48974609375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 6926.43408203125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 899.3017578125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1377.65576171875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1008.6676635742188 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 841.0965576171875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 776.3513793945312 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 922.9591674804688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 4326.74169921875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 849.6723022460938 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1117.1595458984375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 849.1199340820312 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 838.294921875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1181.7052001953125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 675.7449340820312 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1896.853759765625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1118.9635009765625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 622.1959838867188 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 812.4579467773438 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1276.0263671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 750.8877563476562 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 583.729248046875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 848.7839965820312 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 803.9567260742188 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1035.560791015625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 946.8121948242188 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1078.0836181640625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1437.564208984375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 851.5879516601562 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1577.5054931640625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1182.0833740234375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1507.806640625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 869.0042724609375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1977.4090576171875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 970.1253662109375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 832.0559692382812 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3884.84814453125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 4525.703125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1019.6663818359375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 562.3847045898438 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1133.1461181640625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 850.2865600585938 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 976.7584228515625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 863.1079711914062 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 987.8658447265625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 936.1034545898438 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1183.2978515625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 845.4620361328125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1185.162353515625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1107.951171875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 908.76318359375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1187.75537109375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 924.6522216796875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 675.590576171875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1588.077880859375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 912.008056640625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1105.1044921875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 829.465576171875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1087.5081787109375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1196.5244140625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1156.86962890625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1065.439697265625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 919.6829223632812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 839.7154541015625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1805.5706787109375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1318.5545654296875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2571.22802734375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1189.6207275390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 3335.03076171875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1442.6761474609375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1284.60888671875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2381.3427734375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 889.05859375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 2312.14111328125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2639.904296875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2767.013916015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1608.7822265625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1538.6414794921875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1065.4388427734375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1177.50048828125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1603.5528564453125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1611.6590576171875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1230.2763671875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 952.0868530273438 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1748.4222412109375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1451.3409423828125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1359.6025390625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 923.283203125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1284.881103515625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1254.8692626953125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1114.67822265625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1977.7054443359375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1435.1826171875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 645.4007568359375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 992.3388061523438 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1462.769287109375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1324.8983154296875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 962.6423950195312 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1310.448486328125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1501.4730224609375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 980.4105224609375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 3171.62841796875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1547.1932373046875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 701.3136596679688 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 2787.397216796875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1201.9830322265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1162.95166015625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1241.735107421875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 908.557861328125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1442.979736328125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1229.6202392578125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 867.5157470703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1416.7789306640625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1824.8670654296875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1416.5865478515625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 772.592529296875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1282.0433349609375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2495.26611328125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1383.885986328125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1135.9647216796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1420.4886474609375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 855.3645629882812 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 767.550537109375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1202.760009765625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1379.359375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1016.395263671875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1058.586181640625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1036.1507568359375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1400.7147216796875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 814.455810546875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 3002.612060546875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1060.2740478515625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2820.98291015625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1241.1593017578125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 937.8500366210938 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 176364.453125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1064.083984375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1640.057861328125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 3370.0244140625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1108.399169921875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 4705.68212890625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1118.0723876953125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 721.248779296875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1675.2398681640625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 541.1571044921875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 987.0861206054688 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1971.472900390625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1132.156494140625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 973.0776977539062 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1919.3380126953125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1477.465087890625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1165.581787109375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1056.0283203125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1588.428955078125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 712.0059204101562 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 939.1487426757812 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 3659.324951171875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 940.1064453125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1058.6212158203125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 2450.515869140625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 3701.789306640625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1322.2874755859375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 988.3677368164062 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1196.955810546875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2181.706787109375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2321.43505859375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 2299.170166015625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 4215.80126953125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1121.0606689453125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1517.717529296875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1426.843017578125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 756.25830078125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 940.0293579101562 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1141.0411376953125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 616.2516479492188 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 708.123291015625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 702.4962158203125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1179.7169189453125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 576.292236328125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 484.5174560546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 546.6392211914062 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 412.63067626953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 860.1575927734375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1807.796630859375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1064.350341796875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1118.27294921875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 586.3687133789062 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 896.6860961914062 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1566.5157470703125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1023.3674926757812 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 890.77294921875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 617.8565063476562 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 544.6550903320312 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 882.0662231445312 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1106.3863525390625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 828.445556640625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1050.81201171875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1241.8941650390625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 841.8419799804688 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 2083.548095703125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 995.701416015625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 895.822509765625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1391.3238525390625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1612.7906494140625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 846.8960571289062 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 584.65283203125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 717.2196655273438 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 967.7354125976562 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 983.9880981445312 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1027.9508056640625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 2537.8046875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 689.322021484375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1741.9482421875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 615.5110473632812 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1132.884033203125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1635.046630859375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 723.256591796875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1704.0238037109375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1387.30126953125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 919.5557250976562 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1899.76708984375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1217.0704345703125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 707.7980346679688 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1494.915283203125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 2441.123779296875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1300.2860107421875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1441.2845458984375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 941.6084594726562 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1020.2988891601562 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1342.5234375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1848.482666015625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1145.753662109375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1736.1363525390625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1069.6026611328125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:06:33<105385:42:29, 1264.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2585.496337890625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1069.0421142578125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 3873.705322265625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1273.3228759765625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2976.782958984375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 2552.41064453125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1025.0238037109375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1144.614501953125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 2395.68505859375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1902.3817138671875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2181.73388671875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1408.593994140625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 4371.87158203125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1689.47021484375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 693.5455322265625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 2264.97216796875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1672.6748046875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1218.9613037109375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1535.2431640625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1096.283447265625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1653.425537109375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 953.505126953125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1442.039306640625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1630.656494140625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2006.346435546875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 4546.533203125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2169.468505859375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1306.175048828125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3197.21728515625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 2428.896728515625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 852.4268188476562 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1036.021240234375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 931.712646484375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 2277.9609375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1498.9996337890625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1077.721435546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 11262.0859375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 3147.7275390625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1340.1038818359375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1082.962158203125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1562.2265625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1511.603515625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 2057.065673828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 14380.22265625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 2538.801025390625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2432.2333984375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 4226.787109375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1860.36865234375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 950.7725219726562 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 668.6873168945312 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1022.76220703125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 2146.384521484375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1797.201416015625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 3247.27197265625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 843.9199829101562 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1447.2467041015625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1530.3839111328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1671.0498046875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1524.1719970703125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1303.5146484375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1068.6546630859375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1371.7689208984375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1141.3082275390625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 7278.1923828125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1615.0050048828125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 12803.0234375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2540.474609375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1595.5260009765625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 608.3079833984375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1172.6767578125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3912.472900390625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1709.0506591796875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1095.01171875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 893.02880859375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 2306.783203125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1045.0069580078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1095.9769287109375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 939.376953125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1870.9283447265625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 848.3788452148438 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 606.4318237304688 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2941.5048828125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1349.4072265625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 4054.243408203125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2286.6123046875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1150.065673828125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1429.5439453125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1063.1524658203125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1566.634765625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 7096.8193359375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 948.2565307617188 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 4179.99072265625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1356.8607177734375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 846.83251953125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1148.848388671875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 2440.779052734375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1112.37109375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 3153.60693359375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 812.2496948242188 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: -10285.158203125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 2654.09130859375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1720.527587890625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 2683.528076171875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 940.594970703125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1042.9415283203125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1083.9989013671875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 3182.634765625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1252.031982421875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 715.728271484375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1003.620361328125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2284.5029296875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1076.69482421875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 683.2523193359375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 661.4202880859375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 3816.35400390625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 895.0419921875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1206.6478271484375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1293.1451416015625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1244.9991455078125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1352.754638671875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 904.3787841796875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1145.3275146484375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 824.4722900390625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2276.150146484375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 5470.337890625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1264.12646484375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1162.023193359375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1819.863525390625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 8362.66015625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 676.6637573242188 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 889.0394897460938 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 796.1898193359375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1292.83935546875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2124.710205078125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1549.443359375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1376.556640625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 2991.931640625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1457.8095703125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1649.1046142578125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 3207.212646484375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1883.68603515625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 3603.18408203125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1461.7342529296875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 637.7913208007812 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 996.0093383789062 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1090.3157958984375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 913.7430419921875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1319.1826171875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1451.75390625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2349.705810546875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1497.74755859375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1354.7940673828125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1066.125244140625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1664.67724609375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 2275.8505859375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 985.7883911132812 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1758.529052734375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1188.0511474609375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1506.916748046875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1236.1087646484375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1972.5479736328125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3569.33544921875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 3183.4267578125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 879.9063110351562 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 724.3157958984375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 4387.5888671875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1077.4830322265625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 698.0292358398438 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1453.457275390625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 873.4169311523438 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 835.6842041015625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 938.3556518554688 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 728.9712524414062 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1009.020263671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 850.3628540039062 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 997.0931396484375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 707.2303466796875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 543.3615112304688 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 657.8228149414062 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 730.6151123046875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1578.8184814453125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 624.825927734375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 405.051513671875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1003.1002807617188 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 940.716552734375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 591.2001953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 653.8900146484375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2324.0537109375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1563.9925537109375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1148.136474609375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 553.9855346679688 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1591.295654296875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 2588.08642578125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1457.7060546875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 804.309326171875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 4592.7373046875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 2194.408935546875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1258.0963134765625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1003.1661987304688 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1576.5623779296875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 718.00048828125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1818.046142578125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1283.1004638671875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2433.500732421875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 890.2391357421875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3304.5263671875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1067.632568359375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1311.9344482421875 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1359.013427734375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 909.5530395507812 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1014.6167602539062 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 3927.873046875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1271.8734130859375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1142.5284423828125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 803.1341552734375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 993.4262084960938 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2537.76611328125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1279.7662353515625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 822.5660400390625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1603.21044921875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1306.74462890625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 755.4737548828125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 965.1015625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 907.4097290039062 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1036.4642333984375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 747.7901611328125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1291.7213134765625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 859.7237548828125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1468.254150390625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 3135.0546875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1191.8345947265625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 626.7084350585938 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1248.45068359375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1551.336181640625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1653.9593505859375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1775.5330810546875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1217.919921875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 2201.521484375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 514.4207153320312 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1673.2501220703125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 11764.908203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1305.401123046875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1342.0089111328125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1352.3253173828125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: -104884.625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 2187.641845703125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2696.658935546875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 3629.901611328125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 6944.6044921875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2665.0439453125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1936.6162109375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 536.4564208984375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1978.1168212890625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1090.992919921875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1421.216064453125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1926.196044921875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 847.6610107421875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 1285.218505859375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 3083.09033203125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1014.667724609375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 891.0624389648438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 751.671630859375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1721.7664794921875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1114.4755859375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 727.3795776367188 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1254.9205322265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1196.867919921875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1707.4185791015625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1544.256103515625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1328.632080078125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1207.784912109375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1381.234619140625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 2805.9912109375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1578.65087890625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 856.39013671875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1315.4276123046875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 874.9989624023438 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 657.078125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 3131.77197265625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2691.984619140625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1576.0758056640625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1226.2266845703125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 896.4902954101562 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 2491.501953125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1329.3197021484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1731.5343017578125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1510.914794921875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 992.6923828125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 581.9490966796875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1091.060546875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 5682.2529296875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1031.5638427734375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1678.39111328125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 801.70361328125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1235.360595703125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1731.196044921875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1428.38427734375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 2955.213134765625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 2703.459228515625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2387.864501953125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1873.730224609375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1014.0980834960938 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1278.100341796875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 903.3161010742188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 2476.115234375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1618.74267578125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1171.9150390625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 874.3596801757812 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1496.3466796875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 969.8408203125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1493.10595703125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.753, loss_val: nan, pos_over_neg: -10761.21484375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1517.622314453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1026.254638671875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 680.3370971679688 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1230.364501953125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2704.544921875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 992.5192260742188 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 763.915771484375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1417.380859375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1325.1260986328125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 2033.8421630859375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1664.343505859375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1374.8551025390625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1258.237060546875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1101.1898193359375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1873.1240234375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 981.2356567382812 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1067.9949951171875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 855.6306762695312 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 966.96533203125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 806.169921875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1059.8931884765625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1176.8245849609375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1823.954345703125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1193.141845703125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1138.8551025390625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2288.436279296875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1512.635009765625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 982.0906982421875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 981.0663452148438 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1142.005615234375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1885.6973876953125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1384.443359375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1035.6075439453125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 573.6614379882812 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 913.1776123046875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1144.1793212890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1613.1754150390625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 668.2745971679688 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 593.2440185546875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1624.6217041015625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 778.3511352539062 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 876.1387329101562 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1185.0318603515625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1144.6883544921875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1778.2315673828125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 876.1315307617188 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1043.5372314453125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1306.6168212890625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2037.2210693359375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1008.054443359375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1674.689697265625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 871.01904296875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 946.6915283203125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 955.0111694335938 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 587.6868896484375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1063.6656494140625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 3018.470947265625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1790.2054443359375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1516.7998046875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1818.944580078125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 4633.828125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 2070.072509765625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1124.10693359375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1637.9569091796875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1584.9010009765625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1853.940673828125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 16028.1435546875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 931.41748046875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1417.3421630859375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1646.0362548828125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2746.529296875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 8689.9794921875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1168.88037109375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 3039.861572265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 5630.90234375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1043.707763671875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1075.6290283203125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 783.1508178710938 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1981.8201904296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 3312.621337890625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1350.182373046875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1223.79931640625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1097.4525146484375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1103.4722900390625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1065.781494140625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2646.47802734375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 881.8468627929688 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1290.592529296875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 2304.728759765625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1157.1248779296875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1429.0299072265625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1820.958740234375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 681.4248046875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1029.9915771484375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 2011.946044921875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1335.703125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1111.810791015625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1483.74658203125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1526.7510986328125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 3412.549072265625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: -4536113.5 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1357.7265625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 965.1082763671875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1783.158935546875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1242.750732421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1327.4339599609375 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1732.247802734375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2202.3935546875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1359.18603515625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1537.57763671875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 600.2593994140625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 892.4696655273438 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1899.0263671875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 4412.109375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 3084.132568359375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1042.357666015625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1240.2567138671875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 781.7025756835938 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1310.7752685546875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 738.2425537109375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 808.9735717773438 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1230.01904296875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 932.0064086914062 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 850.4781494140625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 814.9833374023438 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1134.4925537109375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1063.3475341796875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1068.4288330078125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 742.0169677734375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 684.4539184570312 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1614.9317626953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1103.2222900390625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1050.78369140625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 941.4404907226562 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 698.75244140625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1773.0208740234375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1142.1546630859375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1065.9708251953125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 963.43603515625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 952.1370239257812 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 3025.38916015625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1948.67529296875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 858.7582397460938 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 571.5906982421875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1276.71240234375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1012.7308349609375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 993.6289672851562 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 733.5995483398438 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1236.2677001953125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1662.887939453125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 2110.62548828125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 2049.843017578125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 627.1026611328125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1315.103515625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 887.201416015625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 114717.703125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1129.694580078125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 813.767333984375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2799.429931640625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1277.6533203125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 915.006103515625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 804.91552734375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1452.669921875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 3456.745849609375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 860.556640625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 992.4623413085938 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 668.1467895507812 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1046.3297119140625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2916.4326171875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 762.7451171875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1089.673095703125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1345.036865234375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1107.5543212890625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1375.21533203125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 635.5238037109375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1021.740478515625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1187.5787353515625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1026.4871826171875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 622.3198852539062 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 868.9507446289062 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 942.9065551757812 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 736.741455078125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 945.4344482421875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1124.3262939453125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 26420.8984375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1183.912841796875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1125.3287353515625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1607.3099365234375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1571.5264892578125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1077.0140380859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1837.537353515625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2474.708984375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1684.791259765625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1801.9637451171875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 863.3595581054688 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1028.642822265625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 2033.122314453125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 16160.19921875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 2384.787353515625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1217.36328125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1096.298828125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 902.339111328125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 4549.01611328125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7398, loss_val: nan, pos_over_neg: 16130.6201171875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 6050.59033203125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1606.0865478515625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 652.2363891601562 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1152.310791015625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1647.51708984375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1227.5980224609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 913.4843139648438 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2642.83642578125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 832.3350830078125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1258.0113525390625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1188.956298828125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1702.2894287109375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1340.0537109375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 770.1312866210938 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1191.4454345703125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1688.7618408203125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1198.60009765625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 807.4884643554688 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1157.6295166015625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1841.080078125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1060.3428955078125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1018.0818481445312 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 559.9519653320312 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 663.7196044921875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 2072.576171875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1629.1776123046875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1483.9049072265625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 1887.330810546875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 679.581787109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1216.9578857421875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 968.3878784179688 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1694.293701171875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 916.7786865234375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7433, loss_val: nan, pos_over_neg: 902.1306762695312 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 745.0396118164062 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1181.9293212890625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2073.4873046875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 3419.384765625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 982.1222534179688 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1008.7018432617188 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7435, loss_val: nan, pos_over_neg: 2154.14892578125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1005.0509033203125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 720.1404418945312 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 939.4847412109375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1705.7188720703125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 2096.353271484375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1560.0360107421875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 849.5489501953125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 651.3394775390625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 8719.89453125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 2452.658203125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 994.8143310546875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1641.202392578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 815.1903076171875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 4714.490234375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1191.69677734375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1351.77001953125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1333.985595703125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1346.1282958984375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1010.8556518554688 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1153.11767578125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1511.3387451171875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1733.158935546875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1310.1981201171875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1992.013916015625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1054.4207763671875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 3509.34033203125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 3245.5166015625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1516.1055908203125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 874.6221313476562 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 840.3653564453125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1421.23583984375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1569.513427734375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 901.6498413085938 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 821.5924682617188 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1755.9610595703125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 798.3407592773438 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 920.3735961914062 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 921.6552124023438 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 911.341064453125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1051.31689453125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1392.5693359375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 931.4902954101562 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1017.3001098632812 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 860.480224609375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1159.525390625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1091.384033203125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 976.6549682617188 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 772.6597900390625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2499.554931640625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 719.1637573242188 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 738.049560546875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 796.7279663085938 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1178.3966064453125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1427.4725341796875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1407.9359130859375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 790.206298828125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1402.9141845703125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 904.3031005859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1222.393798828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1296.7464599609375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1166.828125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 900.2122192382812 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1038.3756103515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 2865.3720703125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1429.4178466796875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1152.9183349609375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1250.40673828125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1549.8162841796875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1235.96630859375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 658.937255859375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 796.2315063476562 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 963.498779296875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 2347.598388671875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1798.853271484375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 3360.9970703125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 965.1270141601562 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 956.9140625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1468.0020751953125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1497.7264404296875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1930.772705078125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1419.2119140625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 889.6922607421875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1384.2261962890625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1483.6658935546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 866.9267578125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1323.0418701171875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2915.326904296875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 795.1110229492188 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1381.96484375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1330.9853515625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1471.953125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1494.7315673828125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1265.827392578125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 912.69287109375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 981.1854858398438 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 862.8771362304688 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1110.5919189453125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1711.3995361328125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 907.50927734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1072.157958984375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 704.9178466796875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1269.7607421875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 779.9134521484375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 1477.6153564453125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1683.262451171875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 555.4287719726562 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 741.7930908203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 2204.427490234375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1103.9957275390625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 2855.40673828125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 903.722900390625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1342.108642578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 987.1395263671875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1392.8763427734375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1022.717529296875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3274.975830078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1388.319580078125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1202.4031982421875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 2279.238037109375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1466.61962890625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 1422.246337890625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1066.0670166015625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1832.736328125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1053.311767578125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1032.3984375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 982.32080078125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1695.879638671875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1498.1732177734375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 2400.11962890625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1739.657470703125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1726.7481689453125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1049.1812744140625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1418.713134765625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1920.322265625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1887.6536865234375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1196.1505126953125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 954.4096069335938 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1441.733154296875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:27:45<105590:34:26, 1267.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 967.9889526367188 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1657.6190185546875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 2460.54638671875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1486.33203125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1297.356201171875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1888.7569580078125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 8103.541015625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7441, loss_val: nan, pos_over_neg: 741.6808471679688 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 714.9130249023438 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1597.3013916015625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1741.596923828125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 597.6911010742188 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 578.7215576171875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 992.544677734375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 996.8966674804688 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1797.9044189453125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 877.791259765625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 698.4531860351562 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 959.2302856445312 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1496.1044921875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1114.1593017578125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 895.1055297851562 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1036.7261962890625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 889.4019165039062 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1071.5877685546875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1044.6563720703125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 2423.054443359375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 3212.09814453125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1085.783203125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 1301.9322509765625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2026.42724609375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1759.5848388671875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1720.6513671875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1374.453125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2308.307861328125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 785.135498046875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 496.57025146484375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 830.2232055664062 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1118.839599609375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 937.1206665039062 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 989.2242431640625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1148.6109619140625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 961.9535522460938 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 973.5025634765625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1487.1416015625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 14256.0869140625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 4077.81103515625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 764.2704467773438 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1117.3570556640625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 2633.64013671875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1192.651611328125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 2751.528564453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1597.88232421875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1387.147216796875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1166.2269287109375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 611.1776123046875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2462.16064453125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 734.7921752929688 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1737.58154296875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 2162.019287109375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1324.840087890625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 605.6820068359375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 851.0018310546875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 728.25146484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1232.7884521484375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1208.4996337890625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 782.637451171875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 986.4630126953125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1465.9267578125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 5027.865234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1177.079345703125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1244.23828125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 966.0055541992188 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 2868.348876953125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1074.5875244140625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 991.4041137695312 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1204.3115234375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1323.915283203125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1555.374755859375 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 2335.8359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1727.47802734375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1365.7041015625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1881.700439453125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 11178.521484375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 846.7894287109375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 783.7461547851562 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1116.3209228515625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1600.0281982421875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1707.0509033203125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1835.5550537109375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1190.1951904296875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3360.30908203125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1184.1510009765625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1652.6116943359375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1423.669677734375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 854.2640991210938 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 776.3240966796875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1043.4906005859375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 929.57958984375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1921.7916259765625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 910.9201049804688 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1531.663818359375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 1333.220458984375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1096.5279541015625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 2051.040771484375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1670.6636962890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 679.1582641601562 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1062.1708984375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1436.3765869140625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 728.7296142578125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1672.4022216796875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1055.397705078125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1457.65234375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1802.1064453125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1005.6231689453125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1037.0941162109375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1270.4161376953125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1051.6148681640625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1972.1588134765625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 734.998291015625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 886.8467407226562 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1414.8546142578125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1614.8602294921875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 4782.29638671875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1725.868896484375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 868.8917846679688 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 2213.05859375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1240.3470458984375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1911.99609375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 943.5665893554688 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2068.970458984375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1935.07177734375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 825.6193237304688 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1564.7012939453125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 3651.44677734375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 2414.74072265625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 942.5271606445312 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 926.69970703125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 904.3182983398438 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1453.317626953125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1502.5872802734375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1315.7462158203125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1252.116943359375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1048.3892822265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 936.5245971679688 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 939.2576293945312 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 2915.02685546875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: -67266.7109375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1588.2352294921875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2467.793212890625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1330.7752685546875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1805.156982421875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1397.772705078125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1143.6953125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1505.8150634765625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1075.040283203125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1157.4267578125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 713.364501953125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 546.5512084960938 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1616.9197998046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2812.645751953125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1262.8162841796875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 655.594482421875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 854.9681396484375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1296.0538330078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 796.8525390625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 898.5757446289062 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 922.0801391601562 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1557.5003662109375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1510.4569091796875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1053.916015625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1517.861083984375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2052.807373046875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1187.6551513671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1230.58203125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 2605.42626953125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 610.2487182617188 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1568.20458984375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1326.1131591796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1218.453369140625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 956.3607177734375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1337.3682861328125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1402.7432861328125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1162.8414306640625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1334.132568359375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 817.7276000976562 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 3260.435546875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 10076.49609375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1350.6846923828125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 2338.542724609375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1277.8720703125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1199.591552734375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 636.9894409179688 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 894.1445922851562 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1035.6690673828125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1120.63134765625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1459.68798828125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 782.0480346679688 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 956.3076782226562 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 967.5388793945312 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 518.1884765625 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1571.5032958984375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1453.5098876953125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 933.7825317382812 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 944.2190551757812 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 971.9327392578125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 825.734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 853.95068359375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 760.8399047851562 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1286.4296875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7446, loss_val: nan, pos_over_neg: 2340.066650390625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 865.058349609375 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 628.7074584960938 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1785.5865478515625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 1515.0946044921875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 2909.591064453125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 746.1345825195312 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 702.987548828125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1536.74267578125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 2312.95458984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 2111.904296875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1572.1982421875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 977.8093872070312 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1523.4759521484375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1061.7806396484375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 718.0179443359375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 2244.201904296875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1178.401123046875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1142.658935546875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 651.680419921875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 1152.1973876953125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1025.173095703125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 3520.396728515625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1409.87841796875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1480.039306640625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1703.135986328125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1366.3729248046875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1757.1439208984375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1704.6495361328125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1027.866943359375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 569.1434936523438 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1617.6534423828125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1090.8131103515625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1633.5390625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1947.06982421875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1772.82568359375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 3059.31396484375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 722.659423828125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 724.0801391601562 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1928.4127197265625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1759.0855712890625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1169.125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1237.0657958984375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1667.124267578125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 946.4391479492188 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 796.5834350585938 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 601.288818359375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 974.7509765625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1192.988037109375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 924.8580322265625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1142.51416015625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1607.61767578125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1046.3072509765625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1723.177001953125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 965.6718139648438 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1050.5396728515625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1365.4849853515625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1189.0772705078125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 2212.72509765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 5686.71435546875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 971.3748168945312 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 798.262451171875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 930.3511352539062 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 924.2298583984375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 968.3650512695312 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 2189.837890625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 4313.486328125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1804.6241455078125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 957.9290161132812 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 717.78076171875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 996.5848388671875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 987.473876953125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1533.4884033203125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1060.095947265625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1063.396484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1676.629150390625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 665.8355102539062 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1054.7044677734375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 2948.84375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 2940.359375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1094.8048095703125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 782.5986938476562 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2237.895263671875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1376.3272705078125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 892.661865234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 979.207763671875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 999.526611328125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 789.6654052734375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 1737.8499755859375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 4112.96728515625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1595.4805908203125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 810.6373291015625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1695.353515625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1193.13232421875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1203.196044921875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 763.661865234375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1951.55712890625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1563.5919189453125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 3010.715576171875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1565.719970703125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1396.99951171875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1071.3214111328125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1602.843017578125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1215.7275390625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 871.1153564453125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1128.1614990234375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1090.6319580078125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1896.4237060546875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1387.2584228515625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 1350.72265625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1366.660888671875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1042.1033935546875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 755.1279296875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 982.0447387695312 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7419, loss_val: nan, pos_over_neg: 1671.5582275390625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1319.0609130859375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 23932.693359375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1879.0540771484375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 942.6226806640625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 1036.207275390625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1098.2103271484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1528.7684326171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1067.318603515625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1576.816650390625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1617.323974609375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1305.5242919921875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1508.80712890625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1039.7332763671875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 2217.180908203125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1318.0740966796875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 978.777099609375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1209.5936279296875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 5679.4677734375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 4733.36962890625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1405.0601806640625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1032.8258056640625 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1284.90869140625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1181.0543212890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7469, loss_val: nan, pos_over_neg: 2208.29296875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 877.397216796875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 977.7403564453125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 982.9894409179688 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 2299.545166015625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 2278.509765625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1379.498291015625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1673.4112548828125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 2970.76171875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2107.1328125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1925.1087646484375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 2574.254150390625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 2210.759033203125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 1858.929443359375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1648.2120361328125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1672.095458984375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 2341.79052734375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1111.463623046875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1423.5126953125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1397.1336669921875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 3788.34521484375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1290.9573974609375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1062.309814453125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1249.5574951171875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1924.8289794921875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 765.71875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1064.512451171875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 733.65625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 3660.015625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1707.462158203125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 514.9143676757812 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2012.6705322265625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1548.2789306640625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 504.9223327636719 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1150.019287109375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 825.342529296875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2109.089599609375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1097.529296875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 433.7658996582031 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 931.588623046875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1307.1163330078125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2444.578369140625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1190.1424560546875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1435.936279296875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1869.4661865234375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1048.1156005859375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1405.1015625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1908.80859375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1521.57666015625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1839.5880126953125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 963.7244873046875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1256.3424072265625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2467.31201171875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 10503.9140625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7434, loss_val: nan, pos_over_neg: 7949.44482421875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 957.4435424804688 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2102.126220703125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 12921.025390625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1425.1102294921875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 3185.957275390625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7455, loss_val: nan, pos_over_neg: 2289.865966796875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1305.400390625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1787.61279296875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1783.36279296875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 2449.099853515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1206.3477783203125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1597.34326171875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1167.403564453125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1857.8851318359375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1761.4884033203125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 3281.494873046875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 1158.1241455078125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7377, loss_val: nan, pos_over_neg: 2107.805419921875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 2855.52099609375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1232.0177001953125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7409, loss_val: nan, pos_over_neg: 5290.86962890625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 948.934814453125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 1809.8367919921875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1470.002685546875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1009.8067626953125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 992.1978759765625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1160.1656494140625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 2424.80224609375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 22343.849609375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1712.6484375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 969.4779052734375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1093.1005859375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1637.1328125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1247.54541015625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1080.5616455078125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 667.2026977539062 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1242.598388671875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1687.926025390625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1160.5616455078125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1241.7789306640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 2337.80908203125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1033.117431640625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1697.8192138671875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1142.848876953125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 957.0098266601562 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 10442.294921875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 4815.2802734375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1566.0919189453125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1788.0245361328125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7412, loss_val: nan, pos_over_neg: 9963.0830078125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 1157.8343505859375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7438, loss_val: nan, pos_over_neg: 2377.981201171875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7427, loss_val: nan, pos_over_neg: 21773.728515625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 3996.17236328125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 704.5606079101562 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1316.0052490234375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1250.04345703125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1359.30224609375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1616.0413818359375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 980.631103515625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 726.831787109375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 3216.251220703125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1161.1834716796875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 2000.7452392578125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1842.9300537109375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1292.2020263671875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 882.5769653320312 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7431, loss_val: nan, pos_over_neg: 1178.6680908203125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1214.600341796875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1217.2200927734375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1936.9530029296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 6134.14404296875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 1432.10986328125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 548.3812255859375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1669.178466796875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: -9499.94140625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1569.0439453125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 936.4603271484375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 974.3214721679688 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1054.138916015625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 2413.247314453125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 1098.5740966796875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 994.41455078125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1527.3671875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1182.543701171875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1904.2745361328125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1136.861572265625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1402.4376220703125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 935.0045776367188 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1107.814697265625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1285.494384765625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 3812.542724609375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1776.57470703125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1158.8255615234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 843.29248046875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1062.6153564453125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 803.4630126953125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 1213.0906982421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 3445.44970703125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1274.71630859375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1630.46142578125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1311.6572265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1503.0892333984375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7432, loss_val: nan, pos_over_neg: 2011.5322265625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1178.6451416015625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1280.590576171875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7451, loss_val: nan, pos_over_neg: 1239.2841796875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 750.5223388671875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 964.1969604492188 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1370.39013671875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 954.51953125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1206.5791015625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1228.281982421875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 906.5855102539062 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 701.4729614257812 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 813.8690795898438 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1115.3843994140625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1269.2169189453125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1228.123291015625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1005.4605712890625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 903.7487182617188 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 767.4508666992188 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1370.70068359375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 852.248779296875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 766.9859008789062 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 644.9713134765625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1352.46826171875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1415.444091796875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 2006.7720947265625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7436, loss_val: nan, pos_over_neg: 849.7510986328125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 832.57861328125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1007.173095703125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1175.032470703125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 937.1153564453125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1750.1253662109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2240.600830078125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 3814.162841796875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1559.3944091796875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 828.7543334960938 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 656.4244384765625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1605.486328125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 2536.178955078125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7426, loss_val: nan, pos_over_neg: 7013.419921875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 988.2349243164062 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1042.5367431640625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 721.9896240234375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 974.2037963867188 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1805.9937744140625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1681.1053466796875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 2282.3203125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 835.278564453125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 1057.047119140625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7447, loss_val: nan, pos_over_neg: 4815.31201171875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 866.6065673828125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7466, loss_val: nan, pos_over_neg: 1022.1168823242188 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 2031.032958984375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 2952.94189453125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1794.9244384765625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1922.2354736328125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.743, loss_val: nan, pos_over_neg: 2815.790771484375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1583.504638671875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1432.6435546875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1955.52099609375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1985.14697265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 7343.49072265625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7437, loss_val: nan, pos_over_neg: 1034.2073974609375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7461, loss_val: nan, pos_over_neg: 736.397705078125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1505.8624267578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 3082.027587890625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7445, loss_val: nan, pos_over_neg: 1001.3018188476562 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 612.6395263671875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 817.7188720703125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7429, loss_val: nan, pos_over_neg: 845.9335327148438 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1083.1693115234375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 915.936767578125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1165.084228515625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7418, loss_val: nan, pos_over_neg: 1192.8048095703125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1310.9534912109375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 1115.7364501953125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1200.3236083984375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1062.249755859375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1670.335693359375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1128.2471923828125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 672.021484375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1389.3765869140625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1901.376220703125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 739.84375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1218.6553955078125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 865.142578125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1743.59228515625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 3419.69287109375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2822.12451171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 2049.764892578125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 2060.1044921875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 791.7069702148438 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 2043.8590087890625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 2247.971923828125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 933.7190551757812 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1424.8944091796875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1186.998779296875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1030.3472900390625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 2019.0003662109375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 970.445068359375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1024.5665283203125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2547.621337890625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.746, loss_val: nan, pos_over_neg: 1773.3006591796875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 22958.0546875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1009.2500610351562 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1491.259521484375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1916.2664794921875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7411, loss_val: nan, pos_over_neg: 1929.8203125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1774.364990234375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1101.726806640625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1037.8118896484375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 921.4912719726562 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 4070.13134765625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 847.2189331054688 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1354.7840576171875 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=clmp'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
