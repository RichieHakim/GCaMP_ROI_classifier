{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/josh/opt/anaconda3/bin/python'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "# base_dir = '/n/data1/hms/neurobio/sabatini/josh'\n",
    "base_dir = '/Users/josh/Documents/'\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition, torch_helpers\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [],
   "source": [
    "# DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.45, 0.45), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAu4UlEQVR4nO2dT6zs2F3nP79zbFfV/fdev+5O0h2a0IwiDWE2ZFoECYSQGDSBTdiMRBZoFpGyCRJILOghC1aRgEWWLFoiggUkygxIk0UkNCBQhARMMihAklb+K3+gSf95/frdP1Vl+5zfLI6rbt26tst2uW75dd+vVLp1XcfHx/bXv//nWFSVW9yiCcy+B3CLRwe3ZLlFY9yS5RaNcUuWWzTGLVlu0Ri3ZLlFY+yMLCLyfhH5qoh8Q0Se39VxbnFzkF3EWUTEAl8DfgH4PvB54IOq+pXeD3aLG8OuJMtPAt9Q1W+pagp8CvjAjo51ixtCtKN+3wl8b+X/7wPvq2qcyEjHHO5oKLdog1Nef1VVnyz7bVdkkZJtV/SdiHwY+DDAmAPeJz+/+uPmI6yqz9X2VWp1vc869bto21ZFNxlHk31XUdZP2/GVjavievyl/q/vVHWzKzX0feCZlf9/CPi31Qaq+oKqPqeqz8WMru6t2v5ib8Kiz6qL1Teq+m9z3KprsDgPkfrjrP6+fk2rvtdgV2T5PPBuEXlWRBLgV4DPtO6l7CTKiNSVWFUXexdkLTtuHXHaHH+9r6ZStOV57kQNqWouIr8G/AVggU+o6pc7dhb+ivR3AxdP5QLbqI++sD6mofS1gl3ZLKjqZ4HP9tjhdr9XtV+/qH2QsqndVLZf0+O3tcF6eAh2Rpa3JPqQSk0M76b99Cw9H81w/66N011gl2Ous0l6xLAkS5WxualNn+jzAvc91q7ueE/nNBzJ0tbV3AVpbpooj1hJ67AkSxm2cYv76qtPVAXZhjC2DRguWbro4ZuyZdp4Ik3bdfGC+niQ9h1n6YRNg666mNsQ5CYN5a7udB3qCLYD9T0csmzCrsX0LtILQ7Srtth/eGRpa8GX3ZRNN2pbUd6H2llt0xepdiwph0eWBbaNZDYRz31Lk01jrnoQdiU116PUWx5nWGRpcyO7ejttQupNMKQA4aqU6hyTqW4yHLK0Oclt3eJdZ5T36Qb3VUdTguEE5Ra4iSe1qjzgpqLDm8oTdnXcLTEcsqxXvrW5oI9AQKsUQyDMIxlnqcI2Yr1MtfUdl2lbedfF++k5x3Ptmu65Uq5f1FWBtUWfUqjDBW+FXamrjmMdlmSpyzBviptUlWA2RR+Jvz7jL11TCjvEcCRLGw+njyjmvozNLmhTxV9m7/V0fsORLJuI0Zfe3icx2pxD35HlHlz64ZBlE7qcaBdiNLmhfRucXbCHMTw6ZGmLLm73uuje1Y3ow8Nrg57O481Lliaoy9FsmtOzT3W2J4k2bAN3V301mVzV99ST9X26nm8fNlvHYw9LslxRA2s8Vl++ffHbqmive/L37YKujm3XdkeZuttCKg6HLCskECPhf1OclFfQtW0LFL+pc1e3V12UfSf6tkXTzPwqIXsqiRgGWaQgCCwJIdZeefLUOUQErAVTEMv78FuWgyjgl+1vHPvwkPoi/hUCVjcbBFlEDDIaBYJYC9YgURS+e496jywkhxjEXkohdR7JczTPIctQ59E8W7a9hnWV1Rf2bfTeAAZBFowgkzESx5DEYC0aWYhsuAlZjuQLsgQDTY0sb7pkOZJmaJrCdAbqUa+X0goCcdSj3lzaP32jrqB8PY+0iVjbVtW1KfJq2OcwyCIGmUzQUSCLJhEaGTQyiFMkc5C5KxdZRYIv5wFrEWsRY8C5QmUVF8BIUF/hH3AedbBUWQvsKhG4CbtUWz33PQyyWIMeH6CxRZMIHxvUGtQK4hRjBbGC5MFGWd5nD6IKRtDIIj6COEHSLCz0IQLGXNo6AGkKuQRSed2dlIHmEmTHYfq+MAiyqBH84QgfG3xi8ZEEApiCIAZEBCMOyRyi/jJW4gNZiGywbSIbVNliu7UQRUHqQNg2C3ac4AopQz83pUrlVLUr+7+MYNsUmfeYvR4EWbCG/CjGWwnqxwASVI2IQTwYq6iTsFidKjgfvCGR0HghPZI4bC/IIsYE1RaHU116WfMUsmAILyXMNhf2EZnOsc1DMQiyeAvpSXEzPdfdt4VXrYALBu+VE/YafjSCjpJAiIXkEUGTGB3HS0JJ4W1plkGaIVmG5vn1WM06yi50X1NLmkR3t4kaV1UNPmpllWqF9NhgMjC5YpyCFsS50lAR7694Rmq4JIYxqLXoOEacBgkD+HGCn0SoEawVTCFxllJGBF2oNK2I1dQFuVqfsNaTrEwV9XHMVaxHkZftqrsYBlkMpMeCScGmgs00kCZXpOxhX3hE1oQAXWSX3tPCrQ4xuqBa/Cgin9hgrxTkEGuRyCIiwX5xrjB6K1zrTcZq15u5q9qWHWBjIlFEPiEiL4vIl1a23ROR/yMiXy/+Prby2/8o1uv/qoj81yaDUAvZsZCdCOkdIT0WsonBR+HG6+o9EgmeUhxBHKHjGD+JcYcJbhLjxhF+FOHGEe4gwR3G5IcRbmLJDwz5YUR+lOAPYnSSoJNRCAjGUXkQ78pASxKQTZKSTfuq+r2qAm4bdBhzk6zzHwHvX9v2PPBXqvpu4K+K/xGR9xCWMf3xYp8/KNbxr0WQLEp6TPER3BhcLKjlMh8kgtrg+RAFN1uTghiTCHcQ4cYWNzIFOSzZQRRIMjHko+LvgcUdxvhxjI4SiKPgMdmFatphMr7tzW7iTa1uXydVjyTbqIZU9XMi8iNrmz8A/Fzx/Y+BvwF+q9j+KVWdA98WkW8Q1vH/u9pjGHAHis/AZMGVFi9FHGWRBwqcsxBsExGwgo8MfmTJJxa1IA7E68I3Dm65FXwUjhOsZVPYRIp4RbMk2EF5DmlWRHo3XZkO6Gr3NE0l7FhddbVZ3q6qLwGo6ksi8rZi+zuBv19p9/1iWz2M4g48kgmahsiseCkyzaCF+7ywOYy9vHBqBZcY8klQWcHW4dLWkUASNeDt4iYJooJzFsk8Mo7BOSQfhz6z/PoN3dbrKYudtCVMFzQp12godfo2cMuOWnqWq2v328fvIhOHWoNfZpQX3UkhBQxqww23cxOkgio+MrixIR+H+IxNi/5NIWGEpdGrhhDwE8V5wcSCH1kkj7BZDKME8hyxBs3bnnlN1nnoCcaGkqsrWX4gIk8VUuUp4OVi+8Y1+y/Hpy8ALwCM/8M71SYOJ4oCXg3ihbzQJeKDd+MSi5koxtlChQTtlE8M+YSClqGt+PBRWWqwy2NLsIVcYoKqyxUzzy/tlQVhmxZU9WGH7AJtxtVgTF3J8hngvwO/W/z93yvb/1REPg48Dbwb+L+bOhNRklFGSoxTQb3g1KNiAEU0SBhxRWBOg5oSDwi4RPDxIi4T2otb7LewVVbiNgWBJA72i01NyEvZUP4QOFfiQm+SGpu8m5vEDqTZRrKIyCcJxuwTIvJ94HcIJPm0iHwI+C7w3wBU9csi8mngK0AOfERVN4RFwYgyinNUQ8zDabBTvAQD1zlWgnThIiwkB4Qbr5bCVhGcV8QVsZZ1aJA2GMHFiorgxgY7izBJDHGMRBFi0hBzwfcfENs1dqT2mnhDH6z46efLNqrqx4CPtRmEiDJJsuX/KQQJo4p3WpClkCxFOmApYbQwYCUQxsdFWx9u0EIlBc9qMUiWnpIayMcGOYoQN8LmDmZzJE0hz9lM9QFiR+QcRATXiHIQF0k9lfDxEqomY8EtarVzMK5wqUtySB4gJggflSWZxIU0Ais3Xle+BJVmMXmMzEfY8wSdxSyz2qu1L33diD5ySjdcvjAIsohAbByJNbg4L7YpmVFcFDwejQySF9LFXZJgcdeDpAkEW6qs4q/JQhpBchDVK0QTDe63jwnqaBxhxgkyS0K6wLnm0qXLzesr+dgFLeuGh0EWlMh4EuNw1iCFDeMnwiyLmF6MyBOL5sF7wUsgTi5XJYwouqhY8MF+kRzsXNAITBq2GUcwgB2XNd4GfCxoEkoaZDQKaijNuCKS1rHuJfVVyth3InFT/w0wGLIkJl+OJlbDyOYkxjHNY+5HjvNZQp6bQnoYXGrQzICTS7IYwCpEPmzPDZIKakKBjEqQMgBm4TF5lvZNiPYGz0ji6LK6boGqG7Ye3Cq7EX1IgT0nFAdBlth43jE+5WE+4kzC+xLvJDOOoxlTlzCKcu5HB8zziNwZMmdJTURu7AphQrANo4gJ8RpU0SgYvhQ/y1I1KdEU4qnHLLLcc4+d5qHmN3eFvdIRfZUx7Ks2uASDIMvIZLxr8iqvZse8Kkd4hKdGb/BU8gYzjZjYlLHNeJiOmeUxF1kMBGPYFQYxOYQACqgzhcejKxPVClunSAXYOSRnnuSNHJM6TO6RzGGmGXIxC4VRzpWXK2xCXzf4JojyqBU/JeJ4Jr5PXCR0vArvHL3OjySvkGlELI5YHPejQ06zEad2jDUeYzwzSciJijqUQiWVxc4WEsWBzZRorsTnjvj1GWaeXU43yfJlBZ3mbWP+N4S+JE7LPgZBFgAjnmMzg+QBFuWZ+DWeiR4wU0uqFq+Go2jORZIwHSc8yCY8zMY8mEx4MB1zcT7G5ysJRh9slgVxtNBU+GDo2rlipw4zz5DpPJAjzULm2blQZpnloT4X+jFa+8IuVVQNBkMWi3JspxzbKYdmzjvtGzxtHTPNyfR1AO75EU4NmVpO/ZgzN+bf5yd8J77HD0SZpTHOBQPYpzZooiKvtKjcMU6xqWJnHjMvShKmM3Q2Q2fzK+WVy0LuquzsvivYbthVHwRZPMJMY8aScWjmHJspxyZjLBEex1hyxpKRWEcsORbl3I849WMOTIpXg1PDg+mENLekWUTqBc1X1NJKzCWaK9HUYWYZMkvReYrO5vj5vPnF3KZMYd8k64hBkMWp4YE74NhMseKJNefUx8SSce4Nr7hjXslPiCXnrr1gbObEYhlLxoFJuRtf8Ph4hBXPRZYwjWLOgHluLr2gvCDKTInOHdF5hkwLouQ56loYskO42dtEfTv2MQiy5Gp4PT/EFxPeDT4Yuw4e+Akv58e8mh9zZGec2BkWJRHHTDwjk3EnmjIfRUTiuYhSTqMxuTOk86hwoQvDNlXsXInOc8xFGqRKVswfaur19HGT+kZX9VgaD6puPgiynLsRX3j9XZwkU+4lFzwRn3HHTrljz8k04r475NSNMeKZ+ZhULK7IPlsCYSY24zCakxbFK77ILy3ySIuPcYrJfZh7tDBinb80ZKuwzVSQIRc/NS3ZZCBkmc1j/uW7T5OMc44mc+5Opjw2uuBtozNGRcjViJL5iHM/WnGxLwurLR6vhtRFTLOYNIvQ1GAyuZJDglD8FL5o8dliXZdNdkgbovQ1Ya0Oq+R4FNWQzIX4WxOyA+XVowmvHR5xcDTn3w6m3B1POYln3E2mjEzOqR9jV2afOUwwcDHMveU8TzifJ2RpBFlBloWhu4bg+fh+Jshvmq3YZP+uaCEdlu077D8Istg53P26JzsUsqOI7Dhieifh/M6E147n3Dmccm9ywcPRmAfxASfRlNg4DMrcR7ySHvPa/ICXL455/XzC9GyEnkfYc4udCXYO0SwcZ5F5BsIEs2JmojponF5eEGNX6mUfBvQOyyp7RTR13P3yKe4oITuOmN+1TJ8wzJ6ImT9meflOzMVJzBvjMUdxymE8J7GOxOTMXMwr0yNeOz/g/GyMP4uxpxabgpkLNoXoAqILJZopJismzS/mPUdRsdyYD2mDrsnCPqTKwCvyBkEWnc2RF79JfHxMfOeI8WNHxOcToqllOrWkU8Pp3HJxMOZ+kpPEOZF1WKNkueX0fIx7mGDPDOMzQ3QR3ORFLii+0PA5d0Qzh7iVQJuRywLtRoPdUqJsIuOAMQiyoIpPM8xshhjBiDCxgslGRBeW+UND+npMfhCRjWGe6LKs0mRCfC5MLtYkSDHZTBzYmSeaOuwsD4nCaYpM5+h8XnhDbrM31OGcgGZG6x7yPF0wDLIskGUwDRc3yh32dEzy+pjsTkJ6YskOhHwSpnCYfBE3gWjmiaYeO/fYuUNSH9zjwtuRuQvrzmX5isucLQNyy+xy1znLC5QZuXvK41SOZwsMhyzqQyRVNaxoMJ/DWUJ8NiZ6eEByGCa5Z4cRPhFMqtiZKz45MguLEEoeVkNYFu6qXiYGXbBN8GGp1EWysLUnVEaAujjMkAqftrCPhkMWCCtMUqgEF8oF8MHGsOkIM0uw5zE+scv6E5k7ZB6isYulTVmUFixIUJBjeYG8D27zar1KXZyk7LeblBTbHGt1/F3KP1cwKLKgi5UkHeoMYsL6tpJmyDTGJAlmPArLnqoGQzV3IVs8n1+u3rRqf6xJDS37bdPksboLu8/obFMp0dMYh0OWa6LbhXvpi5KBLNSbSJqCscUuPixVmqZomhXSo2TaxtBKC/rGDdlEwyFLFRbSZmGAOnfNw9Bi7dtrkmKbwum6dkMkXxNbaZOEkfJI9wKPAFk0SBmVYKhWLbG+2n6XGEJScEGMLmH+KtX1qIT7G2FxclUh+SaiuK9JWV326dubqcpFtSlPWG3bgHjDeTnVttgnUZosw9WEaJv66UOqrbv7LTLdw5Isfafob1qStDU0N633ckNeTlM8mpLlJi5SlXHcp020qb8yQ7qPMaz28ajNG1pik97cMqjUCJvWX1v/XrdPU9R5b1362VH7YZEF9uuG7uJmlaUEmpZBbDrWDc8WGA5ZbsKb2YQqydZGVexyHGXtqsbTNOLc4poOy2bp68K3LTyqM6y7EqXNb7vwgJru18STKzAssuxLBXWtM6mzY9rEO+r6volr0vAYTdbuf0ZE/lpEXhSRL4vIrxfbe12/v3cvY5u2bTyOrt7J+hPdRP0t9ln91I2nqyqrQBPJkgO/qao/BvwU8JFijf5e1+9/S2BX3lsZceqkZUcXfCNZVPUlVf3H4vsp8CJhifUPENbtp/j7y8X3D1Cs36+q3wYW6/ffHIaQv6nCrkorq9z5Hq9FK5uleOHDTwD/wNr6/cDq+v3fW9mt2fr9fWH1InWJwPY5ji5jqEOVRLghW68xWUTkCPgz4DdU9WFd05Jt185GRD4sIl8QkS9kzKsO2nR4NaNp4RV06bvvfE0f+7bxtlqgEVlEJCYQ5U9U9c+LzT8o1u2ny/r9qvqCqj6nqs/FjLqNfpOu7js8v45N/Zf9XnbjhqA2GxC/iTckwB8CL6rqx1d+WqzfD9fX7/8VERmJyLM0XL9/K1TlUNpgF3mfdfQZT2k71h6O3SSC+9PArwL/IiJfLLb9Nj2v31+KoVag9dn+ptDDmJqs3f+3lNsh0OP6/Vuhr+r3bdo0wSKMX5Yv6trfKrrmihqmF4aTG+qCpvmPurZ12IU3VebJ3KQhX4UG1+fRJsubCU3IXJca6KvMoQZvLrI0MeK2yeHsKiu+DVHa9vWmyTpvgx1kWXvddxv0lXne0m1/85BlF9iHV9M3GXvsT3QAbp6IvAKcA6/ueywt8ARvzvG+S1WfLPthEGQBEJEvqOpz+x5HU7wVx3urhm7RGLdkuUVjDIksL+x7AC3xlhvvYGyWWwwfQ5Istxg4bslyi8bYO1lE5P3FLIBviMjz+x4PgIh8QkReFpEvrWzrdzZDv+O9qRkYurcPYIFvAj8KJMA/Ae/Z55iKcf0s8F7gSyvbfh94vvj+PPB7xff3FOMeAc8W52NveLxPAe8tvh8DXyvG1euY9y1ZfhL4hqp+S1VT4FOE2QF7hap+Dri/tnmwsxn0hmZg7Jss+50J0A7DnM2whl3OwNg3WRrNBBg4BnMOfc/AWMe+ydJoJsBAsNVshl1jFzMw1rFvsnweeLeIPCsiCWHa62f2PKYqDGc2wxpubAbGADyPXyJY798EPrrv8RRj+iTwEpARnsIPAY8T5nR/vfh7b6X9R4vxfxX4xT2M92cIauSfgS8Wn1/qe8y34f5bNMbO1NAQg2232A47kSzFEhtfA36BIMY/D3xQVb/S+8FucWPYlWQZZLDtFtthV1NByoI+71ttICIfBj4MYLH/+YCT670I/UYsVqMLbftdj0wM3dRrWqe9dh6nvP6qVtTg7oosG4M+qvoCRUHOidzT95n/Uux5KezEyHbvLly8AKLoU4ys/KRX2yzarb/VrGTfK/tvO7a14zT+fQPWx1s9jKvX4S/9//xOVdtdqaFeAlW9v+Ry8wEbNxUjjW9IJ6ySoyVRoMO1E7PxOLsiS7dg24bBLm5QrzeqwY1YSLiyG1A1jtrxNZUaJTewst8Soi/GvBj36v9V51OHnaghVc1F5NeAvyCUIXxCVb9cu1OHp6c3rB57XS0tv1arxMUNXPy+kcir6m7b877yriVf2V8fUnpnc51V9bPAZxs1lmb2yerNaHTyazZIl32u/lRPgnXSFP9c7bvse01/N66KazCQifECYhDjG12cUuMUtnpKr9zoNYnSGauv4auZRlpGisVx143ySiKGxrXHuDq09iTcdyJxOzR5Urf0Irro9qsdNNu3083c8uFo+yAMhCzayhO5ggZWfKMRbDD6mpLmWpstJqb3SZg+1NlAyHIzKHuSNj5dde+FXtteeUNKV6Jq9nA0JswNOAjDsFl0e+Zf0fslXkEVKRodd62/xmOtMpaXtky199JqjA09q9V+uthiwyDLLlByI1pf9Kqnv8rDKUPd7x2kwTWPq6P6fksYuFcMM/XdbZ11rMcrmhBl9f/FPk3Gs8+Y0hZ45CRL3ROxVEUVrm/ruEWXm9p0nzZqbUXiXTeg693lPuM0wyRLGzFfg1Z6eT2Ku5ZAXF70Li76Jtuk7vd1ibd2rNLk6Mr2skBh15jLMMnShiAt2rYyTIeADSqt7mFQr9ek6rYYyFXpiA2ifD2Btjd0MYLLiFLjItedfxXaXpfBSZZKEdlANXW5YLXHrNq/RE3VYf0pr1IJtWOtq8lZH9tK+6rk5lvCG9qFiqjKy2w7hvXygNX+G4fbuwbc1jyzQWedu6I6CtpTKr9GKtXp+KYXu65d46d8TQU1lj6hcaWts36ObfHoSZYuKPMoSptV54fqIsClN7th3OVaFnlN3YqR9tKlqj7nSpM3QwS3r6KgKrRMvDWN07SRiJVPeBP3uSlqJMxiDG0xLLLUXZSWtRtXLkZDgpSRYZ00GwNdLci+cw+tgqhdMWg11EpU1uj5Tsfq8jRXBM3KxtL1pu20SHwDhiVZFqJzSxW06UY0CoOXJCE37tfVa6nZv9FxW6CVsbyG4UmWqkDbpqq4G6iIuxLzWbnJmyKpNT/W/9+kjxaoG2cTV34YZGkqWW+oyKdPtJm6Ubt9ABiGGmry4PSgntqiUmS3rZMp73yxc/n2knFsI2E2qbImfT9aj+k6+qhnKdm/TCS3mjTWZJ/LRpvbtEDrsbfAMCRLV9SVLVbNHV7LmVzvUi7brV5kr4gpCZ9viGV0LpPogK1KRxtgOGSp8wrqalY3BbJKCLUxBmMtIgLGgLXgPaiizoHX+rFeG8Z2E9ua9lEmTdqQZLl/zevah0OWLVCr09cItgyfV/VlA0HE2kAUa0AVnANnIctQB2VXdSvbYtf2WI3N11T6vSnIck3cr80lljhCogiJIxiNkDiGyLKcouE95C78HY/QgzEa28v+co/M5phZis5mcDENUqZiLD2f3NX/awzgpn10jdsMjyybnrCKJ6SqNlWMIEmCjMfI4QQ9nJAfj/CxRa2AgEk9ZpYhTsmPR6R3Y/KxIB7EQzT1xKdj7BszzKkB55E0rUwNtD/l/uc0N+2vjU01PLKsoM1FvGKYUqgTY4JEOTlGjw/I70xI7ySkJxY3EnwkqAGbKtF0hHHK/MQye0xwExAHJofo3DB6wzI+jEhes1hA8hzJsrBMaCFlmkSOq9rVnutiCmzn9zv34OozNLKUFCKHi3hVrVzfrZhYbw3EMZLEyHiMHozxRxOye2Nm9yLmdwzZsZAdQT5RfAwaayBFZsCDmyj+IEdG4XiqIOcRyX3D6LWEg5cjDo9HJAdj5GKGXsyCasoyyPPON6I+jXD1QejcT0X7N5/NUmecFUSRJAk2ycEYf/eI7N6E2b2Yi7cZpk8K6WMef5hjjzImk5STyYzjZA5A5i1ehUmUcRCljG3OyORExvHS9A7feu1xHr58SHonIh+POE4Myf0R5kEUxjAV1Hnq3ImqG3OtpqUmtVEmnbZVYU0JMxyytPAGrolsI8FzSWLkcII/OSB94oDzd8RM32aYvl3J3pZyfO+ck/GcJybnPDk+4+2jhzyVPCAWhyVIEiseg8eKcmjmjCXju9nj/L/Ju3jx8B18b3QPjRJ8EjM5tkwmMfEoQc4uMNai83kwfitUU9sE5vp5V16DywMs+3lzzhsqWcxnle1lv13Z3dpgxE4CUbJ7B1y8Peb8KcP0HR7/ZMqTT5zyQ8cPuJPMuBef83h8ztvjN3gyergkRSwOrwaH4DBLAr0jesB/OoyZ2IzYOr4zuseD4zGzly3zkzGHL0ck98dEowRzeo7O5uh0ijpP0zVnOl22KjKsqfM3WVBuc2FR7YWxFqIInYzIT8bMH4uZPhGIYt455el7D3n3nVf44cl9DuycA5Ny117wpH3I4/acY5Nx18BIDJl6LlTJFGZqmanlcXvGeJTxdPw6b4tP+crBU3ztzpO8cnQXP4pxo4jDkWEiEIkgxqB5DpoGrbSpIGrHea8+ckswGLKU1LAWUzU3Z20X0VaBOCI/jJifGOZ3wd3Ll0T5j0cv8Ux8n7HJiCVnLBljybAoMUqMYSwRFgc4ZigGRyyeWD1jyRmbjNkoxhfknmURZ/MTTGoxmcFOE0zqsGkWjG2R+hxpy7xWI9tircSjz2KpjXS+yZdLbpoiuunENTK4iSE7ErJjZXw854eOHvDswav8aPIKPxK/ytPR6zxpTzk2MyzKTCNONeJUPac+5UIdWeGqxgIHohxKjkO48CMytcTiOIlnHI/ncJSTnWhxTIubxGgSB2m3OJeq0ooOJRdtJ4bVtm9J1iYj/SPg/Wvbngf+SlXfTXg1yfMAIvIewjKmP17s8wfFOv710O1E5OL9AxoZ8pEhOwJ37Lh7dMEzB6/zw8mrPBO/xtPRlCfNnMfNfEmAh37MAz/mgY94wyunXjlVYV4MZyyCATK1nPsRqUYY8UxMylGcEk8y8kNPfgDZRHATi8ZRcHdNz6plJcvexUXeFhvVkKp+rnjv3io+APxc8f2Pgb8BfouVFzUC3xaRxYsa/27rka5j1YhbxiEEteAjwChGlNwbZpow05hznwGQYTj1Cf+e3+U1dwTAoZkTSw6AV4MRvzR8ZxrzSn7C/fyI+/khD/Mxr86POMsSvDPgBeNCcM+kHnEOnA/pg5bn0ggrarpJu77Q1Wa58qJGEVl9UePfr7SrfFHj6tr9Yw46DoNlGYGIBLIYlvIyc5aH+YTX80NesSfLs/VqeM0d8b3sHv86f4xML4WfV1mSZWIzDkyKR7hwCVOX8CCb8DAb82A24cH5BJdaTCrYGcQXnug8Q2Yp6lyQeAOufGuLvg3cMqOilP5X1u4393QrF88URFkc3YNkhrPpiJemJ0xsyshkZGoxEm7eK/kJ/zp/jH+d3uUiT8jVkHuDK/4aUUY2Z2wzjOiSRKfZiPM04Ww2YjZN0LnB5GAyxaSKpDnk7rKcoU9sWnasrHyjx3lYXcnyAxF5qpAqO325ZJ1FL0aCRFkE5BSiqTJ6AzS2zNwhL85iXr13yP27h/zw5D4jkxOL4w034QfzY+7PD5jlMZk3ZM6Su0AWgMh4kshhi6Inr0KaW+ZZTDqP8KlFcoM4UCO4scFPYkxkQ55AdxBjabvOy2r55paE6UqWzxBe0Pi7XH9R45+KyMeBp7mJl0saA1EUCKNKfO4Z3xckF6JzS/pwzA+mEZkzXNyJOYgyJjZj6mLuzw84TUfMsigQJbfkucE7iyoYoxjrMUYRCTfdOYN3BpcbSA2Shuy0WsjHgptE2Dhc1rZEKav5bT11o07KrLcpOW4dNpJFRD5JMGafEJHvA79DIMmnReRDwHeB/wagql8WkU8DXwFy4COqWlN7tRj8Fta690X1miKznPhhiviYaGrJToX4TEAj7kfHeG84HKUcJXOcGs7ShIt5QppF5JnFZQbNDWThwnpRsAoGxCiIggrqBZwgTpDi7HwUyJJPLPE4RkYjjPchX7QqYSqe8CY3bOtobEVtTG+5IVX9YMVPP1/R/mPAxzYeuQVq5xerIvM5CpjcIfMU+zAhOh4RH8fYLMKNDH6U8ECFi8OUi0mMiJLmEWkakacWP4uQ1GAyQbKFtFDUhr8YULOybEZBGCRIFY3Ax+DGBnc4Ij45ClV28zRkpJ2/LJhqsZLm6o1sTJS6pcwqJE+TvgcSwa1AEz3rHH46C8VIFxchTxRHxGdH2ItDJD/AJTFuZICYNDU4Z7DWB5WSG3RmkanFTgWTgU0lSDsr+EhDkZQBFYK3JXrFlFcL3oJPhHwkuIMIe3yIESlc3FDDKyos5ezagkCdJMaijx0tq7aOYZOlJpJ7dZGc4H2IyYPrZS3GK0aVRBWNjoAIOzfMZ5Y0G+FiBQ0Swk6F6CL8tXOwaTiOG4FLQuwmkCR8tCBOkDZQhGdQAR8HVWSPR4j3SO4gS1kWfJfgWolCybnvfBJ9AwyHLD1UzC9FtnNomsLZOSZ3jJ0SnU9IThMuzi3TmSWfXPYRTYXoAqILLcgSJEd6JHAk6OIqKYEsUqifQgWJBxPifXgLrpAuJo2x08L4lryQAg1iL02Jss3c6g77Docsq9jCzVtOJJ/P0TRDzi+Qiynx/RHRgzuY9AR8RHZ0qVqiqRKfh6CanSl25tDIgFp8JPiYZT3uEhKIsYjniQu/q1mRLrMYUxb632Idll3U6y76BR7BqSC9raLgwg2Yz0E9cp6QPJwwOjKIN8EotRDNQlF2dOGxM4+dO9R44rHgEsEni44X4wskE8Oy6BsPximmII2oBi/N+xD6r1BB1867zUpRXdBi8cZ1DIcsO6jnWM4RWrjXWY6ZZcRnMYjgEvCRYOe6zOuY1GHmOWqE6MyQWAnF3VYu7RZWjF2ngTieopZXVz4OmWf4NEWzvNJm6Q09LTZdheGQpQpNdewG1aXOQZ5jphnxWQ4CZmzIR4sEoGJyxeQeyRwCRFOLRgbjBBeH2QBXyFIQSIUgUVwgnZ0H6SSzPExKS7NrbnOlO7ztGi8VaLxiVQ2GTZb1hQPrLmTlXKJwkwQgDUk+exGHZKNGgEGcIsqSCKgW+SWHSS1gMJmGqSMLghjBq+KtIIDJtZhS4olPc6LTOeZ8is7Ty/Poc6nRKinSdF4V7SvohkOWXWRnF7UfC+mf58h0dml0AstLoBq8HAhEcQ5xFpM6pPjNGNDILI1eVl5pf4Uob0wxb5yj5xfBK+vpPK6RoOzdi1tK4DoMhyxN0PVE1aOOMCksTZHz6ZWqL114AsqVTLG4oJZCpLhokhjcKEibohXilWjmiaYOe5FizmboWVG4vTqXaEcqpnVfVQnHDccaFlk2ZVTboERMq/MwC/OERBXjPZKN0DjYJirhxmNACT6x5B5RDXaMU/xBElRQFKK9JlfEK3bqMTOHmeWQZpDlwQPqw3upOveFVOmryGlDH8MhS4PJVNd3KcnIbli6Q9MMzXNMlgeDN8vRJNTNamyDyysSXl2uGsiSOZinSJZjAJNYZGQR5xEFkwWi2IsU5ilaSLBFEnHTOXWKnayvHNXEEVgQb8MaNVUYDlk2oYJMbbA0eB14ChvXOWQ0QrJ8pXZ2bRGfLA8kSDPEGEwSEQmIX5DJY9LQRqYhGLheJbdpvJ2DbQ3Ux7X2HTFYsuwq+LSE12B8Fi418xCWl8UCPksRHya+azGXmanBimBmcVAxzhX5nzyQJAukumL7bJgQ3+V8rvR5Q2vtDZYsndAwJiNGgkrKfZjMnmZQVN1pHC8X9GFFtWmWFy61BtJYWyzw4/H5wj4p6ldWx9ICTWYYLse/aZ8d4JEhS9301o3LflXgSlrAEdSP6mXib9kwhO21+ItqIFYxp1kXJCopo+x7RezbFbZLUHZRauc89+UROBf6XbNblodBQVMQqSVJGereDdAFXfa98oC1zEAPjizbrNfffLeayjtgKWkqIBqmpdauWNmgFqcttllKvXH/Ned9M5ZRC9TdyPoFb27uVErzORVrquxCkvSxHssSLa7b4CQLtLgYWxJk2ye1zatqGs033sHKTrX7t6zGG45k6TusPaCZgDddErkrI3g4ZOkbu3hK6xZfbosFoatWWagifIMHYat4Tg2GRZabkgZtb3rJDbpi3K5+Ohy/MdZLNja17aPNCoZFlr6M1E6xlsY7tBxMz/tX9blhceVr59lhHMMxcG/Qm2l9rNrMb4u+ts3hVE0Sq5tUtmkcj7o31AS7jjnUokEAsK913JbHC51uf9wt6nSHpYZaYAiTrupQGhdaN2YbLmPaKxocuwqPrGSpRB8V7pvC4GXb2x63QZtlHGd1lasNkqPRQ7Rp/BV4ZCXLjaCMAE0udMWFX7xl7CaTgX0e85Ehy5WTXVmEbyfqqIoYdZKmgcHZOVzf0YNalUI3sgDhjaFFMq4WbcR73faaNVRW921bhFRXwlCZUa/qo8bQbr3eS4OxD1uy1D1RHYuLdqUCahd3rhlrr/OHWqDLdRiOZNmArpnSyj56/r30t4Y3ciNhOpxv00L3NiUTw5Ms6+7ljlMAdU/YNUnUNDxe0670eC3OcZvprttK1uGR5YZR9VQ1vajXjMe2Sb6Sqa2N0WT24bJp/UPRBBvJIiLPiMhfi8iLIvJlEfn1Ynu/6/cvpMnqU9l2CawesSDBNfW3aTxlHlTPYffG6FnqNOktB35TVX8M+CngI8Ua/f2u33+TqMgU9xaT2BQl3abepsVcpLbtNmEjWVT1JVX9x+L7KfAiYYn1DxDW7af4+8vF9w9QrN+vqt8GFuv3t8M2F3PTzVi1i9ZKIHuL3TRZ8aGqlqUhmi6hUVba2eVcW42yeOHDTwD/wNr6/cDq+v3fW9mtdP1+EfmwiHxBRL6QMS8/4MAq3nrFWgh/K4m2Likrlviosq9694ZE5Aj4M+A3VPVhXdOSbddGo6ovqOpzqvpczKj+4DskTNcbtOkGN735rY/fVgpVXbsOhnWjOIuIxASi/Imq/nmxeTfr96+6zF0MwyYXs6c5Rn2oq06h/6aLGm1SxTWqqgxNvCEB/hB4UVU/vvLTZwjr9sP19ft/RURGIvIsXdfvb0KUxf91T8+mJ4tu0uWaOG+pMjtJtE39VxVMbWrfcNxNJMtPA78K/IuIfLHY9tv0vX7/GtYvZqM62R1FdtfH1Cau0nAAzbLZdYnMrrm19XBFDZqs3f+3lNshsIP1+ze+JLsO6xe9QT3K1hVlGzLMjdFGvXT5fdO+fZDlRiDVYrmvp7hVGWaZqmuYzS7rv/Ec5xblmrX97AjDIEsFhl46uY5N1Wsbk3YtjfVer0+Dh2EYZNHyEy+dHtq3G132NDc9TodwfSlhdjl1pUcMPpF4TT1VWfxtZvU1O3Dp98qVE1oca9dllW37bxoUHB5ZSi763haw2UMSs9G5dpkF2QNk8QLtfUJEXgHOgVf3PZYWeII353jfpapPlv0wCLIAiMgXVPW5fY+jKd6K4x2eGrrFYHFLlls0xpDI8sK+B9ASb7nxDsZmucXwMSTJcouBY+9kEZH3F4Xd3xCR5/c9HgAR+YSIvCwiX1rZ1m+Ber/jvZmielXd24fw7o1vAj8KJMA/Ae/Z55iKcf0s8F7gSyvbfh94vvj+PPB7xff3FOMeAc8W52NveLxPAe8tvh8DXyvG1euY9y1ZfhL4hqp+S1VT4FOEgu+9QlU/B9xf27zbAvUtoDdUVL9vsjQq7h4ItipQvyn0WVS/jn2TpVFx98AxmHPou6h+Hfsmy3bF3TeLHxSF6fRaoN4T6orqi9+3HvO+yfJ54N0i8qyIJISZjJ/Z85iqsNsC9S1wY0X1A/A8folgvX8T+Oi+x1OM6ZPAS0BGeAo/BDxOmKb79eLvvZX2Hy3G/1XgF/cw3p8hqJF/Br5YfH6p7zHfRnBv0Rj7VkO3eIRwS5ZbNMYtWW7RGLdkuUVj3JLlFo1xS5ZbNMYtWW7RGLdkuUVj/H8qOS9znknItgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAapUlEQVR4nO2dSYwk53Xnfy/WXKqru6u7RTWXkVoGPRAFHyxzpJmxYRgwPKYEDOSLB9LBsAECvMgYG/DBLevgkwDZBwFz8YGACXsAjWhhbMA6CBAkwoZG8DLkaGSJi7mIskWq12IvlVW5RsTzIaKayexcvoyMyIis/H5AoaoiI754+cU/3rfE+16IqmKxmOBUbYBlc7BisRhjxWIxxorFYowVi8UYKxaLMaWJRUSeEJFXReQNEblc1nks60PKmGcRERd4DfgV4G3geeAzqvpy4SezrI2yPMvHgDdU9U1VHQLPAp8q6VyWNeGVVO5DwFtj/78NfHzWzoGE2qANMrZx3OHN2j6JTPxvJ6fnM6VeO9zeV9UL03YvSyyTl23MnGwHkaeApwAatPi4+1/S7U56qCbv7n68bXL7fSd13nvaeftWjiYgho5dk/S36f7jx0weN1GWOPKeevpW/Bf/OqvIspqht4FHxv5/GLgyvoOqPq2qj6vq4z7hu9sTve8iH29bdPFrLY5Jlrnw4iy3/7zyJ8paps7KEsvzwKMicklEAuDTwNeKKlwcuc+LHLMuwcyzYdVyCyxseZHNoZRmSFUjEflt4BuACzyjqi+Vca6qmdZsntTzltVnQVW/Dny9lLINKqjsSjwu//iiTbb9q5abd78y+20ncgZ3o/ouy6DJezuuBhTZrG2mWJassKKYVvHHwjyxAh1jM8VSYKfN7HTzO7NrE8qs7z3mcabZYuRdDLxWaX2WpZDi2vzcJhTYYTSdF8pZ+MJdJs85LvZV7KmVZ7nvDjBQe1Ft8rxKNJnjmVbWWsRv0CQb2WEwzK6VWGD5i296Qe5rSo6FOFbZJmWZ2jezrByd1CJYVvDTqEczVDWzpt7zTLMvOs+0v4s8R4nUQyw6404sswIXlT3l7p+cW1n6ThVnuldZ5XuuUWT1EMsaWOrCGjYThXTKc17sojvR98qLZ+9Tf99XNUXfuSV4gjKeUU1jazzLUiy4oJroahdoRcEsfe4FYQnpLou9kxVLzk7s3MqdFUsyh1WbtJnN0owmNc/5bDO0gFyhCDk8R555nBwnuXd8njI2UyxFzlPkjPlYKKCCY0kmmXWx79teoB2bJ5ZZQpkx2ZW3bzHPo4yHJRRxrrws1e84Fs34ZOSSE4Sb12eZNVdx/NkEeV32oun/WUIpou+xliYpB5vnWWC6WzVxtQU2X8ft/lJ9mgV38iyPVSjTmiXDeqmPWMp+XlJE+Wt8prNUWEEeu6YJZkE59RFL2dPWJZW/VPzIgs7m+NNq4yfF7x68eP9Z9oz/zKE+YjFkahiD+cH5TzznPLMEk6c5WbQu6r4yVxXMEtRaLLMqfBnB5G7/l6z48Ytsus7JlLKWnSx7jlqPhqZVdllPp+8bhUyWOW8UNs22osMbJssfp4BzmAi7Pp6lKBc6pdNnelcaTbQZ21CcV6kL9RFLXuYt06SYCLGlKLjfUKfVA/Vphopy1znLKexiHDdXUpxHqINQYMM8y3EnbFFnrHLXfSzYsZCAk8BGiWVyxDGN2lyYnJkK6kx9miFDlq74ZfKg1JmiRlc5Ym2OqUctSonrfyoSSmkebt5T90XHrdj5rodYCqYObn/tKxGLSvYzh41rhoqg6qWyJkxdblKEl1yhjNqIpdCLZ9BPqZVgVuxXzcvuMLbT3OM2J2C76Gs2p+JXXSBudGGWL3SlMssI8JpGPcSyJspKrbXsHZpngVgdPGGtxVLIxR1z8VVXNuRfSWi09KTkkd/C0kXkGRG5ISIvjm3bE5Fvisjr2e+zY599LsvX/6qI/Ooqxq38XKeE+I5FsbmmZRQq3JJXEhxjcoY/A56Y2HYZeE5VHwWey/5HRB4jTWP6keyYP8ny+K+feRkLZu1nXPT0XL2LMInDWTg/U1HKDjAQi6p+G7g1sflTwJ9nf/858Gtj259V1YGq/gh4gzSPf/UssSLAvMjlvMN9Isvbsa1oojHvWR9Q1asA2e/3Zdun5ex/KL95szF9qFj0OfN8VrAR6znPFIo+88Kc/fd2FHlKRF4QkRdGDAo2g8IrddEyjTp0nudRhJjz1uh1EbkIkP2+kW1fmLP/mFm5+wtlicj1RcwKQlq1s7qKd1zm2CLEnLcGvwb8Zvb3bwJ/Pbb90yISisgl4FHg/+a2zjCifuURk2GnsaikOfOWv84KUF8kiFXEZsrCeRYR+QrwS8B5EXkb+EPgi8BXReRJ4MfArwOo6ksi8lXgZSACPquqc3IJrUbhrn+N4Qx5L26Vzd1CsajqZ2Z89Msz9v8C8IVVjLrHOi6caW65NYnIeEXDnO2rnGse9ZjBrUlwWxXUvWM8Tj3EUuf6KtijbJI4JjmRwU91ozZxwRl57dl4sRgtb12h7KIm/RZN6C2VtqMkOxZRj2YITk5g9RQWNT1LNU0FzRfloT5iqRlVD1OB+73IhFCm2rjE6G2ql5kz0VFrsZjEfpR1QWsnlAmmXuiSn0bXWizH5Hm6W1tMm1uDrA1Tj8llkll91VYsy2Q+WPhlDd7AUeoLpWbZsYhjOycEto53LE2jtmIZp4z+gzgCrou4Y7FZSYLGCeIk9fZOY6yzb1UfsSy4c2ZVSp7XuIgjSBAgzQbSaKQiiWKIIogidDiEJFr6K5icu4jjVn53QE7qIxYDjO+iRf0CcZDAR3ba6E4LkgQZjGAwhH4fRqPFk8oVD/XHBTMpHOMbaMlmsZZimfyyK+fOzzqL4rqI56Ve5dQp4r1dRmcbOHGC241wjgaICDocwXC03Dkq4LieTOpn7o1m+F1qKZZJCmmTxUn7KI0QabdJzu4weKBF75yLOwL/0CMIXLw4QY66q59vjRx7mUVpSBbmzVtAPcQylkWhzFTm4rpIo4G2m0Rnm/TOuXQfcHCHEDQEdQRn0MC9G8wsK8+KxHl3ftlroqYFWOU9Zz3EklFmfIY4AiLgeWjgM2p7DM449M8r7lCIA0HFxeuFuLeaSBimnd3Rezu6q9hYxQhrsjO8+dP9en/7O3Uae9V+guOA56KhS9RyGO7CaC8mioTEdwAhOPII9pvIQQN6fUgUjfMH++X2mCbT9ob1UVRd1kMsiziuuJxf8l6bLYL6HknoMWo5jE4rjXM94shh6IWouAR3HVo7IX6jAXGCRtHc5yX32VmDju9cVngksBliWTUdhTiI60AYomFA1PIY7gij0xEf2LtDosLNZpuO12ZwJ2C46+M1QxgMU4GtYGdut1+W6E5CfhYoL0hIXCcdLocBSdMnartEO+CeHvLvd2/gOTHXmru87p6ns3+O0SkXbQY4XR91clTuuuJ21+zJaiWWeUPAlaa1XRd8DwKfuJE1QW3Y3enx4fYV2s6AW+EOgRPxf86cYdh20dBPj8mTz3b8jWF1b5aWoFZigRJCEcRJO7aOi/oeUcslagpRSznb6vHB4CYX3A5dP8SXmH84fYmo1SbxHVwnOzbvectmzUKsnVjKQEQQzyXxPZLAIWoJSSvmXOOIh9y7fMCLcSSiIW9yZvdj9JptksBFPRcpMFO2ka1lPBgsqFk8OT7SBAfUhcQFdZQocTjQkI4mdJOYIdkTaAF1Bdztqp5FbJxnyTtvoUkCCaAgCjIS9ns7/KD/CLf82/gS8ebgATrdBn4MKqT9lZyd7vx21jc0YqPEkne0pKpIokgcIzFIDM5QuHXU4vuHD3M1OEPDGXFlcIZh3yNQUM9BHQfJQhoWzQ4XNW2/DMaCnBV1t2TztFFiyXVBNIE4htEQ6Q3wDyPCuw6jfYduY4e/k0u8f7fDxdZdEnUQVxntwOCMi383xAv89AEk8bvnn6jkQuJsclDIqoAlRFprsRSVXVJHESQJHPXwb3VpBQ7q+Eji0uvv8ub7Q0bvc3lfq0MQRAxPJ/TOO4R3AvxGiLhuZkNcTlB0VcPrTX7qPC6K8SbnPZ8t6TrTuZsEjRLodnHuNggdB5ImzsjH7Tl0nJCD3ZCHd+6w2+pz/WyDfidgeMOl2QgRz8tCLuPJwqu70BP1sI7wynqIZQrjT0tXTUl+7yFlHKPdPo7jELiCuoK6HkcPCTvhkJ859RN8JyZOHN7pnaV/1eHUTogThhDHiBPxngQiNZpwW0ezV1uxQPEVoHHqXUhiXCB0HNQTnMhjr9HlP7Vf56x3hIPynaMGg7M7xK0ANwxgNEQHy6czMwoPWOChpnrWvKww51IPseiahoyapCGTcQyOixP4+C0PJ/JpeCMecQ8hvMLhToPXz1zgRrtN1HIJfA88L61gKcHOggK5VzMhE/WcJ+z18aMLyJs+axxNNBVMnEAUIaMIGSY4QzgchXTVJSDmvHfA+eYhcQMS30E9d+UFXIUFTa2aI2/GsSa58eojFoMXeRfxVPpYMOlwOsIZRjgRHI0COkkAwDnvkHNhlzhUkkDAc9MQh7zzPHkCn4pIjjzj+Lz1WI9maAGLApLzlKdxjEQRMorxunCzs8PzvQ/hS0QnaXC1t4vbE5xhkgZBFfSe5qVYtZlZsPJyWUwSED4C/E/g/aQT5k+r6v8QkT3gL4APAv8C/DdVvZ0d8zngSdIW8L+r6jcWWrKgYpaZ+DKan1FFkwQZxfiHyp0bbb557sM4KL3I50c3zhHeFvzDCBkM04i5dU3Fm4hkzbnuwMyzRMDvqep3ReQU8P9E5JvAb5Hm7/+iiFwmzd//+xP5+x8EviUiP50ra+UyFTJlRDFXNEnaFMlgRNBJCG66vHzqIoiSDF2cfZ/mTcXrDND+AEYlrFDMQ0V5+8EsW+VV4Dj1ekdEXiFNsf4p0pSnkObv/1vg9xnL3w/8SESO8/f/fdHGmzAzPkYViSIYjmjcGrHzdshR0kQScIYQ3lHaVyOcu10YDFLPUuGFuseM5zymzfTaovtF5IPAzwL/yET+fhEZz9//D2OH5c/fP+4pcsyWzq2YRNPHAP0+/s0uu6FLcODijhS3rwSdEf7NI7hzgPYH6QiqLoxnV1gjxmIRkR3gL4HfVdWDOUFBRvn7ReQp4CmABi1TM9Jjp91Fy750OxsR6WCIc/uAZhwTXg9wBiOk20d7fej1SQaDdL86hg6U+Hq/aRiJRUR8UqF8WVX/Ktt8XUQuZl5l6fz9qvo08DTAruwt/kazxLDC8xmNE4QR2u0hSYIb+DAcocNhOnk3GtVXKBPMeq5WJCZvMhPgT4FXVPVLYx+tJ3//PMbXEy19aDZBF0Vor0dy0EFv301/9/qpYOLpeVrW+dqaZZlpVwFNloln+XngN4AfiMj3sm1/QE3y96/CvdnV4Qh4N2tCWS/eLA1T7zojg1RhacJU9TtM74fAOvL3V0ChqUjXzMxEPwUsgKvPdP8qGPZZZmZ4rGJIXORU/JTMUGUIeiOm+2dS5FB63RQ8FT+N3IvxZ3AyPMuq1CSIaWYC5CVYqfO9oB7qUUtjFDLSWKaSZ9zhRY54lg6jqCm1E8vK1GFKfox1DrHLPpdU8uh90giRm8ARsF+1LUtwnpNp7wdU9cK0D2ohFgAReUFVH6/aDlO20d6T1wxZSsOKxWJMncTydNUGLMnW2VubPoul/tTJs1hqjhWLxZjKxSIiT4jIqyLyRhb4XTki8oyI3BCRF8e27YnIN0Xk9ez32bHPPpfZ/6qI/GoF9j4iIn8jIq+IyEsi8jul2Kyqlf0ALvBD4ENAAPwT8FiVNmV2/SLwUeDFsW1/DFzO/r4M/FH292OZ3SFwKfs+7prtvQh8NPv7FPBaZlehNlftWT4GvKGqb6rqEHiWdHVApajqt4FbE5s/RbqKgez3r41tf1ZVB6r6I+B4NcPaUNWrqvrd7O8OML4CozCbqxbLQ8BbY//nXwlQPu9ZzQCMr2aozXeYtwKDFW2uWixGKwFqTm2+w+QKjHm7Ttm20OaqxWK0EqAmXM9WMZBnNUPZzFuBkX2+ss1Vi+V54FERuSQiAemy169VbNMsql/NMIO1rcCowcjjk6S99x8Cn6/ansymr5Au2R2R3oVPAueA54DXs997Y/t/PrP/VeATFdj7C6TNyPeB72U/nyzaZjvdbzGmtGaojpNtltUoxbOIiEvatPwKqRt/HviMqr5c+Mksa6Msz1LLyTbLapS1bmjapM/Hx3cYz6Lg4v5ci92STLEsQ4fb+zojBrcssSyc9NGJLAofl6krYbeb8bQmaxqIfEv/97/O+qwssdRiomrjqdlItaw+yyZNtlkMKcWzqGokIr8NfIM0DOEZVX2pjHNZ1kdpC+NV9evA18sq37J+qn42ZNkgrFgsxlixWIzZbLGIvHcuwlIqmy2Wms1DnHQ2WyxgBbNGNl8slrVhxWIxxorFYowVi8UYKxaLMVYsFmOsWCzGWLFYjLFisRhjxWIxxorFYowVi8UYKxaLMVYsFmOsWCzGWLFYjDk5YrHhlaVzcsQCVjAlc7LEYikVKxaLMZv9XudxbOB26VjPYjHGisVijBWLxRgrFosxViwnmYLnnaxYTjIFjxCtWCzGWLFYjFkollq/XPI4P4vN07IWTDzLnwFPTGy7DDynqo+SvprkMoCIPEaaxvQj2TF/kuXxXw9WMKWyUCxa55dLqt7/s4lsiMjz9llWflGjiDwlIi+IyAsjBjnNOEFsgGCK7uAav6hRVZ9W1cdV9XGfsGAzLGWQVyy1frnkxrEhTWhesdT25ZKlUecR15rsWhjPIiJfAX4JOC8ibwN/CHwR+KqIPAn8GPh1AFV9SUS+CrwMRMBnVTUuyfb1olpPsazRpoViUdXPzPho6guCVPULwBdWMaq21LGpWKNNdgbXYowVi8UYKxaLMScnYPu4o1fHfsUmcK/+Zu9ycsRiWQ2Dm+zkiMV6lNKxfRaLMVYsFmOsWOo4K1tTrFhsX8cYKxaLMZsplmWaDtvMFMZmimWZpsM2M4WxmWKxVIIVi8UYKxaLMVYsFmOsWCzGWLFYjLFiKYsTOL9zckIUlkEExEFcF/G99H/HQUTQKEJHERrHoImdpxlj+8SSCcVphEi7jbQaaOBD4KMiOIdd9G6HpNuFOM5Ek0MwJ1Bk2yWWrGkQR5DAR3ZaxGd2iNs+cctDHSF8x8dVxYljdDhEEwWS9PgTKIBl2C6xjCMO6nskTY/hmYD+WZc4hKjVouk7eIGP0+0jR0dof5A2T3m9zAlhu8SSrSpMvQX3LnzUcuifF4anYbTjMmo2aZ7yCd/p477jQ+cQen2014cTssAyD9slFsgEksCYl4gaQn9PGT4QMdpxiZoOUcsnbji0RHCz42QwQFW21rtsn1gyVBUninFGCYkLcUtp7vWIdh2653z6F3yGpzwSv0XLd/BFkP4ABoOtbY62ViwkCYwiZBil/zaUi2cOOB30CNyYK4eneat5AcQDbbAzjHE6R6lHAjSKqrS+ErZXLHGMDkdIb4gTgYrS9of8zOkrfKT5NtfPnOHL8X9g//ACfschvBPSCAO070OcpCOrLfMuWzuDq3ECgwFy1KNxO6ZxzeO16xf4cW+PtjPg0fAae80uSTMhDoU4cCDwEc9D3O2stu381qpoHJP0+iQHHRr7fdpXlOitNv/S2cNFedC7y17YRYOEJIAkENT3wPPA2c5q285vDZDE6GiI9nq4t45oX4tpXXG4cus0Pxmdpa8ujiTgKYkP6go42c+Wsr19lgxNFOn2Cd8Z0LrmcHitxXMPf5hbp9tc7+6m+xzfUlEMUZR2jreQrRcLmqBHXbybB+w0XDrXGvzgxkUidbjVa4EoiQcqIHGCxjG6ZR3bY7a3GRpDBwPoHOG/06W5rxzeaPPjg7O4TsLZc4cMHojpn3FIdhoQhohXk3tszWEQJrn7HxGRvxGRV0TkJRH5nWx7PfL3F4COIvTwCOegS3hX8W953Dlsstfs8ssPv8bFn7pJ90FheL4NuztIEFRtcsqaPZyJZ4mA31PVDwP/EfhslqO/nvn7l0UVjUYkvT7aOSS8NaKxLwwOQ84EPf7rmf/PJx58md6DEf3zPsluE2mEIAU65SI9RIkpWE1y919V1e9mf3eAV0hTrNcjf38RqKajo+EI/2BIY1/x9n1uDVoExPy7YJ/wXI/Oww69i210dycNmnLcYi5MkR6iRG+z1O0hIh8Efhb4R1bM31/L3P2jEW6nT+tGROOGcPVglztJi123z/vPdDh6OOHookt8toUEAVLXYXRJgjEWi4jsAH8J/K6qHszbdcq2+6yvY+5+jROk06VxrUv7WsLBfpsXupd4a3gO341JdiOGp4S4kc3iilNsc1RzjLr1IuKTCuXLqvpX2ebrInJRVa+emPz92TDa9Vx2fhJy9M8NnvH/M14YEb3TpHHDpbmveJ0BOoqyGN3tmXMxSccuwJ8Cr6jql8Y+Os7f/0Xuz9//v0TkS8CDbFD+fo1jkm4XiSJCES74FwjvhMSNBm5f8XoJ7Wsj3NtHxFH0bhDVlmDiWX4e+A3gByLyvWzbH3AS8/erooMBOhxCHNNUJXjnFEnDv/e5d7ePdo7GPMv2CMYkd/93mN4PgZOav181nXvpHOKq4rou6giSKPT6JP1+1RZWQk2mIuuHRiOSg0Ok1783PNbsaTVbuqbIimUWqulT6dFwKwOdprE9475VsEIBrFiW4wSuX14GKxaLMVKH2AwRuQkcAftV27IE5zmZ9n5AVS9M+6AWYgEQkRdU9fGq7TBlG+21zZDFGCsWizF1EsvTVRuwJFtnb236LJb6UyfPYqk5lYtFRJ7IArvfEJHLVdsDICLPiMgNEXlxbFttA9TXFlSvqpX9AC7wQ+BDQAD8E/BYlTZldv0i8FHgxbFtfwxczv6+DPxR9vdjmd0hcCn7Pu6a7b0IfDT7+xTwWmZXoTZX7Vk+Bryhqm+q6hB4ljTgu1JU9dvArYnNtQ1Q1zUF1VctFqPg7pqwUoD6uigyqH6SqsViFNxdc2rzHYoOqp+karFsUnD39SwwnToGqM8Lqs8+X9nmqsXyPPCoiFwSkYB0JePXKrZpFscB6nB/gPqnRSQUkUtUEKBuEFQPRdhcg5HHJ0l77z8EPl+1PZlNXwGuAiPSu/BJ4BzpMt3Xs997Y/t/PrP/VeATFdj7C6TNyPeB72U/nyzaZjuDazGm6mbIskFYsViMsWKxGGPFYjHGisVijBWLxRgrFosxViwWY/4N+D7XK3KJj8QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=transl045'\n",
    "model.forward = model.forward_latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5614, loss_val: nan, pos_over_neg: 1.0447083711624146 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.3778, loss_val: nan, pos_over_neg: 3.3395397663116455 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 7.1618, loss_val: nan, pos_over_neg: 1.6413317918777466 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.9915, loss_val: nan, pos_over_neg: 3.036532163619995 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.881, loss_val: nan, pos_over_neg: 9.184853553771973 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.8503, loss_val: nan, pos_over_neg: 15.330060005187988 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.7693, loss_val: nan, pos_over_neg: 13.769256591796875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.6605, loss_val: nan, pos_over_neg: 13.378978729248047 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.6281, loss_val: nan, pos_over_neg: 15.212271690368652 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.587, loss_val: nan, pos_over_neg: 23.29730987548828 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.5421, loss_val: nan, pos_over_neg: 42.011573791503906 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.5588, loss_val: nan, pos_over_neg: 43.178348541259766 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.5145, loss_val: nan, pos_over_neg: 39.56291580200195 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.4964, loss_val: nan, pos_over_neg: 45.86143493652344 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.4766, loss_val: nan, pos_over_neg: 59.46516036987305 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.4857, loss_val: nan, pos_over_neg: 57.12859344482422 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.449, loss_val: nan, pos_over_neg: 90.30148315429688 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.4409, loss_val: nan, pos_over_neg: 107.88484954833984 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.4304, loss_val: nan, pos_over_neg: 115.69216918945312 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.4285, loss_val: nan, pos_over_neg: 105.70407104492188 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.4205, loss_val: nan, pos_over_neg: 87.18276977539062 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.4159, loss_val: nan, pos_over_neg: 85.43584442138672 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.4051, loss_val: nan, pos_over_neg: 124.76329803466797 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.3884, loss_val: nan, pos_over_neg: 97.57488250732422 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.3768, loss_val: nan, pos_over_neg: 166.6060028076172 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.3642, loss_val: nan, pos_over_neg: 160.82420349121094 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.34, loss_val: nan, pos_over_neg: 192.2418670654297 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.3748, loss_val: nan, pos_over_neg: 179.4260711669922 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.3451, loss_val: nan, pos_over_neg: 168.7216339111328 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.3476, loss_val: nan, pos_over_neg: 152.32601928710938 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.3396, loss_val: nan, pos_over_neg: 209.3761749267578 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.3279, loss_val: nan, pos_over_neg: 337.53271484375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.3124, loss_val: nan, pos_over_neg: 142.59823608398438 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.3512, loss_val: nan, pos_over_neg: 234.0596160888672 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.2939, loss_val: nan, pos_over_neg: 238.94461059570312 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.2963, loss_val: nan, pos_over_neg: 397.1705322265625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.295, loss_val: nan, pos_over_neg: 157.38270568847656 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.269, loss_val: nan, pos_over_neg: 262.0244140625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.2674, loss_val: nan, pos_over_neg: 192.04788208007812 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.2672, loss_val: nan, pos_over_neg: 304.905517578125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.2769, loss_val: nan, pos_over_neg: 166.5602569580078 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.2598, loss_val: nan, pos_over_neg: 230.6359405517578 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.2502, loss_val: nan, pos_over_neg: 178.95559692382812 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.2587, loss_val: nan, pos_over_neg: 238.79042053222656 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.2399, loss_val: nan, pos_over_neg: 442.5587463378906 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.2564, loss_val: nan, pos_over_neg: 195.85414123535156 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.2208, loss_val: nan, pos_over_neg: 139.24087524414062 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.2223, loss_val: nan, pos_over_neg: 117.82904815673828 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.2592, loss_val: nan, pos_over_neg: 168.0215301513672 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.2304, loss_val: nan, pos_over_neg: 533.1426391601562 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.2135, loss_val: nan, pos_over_neg: 163.00454711914062 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.2004, loss_val: nan, pos_over_neg: 189.39364624023438 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.2205, loss_val: nan, pos_over_neg: 229.6129608154297 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.2215, loss_val: nan, pos_over_neg: 105.06242370605469 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.2035, loss_val: nan, pos_over_neg: 437.5560607910156 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.1942, loss_val: nan, pos_over_neg: 524.2943115234375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.2081, loss_val: nan, pos_over_neg: 322.4298095703125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.1971, loss_val: nan, pos_over_neg: 420.36248779296875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.1871, loss_val: nan, pos_over_neg: 114.42517852783203 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.2049, loss_val: nan, pos_over_neg: 298.29388427734375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.1916, loss_val: nan, pos_over_neg: 497.9173583984375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.1877, loss_val: nan, pos_over_neg: 148.88473510742188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.1962, loss_val: nan, pos_over_neg: 388.10296630859375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.1655, loss_val: nan, pos_over_neg: 303.6028137207031 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.1648, loss_val: nan, pos_over_neg: 241.15823364257812 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.1747, loss_val: nan, pos_over_neg: 213.5969696044922 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.172, loss_val: nan, pos_over_neg: 341.55487060546875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.1577, loss_val: nan, pos_over_neg: 445.1236572265625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.1635, loss_val: nan, pos_over_neg: 291.833251953125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.1682, loss_val: nan, pos_over_neg: 659.9988403320312 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.1495, loss_val: nan, pos_over_neg: 270.26434326171875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.1697, loss_val: nan, pos_over_neg: 221.44383239746094 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.1647, loss_val: nan, pos_over_neg: 522.3010864257812 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.15, loss_val: nan, pos_over_neg: 510.1427307128906 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.1677, loss_val: nan, pos_over_neg: 790.5547485351562 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.1433, loss_val: nan, pos_over_neg: 388.18243408203125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.1575, loss_val: nan, pos_over_neg: 311.9070129394531 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.136, loss_val: nan, pos_over_neg: 296.4540710449219 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.1496, loss_val: nan, pos_over_neg: 370.6463928222656 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.1262, loss_val: nan, pos_over_neg: 493.8175354003906 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.1473, loss_val: nan, pos_over_neg: 409.2738342285156 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.1293, loss_val: nan, pos_over_neg: 2194.76806640625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.1505, loss_val: nan, pos_over_neg: 280.5133056640625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.1351, loss_val: nan, pos_over_neg: 405.61724853515625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.1414, loss_val: nan, pos_over_neg: 387.1517639160156 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.1371, loss_val: nan, pos_over_neg: 1773.7943115234375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.1295, loss_val: nan, pos_over_neg: 324.0937805175781 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.1371, loss_val: nan, pos_over_neg: 814.1448364257812 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.1325, loss_val: nan, pos_over_neg: 528.6254272460938 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.1231, loss_val: nan, pos_over_neg: 709.9321899414062 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.1335, loss_val: nan, pos_over_neg: 400.6474304199219 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.1367, loss_val: nan, pos_over_neg: 475.19140625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.1455, loss_val: nan, pos_over_neg: 2108.8720703125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.1198, loss_val: nan, pos_over_neg: 2168.650146484375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.1278, loss_val: nan, pos_over_neg: 311.5567626953125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.1319, loss_val: nan, pos_over_neg: 638.3256225585938 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.1259, loss_val: nan, pos_over_neg: 570.3151245117188 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.1312, loss_val: nan, pos_over_neg: 446.7638854980469 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.1323, loss_val: nan, pos_over_neg: 756.4742431640625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.1088, loss_val: nan, pos_over_neg: 358.8970031738281 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.116, loss_val: nan, pos_over_neg: 200.2230224609375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.1252, loss_val: nan, pos_over_neg: 372.704833984375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.1164, loss_val: nan, pos_over_neg: 325.8014221191406 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.1265, loss_val: nan, pos_over_neg: 306.5794677734375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.1175, loss_val: nan, pos_over_neg: 286.9820251464844 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.1186, loss_val: nan, pos_over_neg: 232.87803649902344 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.1061, loss_val: nan, pos_over_neg: 879.042236328125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.114, loss_val: nan, pos_over_neg: 357.3091735839844 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.0904, loss_val: nan, pos_over_neg: 676.982421875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.1108, loss_val: nan, pos_over_neg: 366.6370544433594 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.1195, loss_val: nan, pos_over_neg: 338.3680114746094 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.1228, loss_val: nan, pos_over_neg: 323.7328186035156 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.0979, loss_val: nan, pos_over_neg: 673.1757202148438 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.0871, loss_val: nan, pos_over_neg: 282.44866943359375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.0998, loss_val: nan, pos_over_neg: 492.8987731933594 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.1082, loss_val: nan, pos_over_neg: 446.33770751953125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.1198, loss_val: nan, pos_over_neg: 571.6319580078125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.1049, loss_val: nan, pos_over_neg: 272.1192932128906 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.1103, loss_val: nan, pos_over_neg: 377.12457275390625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.102, loss_val: nan, pos_over_neg: 362.31585693359375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.1112, loss_val: nan, pos_over_neg: 599.1998901367188 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.1005, loss_val: nan, pos_over_neg: 163.7645721435547 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.1092, loss_val: nan, pos_over_neg: 209.45928955078125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.1022, loss_val: nan, pos_over_neg: 855.9915771484375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.1198, loss_val: nan, pos_over_neg: 184.00347900390625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.1092, loss_val: nan, pos_over_neg: 265.0415954589844 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.0893, loss_val: nan, pos_over_neg: 439.3561706542969 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.0904, loss_val: nan, pos_over_neg: 517.3865356445312 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.1126, loss_val: nan, pos_over_neg: 223.5556182861328 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.122, loss_val: nan, pos_over_neg: 240.52743530273438 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.1017, loss_val: nan, pos_over_neg: 351.4296875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.1078, loss_val: nan, pos_over_neg: 182.03358459472656 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.1067, loss_val: nan, pos_over_neg: 535.2627563476562 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.0869, loss_val: nan, pos_over_neg: 498.54620361328125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.1066, loss_val: nan, pos_over_neg: 241.61083984375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.0953, loss_val: nan, pos_over_neg: 279.10235595703125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.0965, loss_val: nan, pos_over_neg: 447.0877990722656 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.0957, loss_val: nan, pos_over_neg: 419.3135986328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.0898, loss_val: nan, pos_over_neg: 502.5914001464844 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.0994, loss_val: nan, pos_over_neg: 311.58270263671875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.0861, loss_val: nan, pos_over_neg: 341.84393310546875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.0798, loss_val: nan, pos_over_neg: 340.3411560058594 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.0968, loss_val: nan, pos_over_neg: 475.9255065917969 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.0987, loss_val: nan, pos_over_neg: 496.98779296875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.0717, loss_val: nan, pos_over_neg: 590.3491821289062 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.0802, loss_val: nan, pos_over_neg: 685.1510620117188 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.0839, loss_val: nan, pos_over_neg: 364.9896545410156 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.0882, loss_val: nan, pos_over_neg: 341.9855651855469 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.0821, loss_val: nan, pos_over_neg: 304.1137390136719 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.0706, loss_val: nan, pos_over_neg: 948.617431640625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.0812, loss_val: nan, pos_over_neg: 327.23919677734375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.0795, loss_val: nan, pos_over_neg: 1591.5638427734375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.0728, loss_val: nan, pos_over_neg: 936.0488891601562 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.0769, loss_val: nan, pos_over_neg: 359.9535217285156 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.0893, loss_val: nan, pos_over_neg: 771.5518188476562 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.0788, loss_val: nan, pos_over_neg: 866.5228271484375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.079, loss_val: nan, pos_over_neg: 539.6994018554688 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.0737, loss_val: nan, pos_over_neg: 932.593017578125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.0838, loss_val: nan, pos_over_neg: 483.3731994628906 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.0848, loss_val: nan, pos_over_neg: 683.7586059570312 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.0813, loss_val: nan, pos_over_neg: 1233.5386962890625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.072, loss_val: nan, pos_over_neg: 943.22021484375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.0707, loss_val: nan, pos_over_neg: 639.5836791992188 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.0665, loss_val: nan, pos_over_neg: 716.0609130859375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.0741, loss_val: nan, pos_over_neg: 335.1690673828125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.0743, loss_val: nan, pos_over_neg: 295.1044616699219 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.0633, loss_val: nan, pos_over_neg: 1031.24072265625 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.0654, loss_val: nan, pos_over_neg: 687.462646484375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.0556, loss_val: nan, pos_over_neg: 1057.663330078125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.0837, loss_val: nan, pos_over_neg: 220.76300048828125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.0749, loss_val: nan, pos_over_neg: 229.0476531982422 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.0697, loss_val: nan, pos_over_neg: 277.1019287109375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.0675, loss_val: nan, pos_over_neg: 841.930419921875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.0784, loss_val: nan, pos_over_neg: 299.1085205078125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.0774, loss_val: nan, pos_over_neg: 971.7303466796875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.055, loss_val: nan, pos_over_neg: 350.5407409667969 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.0549, loss_val: nan, pos_over_neg: 394.1139221191406 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.0601, loss_val: nan, pos_over_neg: 522.348876953125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.0729, loss_val: nan, pos_over_neg: 296.738525390625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.0443, loss_val: nan, pos_over_neg: 1040.54638671875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.0601, loss_val: nan, pos_over_neg: 287.0633239746094 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.0714, loss_val: nan, pos_over_neg: 333.0135192871094 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.0654, loss_val: nan, pos_over_neg: 549.490478515625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.0636, loss_val: nan, pos_over_neg: 458.9458312988281 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.0787, loss_val: nan, pos_over_neg: 391.1045227050781 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.0611, loss_val: nan, pos_over_neg: 457.8114929199219 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.0733, loss_val: nan, pos_over_neg: 281.6115417480469 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.0773, loss_val: nan, pos_over_neg: 374.2673645019531 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.0684, loss_val: nan, pos_over_neg: 379.35455322265625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.067, loss_val: nan, pos_over_neg: 463.35986328125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.0538, loss_val: nan, pos_over_neg: 325.5542907714844 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.0675, loss_val: nan, pos_over_neg: 394.2484436035156 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.0561, loss_val: nan, pos_over_neg: 421.3294677734375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.0534, loss_val: nan, pos_over_neg: 213.2724609375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.0681, loss_val: nan, pos_over_neg: 512.46728515625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.0494, loss_val: nan, pos_over_neg: 345.7333679199219 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.067, loss_val: nan, pos_over_neg: 539.2222900390625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.0526, loss_val: nan, pos_over_neg: 404.81890869140625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.0549, loss_val: nan, pos_over_neg: 305.9976806640625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.0645, loss_val: nan, pos_over_neg: 1131.072265625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.0608, loss_val: nan, pos_over_neg: 742.1919555664062 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.0422, loss_val: nan, pos_over_neg: 1102.361083984375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.0462, loss_val: nan, pos_over_neg: 1162.9951171875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.0596, loss_val: nan, pos_over_neg: 579.115966796875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.0681, loss_val: nan, pos_over_neg: 616.576904296875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 6.0612, loss_val: nan, pos_over_neg: 267.4844970703125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 6.0473, loss_val: nan, pos_over_neg: 1370.538330078125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.061, loss_val: nan, pos_over_neg: 668.75439453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.047, loss_val: nan, pos_over_neg: 673.1507568359375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 6.0436, loss_val: nan, pos_over_neg: 662.4059448242188 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.0706, loss_val: nan, pos_over_neg: 449.2174072265625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 6.0549, loss_val: nan, pos_over_neg: 488.5371398925781 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.0413, loss_val: nan, pos_over_neg: 1600.236572265625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.0634, loss_val: nan, pos_over_neg: 647.94287109375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.066, loss_val: nan, pos_over_neg: 486.79730224609375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.061, loss_val: nan, pos_over_neg: 496.6512756347656 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.056, loss_val: nan, pos_over_neg: 827.323974609375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.0586, loss_val: nan, pos_over_neg: 400.4051818847656 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.06, loss_val: nan, pos_over_neg: 846.1201782226562 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.0461, loss_val: nan, pos_over_neg: 585.1315307617188 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 6.0556, loss_val: nan, pos_over_neg: 898.7090454101562 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 6.0454, loss_val: nan, pos_over_neg: 2254.171630859375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 6.0393, loss_val: nan, pos_over_neg: 1039.251220703125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.0425, loss_val: nan, pos_over_neg: 680.4540405273438 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.0467, loss_val: nan, pos_over_neg: 393.8467102050781 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.0503, loss_val: nan, pos_over_neg: 1086.8123779296875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.0467, loss_val: nan, pos_over_neg: 411.5586853027344 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.0501, loss_val: nan, pos_over_neg: 779.8031616210938 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.0524, loss_val: nan, pos_over_neg: 852.1635131835938 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.0581, loss_val: nan, pos_over_neg: 498.5189514160156 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 6.0507, loss_val: nan, pos_over_neg: 878.1129760742188 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.0424, loss_val: nan, pos_over_neg: 519.0023803710938 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 6.0619, loss_val: nan, pos_over_neg: 548.3627319335938 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.0441, loss_val: nan, pos_over_neg: 663.0958862304688 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 6.0538, loss_val: nan, pos_over_neg: 1773.2025146484375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 1076.4547119140625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.0544, loss_val: nan, pos_over_neg: 554.8545532226562 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 863.6202392578125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.0527, loss_val: nan, pos_over_neg: 2158.2822265625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 6.0394, loss_val: nan, pos_over_neg: 436.1489562988281 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.0497, loss_val: nan, pos_over_neg: 460.65203857421875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 6.0492, loss_val: nan, pos_over_neg: 480.2988586425781 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 6.0471, loss_val: nan, pos_over_neg: 440.5875549316406 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 6.0427, loss_val: nan, pos_over_neg: 349.9509582519531 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 6.0428, loss_val: nan, pos_over_neg: 361.1853332519531 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 6.0325, loss_val: nan, pos_over_neg: 777.71728515625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 6.0594, loss_val: nan, pos_over_neg: 2358.869140625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 6.0329, loss_val: nan, pos_over_neg: 750.19140625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 6.0476, loss_val: nan, pos_over_neg: 520.2777099609375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 6.0535, loss_val: nan, pos_over_neg: 667.0003662109375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.035, loss_val: nan, pos_over_neg: 1105.5936279296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 6.0261, loss_val: nan, pos_over_neg: 510.42657470703125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 6.0471, loss_val: nan, pos_over_neg: 485.95654296875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 6.0343, loss_val: nan, pos_over_neg: 325.2689514160156 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 6.0613, loss_val: nan, pos_over_neg: 262.666015625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 6.0334, loss_val: nan, pos_over_neg: 293.1493225097656 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.052, loss_val: nan, pos_over_neg: 305.0567626953125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.0343, loss_val: nan, pos_over_neg: 385.33099365234375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 6.0236, loss_val: nan, pos_over_neg: 270.41650390625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 6.0456, loss_val: nan, pos_over_neg: 180.11773681640625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.0253, loss_val: nan, pos_over_neg: 275.3757019042969 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.0542, loss_val: nan, pos_over_neg: 157.3345184326172 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 6.0426, loss_val: nan, pos_over_neg: 575.8424682617188 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.0388, loss_val: nan, pos_over_neg: 997.0798950195312 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 6.0541, loss_val: nan, pos_over_neg: 192.09466552734375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 6.0434, loss_val: nan, pos_over_neg: 386.3359069824219 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 6.0287, loss_val: nan, pos_over_neg: 576.4227905273438 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 6.0532, loss_val: nan, pos_over_neg: 239.31573486328125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 6.0517, loss_val: nan, pos_over_neg: 690.172119140625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 6.0413, loss_val: nan, pos_over_neg: 586.1885986328125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 6.035, loss_val: nan, pos_over_neg: 194.2322998046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 6.0287, loss_val: nan, pos_over_neg: 541.2302856445312 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 6.029, loss_val: nan, pos_over_neg: 244.7694091796875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 6.0445, loss_val: nan, pos_over_neg: 469.5073547363281 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 6.0522, loss_val: nan, pos_over_neg: 375.9588317871094 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 6.0342, loss_val: nan, pos_over_neg: 558.8865356445312 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 6.0331, loss_val: nan, pos_over_neg: 299.59906005859375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.0358, loss_val: nan, pos_over_neg: 291.46136474609375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 6.0363, loss_val: nan, pos_over_neg: 206.1731414794922 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 6.0486, loss_val: nan, pos_over_neg: 286.50189208984375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 6.0389, loss_val: nan, pos_over_neg: 482.79107666015625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 6.0412, loss_val: nan, pos_over_neg: 438.80596923828125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 6.0269, loss_val: nan, pos_over_neg: 268.8370361328125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 6.0346, loss_val: nan, pos_over_neg: 236.72897338867188 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 6.0369, loss_val: nan, pos_over_neg: 919.4022827148438 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 6.0268, loss_val: nan, pos_over_neg: 788.6759033203125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 6.0343, loss_val: nan, pos_over_neg: 544.2269287109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 6.0415, loss_val: nan, pos_over_neg: 350.9344787597656 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 6.0331, loss_val: nan, pos_over_neg: 321.3683776855469 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 6.0325, loss_val: nan, pos_over_neg: 283.15362548828125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 6.0336, loss_val: nan, pos_over_neg: 1003.0910034179688 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 6.0298, loss_val: nan, pos_over_neg: 751.6046752929688 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.0317, loss_val: nan, pos_over_neg: 1099.1058349609375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 6.0332, loss_val: nan, pos_over_neg: 259.2654724121094 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 6.0327, loss_val: nan, pos_over_neg: 480.8229675292969 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 6.0349, loss_val: nan, pos_over_neg: 310.291015625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 484.72711181640625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 6.0342, loss_val: nan, pos_over_neg: 726.8878784179688 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 6.0362, loss_val: nan, pos_over_neg: 870.4984741210938 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 608.7975463867188 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 6.0194, loss_val: nan, pos_over_neg: 455.5055847167969 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 6.0309, loss_val: nan, pos_over_neg: 376.1914367675781 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 6.0298, loss_val: nan, pos_over_neg: 583.1547241210938 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 6.0204, loss_val: nan, pos_over_neg: 946.8668823242188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 6.0316, loss_val: nan, pos_over_neg: 652.010986328125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 6.033, loss_val: nan, pos_over_neg: 371.0552673339844 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 6.0278, loss_val: nan, pos_over_neg: 1513.21337890625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 6.0296, loss_val: nan, pos_over_neg: 1158.2325439453125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 6.023, loss_val: nan, pos_over_neg: 390.5525207519531 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 6.0394, loss_val: nan, pos_over_neg: 504.6603698730469 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 6.0367, loss_val: nan, pos_over_neg: 460.50494384765625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 6.0244, loss_val: nan, pos_over_neg: 577.4094848632812 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 6.0271, loss_val: nan, pos_over_neg: 439.1418762207031 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 6.0374, loss_val: nan, pos_over_neg: 436.30413818359375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 6.0334, loss_val: nan, pos_over_neg: 765.9389038085938 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 6.0304, loss_val: nan, pos_over_neg: 327.9260559082031 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 450.2501220703125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 6.015, loss_val: nan, pos_over_neg: 413.98577880859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 6.0428, loss_val: nan, pos_over_neg: 429.80560302734375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 6.0223, loss_val: nan, pos_over_neg: 461.87701416015625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 418.90008544921875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 384.951171875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 6.0047, loss_val: nan, pos_over_neg: 780.9829711914062 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 6.0258, loss_val: nan, pos_over_neg: 358.01605224609375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 6.0436, loss_val: nan, pos_over_neg: 311.3695068359375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 6.0252, loss_val: nan, pos_over_neg: 736.0186767578125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 6.0158, loss_val: nan, pos_over_neg: 652.140869140625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 6.0379, loss_val: nan, pos_over_neg: 312.00103759765625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 6.0227, loss_val: nan, pos_over_neg: 987.192626953125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 6.02, loss_val: nan, pos_over_neg: 618.2185668945312 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 6.0339, loss_val: nan, pos_over_neg: 338.9792785644531 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 6.0334, loss_val: nan, pos_over_neg: 529.982177734375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 535.4049072265625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 6.0409, loss_val: nan, pos_over_neg: 713.0674438476562 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 6.0228, loss_val: nan, pos_over_neg: 611.02392578125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 6.0384, loss_val: nan, pos_over_neg: 436.9829406738281 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 6.0202, loss_val: nan, pos_over_neg: 602.2349243164062 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 6.0092, loss_val: nan, pos_over_neg: 1023.5556030273438 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 6.0211, loss_val: nan, pos_over_neg: 559.3497924804688 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 6.0386, loss_val: nan, pos_over_neg: 462.798583984375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 6.0354, loss_val: nan, pos_over_neg: 372.4967041015625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 6.0197, loss_val: nan, pos_over_neg: 1038.1435546875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 787.003173828125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 6.0301, loss_val: nan, pos_over_neg: 415.5928955078125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 6.011, loss_val: nan, pos_over_neg: 1562.9964599609375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 6.0336, loss_val: nan, pos_over_neg: 526.05029296875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 6.0197, loss_val: nan, pos_over_neg: 316.454833984375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 1456.7515869140625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 6.0255, loss_val: nan, pos_over_neg: 1009.991943359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 521.692626953125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 6.0232, loss_val: nan, pos_over_neg: 568.2914428710938 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 6.0053, loss_val: nan, pos_over_neg: 729.4674072265625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 6.0277, loss_val: nan, pos_over_neg: 236.23780822753906 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 6.0108, loss_val: nan, pos_over_neg: 377.6046142578125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 6.0268, loss_val: nan, pos_over_neg: 1044.2569580078125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 6.0173, loss_val: nan, pos_over_neg: 1287.7646484375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 6.0328, loss_val: nan, pos_over_neg: 296.39349365234375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 6.0179, loss_val: nan, pos_over_neg: 631.3807373046875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 6.0184, loss_val: nan, pos_over_neg: 460.04443359375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 6.0255, loss_val: nan, pos_over_neg: 971.6778564453125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 6.0022, loss_val: nan, pos_over_neg: 626.0994873046875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 6.0429, loss_val: nan, pos_over_neg: 1173.125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 6.0188, loss_val: nan, pos_over_neg: 1234.5743408203125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 6.0193, loss_val: nan, pos_over_neg: 331.999755859375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 6.0175, loss_val: nan, pos_over_neg: 661.44091796875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 6.0213, loss_val: nan, pos_over_neg: 774.9170532226562 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 6.0374, loss_val: nan, pos_over_neg: 690.686767578125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 6.011, loss_val: nan, pos_over_neg: 1757.1719970703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 6.029, loss_val: nan, pos_over_neg: 375.9028625488281 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 6.007, loss_val: nan, pos_over_neg: 462.12091064453125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 6.0289, loss_val: nan, pos_over_neg: 432.74932861328125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 6.0409, loss_val: nan, pos_over_neg: 565.8624267578125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 6.02, loss_val: nan, pos_over_neg: 1093.6138916015625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 784.063232421875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 6.0301, loss_val: nan, pos_over_neg: 974.6931762695312 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 6.0299, loss_val: nan, pos_over_neg: 977.817138671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 6.0251, loss_val: nan, pos_over_neg: 584.2345581054688 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 6.0141, loss_val: nan, pos_over_neg: 998.0592041015625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 6.008, loss_val: nan, pos_over_neg: 431.26031494140625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 6.0254, loss_val: nan, pos_over_neg: 539.3221435546875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 6.0372, loss_val: nan, pos_over_neg: 394.4039611816406 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 6.0245, loss_val: nan, pos_over_neg: 513.1856689453125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 6.0304, loss_val: nan, pos_over_neg: 544.3283081054688 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 6.0198, loss_val: nan, pos_over_neg: 527.997314453125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 6.0222, loss_val: nan, pos_over_neg: 887.74951171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 6.0318, loss_val: nan, pos_over_neg: 863.8207397460938 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 6.025, loss_val: nan, pos_over_neg: 296.92144775390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 6.0138, loss_val: nan, pos_over_neg: 608.3357543945312 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 6.0352, loss_val: nan, pos_over_neg: 533.307861328125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 6.0388, loss_val: nan, pos_over_neg: 220.11660766601562 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 6.0158, loss_val: nan, pos_over_neg: 288.90771484375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 6.0208, loss_val: nan, pos_over_neg: 920.631591796875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 6.009, loss_val: nan, pos_over_neg: 203.1959991455078 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 6.0236, loss_val: nan, pos_over_neg: 291.3397216796875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 6.0145, loss_val: nan, pos_over_neg: 737.6439819335938 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 6.0078, loss_val: nan, pos_over_neg: 466.16790771484375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 6.0122, loss_val: nan, pos_over_neg: 314.90069580078125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 6.0087, loss_val: nan, pos_over_neg: 435.19000244140625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 6.0182, loss_val: nan, pos_over_neg: 675.5269165039062 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 6.0118, loss_val: nan, pos_over_neg: 641.8616333007812 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 6.0102, loss_val: nan, pos_over_neg: 504.7388610839844 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 6.0194, loss_val: nan, pos_over_neg: 451.6847839355469 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 6.0308, loss_val: nan, pos_over_neg: 394.28680419921875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9961, loss_val: nan, pos_over_neg: 872.5829467773438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 6.026, loss_val: nan, pos_over_neg: 786.3048095703125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 6.0016, loss_val: nan, pos_over_neg: 702.2551879882812 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 6.0211, loss_val: nan, pos_over_neg: 952.213134765625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 6.0214, loss_val: nan, pos_over_neg: 2436.2236328125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 6.0211, loss_val: nan, pos_over_neg: 340.3417053222656 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 6.0079, loss_val: nan, pos_over_neg: 350.2194519042969 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 521.0626831054688 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 6.003, loss_val: nan, pos_over_neg: 368.9083251953125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 438.6371154785156 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 6.0209, loss_val: nan, pos_over_neg: 336.5673522949219 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 6.0238, loss_val: nan, pos_over_neg: 875.6814575195312 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 6.024, loss_val: nan, pos_over_neg: 433.07025146484375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 6.02, loss_val: nan, pos_over_neg: 370.3247985839844 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 6.0214, loss_val: nan, pos_over_neg: 334.5733947753906 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 6.0204, loss_val: nan, pos_over_neg: 698.5596313476562 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 6.0418, loss_val: nan, pos_over_neg: 854.0936279296875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 6.0388, loss_val: nan, pos_over_neg: 860.8716430664062 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 6.0101, loss_val: nan, pos_over_neg: 1435.486572265625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 6.0046, loss_val: nan, pos_over_neg: 608.7584838867188 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 6.0159, loss_val: nan, pos_over_neg: 348.542724609375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 6.0107, loss_val: nan, pos_over_neg: 664.6883544921875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 6.0152, loss_val: nan, pos_over_neg: 737.8226318359375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 6.0186, loss_val: nan, pos_over_neg: 378.5908203125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 6.0078, loss_val: nan, pos_over_neg: 910.477294921875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 6.0242, loss_val: nan, pos_over_neg: 386.6105651855469 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 6.0074, loss_val: nan, pos_over_neg: 276.2027282714844 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 6.0031, loss_val: nan, pos_over_neg: 478.1535339355469 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 6.0199, loss_val: nan, pos_over_neg: 699.0308227539062 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 6.0121, loss_val: nan, pos_over_neg: 485.3247985839844 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 6.0075, loss_val: nan, pos_over_neg: 584.1499633789062 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9981, loss_val: nan, pos_over_neg: 1415.682373046875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 6.0162, loss_val: nan, pos_over_neg: 670.695556640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 6.0125, loss_val: nan, pos_over_neg: 283.9752197265625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 6.0002, loss_val: nan, pos_over_neg: 749.8148193359375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 6.012, loss_val: nan, pos_over_neg: 295.84393310546875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 6.0136, loss_val: nan, pos_over_neg: 385.15966796875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 6.0232, loss_val: nan, pos_over_neg: 305.1569519042969 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 6.0188, loss_val: nan, pos_over_neg: 1780.5045166015625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 6.0175, loss_val: nan, pos_over_neg: 299.8678894042969 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 6.0105, loss_val: nan, pos_over_neg: 465.5108947753906 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 6.0028, loss_val: nan, pos_over_neg: 53082.59765625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 6.006, loss_val: nan, pos_over_neg: 558.9940185546875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 6.023, loss_val: nan, pos_over_neg: 411.5485534667969 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 6.0126, loss_val: nan, pos_over_neg: 958.1452026367188 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 6.0132, loss_val: nan, pos_over_neg: 652.39306640625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 6.0092, loss_val: nan, pos_over_neg: 602.1067504882812 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 6.0082, loss_val: nan, pos_over_neg: 719.3194580078125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 6.015, loss_val: nan, pos_over_neg: 362.0550537109375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 6.0052, loss_val: nan, pos_over_neg: 3842.864013671875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9873, loss_val: nan, pos_over_neg: 1423.817626953125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 6.0358, loss_val: nan, pos_over_neg: 564.4572143554688 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 6.009, loss_val: nan, pos_over_neg: 1964.6011962890625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 6.0148, loss_val: nan, pos_over_neg: 2019.6968994140625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 6.0141, loss_val: nan, pos_over_neg: 603.6923217773438 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 6.017, loss_val: nan, pos_over_neg: 654.0411987304688 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 6.0239, loss_val: nan, pos_over_neg: 1678.7911376953125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 746.6355590820312 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 6.0106, loss_val: nan, pos_over_neg: 434.109375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 6.0003, loss_val: nan, pos_over_neg: 1993.2427978515625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 6268.3603515625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 639.841796875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 6.0182, loss_val: nan, pos_over_neg: 582.9955444335938 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 6.0237, loss_val: nan, pos_over_neg: 607.0393676757812 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 6.0073, loss_val: nan, pos_over_neg: 914.5455322265625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 6.0204, loss_val: nan, pos_over_neg: 469.5321044921875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 6.003, loss_val: nan, pos_over_neg: 640.8543701171875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 6.0094, loss_val: nan, pos_over_neg: 1220.12451171875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 6.0198, loss_val: nan, pos_over_neg: 502.40869140625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 6.0018, loss_val: nan, pos_over_neg: 739.9905395507812 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 6.0064, loss_val: nan, pos_over_neg: 411.8140563964844 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 6.0219, loss_val: nan, pos_over_neg: 967.5386962890625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9889, loss_val: nan, pos_over_neg: 322.7231750488281 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 6.0113, loss_val: nan, pos_over_neg: 458.5781555175781 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9874, loss_val: nan, pos_over_neg: 670.353271484375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9999, loss_val: nan, pos_over_neg: 514.2391967773438 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 6.0128, loss_val: nan, pos_over_neg: 352.28387451171875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 6.0026, loss_val: nan, pos_over_neg: 674.0067138671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 6.006, loss_val: nan, pos_over_neg: 444.4919738769531 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 6.0128, loss_val: nan, pos_over_neg: 741.7863159179688 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 6.0023, loss_val: nan, pos_over_neg: 351.08489990234375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 6.0128, loss_val: nan, pos_over_neg: 1074.291748046875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 772.287353515625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 6.0005, loss_val: nan, pos_over_neg: 273.9508972167969 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 6.0108, loss_val: nan, pos_over_neg: 343.5426330566406 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 567.9720458984375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 6.0159, loss_val: nan, pos_over_neg: 600.4698486328125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 6.0008, loss_val: nan, pos_over_neg: 999.142333984375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 6.0102, loss_val: nan, pos_over_neg: 682.1726684570312 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 6.0228, loss_val: nan, pos_over_neg: 606.8903198242188 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 6.0059, loss_val: nan, pos_over_neg: 669.612548828125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 6.0066, loss_val: nan, pos_over_neg: 395.2066345214844 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 6.0176, loss_val: nan, pos_over_neg: 475.0638427734375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 1577.7000732421875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 6.019, loss_val: nan, pos_over_neg: 634.8281860351562 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 6.0064, loss_val: nan, pos_over_neg: 551.70263671875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 6.0176, loss_val: nan, pos_over_neg: 571.6422119140625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 6.0172, loss_val: nan, pos_over_neg: 14167.009765625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 952.8821411132812 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 6.0012, loss_val: nan, pos_over_neg: 615.519775390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 6.0142, loss_val: nan, pos_over_neg: 845.437744140625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 6.0125, loss_val: nan, pos_over_neg: 383.51885986328125 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 6.0085, loss_val: nan, pos_over_neg: 483.46923828125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 6.0126, loss_val: nan, pos_over_neg: 529.8058471679688 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 6.0093, loss_val: nan, pos_over_neg: 1656.3577880859375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 6.0041, loss_val: nan, pos_over_neg: 288.1192626953125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9991, loss_val: nan, pos_over_neg: 635.0138549804688 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 952.4185180664062 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 6.0122, loss_val: nan, pos_over_neg: 411.2643127441406 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 6.0039, loss_val: nan, pos_over_neg: 751.8464965820312 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9935, loss_val: nan, pos_over_neg: 12334.6240234375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 6.0075, loss_val: nan, pos_over_neg: 623.6766357421875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 6.0157, loss_val: nan, pos_over_neg: 528.523681640625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 6.0134, loss_val: nan, pos_over_neg: 626.3558349609375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 6.0046, loss_val: nan, pos_over_neg: 405.39678955078125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9938, loss_val: nan, pos_over_neg: 593.8229370117188 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 6.0078, loss_val: nan, pos_over_neg: 483.1908874511719 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9888, loss_val: nan, pos_over_neg: 2101.742919921875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9998, loss_val: nan, pos_over_neg: 2141.473876953125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 745.818359375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 6.0042, loss_val: nan, pos_over_neg: 310.7790832519531 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 6.0073, loss_val: nan, pos_over_neg: 587.8541870117188 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 2596.74853515625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 6.0002, loss_val: nan, pos_over_neg: 888.1239013671875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 6.0089, loss_val: nan, pos_over_neg: 543.33349609375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 6.0198, loss_val: nan, pos_over_neg: 729.2456665039062 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 6.0103, loss_val: nan, pos_over_neg: 649.2247314453125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 6.002, loss_val: nan, pos_over_neg: 294.13214111328125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 6.0118, loss_val: nan, pos_over_neg: 996.8489379882812 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 6.0011, loss_val: nan, pos_over_neg: 2155.902099609375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 6.0031, loss_val: nan, pos_over_neg: 579.5631713867188 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 6.0018, loss_val: nan, pos_over_neg: 698.5416870117188 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9857, loss_val: nan, pos_over_neg: 491.5179748535156 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9995, loss_val: nan, pos_over_neg: 691.8154296875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9974, loss_val: nan, pos_over_neg: 617.2742919921875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 6.0129, loss_val: nan, pos_over_neg: 488.15167236328125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 6.007, loss_val: nan, pos_over_neg: 728.3468017578125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 6.0085, loss_val: nan, pos_over_neg: 887.3841552734375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 6.0012, loss_val: nan, pos_over_neg: 657.9371948242188 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 6.0075, loss_val: nan, pos_over_neg: 519.8361206054688 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 6.0002, loss_val: nan, pos_over_neg: 1116.2716064453125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 6.0118, loss_val: nan, pos_over_neg: 1395.30224609375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9992, loss_val: nan, pos_over_neg: 366.7082824707031 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 498.9996337890625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 6.0059, loss_val: nan, pos_over_neg: 599.3072509765625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 6.0106, loss_val: nan, pos_over_neg: 1136.9849853515625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 630.8951416015625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9984, loss_val: nan, pos_over_neg: 2713.24853515625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 6.006, loss_val: nan, pos_over_neg: 777.2307739257812 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 1242.844970703125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9936, loss_val: nan, pos_over_neg: 1099.277587890625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9803, loss_val: nan, pos_over_neg: 506.83154296875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9997, loss_val: nan, pos_over_neg: 396.0524597167969 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9936, loss_val: nan, pos_over_neg: 736.550048828125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9899, loss_val: nan, pos_over_neg: 633.5733642578125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 6.0041, loss_val: nan, pos_over_neg: 2012.0068359375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9983, loss_val: nan, pos_over_neg: 444.1365661621094 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9954, loss_val: nan, pos_over_neg: 1031.600830078125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9974, loss_val: nan, pos_over_neg: 766.8360595703125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9781, loss_val: nan, pos_over_neg: 679.872314453125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9791, loss_val: nan, pos_over_neg: 721.589111328125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 575.6448974609375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 663.1730346679688 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 1103.2823486328125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9918, loss_val: nan, pos_over_neg: 773.22412109375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9792, loss_val: nan, pos_over_neg: 552.970703125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9849, loss_val: nan, pos_over_neg: 1322.9189453125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 6.0062, loss_val: nan, pos_over_neg: 639.7711791992188 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9962, loss_val: nan, pos_over_neg: 479.792236328125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9933, loss_val: nan, pos_over_neg: 427.4188232421875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9979, loss_val: nan, pos_over_neg: 577.4010620117188 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9982, loss_val: nan, pos_over_neg: 492.20880126953125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 640.0450439453125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 966.9716796875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 6.0162, loss_val: nan, pos_over_neg: 610.6343383789062 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9894, loss_val: nan, pos_over_neg: 589.075927734375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 6.0068, loss_val: nan, pos_over_neg: 786.7639770507812 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9904, loss_val: nan, pos_over_neg: 1290.3717041015625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9913, loss_val: nan, pos_over_neg: 971.5384521484375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9921, loss_val: nan, pos_over_neg: 6179.298828125 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9997, loss_val: nan, pos_over_neg: 668.631591796875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 6.0163, loss_val: nan, pos_over_neg: 364.81109619140625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 6.0114, loss_val: nan, pos_over_neg: 510.75018310546875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 6.0047, loss_val: nan, pos_over_neg: 345.0652160644531 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 817.3894653320312 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 702.5608520507812 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 1626.9815673828125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9932, loss_val: nan, pos_over_neg: 618.5602416992188 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9976, loss_val: nan, pos_over_neg: 557.5751342773438 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9921, loss_val: nan, pos_over_neg: 430.80029296875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 440.37115478515625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 6.0091, loss_val: nan, pos_over_neg: 857.6136474609375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 6.0054, loss_val: nan, pos_over_neg: 664.828369140625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 935.0451049804688 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 572.7852172851562 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9907, loss_val: nan, pos_over_neg: 1183.2138671875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 6.0045, loss_val: nan, pos_over_neg: 469.8479309082031 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9865, loss_val: nan, pos_over_neg: 543.1097412109375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9979, loss_val: nan, pos_over_neg: 352.37677001953125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 6.0023, loss_val: nan, pos_over_neg: 529.85986328125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9924, loss_val: nan, pos_over_neg: 1620.6666259765625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9967, loss_val: nan, pos_over_neg: 736.677978515625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 6.0145, loss_val: nan, pos_over_neg: 520.8748779296875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9853, loss_val: nan, pos_over_neg: 1027.973876953125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 6.0005, loss_val: nan, pos_over_neg: 778.9596557617188 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 6.0234, loss_val: nan, pos_over_neg: 263.931640625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9981, loss_val: nan, pos_over_neg: 358.7725524902344 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 6.012, loss_val: nan, pos_over_neg: 1326.798828125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 858.8084106445312 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9956, loss_val: nan, pos_over_neg: 526.5324096679688 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 1054.9241943359375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 6.002, loss_val: nan, pos_over_neg: 692.3777465820312 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 6.0116, loss_val: nan, pos_over_neg: 833.784423828125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9952, loss_val: nan, pos_over_neg: 930.1884765625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9979, loss_val: nan, pos_over_neg: 580.6739501953125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9871, loss_val: nan, pos_over_neg: 461.4008483886719 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 6.003, loss_val: nan, pos_over_neg: 4795.59228515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9808, loss_val: nan, pos_over_neg: 1481.595458984375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 592.300537109375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9843, loss_val: nan, pos_over_neg: 1162.41552734375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 6.0137, loss_val: nan, pos_over_neg: 489.2010803222656 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9903, loss_val: nan, pos_over_neg: 519.7482299804688 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 713.852783203125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 6.01, loss_val: nan, pos_over_neg: 434.8901672363281 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9842, loss_val: nan, pos_over_neg: 1264.494873046875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 740.26953125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9959, loss_val: nan, pos_over_neg: 1611.9014892578125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 6.0077, loss_val: nan, pos_over_neg: 1089.072509765625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9761, loss_val: nan, pos_over_neg: 2961.24072265625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 6.0109, loss_val: nan, pos_over_neg: 457.7064208984375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9851, loss_val: nan, pos_over_neg: 641.6893310546875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 1718.5003662109375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9837, loss_val: nan, pos_over_neg: 5634.64208984375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9882, loss_val: nan, pos_over_neg: 1052.9459228515625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9941, loss_val: nan, pos_over_neg: 1229.6126708984375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 1582.34423828125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 1070.4698486328125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 798.6128540039062 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9989, loss_val: nan, pos_over_neg: 766.283935546875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9914, loss_val: nan, pos_over_neg: 914.8311157226562 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.995, loss_val: nan, pos_over_neg: 638.9437866210938 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 6.0016, loss_val: nan, pos_over_neg: 618.5375366210938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9977, loss_val: nan, pos_over_neg: 4546.6201171875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9826, loss_val: nan, pos_over_neg: 1286.3905029296875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 6.0018, loss_val: nan, pos_over_neg: 675.0919799804688 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9915, loss_val: nan, pos_over_neg: 942.1113891601562 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9828, loss_val: nan, pos_over_neg: 253.60394287109375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 658.3189697265625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9992, loss_val: nan, pos_over_neg: 1161.23291015625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 1122.792236328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9767, loss_val: nan, pos_over_neg: 758.2197875976562 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 960.7869873046875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9931, loss_val: nan, pos_over_neg: 2287.396240234375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 565.8602905273438 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9799, loss_val: nan, pos_over_neg: 740.9690551757812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9949, loss_val: nan, pos_over_neg: 924.193359375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9879, loss_val: nan, pos_over_neg: 636.3250122070312 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 6.0041, loss_val: nan, pos_over_neg: 562.8175048828125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9987, loss_val: nan, pos_over_neg: 551.3480224609375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9853, loss_val: nan, pos_over_neg: 991.2622680664062 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9846, loss_val: nan, pos_over_neg: 1513.2845458984375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9871, loss_val: nan, pos_over_neg: 438.4792175292969 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9944, loss_val: nan, pos_over_neg: 4423.146484375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9872, loss_val: nan, pos_over_neg: 1644.1328125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 934.8778686523438 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9894, loss_val: nan, pos_over_neg: 560.70556640625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9965, loss_val: nan, pos_over_neg: 861.8132934570312 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 699.5509033203125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9839, loss_val: nan, pos_over_neg: 617.258056640625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 6.0113, loss_val: nan, pos_over_neg: 1019.3549194335938 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9953, loss_val: nan, pos_over_neg: 823.9791259765625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9896, loss_val: nan, pos_over_neg: 449.01220703125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 6.0032, loss_val: nan, pos_over_neg: 1119.423828125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 638.7005004882812 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 6.0051, loss_val: nan, pos_over_neg: 1424.0810546875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 6.002, loss_val: nan, pos_over_neg: 793.966552734375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9954, loss_val: nan, pos_over_neg: 1637.8704833984375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 694.2789306640625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 1739.205322265625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9927, loss_val: nan, pos_over_neg: 1225.93310546875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 6.0061, loss_val: nan, pos_over_neg: -2879.77099609375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9996, loss_val: nan, pos_over_neg: 495.3711242675781 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 606.903076171875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9838, loss_val: nan, pos_over_neg: 835.533935546875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9927, loss_val: nan, pos_over_neg: 357.6175842285156 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9812, loss_val: nan, pos_over_neg: 445.4422607421875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9863, loss_val: nan, pos_over_neg: 803.453125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 6.0019, loss_val: nan, pos_over_neg: 288.0970458984375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 3085.9580078125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 1168.8812255859375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9895, loss_val: nan, pos_over_neg: 631.4215698242188 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 6.0065, loss_val: nan, pos_over_neg: 494.2590637207031 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [20:04<100396:57:48, 1204.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.9875, loss_val: nan, pos_over_neg: 469.95880126953125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9861, loss_val: nan, pos_over_neg: 772.0654296875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9805, loss_val: nan, pos_over_neg: 310.6705322265625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 632.7103881835938 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9986, loss_val: nan, pos_over_neg: 1758.383056640625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9873, loss_val: nan, pos_over_neg: 1936.077392578125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9882, loss_val: nan, pos_over_neg: 689.5416870117188 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.0079, loss_val: nan, pos_over_neg: 534.0975952148438 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 1215.5550537109375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 1119.2432861328125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9989, loss_val: nan, pos_over_neg: 915.7656860351562 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9977, loss_val: nan, pos_over_neg: 2230.77294921875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9935, loss_val: nan, pos_over_neg: 1698.4368896484375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9887, loss_val: nan, pos_over_neg: 853.6455078125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.0001, loss_val: nan, pos_over_neg: 672.1329345703125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9967, loss_val: nan, pos_over_neg: 1618.4080810546875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9999, loss_val: nan, pos_over_neg: 1540.5980224609375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9679, loss_val: nan, pos_over_neg: 3234.64599609375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9921, loss_val: nan, pos_over_neg: 889.5944213867188 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9887, loss_val: nan, pos_over_neg: 1082.2647705078125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9949, loss_val: nan, pos_over_neg: 633.7324829101562 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9999, loss_val: nan, pos_over_neg: 1203.9326171875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.978, loss_val: nan, pos_over_neg: 938.41943359375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9936, loss_val: nan, pos_over_neg: 559.7515258789062 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 1236.3876953125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9844, loss_val: nan, pos_over_neg: 1322.3509521484375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9939, loss_val: nan, pos_over_neg: 1191.2103271484375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 1823.971435546875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9737, loss_val: nan, pos_over_neg: 1372.831787109375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 1095.6265869140625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9963, loss_val: nan, pos_over_neg: 1339.690673828125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.0097, loss_val: nan, pos_over_neg: 482.0512390136719 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9947, loss_val: nan, pos_over_neg: 696.2477416992188 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9859, loss_val: nan, pos_over_neg: 986.9342651367188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.0071, loss_val: nan, pos_over_neg: 1360.665283203125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.987, loss_val: nan, pos_over_neg: 719.37890625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9812, loss_val: nan, pos_over_neg: 785.8077392578125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 1283.38623046875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9738, loss_val: nan, pos_over_neg: 680.6217651367188 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9961, loss_val: nan, pos_over_neg: 337.9303894042969 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9922, loss_val: nan, pos_over_neg: 668.7075805664062 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 698.581298828125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.0085, loss_val: nan, pos_over_neg: 446.8855285644531 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.992, loss_val: nan, pos_over_neg: 760.2935180664062 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 6651.677734375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 516.298828125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9813, loss_val: nan, pos_over_neg: 780.6212158203125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 749.2164916992188 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 8267.9697265625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.0013, loss_val: nan, pos_over_neg: 578.846923828125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 563.7952880859375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9851, loss_val: nan, pos_over_neg: 3993.560791015625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9883, loss_val: nan, pos_over_neg: 652.3406982421875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 632.6306762695312 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 1164.10498046875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 1539.1484375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9831, loss_val: nan, pos_over_neg: 925.8917236328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9916, loss_val: nan, pos_over_neg: 575.99169921875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9705, loss_val: nan, pos_over_neg: -7848.08203125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 15429.1533203125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9751, loss_val: nan, pos_over_neg: 847.3267211914062 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9985, loss_val: nan, pos_over_neg: 686.9932250976562 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 1479.008056640625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 670.9856567382812 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9961, loss_val: nan, pos_over_neg: 592.7208251953125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9913, loss_val: nan, pos_over_neg: 740.1051025390625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9941, loss_val: nan, pos_over_neg: 608.6661376953125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 465.9967041015625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9923, loss_val: nan, pos_over_neg: 1727.575439453125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 4593.44384765625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9874, loss_val: nan, pos_over_neg: 335.87298583984375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9818, loss_val: nan, pos_over_neg: 677.199951171875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9837, loss_val: nan, pos_over_neg: 1213.970703125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9852, loss_val: nan, pos_over_neg: 514.5719604492188 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 391.6199035644531 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9741, loss_val: nan, pos_over_neg: 441.7149658203125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9866, loss_val: nan, pos_over_neg: 697.9732055664062 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.0123, loss_val: nan, pos_over_neg: 688.0787963867188 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9872, loss_val: nan, pos_over_neg: 638.0595703125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9785, loss_val: nan, pos_over_neg: 1023.1522216796875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 824.3236694335938 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9833, loss_val: nan, pos_over_neg: 827.9112548828125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9745, loss_val: nan, pos_over_neg: 1041.10009765625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9905, loss_val: nan, pos_over_neg: 673.9204711914062 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9868, loss_val: nan, pos_over_neg: 870.5256958007812 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9899, loss_val: nan, pos_over_neg: 636.2478637695312 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9824, loss_val: nan, pos_over_neg: 944.4832763671875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9889, loss_val: nan, pos_over_neg: 1335.2286376953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 458.9210205078125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9735, loss_val: nan, pos_over_neg: 384.53082275390625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.991, loss_val: nan, pos_over_neg: 1147.65576171875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 481.1462097167969 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9796, loss_val: nan, pos_over_neg: 559.3778686523438 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9722, loss_val: nan, pos_over_neg: 1061.6339111328125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1312.9197998046875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9759, loss_val: nan, pos_over_neg: 739.24658203125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 601.40087890625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9955, loss_val: nan, pos_over_neg: 1529.624267578125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9678, loss_val: nan, pos_over_neg: 1324.8895263671875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9901, loss_val: nan, pos_over_neg: 908.1839599609375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9771, loss_val: nan, pos_over_neg: 1405.2017822265625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9772, loss_val: nan, pos_over_neg: 741.8477172851562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9853, loss_val: nan, pos_over_neg: 678.5794677734375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9755, loss_val: nan, pos_over_neg: 557.8493041992188 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 3301.474609375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9624, loss_val: nan, pos_over_neg: 1122.82177734375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9809, loss_val: nan, pos_over_neg: 723.4447631835938 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9989, loss_val: nan, pos_over_neg: 2168.793212890625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.993, loss_val: nan, pos_over_neg: 702.9869384765625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9796, loss_val: nan, pos_over_neg: 416.7487487792969 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 1245.0238037109375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9892, loss_val: nan, pos_over_neg: 5216.251953125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9826, loss_val: nan, pos_over_neg: 699.7113647460938 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9898, loss_val: nan, pos_over_neg: 958.27978515625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9877, loss_val: nan, pos_over_neg: 977.1764526367188 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9799, loss_val: nan, pos_over_neg: 1446.2904052734375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 1137.6038818359375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9986, loss_val: nan, pos_over_neg: 398.0068664550781 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 675.094970703125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9869, loss_val: nan, pos_over_neg: 836.86474609375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 1649.87255859375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9809, loss_val: nan, pos_over_neg: 1027.2110595703125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9877, loss_val: nan, pos_over_neg: 806.773193359375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 752.9053344726562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9765, loss_val: nan, pos_over_neg: 373.3514099121094 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9914, loss_val: nan, pos_over_neg: 478.8707275390625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9881, loss_val: nan, pos_over_neg: 821.047119140625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 1136.5592041015625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 977.599609375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9993, loss_val: nan, pos_over_neg: 1174.6563720703125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9793, loss_val: nan, pos_over_neg: 1026.0428466796875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9932, loss_val: nan, pos_over_neg: 1025.682861328125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9882, loss_val: nan, pos_over_neg: 468.5310974121094 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9712, loss_val: nan, pos_over_neg: 499.6485290527344 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 538.0055541992188 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 940.30859375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9761, loss_val: nan, pos_over_neg: 1819.2005615234375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9897, loss_val: nan, pos_over_neg: 492.61260986328125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.988, loss_val: nan, pos_over_neg: 810.0043334960938 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9862, loss_val: nan, pos_over_neg: 1645.2650146484375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9971, loss_val: nan, pos_over_neg: 851.0156860351562 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 3134.5517578125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.982, loss_val: nan, pos_over_neg: 862.4802856445312 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 376.8258972167969 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9665, loss_val: nan, pos_over_neg: 949.5859985351562 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9977, loss_val: nan, pos_over_neg: 1625.1573486328125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9785, loss_val: nan, pos_over_neg: 1549.0799560546875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.979, loss_val: nan, pos_over_neg: 5791.6767578125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9722, loss_val: nan, pos_over_neg: 1341.2021484375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.97, loss_val: nan, pos_over_neg: 658.91650390625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9911, loss_val: nan, pos_over_neg: 514.5269775390625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.976, loss_val: nan, pos_over_neg: 620.8794555664062 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 319.5826721191406 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 1562.0616455078125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 17164.671875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9673, loss_val: nan, pos_over_neg: 1011.3389892578125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 1518.2052001953125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 1551.9732666015625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 1873.181640625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9778, loss_val: nan, pos_over_neg: 437.23101806640625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 1530.506103515625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9745, loss_val: nan, pos_over_neg: 2226.062255859375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9795, loss_val: nan, pos_over_neg: 1939.699462890625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9852, loss_val: nan, pos_over_neg: 1375.575927734375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9783, loss_val: nan, pos_over_neg: 538.0488891601562 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9826, loss_val: nan, pos_over_neg: 865.1670532226562 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 509.67303466796875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9715, loss_val: nan, pos_over_neg: 1474.1998291015625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 508.40966796875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.981, loss_val: nan, pos_over_neg: 1293.025634765625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9776, loss_val: nan, pos_over_neg: 805.3519897460938 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9753, loss_val: nan, pos_over_neg: 1058.2003173828125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.99, loss_val: nan, pos_over_neg: 498.79437255859375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9779, loss_val: nan, pos_over_neg: 1057.5216064453125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.985, loss_val: nan, pos_over_neg: 529.5321044921875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 917.1822509765625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9821, loss_val: nan, pos_over_neg: 1169.546630859375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.978, loss_val: nan, pos_over_neg: 856.83837890625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9731, loss_val: nan, pos_over_neg: 638.5450439453125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9993, loss_val: nan, pos_over_neg: 691.8851318359375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 632.3707275390625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.975, loss_val: nan, pos_over_neg: 944.8721923828125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 1078.6890869140625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.984, loss_val: nan, pos_over_neg: 727.0203247070312 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9883, loss_val: nan, pos_over_neg: 988.8619995117188 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9763, loss_val: nan, pos_over_neg: 2642.31298828125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 1085.6593017578125 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.98, loss_val: nan, pos_over_neg: 599.451171875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9858, loss_val: nan, pos_over_neg: 646.1713256835938 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 732.798583984375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9863, loss_val: nan, pos_over_neg: 463.0094299316406 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 602.6416015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9877, loss_val: nan, pos_over_neg: 1041.7828369140625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9643, loss_val: nan, pos_over_neg: 1182.92724609375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9759, loss_val: nan, pos_over_neg: 1215.00927734375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 452.0303039550781 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9717, loss_val: nan, pos_over_neg: 568.6091918945312 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9639, loss_val: nan, pos_over_neg: 1132.272216796875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9873, loss_val: nan, pos_over_neg: 908.4934692382812 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 852.081787109375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9765, loss_val: nan, pos_over_neg: 933.4096069335938 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 6002.93798828125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9694, loss_val: nan, pos_over_neg: 3122.68017578125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9814, loss_val: nan, pos_over_neg: 549.9210205078125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9944, loss_val: nan, pos_over_neg: 639.3357543945312 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.965, loss_val: nan, pos_over_neg: 7861.5234375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 2645.697998046875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9778, loss_val: nan, pos_over_neg: 587.5460815429688 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 815.8924560546875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9762, loss_val: nan, pos_over_neg: 763.750732421875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9788, loss_val: nan, pos_over_neg: 461.8069763183594 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9789, loss_val: nan, pos_over_neg: 1184.5350341796875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 986.392333984375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9816, loss_val: nan, pos_over_neg: 988.4703369140625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 736.7063598632812 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9764, loss_val: nan, pos_over_neg: 1228.529541015625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9859, loss_val: nan, pos_over_neg: 447.4761962890625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9718, loss_val: nan, pos_over_neg: 1442.0445556640625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.979, loss_val: nan, pos_over_neg: 804.4490356445312 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9828, loss_val: nan, pos_over_neg: 5055.88671875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9677, loss_val: nan, pos_over_neg: 2238.234375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9854, loss_val: nan, pos_over_neg: 580.0635986328125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9722, loss_val: nan, pos_over_neg: 1196.841552734375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 1063.4736328125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9694, loss_val: nan, pos_over_neg: 767.97509765625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9845, loss_val: nan, pos_over_neg: 2106.756591796875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 652.819091796875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 1607.9405517578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 2850.692626953125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9557, loss_val: nan, pos_over_neg: 1105.625732421875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9763, loss_val: nan, pos_over_neg: 1278.074951171875 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9847, loss_val: nan, pos_over_neg: 771.2838745117188 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.971, loss_val: nan, pos_over_neg: 1062.711181640625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.991, loss_val: nan, pos_over_neg: 2462.49072265625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.971, loss_val: nan, pos_over_neg: 2554.848876953125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9788, loss_val: nan, pos_over_neg: 602.814208984375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9801, loss_val: nan, pos_over_neg: 729.6902465820312 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9834, loss_val: nan, pos_over_neg: 956.7289428710938 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9803, loss_val: nan, pos_over_neg: 1220.564208984375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9833, loss_val: nan, pos_over_neg: 958.101806640625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.978, loss_val: nan, pos_over_neg: 396.1448059082031 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9829, loss_val: nan, pos_over_neg: 829.7393188476562 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9747, loss_val: nan, pos_over_neg: 458.4014892578125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 612.3461303710938 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9777, loss_val: nan, pos_over_neg: 489.8993835449219 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9897, loss_val: nan, pos_over_neg: 968.3895874023438 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9709, loss_val: nan, pos_over_neg: 595.4220581054688 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9807, loss_val: nan, pos_over_neg: 916.4125366210938 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 1202.8458251953125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9825, loss_val: nan, pos_over_neg: 2188.975341796875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9639, loss_val: nan, pos_over_neg: 1641.7030029296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9724, loss_val: nan, pos_over_neg: 357.1915283203125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9749, loss_val: nan, pos_over_neg: 689.91064453125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9773, loss_val: nan, pos_over_neg: 859.2175903320312 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 1430.8817138671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9689, loss_val: nan, pos_over_neg: 1198.0089111328125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9768, loss_val: nan, pos_over_neg: 1353.5460205078125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9924, loss_val: nan, pos_over_neg: 9147.45703125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 999.033447265625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 333.26458740234375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9783, loss_val: nan, pos_over_neg: 1136.052001953125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9748, loss_val: nan, pos_over_neg: 1373.11376953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9797, loss_val: nan, pos_over_neg: 553.9606323242188 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.982, loss_val: nan, pos_over_neg: 534.4692993164062 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 479.22137451171875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9699, loss_val: nan, pos_over_neg: 709.4196166992188 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9798, loss_val: nan, pos_over_neg: 658.6203002929688 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9797, loss_val: nan, pos_over_neg: 2738.8115234375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9644, loss_val: nan, pos_over_neg: 1664.9930419921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9589, loss_val: nan, pos_over_neg: 944.00537109375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9717, loss_val: nan, pos_over_neg: 505.8251953125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9813, loss_val: nan, pos_over_neg: 1447.5550537109375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9879, loss_val: nan, pos_over_neg: 577.3998413085938 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9869, loss_val: nan, pos_over_neg: 593.8554077148438 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.965, loss_val: nan, pos_over_neg: 1666.4521484375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 796.0234985351562 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9839, loss_val: nan, pos_over_neg: 905.9382934570312 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9775, loss_val: nan, pos_over_neg: 825.4689331054688 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 610.896484375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9729, loss_val: nan, pos_over_neg: 948.7988891601562 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9682, loss_val: nan, pos_over_neg: 508.6755065917969 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.987, loss_val: nan, pos_over_neg: 562.1314086914062 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9775, loss_val: nan, pos_over_neg: 857.8858032226562 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9619, loss_val: nan, pos_over_neg: 750.599365234375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 1866.9029541015625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9769, loss_val: nan, pos_over_neg: 1986.521240234375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9913, loss_val: nan, pos_over_neg: 232.84912109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9755, loss_val: nan, pos_over_neg: 478.8553771972656 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9758, loss_val: nan, pos_over_neg: 324.46026611328125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9852, loss_val: nan, pos_over_neg: 3307.375244140625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.975, loss_val: nan, pos_over_neg: 1033.1656494140625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9906, loss_val: nan, pos_over_neg: 481.8027038574219 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9699, loss_val: nan, pos_over_neg: 1854.3489990234375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9793, loss_val: nan, pos_over_neg: 1259.497314453125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9674, loss_val: nan, pos_over_neg: 345.2754211425781 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9719, loss_val: nan, pos_over_neg: 759.5073852539062 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9958, loss_val: nan, pos_over_neg: 380.3766784667969 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 706.6402587890625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 1740.458740234375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9692, loss_val: nan, pos_over_neg: 2995.749755859375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9617, loss_val: nan, pos_over_neg: 1761.1358642578125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9837, loss_val: nan, pos_over_neg: 696.5675048828125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 1185.3621826171875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 586.330810546875 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 460.251220703125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9692, loss_val: nan, pos_over_neg: 523.074462890625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9684, loss_val: nan, pos_over_neg: 1284.4139404296875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9779, loss_val: nan, pos_over_neg: 520.9823608398438 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9724, loss_val: nan, pos_over_neg: 463.5460205078125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 4147.9189453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9776, loss_val: nan, pos_over_neg: 1406.49462890625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9755, loss_val: nan, pos_over_neg: 877.779296875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9747, loss_val: nan, pos_over_neg: 425.17236328125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9764, loss_val: nan, pos_over_neg: 2120.573974609375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 1697.191162109375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 763.9672241210938 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 455.0662841796875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9789, loss_val: nan, pos_over_neg: 1878.7562255859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 939.6615600585938 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9652, loss_val: nan, pos_over_neg: 749.1170654296875 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9855, loss_val: nan, pos_over_neg: 526.7742309570312 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 1187.348388671875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9683, loss_val: nan, pos_over_neg: 357.8694152832031 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9692, loss_val: nan, pos_over_neg: 841.5701904296875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9759, loss_val: nan, pos_over_neg: 707.94140625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9774, loss_val: nan, pos_over_neg: 1428.4688720703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 645.9702758789062 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9703, loss_val: nan, pos_over_neg: 551.0911254882812 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 1990.3326416015625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.964, loss_val: nan, pos_over_neg: 6301.95166015625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9733, loss_val: nan, pos_over_neg: 521.3844604492188 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 1717.418212890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9713, loss_val: nan, pos_over_neg: 936.2583618164062 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9608, loss_val: nan, pos_over_neg: 1121.7945556640625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9764, loss_val: nan, pos_over_neg: 567.8416748046875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9753, loss_val: nan, pos_over_neg: 960.857666015625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9779, loss_val: nan, pos_over_neg: 1620.406982421875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 970.1314697265625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9795, loss_val: nan, pos_over_neg: 433.4027404785156 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 3913.786865234375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 4515.2890625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9749, loss_val: nan, pos_over_neg: 8994.0126953125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9643, loss_val: nan, pos_over_neg: 508.52056884765625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9772, loss_val: nan, pos_over_neg: 732.0601196289062 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9632, loss_val: nan, pos_over_neg: 1328.044921875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9676, loss_val: nan, pos_over_neg: 667.2401733398438 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9772, loss_val: nan, pos_over_neg: 1014.1170043945312 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 502.0700378417969 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 286.74896240234375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9744, loss_val: nan, pos_over_neg: 870.512939453125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9669, loss_val: nan, pos_over_neg: 922.410888671875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 605.6013793945312 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9828, loss_val: nan, pos_over_neg: 1138.597412109375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9647, loss_val: nan, pos_over_neg: 1923.8277587890625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9786, loss_val: nan, pos_over_neg: 953.470703125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9768, loss_val: nan, pos_over_neg: 453.6964416503906 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9797, loss_val: nan, pos_over_neg: 551.3418579101562 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9816, loss_val: nan, pos_over_neg: 1188.3787841796875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 946.6058959960938 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9737, loss_val: nan, pos_over_neg: 780.218994140625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9636, loss_val: nan, pos_over_neg: 394.9588928222656 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9783, loss_val: nan, pos_over_neg: 529.536376953125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9765, loss_val: nan, pos_over_neg: 1367.9495849609375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9673, loss_val: nan, pos_over_neg: 562.7493896484375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.967, loss_val: nan, pos_over_neg: 743.1942749023438 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 978.0079956054688 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9601, loss_val: nan, pos_over_neg: 841.6701049804688 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9768, loss_val: nan, pos_over_neg: 410.2208557128906 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9718, loss_val: nan, pos_over_neg: 512.779052734375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 1156.7916259765625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9746, loss_val: nan, pos_over_neg: 751.9068603515625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 566.1837158203125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9607, loss_val: nan, pos_over_neg: 557.264404296875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9711, loss_val: nan, pos_over_neg: 1011.8155517578125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9634, loss_val: nan, pos_over_neg: 606.95703125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 494.7795715332031 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9897, loss_val: nan, pos_over_neg: 486.4346618652344 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 1202.2496337890625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.967, loss_val: nan, pos_over_neg: 547.0509643554688 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9763, loss_val: nan, pos_over_neg: 365.3199157714844 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 343.45758056640625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9676, loss_val: nan, pos_over_neg: 1213.845703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 696.6041259765625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 552.7570190429688 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9639, loss_val: nan, pos_over_neg: 560.32177734375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9699, loss_val: nan, pos_over_neg: 1099.6044921875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 638.0543212890625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9645, loss_val: nan, pos_over_neg: 447.4938049316406 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9835, loss_val: nan, pos_over_neg: 391.8464660644531 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 4520.119140625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9674, loss_val: nan, pos_over_neg: 497.76092529296875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9719, loss_val: nan, pos_over_neg: 677.8692016601562 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9724, loss_val: nan, pos_over_neg: 762.1080322265625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 522.0728149414062 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9586, loss_val: nan, pos_over_neg: 1014.978271484375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9742, loss_val: nan, pos_over_neg: 464.874755859375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9718, loss_val: nan, pos_over_neg: 404.5363464355469 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 739.2767333984375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 646.8580932617188 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9795, loss_val: nan, pos_over_neg: 1302.1578369140625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 922.47509765625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.964, loss_val: nan, pos_over_neg: 951.628173828125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9673, loss_val: nan, pos_over_neg: 688.96875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9815, loss_val: nan, pos_over_neg: 725.5242309570312 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9888, loss_val: nan, pos_over_neg: 437.7524719238281 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9735, loss_val: nan, pos_over_neg: 450.69781494140625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9666, loss_val: nan, pos_over_neg: 1369.7144775390625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9648, loss_val: nan, pos_over_neg: 3835.99560546875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 1188.7506103515625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9746, loss_val: nan, pos_over_neg: 7406.42041015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9645, loss_val: nan, pos_over_neg: 3120.013916015625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 1151.62353515625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 827.2958984375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9631, loss_val: nan, pos_over_neg: 745.8223876953125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 1986.5555419921875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9666, loss_val: nan, pos_over_neg: 903.6984252929688 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9592, loss_val: nan, pos_over_neg: 1317.2152099609375 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 508.4697265625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.977, loss_val: nan, pos_over_neg: 416.3586730957031 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9813, loss_val: nan, pos_over_neg: 552.0960083007812 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9766, loss_val: nan, pos_over_neg: 1004.97265625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 462.0447692871094 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9549, loss_val: nan, pos_over_neg: 3320.1181640625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1127.823974609375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9588, loss_val: nan, pos_over_neg: 788.3720092773438 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 488.1983642578125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 602.4476318359375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1030.4886474609375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 412.1879577636719 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9713, loss_val: nan, pos_over_neg: 390.13726806640625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 1233.0308837890625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 874.751220703125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9622, loss_val: nan, pos_over_neg: 504.50067138671875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 515.1923828125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9728, loss_val: nan, pos_over_neg: 449.8815612792969 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9612, loss_val: nan, pos_over_neg: 449.74273681640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 580.4939575195312 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9676, loss_val: nan, pos_over_neg: 903.388671875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 1178.62109375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9697, loss_val: nan, pos_over_neg: 308.0848388671875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9708, loss_val: nan, pos_over_neg: 837.8548583984375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9659, loss_val: nan, pos_over_neg: 1242.908203125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9743, loss_val: nan, pos_over_neg: 1540.0098876953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9677, loss_val: nan, pos_over_neg: 814.2664794921875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9791, loss_val: nan, pos_over_neg: 749.692138671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9732, loss_val: nan, pos_over_neg: 3404.0810546875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9628, loss_val: nan, pos_over_neg: 1310.4049072265625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 1011.6948852539062 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9775, loss_val: nan, pos_over_neg: 1221.2928466796875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9643, loss_val: nan, pos_over_neg: 3457.349365234375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 12768.3115234375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9697, loss_val: nan, pos_over_neg: 404.7817687988281 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 605.9224853515625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9702, loss_val: nan, pos_over_neg: 1139.44775390625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 1742.3363037109375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9647, loss_val: nan, pos_over_neg: 496.8490295410156 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9592, loss_val: nan, pos_over_neg: 839.8062133789062 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9564, loss_val: nan, pos_over_neg: 1320.5130615234375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9725, loss_val: nan, pos_over_neg: 780.1524047851562 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 703.2794799804688 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9821, loss_val: nan, pos_over_neg: 662.8763427734375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 1306.54833984375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 800.7947998046875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9623, loss_val: nan, pos_over_neg: 1609.025634765625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9524, loss_val: nan, pos_over_neg: 1083.283935546875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9581, loss_val: nan, pos_over_neg: 1044.728271484375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.967, loss_val: nan, pos_over_neg: 691.0792236328125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 720.40966796875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9474, loss_val: nan, pos_over_neg: 2525.342041015625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 3214.02978515625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1317.2322998046875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 942.0738525390625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9612, loss_val: nan, pos_over_neg: 570.2174072265625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9568, loss_val: nan, pos_over_neg: 4640.125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9723, loss_val: nan, pos_over_neg: 1599.954345703125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 1055.3841552734375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9696, loss_val: nan, pos_over_neg: 907.3294067382812 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9605, loss_val: nan, pos_over_neg: 870.5505981445312 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 809.7474975585938 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9622, loss_val: nan, pos_over_neg: 1030.9771728515625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9513, loss_val: nan, pos_over_neg: 599.5807495117188 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9817, loss_val: nan, pos_over_neg: 7224.55712890625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9683, loss_val: nan, pos_over_neg: 2376.810302734375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9757, loss_val: nan, pos_over_neg: 688.2020874023438 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 3964.36474609375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9674, loss_val: nan, pos_over_neg: 1285.2430419921875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 2082.731201171875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9592, loss_val: nan, pos_over_neg: 598.3094482421875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 1603.3421630859375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9656, loss_val: nan, pos_over_neg: 767.6441650390625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9789, loss_val: nan, pos_over_neg: 457.29718017578125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9606, loss_val: nan, pos_over_neg: 1200.744873046875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9646, loss_val: nan, pos_over_neg: 2130.322021484375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9552, loss_val: nan, pos_over_neg: 1105.4166259765625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9712, loss_val: nan, pos_over_neg: 751.512451171875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 470.40277099609375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9738, loss_val: nan, pos_over_neg: 946.0438842773438 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9647, loss_val: nan, pos_over_neg: 369.2502136230469 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9619, loss_val: nan, pos_over_neg: 731.0771484375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 470.96978759765625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 529.594482421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 943.843994140625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 887.41162109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 936.0270385742188 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9626, loss_val: nan, pos_over_neg: 469.7942810058594 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9532, loss_val: nan, pos_over_neg: 576.9178466796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 587.2633056640625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 839.05859375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 589.7533569335938 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9714, loss_val: nan, pos_over_neg: 948.5181274414062 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9746, loss_val: nan, pos_over_neg: 690.265869140625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9769, loss_val: nan, pos_over_neg: 903.802978515625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.983, loss_val: nan, pos_over_neg: 696.721435546875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 620.91796875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.964, loss_val: nan, pos_over_neg: 829.524169921875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9703, loss_val: nan, pos_over_neg: 588.1602172851562 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 742.8314819335938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9588, loss_val: nan, pos_over_neg: 769.0362548828125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9675, loss_val: nan, pos_over_neg: 1512.1510009765625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9702, loss_val: nan, pos_over_neg: 881.3226928710938 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 1347.671875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 632.6713256835938 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 4244.26953125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9624, loss_val: nan, pos_over_neg: 690.6947021484375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9514, loss_val: nan, pos_over_neg: 697.7433471679688 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1737.59619140625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9685, loss_val: nan, pos_over_neg: 3982.197021484375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 673.8309326171875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9777, loss_val: nan, pos_over_neg: 726.1320190429688 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9585, loss_val: nan, pos_over_neg: 2247.962646484375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9553, loss_val: nan, pos_over_neg: 296183.3125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9659, loss_val: nan, pos_over_neg: 2436.38818359375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 715.6333618164062 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9655, loss_val: nan, pos_over_neg: 1218.845947265625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: -3969.79296875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 2281.530029296875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9721, loss_val: nan, pos_over_neg: 711.8531494140625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 2051.347412109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: -4557.1279296875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: -515017.875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1258.205322265625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 1166.129638671875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9655, loss_val: nan, pos_over_neg: 855.7432861328125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9651, loss_val: nan, pos_over_neg: 2511.8310546875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9725, loss_val: nan, pos_over_neg: 517.4549560546875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 4197.2431640625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 1286.38720703125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9642, loss_val: nan, pos_over_neg: 1157.4814453125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 2733.314208984375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 999.3907470703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9615, loss_val: nan, pos_over_neg: 1442.4149169921875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9736, loss_val: nan, pos_over_neg: 1282.9268798828125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 11696.013671875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9736, loss_val: nan, pos_over_neg: 811.56201171875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9723, loss_val: nan, pos_over_neg: 1712.741455078125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9656, loss_val: nan, pos_over_neg: 1080.309326171875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 913.7352294921875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9705, loss_val: nan, pos_over_neg: 769.8062744140625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9611, loss_val: nan, pos_over_neg: 2817.762451171875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9607, loss_val: nan, pos_over_neg: 1328.258056640625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.961, loss_val: nan, pos_over_neg: 1279.1083984375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9583, loss_val: nan, pos_over_neg: 785.1640625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.963, loss_val: nan, pos_over_neg: 1029.4671630859375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 2185.88818359375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 751.76904296875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 1307.439453125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 820.748291015625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9563, loss_val: nan, pos_over_neg: 701.924072265625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9594, loss_val: nan, pos_over_neg: 1102.3328857421875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9568, loss_val: nan, pos_over_neg: 2187.98876953125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 1009.7761840820312 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 11661.1650390625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9654, loss_val: nan, pos_over_neg: 2835.457763671875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9608, loss_val: nan, pos_over_neg: 604.6597290039062 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 1280.874267578125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 537.4255981445312 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9697, loss_val: nan, pos_over_neg: 519.0641479492188 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 558.7068481445312 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1011.2106323242188 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 494.88677978515625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 2112.949951171875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 398.3558044433594 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 290.4610900878906 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 972.3668212890625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1478.8951416015625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 448.6534118652344 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9656, loss_val: nan, pos_over_neg: 2277.450927734375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 488.0547180175781 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: 783.10400390625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 497.8551330566406 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 419.9805908203125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 788.6354370117188 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9481, loss_val: nan, pos_over_neg: 959.4708862304688 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9665, loss_val: nan, pos_over_neg: 585.2483520507812 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 806.2801513671875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.972, loss_val: nan, pos_over_neg: 486.4857177734375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9669, loss_val: nan, pos_over_neg: 542.2825317382812 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.959, loss_val: nan, pos_over_neg: 541.7899169921875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 602.3126220703125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 461.54681396484375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 734.4088745117188 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9668, loss_val: nan, pos_over_neg: 790.1854858398438 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9544, loss_val: nan, pos_over_neg: 811.9652709960938 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9714, loss_val: nan, pos_over_neg: 892.7135009765625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 637.9096069335938 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 871.8743896484375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9679, loss_val: nan, pos_over_neg: 1199.1195068359375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 4600.462890625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 890.80908203125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9591, loss_val: nan, pos_over_neg: 5504.2109375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 2510.045166015625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9539, loss_val: nan, pos_over_neg: 746.1331787109375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9504, loss_val: nan, pos_over_neg: 2426.5517578125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 4879.02880859375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 553.228515625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9651, loss_val: nan, pos_over_neg: 717.2623291015625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 1275.279296875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 990.8941650390625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9758, loss_val: nan, pos_over_neg: 1640.0068359375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: -11573.78515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 1369.968017578125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9589, loss_val: nan, pos_over_neg: 1477.1622314453125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 680.2684326171875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 1807.206298828125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9576, loss_val: nan, pos_over_neg: 777.9884643554688 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 1189.127197265625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9466, loss_val: nan, pos_over_neg: 820.2384033203125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9623, loss_val: nan, pos_over_neg: 624.1328125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1801.5367431640625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1885.7322998046875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 443.67474365234375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 1447.322265625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.96, loss_val: nan, pos_over_neg: 8363.587890625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 2132.083740234375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 1082.0228271484375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9605, loss_val: nan, pos_over_neg: 575.5985107421875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 838.27783203125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 472.874267578125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 803.6258544921875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9704, loss_val: nan, pos_over_neg: 961.8768310546875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 1965.4407958984375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9627, loss_val: nan, pos_over_neg: 1118.4678955078125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9698, loss_val: nan, pos_over_neg: 506.23828125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 1465.3804931640625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 732.2445678710938 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 436.22998046875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 924.322021484375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1419.9583740234375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9645, loss_val: nan, pos_over_neg: 637.22509765625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9634, loss_val: nan, pos_over_neg: 1040.20458984375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 950.6099853515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9533, loss_val: nan, pos_over_neg: 917.0897827148438 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9624, loss_val: nan, pos_over_neg: 721.452880859375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 2030.283935546875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 1586.4093017578125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: -24419.587890625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9539, loss_val: nan, pos_over_neg: 6483.4462890625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 14997.5771484375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9585, loss_val: nan, pos_over_neg: 1146.3111572265625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 1420.204833984375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9704, loss_val: nan, pos_over_neg: 1601.3232421875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 1545.6761474609375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9686, loss_val: nan, pos_over_neg: 1268.9254150390625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9578, loss_val: nan, pos_over_neg: 786.5825805664062 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9524, loss_val: nan, pos_over_neg: 3806.65576171875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9609, loss_val: nan, pos_over_neg: 3471.994384765625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 660.742919921875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 3314.91064453125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9703, loss_val: nan, pos_over_neg: 1817.2921142578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 927.0296630859375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9543, loss_val: nan, pos_over_neg: 952.655517578125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 2991.224609375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9525, loss_val: nan, pos_over_neg: 3403.689208984375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9594, loss_val: nan, pos_over_neg: 489.39215087890625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 646.7020874023438 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 1688.8878173828125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9597, loss_val: nan, pos_over_neg: 5895.12158203125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 1418.540283203125 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 661.1383666992188 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 2200.21240234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 3191.883056640625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9513, loss_val: nan, pos_over_neg: 660.2813110351562 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 1909.222412109375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.954, loss_val: nan, pos_over_neg: 1007.8494262695312 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9685, loss_val: nan, pos_over_neg: 996.0789184570312 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 3909.733154296875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 1378.0133056640625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 450.6241760253906 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 1434.15576171875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 5036.44580078125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 1033.6033935546875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9621, loss_val: nan, pos_over_neg: 1193.52392578125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9644, loss_val: nan, pos_over_neg: 675.296142578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1983.935791015625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9784, loss_val: nan, pos_over_neg: 639.9163818359375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [39:59<99882:36:26, 1198.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.9515, loss_val: nan, pos_over_neg: 898.3248291015625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 1315.1134033203125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 1578.4510498046875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 759.9706420898438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9578, loss_val: nan, pos_over_neg: -269886.21875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 1101.626220703125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1212.3043212890625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 1486.691162109375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9626, loss_val: nan, pos_over_neg: 1935.41796875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 816.0136108398438 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 2685.3955078125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 1424.27001953125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 1007.8795166015625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 1199.0927734375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 1070.1007080078125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 1705.08349609375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 719.0358276367188 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9633, loss_val: nan, pos_over_neg: 1277.9774169921875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9637, loss_val: nan, pos_over_neg: 985.5642700195312 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9668, loss_val: nan, pos_over_neg: 15415.9130859375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1100.745361328125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9616, loss_val: nan, pos_over_neg: 1217.9166259765625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 885.1468505859375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 1325.717529296875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 1400.3780517578125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1043.866943359375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 1581.3206787109375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 2716.7841796875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 686.2576904296875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 464.1960754394531 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 371.7525634765625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: 786.47265625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 513.1532592773438 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9632, loss_val: nan, pos_over_neg: 649.6608276367188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 2583.822998046875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 1377.674560546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 835.5100708007812 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 518.4733276367188 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 642.1566162109375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1157.27197265625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 1014.2764892578125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 1057.9716796875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9576, loss_val: nan, pos_over_neg: 1513.804931640625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9691, loss_val: nan, pos_over_neg: 5403.34228515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.968, loss_val: nan, pos_over_neg: 31765.1640625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 1223.6053466796875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9663, loss_val: nan, pos_over_neg: 1369.6419677734375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 1347.02880859375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 1628.74609375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 756.1696166992188 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 663.552734375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 614.8692626953125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9648, loss_val: nan, pos_over_neg: 1222.52734375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9596, loss_val: nan, pos_over_neg: 1219.9525146484375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 2156.175048828125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 813.4017944335938 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9515, loss_val: nan, pos_over_neg: 2887.8017578125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 1123.6185302734375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 748.7470703125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 713.3355712890625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 1635.6240234375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 794.7320556640625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9588, loss_val: nan, pos_over_neg: 584.4747924804688 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9636, loss_val: nan, pos_over_neg: 3825.06396484375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 159049.0 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.946, loss_val: nan, pos_over_neg: 1546.226806640625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9678, loss_val: nan, pos_over_neg: 526.2426147460938 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 4894.5380859375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 2520.0712890625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.957, loss_val: nan, pos_over_neg: 496.53070068359375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 807.8234252929688 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 1120.7161865234375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 2389.380615234375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 871.2636108398438 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9465, loss_val: nan, pos_over_neg: 967.8319702148438 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 580.412841796875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 2528.23388671875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 1002.6829223632812 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 948.2163696289062 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 929.9120483398438 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9594, loss_val: nan, pos_over_neg: 654.8511962890625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 778.741455078125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9523, loss_val: nan, pos_over_neg: 608.773681640625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 625.234130859375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 677.2163696289062 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9583, loss_val: nan, pos_over_neg: 798.3263549804688 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 1271.94921875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 2429.376953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 997.812744140625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9583, loss_val: nan, pos_over_neg: 675.6461791992188 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9603, loss_val: nan, pos_over_neg: 799.535400390625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 731.5720825195312 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9638, loss_val: nan, pos_over_neg: 4186.01123046875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9663, loss_val: nan, pos_over_neg: 768.928466796875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 550.9379272460938 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 1008.6574096679688 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 4665.27294921875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 1204.718994140625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 867.858642578125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 1006.3767700195312 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9629, loss_val: nan, pos_over_neg: 1251.1087646484375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9607, loss_val: nan, pos_over_neg: 1509.0972900390625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 781.47705078125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9597, loss_val: nan, pos_over_neg: 1416.4951171875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1988.9091796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 767.012939453125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 1394.352294921875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 32425.33984375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 940.2904052734375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 1016.434326171875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 417.75897216796875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1874.093017578125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9617, loss_val: nan, pos_over_neg: 1170.6910400390625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9786, loss_val: nan, pos_over_neg: 1647.4954833984375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 617.9005737304688 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 528.5689697265625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 1124.2291259765625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9549, loss_val: nan, pos_over_neg: 798.12353515625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 455.1931457519531 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9495, loss_val: nan, pos_over_neg: 743.5391235351562 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 924.8502807617188 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 2237.84130859375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 869.3904418945312 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9542, loss_val: nan, pos_over_neg: 833.8619995117188 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9616, loss_val: nan, pos_over_neg: 929.8959350585938 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 1044.5474853515625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.948, loss_val: nan, pos_over_neg: 2394.527587890625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 994.7643432617188 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9672, loss_val: nan, pos_over_neg: 4087.9052734375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.953, loss_val: nan, pos_over_neg: -11571.8291015625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 5463.74560546875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 5034.705078125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 1816.13330078125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9589, loss_val: nan, pos_over_neg: 1052.8199462890625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9529, loss_val: nan, pos_over_neg: 619.7344970703125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 801.9523315429688 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9606, loss_val: nan, pos_over_neg: 1382.4490966796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 2366.36181640625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9615, loss_val: nan, pos_over_neg: 695.364501953125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.962, loss_val: nan, pos_over_neg: 450.88250732421875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9514, loss_val: nan, pos_over_neg: 1003.4681396484375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 755.3826904296875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9576, loss_val: nan, pos_over_neg: 614.8388061523438 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 498.4332580566406 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9558, loss_val: nan, pos_over_neg: 441.0530090332031 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 280.06207275390625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 661.6527709960938 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 928.6913452148438 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 712.7547607421875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 774.9961547851562 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 882.9647827148438 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 846.48779296875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9386, loss_val: nan, pos_over_neg: 748.4624633789062 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 1172.0419921875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 2394.00390625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 3492.283447265625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 1100.4149169921875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 1772.5474853515625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 1126.887451171875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 739.6817016601562 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 1587.4263916015625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.948, loss_val: nan, pos_over_neg: 1002.6571655273438 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9494, loss_val: nan, pos_over_neg: 1002.3563232421875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1321.1397705078125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9529, loss_val: nan, pos_over_neg: 50281.3515625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 1654.76123046875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 764.0595703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9569, loss_val: nan, pos_over_neg: 584.7869262695312 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9663, loss_val: nan, pos_over_neg: 729.7976684570312 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 469.6231689453125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 989.9741821289062 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 438.4841003417969 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 875.4435424804688 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 734.89892578125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 883.6021728515625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 789.2786865234375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 670.4700317382812 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9428, loss_val: nan, pos_over_neg: 5101.294921875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1427.018310546875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1868.5921630859375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 753.4912719726562 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 932.6312866210938 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: -19841.7265625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 1632.9779052734375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 1298.3316650390625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 1367.239013671875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 944.6310424804688 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 5251.31884765625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 4244.24658203125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9611, loss_val: nan, pos_over_neg: 5711.03662109375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 1512.8360595703125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9579, loss_val: nan, pos_over_neg: 3126.837646484375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 2665.362548828125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 4673.98583984375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 4579.15380859375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 4884.82958984375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 3082.379638671875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 1996.89794921875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9695, loss_val: nan, pos_over_neg: 1725.4793701171875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1100.8916015625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 950.3292846679688 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9554, loss_val: nan, pos_over_neg: 6552.16943359375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 2283.80322265625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 2242.800048828125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 1401.7359619140625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 946.8248901367188 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9648, loss_val: nan, pos_over_neg: 1462.888671875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1623.95166015625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 543.083740234375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 1260.3734130859375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9644, loss_val: nan, pos_over_neg: 1731.337646484375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 823.72265625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 1699.1092529296875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 4249.50634765625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 578.8226318359375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9495, loss_val: nan, pos_over_neg: 814.8133544921875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 755.1744384765625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9564, loss_val: nan, pos_over_neg: 1170.653564453125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 3502.35888671875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1661.1234130859375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 1019.2472534179688 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 1310.3314208984375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 879.1287231445312 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 922.6629638671875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9435, loss_val: nan, pos_over_neg: 1075.571533203125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 890.2555541992188 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 2912.87255859375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 575.4324340820312 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9449, loss_val: nan, pos_over_neg: 1803.376953125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 1589.302490234375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9433, loss_val: nan, pos_over_neg: 2577.4833984375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 940.821044921875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1233.8905029296875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1711.09814453125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 19706.578125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1258.025634765625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 515.0302734375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1585.341064453125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 3771.37451171875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 968.881591796875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9591, loss_val: nan, pos_over_neg: 1298.946533203125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 831.6747436523438 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 605.8475341796875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9553, loss_val: nan, pos_over_neg: 996.0657958984375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 1063.8603515625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9492, loss_val: nan, pos_over_neg: 955.973388671875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 737.7120361328125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1201.0677490234375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1085.6529541015625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9611, loss_val: nan, pos_over_neg: 1091.72900390625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 616.3096923828125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 675.6611938476562 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 1435.60888671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9483, loss_val: nan, pos_over_neg: 853.1909790039062 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9593, loss_val: nan, pos_over_neg: 2412.2939453125 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9514, loss_val: nan, pos_over_neg: 605.1121215820312 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 710.1233520507812 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1662.5439453125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 890.9502563476562 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 833.1682739257812 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 709.6034545898438 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9538, loss_val: nan, pos_over_neg: 15773.7294921875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 1217.700439453125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 942.5308227539062 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 698.2373046875 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 1293.70654296875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9539, loss_val: nan, pos_over_neg: 934.2161254882812 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 1551.6512451171875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1601.721435546875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 1533.317138671875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 1539.0804443359375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 1794.1060791015625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9525, loss_val: nan, pos_over_neg: 899.0068969726562 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: 1064.7022705078125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 178956.671875 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 1290.1510009765625 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 1049.53564453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1502.8448486328125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9483, loss_val: nan, pos_over_neg: 3760.54296875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1482.71484375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 393.4430847167969 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 587.6080932617188 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9523, loss_val: nan, pos_over_neg: 1006.152099609375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 943.5359497070312 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9492, loss_val: nan, pos_over_neg: 474.5882873535156 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 545.236572265625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 1320.4544677734375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 586.9022827148438 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9543, loss_val: nan, pos_over_neg: 645.6513061523438 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9557, loss_val: nan, pos_over_neg: 580.4657592773438 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 1486.63134765625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9596, loss_val: nan, pos_over_neg: 745.4214477539062 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1201.2772216796875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 797.9249267578125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9651, loss_val: nan, pos_over_neg: 815.4697875976562 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9695, loss_val: nan, pos_over_neg: 1095.822021484375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 1906.451416015625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.954, loss_val: nan, pos_over_neg: 2221.53955078125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 1944.85546875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 2106.761962890625 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 2360.235107421875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 2067.797119140625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 1843.8319091796875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9662, loss_val: nan, pos_over_neg: 873.0645751953125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9474, loss_val: nan, pos_over_neg: 1804.56640625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 2211.385009765625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 1282.3829345703125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 1106.7210693359375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 1233.7210693359375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9481, loss_val: nan, pos_over_neg: 942.2764892578125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 989.709716796875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 2134.124267578125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 1050.3382568359375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9468, loss_val: nan, pos_over_neg: 1326.4652099609375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: -15824.947265625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.948, loss_val: nan, pos_over_neg: -3608.968017578125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 1212.9271240234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1621.12255859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1366.66796875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 6586.2333984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 1025.92822265625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 555.3233032226562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1996.767578125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 1806.4869384765625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 552.5572509765625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 549.0469970703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9513, loss_val: nan, pos_over_neg: 813.1569213867188 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9613, loss_val: nan, pos_over_neg: 911.6792602539062 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 1921.3367919921875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9613, loss_val: nan, pos_over_neg: 845.2313842773438 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 602.1646118164062 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1169.043701171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 675.0516357421875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.952, loss_val: nan, pos_over_neg: 1040.09130859375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9723, loss_val: nan, pos_over_neg: 587.3792114257812 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 823.0771484375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 943.9797973632812 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 648.8018798828125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1237.47119140625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 761.6224975585938 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 682.0423583984375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1320.9739990234375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 1045.2098388671875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 6372.2119140625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.95, loss_val: nan, pos_over_neg: 630.4369506835938 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 904.0635375976562 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: -2847.295166015625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 2122.93505859375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 565.1217041015625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 1155.7777099609375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 2413.240234375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 571.737060546875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 294.0021667480469 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 746.531494140625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 418.6093444824219 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.947, loss_val: nan, pos_over_neg: 734.0158081054688 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 570.1149291992188 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9563, loss_val: nan, pos_over_neg: 592.4583740234375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 486.69012451171875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 893.9168701171875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 510.93646240234375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9544, loss_val: nan, pos_over_neg: 456.8091735839844 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 674.7470703125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9504, loss_val: nan, pos_over_neg: 539.9193115234375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 986.370361328125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.956, loss_val: nan, pos_over_neg: 1016.111572265625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 1513.4246826171875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 4268.736328125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 924.5595703125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 872.685302734375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9502, loss_val: nan, pos_over_neg: -12469.5234375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9555, loss_val: nan, pos_over_neg: 3343.236572265625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9463, loss_val: nan, pos_over_neg: 3791.0849609375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 6449.1552734375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 2869.7255859375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 9734.986328125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9603, loss_val: nan, pos_over_neg: 1271.72021484375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 795.8388061523438 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: -7309.498046875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 1677.724853515625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1184.147216796875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 2556.500732421875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 1600.7847900390625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 1709.017822265625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 2947.873291015625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 976.4673461914062 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 1332.2109375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 731.9481811523438 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 2106.74609375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 2317.48583984375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 1235.09912109375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 938.6694946289062 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 2872.79931640625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 2473.155029296875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 684.1209716796875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 721.2564086914062 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 8651.24609375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 951.95458984375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 609.0593872070312 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 1142.822265625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 4511.8046875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1571.9342041015625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 1022.5827026367188 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 732.9413452148438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9502, loss_val: nan, pos_over_neg: 2172.448486328125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: 544.9813232421875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 2439.751708984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 1624.826904296875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1570.5836181640625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 1257.6180419921875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 4910.19482421875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 1246.1009521484375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 711.1602783203125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 702.7492065429688 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 1415.9869384765625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1849.071533203125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 1022.0372314453125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9536, loss_val: nan, pos_over_neg: 1257.4383544921875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 878.2894897460938 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.961, loss_val: nan, pos_over_neg: 2545.816650390625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 3021.902099609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 5059.19140625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 2795.567138671875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9502, loss_val: nan, pos_over_neg: 1179.118408203125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9605, loss_val: nan, pos_over_neg: 1442.2847900390625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9495, loss_val: nan, pos_over_neg: 2759.798828125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9534, loss_val: nan, pos_over_neg: 500.65374755859375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9498, loss_val: nan, pos_over_neg: 14667.2099609375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 2893.49169921875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1673.11767578125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.949, loss_val: nan, pos_over_neg: 1097.5499267578125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 2694.774169921875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 1571.5587158203125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 1262.5372314453125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9463, loss_val: nan, pos_over_neg: 1220.4066162109375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 902.1112670898438 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9591, loss_val: nan, pos_over_neg: 995.1166381835938 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 2491.517333984375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1394.1295166015625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 1420.4322509765625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 1827.8702392578125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 1252.028076171875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 1919.2552490234375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 912.9569702148438 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1615.2847900390625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 1295.4539794921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 945.2794799804688 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 862.626708984375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 1407.288330078125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1652.2520751953125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 4719.25439453125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9452, loss_val: nan, pos_over_neg: 4321.6875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 17845.626953125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 1883.4708251953125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1987.6904296875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 2813.150634765625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 1731.98095703125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 1197.2259521484375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 701.3738403320312 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 5131.2919921875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 5061.50244140625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 1361.5057373046875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: -2760.792236328125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9546, loss_val: nan, pos_over_neg: 2485.984130859375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 2050.5029296875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 1939.9114990234375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1842.037109375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 1072.8316650390625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 942.671875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9444, loss_val: nan, pos_over_neg: 1021.0314331054688 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9528, loss_val: nan, pos_over_neg: 4744.27685546875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: -29680.193359375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 1032.21240234375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 3407.66748046875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9435, loss_val: nan, pos_over_neg: 3516.709716796875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9481, loss_val: nan, pos_over_neg: 1396.1983642578125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1592.060791015625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 1960.2271728515625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 1925.9541015625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: -5852.3828125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 2028.4923095703125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 2006.1243896484375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 2456.636962890625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 2457.5498046875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9532, loss_val: nan, pos_over_neg: 71954.5546875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 3110.738037109375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 744.6602172851562 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9489, loss_val: nan, pos_over_neg: 693.9307861328125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 1199.011962890625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 13459.078125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9533, loss_val: nan, pos_over_neg: 1463.426025390625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 2656.905029296875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 839.0164794921875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 4083.205078125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: -2946.3310546875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 2262.989501953125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 2562.481201171875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: -7965.56298828125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 2108.85693359375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 1177.59228515625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: -23662.388671875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 4155.6279296875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: -40862.1640625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 1086.39697265625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 1133.4918212890625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 2213.27001953125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1241.497314453125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 907.7267456054688 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 616.5338745117188 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 3297.71533203125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 9250.0400390625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.95, loss_val: nan, pos_over_neg: 865.5392456054688 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 3627.389892578125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 1946.3790283203125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 735.5993041992188 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 1468.1087646484375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 4048.956787109375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9535, loss_val: nan, pos_over_neg: 567.2258911132812 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 553.64013671875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 758.280517578125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 1293.603515625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 1312.6402587890625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 701.0588989257812 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 613.1643676757812 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 891.6689453125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1103.5546875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 428.3733825683594 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1941.6676025390625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 2725.63525390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9418, loss_val: nan, pos_over_neg: 909.7896728515625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 1835.8541259765625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 1251.625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 3341.005615234375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 739.6395263671875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1211.5877685546875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 6590.642578125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 1619.4681396484375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1405.4759521484375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 1841.7105712890625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 813.8285522460938 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 5291.30517578125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 1028.2066650390625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9599, loss_val: nan, pos_over_neg: 881.4107055664062 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 709.8555297851562 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 2612.191162109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 1520.558349609375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 558.7875366210938 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 692.3318481445312 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 2618.724609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 1171.4075927734375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9475, loss_val: nan, pos_over_neg: 761.5125732421875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 730.8960571289062 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.948, loss_val: nan, pos_over_neg: 1044.87109375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9493, loss_val: nan, pos_over_neg: -8203.1279296875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 5464.6142578125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9581, loss_val: nan, pos_over_neg: 586.455810546875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 1436.4366455078125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 809.5343017578125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 1106.6170654296875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 3584.83154296875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.938, loss_val: nan, pos_over_neg: -7889.84130859375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 970.04541015625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.945, loss_val: nan, pos_over_neg: 2080.252685546875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9511, loss_val: nan, pos_over_neg: 659.1241455078125 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 800.4706420898438 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1020.1441650390625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 2217.770263671875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 1282.9757080078125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.942, loss_val: nan, pos_over_neg: -41352.34375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 2421.274658203125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 1945.3580322265625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9386, loss_val: nan, pos_over_neg: 1012.53955078125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: -24516.115234375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: -6287.23681640625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1240.5079345703125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9616, loss_val: nan, pos_over_neg: 618.7766723632812 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: -121373.203125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 1133.696044921875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 910.9207153320312 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 1039.2222900390625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 3714.235107421875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 2356.830078125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9523, loss_val: nan, pos_over_neg: 1639.6005859375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.949, loss_val: nan, pos_over_neg: 3022.525634765625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 2901.513427734375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 4367.296875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 1055.1490478515625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 2141.05517578125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: -31796.720703125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 3335.42431640625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9461, loss_val: nan, pos_over_neg: 2381.3232421875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9484, loss_val: nan, pos_over_neg: 1469.4571533203125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 3365.14990234375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 2312.551513671875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1910.2734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 3005.161376953125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: -5211.00732421875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 1204.2952880859375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9386, loss_val: nan, pos_over_neg: 1279.739013671875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1778.655029296875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1877.78662109375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: -13535.17578125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 8663.2685546875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 1614.288818359375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1181.5313720703125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 16726.17578125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 2142.3115234375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 1921.097900390625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 5381.26123046875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 2969.383056640625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1574.511962890625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 657.3654174804688 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9354, loss_val: nan, pos_over_neg: 958.7848510742188 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 899.5418701171875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 2640.749755859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 4130.03515625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 2098.70751953125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9332, loss_val: nan, pos_over_neg: 1491.6488037109375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 1617.9654541015625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 1751.481689453125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9563, loss_val: nan, pos_over_neg: 1450.055419921875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 814.7297973632812 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 723.555908203125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 4231.46875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9433, loss_val: nan, pos_over_neg: 21970.83203125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 709.8941650390625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 1807.574951171875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.935, loss_val: nan, pos_over_neg: -2892.8740234375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 3343.4111328125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 624.8284912109375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 711.3740234375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9332, loss_val: nan, pos_over_neg: 2014.1527099609375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 962.080322265625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 2173.735595703125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 2689.752685546875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 3055.505615234375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 1701.72412109375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 4117.3671875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 644.8296508789062 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 991.0694580078125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: 918.6112060546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1893.2828369140625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1815.3311767578125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 1155.5634765625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 8086.25 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 738.68115234375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 1791.4007568359375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 793.433349609375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 3354.14501953125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9474, loss_val: nan, pos_over_neg: -4301.67919921875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2519.873291015625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 718.6262817382812 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 896.2802734375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 137681.84375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1099.232177734375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 2145.1435546875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 2226.798828125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1853.751953125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 1195.5888671875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9352, loss_val: nan, pos_over_neg: 898.253662109375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 870.0838623046875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9552, loss_val: nan, pos_over_neg: 1456.495361328125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 659.2472534179688 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 863.207275390625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 1067.7861328125 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 3920.90283203125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9464, loss_val: nan, pos_over_neg: 1889.2808837890625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9506, loss_val: nan, pos_over_neg: 676.6846923828125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 519.982421875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9428, loss_val: nan, pos_over_neg: 2437.455078125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: -42518.1953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 1409.002685546875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9519, loss_val: nan, pos_over_neg: 864.9756469726562 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 772.0787963867188 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 1400.1741943359375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 3715.62109375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1075.2244873046875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9494, loss_val: nan, pos_over_neg: 1003.0125732421875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: -54367.625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 981.4904174804688 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 2645.778564453125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 3165.1435546875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2024.2000732421875 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 2416.658203125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 593.2432861328125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 1457.5411376953125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9485, loss_val: nan, pos_over_neg: 3742.18798828125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.936, loss_val: nan, pos_over_neg: 1335.3018798828125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1107.2178955078125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 937.306884765625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 3031.439697265625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 7424.205078125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 1668.220947265625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 477.1262512207031 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 520.8916625976562 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [59:50<99619:48:23, 1195.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 1034.48193359375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 675.9262084960938 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 1689.462890625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 959.8406372070312 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 17270.59765625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 848.999267578125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 720.552978515625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.947, loss_val: nan, pos_over_neg: 412.9375305175781 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1021.880126953125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 1310.8663330078125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 367.4347229003906 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 643.34716796875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1498.1304931640625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 2631.7568359375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 910.299560546875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 760.3705444335938 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 5036.26025390625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1456.8609619140625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 1936.0679931640625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 1432.9130859375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 669.3054809570312 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.957, loss_val: nan, pos_over_neg: 956.2821655273438 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 1036.4647216796875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 1882.1065673828125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 2385.0771484375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 1048.6630859375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 898.3321533203125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: 2174.47021484375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9584, loss_val: nan, pos_over_neg: 1383.8812255859375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9503, loss_val: nan, pos_over_neg: 666.0303344726562 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: -96521.4765625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 775.9369506835938 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 1283.634033203125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9448, loss_val: nan, pos_over_neg: 530.7635498046875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9491, loss_val: nan, pos_over_neg: 1087.49658203125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 1728.5426025390625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 2247.215087890625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9543, loss_val: nan, pos_over_neg: 1708.3687744140625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9501, loss_val: nan, pos_over_neg: 1017.3115844726562 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 1705.478759765625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1171.8592529296875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: -47169.60546875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1551.78759765625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 2600.05517578125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 936.7288208007812 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 802.1216430664062 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 2557.684814453125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: -47184.30078125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 2835.0146484375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1587.4591064453125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 1092.2972412109375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 1157.6304931640625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 4111.79638671875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 1746.058349609375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1863.479736328125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 37199.0390625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9561, loss_val: nan, pos_over_neg: -5380.22509765625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 962.3994140625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 1275.60205078125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 682.4556274414062 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 3045.716064453125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 1828.2662353515625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1900.950927734375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 960.6256713867188 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1281.864990234375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1438.2735595703125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 576.2628173828125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 1015.811767578125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 5075.91162109375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 597.55859375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 488.5722961425781 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 1363.348388671875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 2235.04150390625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 816.052490234375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 658.570068359375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 1720.6771240234375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 2329.523681640625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 983.8165893554688 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 479.2088928222656 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 1256.3460693359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 1024.8109130859375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 1046.89111328125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1094.9169921875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1752.81689453125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 2222.6796875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 417.1174621582031 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 801.8638916015625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 2178.4931640625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 6845.9365234375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 2940.63232421875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.947, loss_val: nan, pos_over_neg: 707.5148315429688 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 964.2506713867188 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 3217.747802734375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9488, loss_val: nan, pos_over_neg: 624.5259399414062 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 1191.466796875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9492, loss_val: nan, pos_over_neg: 494.88677978515625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 1233.7347412109375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 2886.986328125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 792.4373779296875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 362.46575927734375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1273.4210205078125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 580.9761962890625 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 302.2047119140625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 647.7006225585938 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 753.03125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1338.5887451171875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 557.3758544921875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 556.2548828125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 1431.85888671875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 593.8934936523438 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1065.5389404296875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 1181.1136474609375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1092.46142578125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 1192.6070556640625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 2219.18017578125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2318.119140625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 544.2958374023438 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 567.3057861328125 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 886.101318359375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9472, loss_val: nan, pos_over_neg: 467.3658752441406 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 750.98046875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 1507.0452880859375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9449, loss_val: nan, pos_over_neg: 807.263427734375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1461.47021484375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 579.8969116210938 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 3634.47607421875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 2752.833984375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1262.68310546875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 2929.50390625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 1249.7628173828125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9468, loss_val: nan, pos_over_neg: 2197.49951171875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 10607.0732421875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1084.11865234375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 937.9573974609375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 933.9763793945312 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 81908.9921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1107.8477783203125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 800.00390625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 1519.3173828125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1801.09423828125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 945.4783935546875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 2606.357177734375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 1203.3795166015625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1120.3951416015625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1165.663330078125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 570.7871704101562 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 1776.2088623046875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 1495.179931640625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 1265.8154296875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 3168.447265625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 484.6595153808594 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.939, loss_val: nan, pos_over_neg: 1175.967529296875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.936, loss_val: nan, pos_over_neg: 684.843505859375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9435, loss_val: nan, pos_over_neg: 517.09716796875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 1481.580810546875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1582.9259033203125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 935.1693115234375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.946, loss_val: nan, pos_over_neg: 1332.2894287109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 634.1516723632812 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 904.9555053710938 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 1134.565185546875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 593.6141357421875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9416, loss_val: nan, pos_over_neg: 1021.4427490234375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 599.3093872070312 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2671.944580078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 4321.357421875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 857.1209106445312 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 1011.8388061523438 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1891.4783935546875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 4104.78076171875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 2218.31298828125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 795.2024536132812 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9416, loss_val: nan, pos_over_neg: 1226.4180908203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 943.32763671875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 4833.322265625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 694.3887939453125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 1176.6748046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9448, loss_val: nan, pos_over_neg: 1319.1658935546875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9449, loss_val: nan, pos_over_neg: -34871.34765625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9354, loss_val: nan, pos_over_neg: 2164.496826171875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 1596.359619140625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 1057.521240234375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 934.8358154296875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 4852.03076171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 5842.83837890625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 2184.80126953125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 604.61279296875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 3089.62353515625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.936, loss_val: nan, pos_over_neg: 1523.3748779296875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1257.7293701171875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 2771.3134765625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9447, loss_val: nan, pos_over_neg: 839.8101806640625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 11213.591796875 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 4824.48046875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 2308.500732421875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 784.847900390625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 2127.12158203125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1710.9229736328125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9565, loss_val: nan, pos_over_neg: 1688.5443115234375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.94, loss_val: nan, pos_over_neg: -90212.4140625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 814.4810180664062 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 29411.28515625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 1547.884521484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 2789.320068359375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 10929.6064453125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 2856.712646484375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 688.0177001953125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 8257.9462890625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: -60664.21484375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1049.330810546875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 1187.9073486328125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 1568.613525390625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 5465.99658203125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 3484.298828125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 1006.0107421875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 51328.1015625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 8284.7001953125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 1531.08203125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 713.9505615234375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1898.8704833984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 637.9634399414062 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 721.783935546875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 4020.04345703125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 1092.258056640625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 1220.836669921875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 1802.81787109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 4096.6123046875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: -11101.998046875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 1572.19970703125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 710.2046508789062 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 685.5861206054688 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 2249.60986328125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 1234.4566650390625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 739.0968017578125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 1030.4527587890625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.945, loss_val: nan, pos_over_neg: 2466.20068359375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 5961.88818359375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9507, loss_val: nan, pos_over_neg: 1712.194580078125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 8115.6962890625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1187.8446044921875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1022.0902099609375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1596.92529296875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 2403.89013671875 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 865.4744262695312 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 1196.1207275390625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 776.5533447265625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1363.8721923828125 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 1363.590576171875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 2219.392578125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 629.35986328125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 571.9256591796875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 1616.5054931640625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 4312.5263671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 462.4133605957031 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 1170.1787109375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1408.548095703125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 1096.77001953125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 5066.728515625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 2669.868896484375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 1359.66259765625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 1234.168701171875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1944.24609375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 2315.323486328125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 872.4493408203125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 729.8818359375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 1308.5928955078125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: 2343.657470703125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 4212.48095703125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 1516.9844970703125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 902.5718383789062 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 1070.0802001953125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 2453.529541015625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9433, loss_val: nan, pos_over_neg: 4478.5712890625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 841.8355712890625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 736.4594116210938 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2725.114990234375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 7889.21337890625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 3521.117431640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 764.404296875 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 650.7237548828125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 594.6361694335938 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 1355.9024658203125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1226.017578125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 705.9122314453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 3889.253173828125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 1137.194091796875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 825.7752075195312 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 960.485595703125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 762.3161010742188 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 623.3145751953125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1354.460693359375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 565.2747192382812 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2335.9296875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 1308.747802734375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 2675.04541015625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 925.3875122070312 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 702.5111694335938 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 607.8994140625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 942.847900390625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 1154.3128662109375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 1252.6263427734375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 880.4989624023438 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 1206.5032958984375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 987.626220703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1537.42431640625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 1008.6504516601562 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 891.9098510742188 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1827.550048828125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 8772.0400390625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 3418.27490234375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 1044.2794189453125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 498.2697448730469 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1414.512939453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 9376.4990234375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 1000.4574584960938 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 1495.819580078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 1091.058837890625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1390.865478515625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 975.70849609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 2240.76806640625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 3231.962646484375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9436, loss_val: nan, pos_over_neg: 1412.0771484375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 521.1834716796875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 2842.403564453125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 1444.81689453125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 1441.0921630859375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 705.7520751953125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 1477.8033447265625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 978.7978515625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 8131.26171875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 2346.878173828125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: -53545.62890625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1370.4420166015625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 3116.654296875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9441, loss_val: nan, pos_over_neg: 3500.761474609375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 4506.99462890625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1829.8062744140625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1418.6214599609375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 3088.11962890625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 50190.81640625 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 3839.55078125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: -5669.6953125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9553, loss_val: nan, pos_over_neg: 3041.990234375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 1377.223876953125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 6737.92236328125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 5354.13720703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 963.852294921875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 857.0932006835938 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 811.4088134765625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1097.7611083984375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 2275.051025390625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 1023.2720947265625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 692.5277709960938 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 1027.26025390625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 57074.0078125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: -16529.970703125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 4040.960693359375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1249.0528564453125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1388.575927734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1598.767333984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 903.0115966796875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 684.9647216796875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 608.1091918945312 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 4357.04541015625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: 1483.07275390625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9332, loss_val: nan, pos_over_neg: 1823.018310546875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 1467.6334228515625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 2976.7724609375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2065.6611328125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 769.7755737304688 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 616.2876586914062 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 586.5523681640625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 2312.8056640625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 515.3346557617188 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1282.170654296875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1621.3759765625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 795.2073974609375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 941.5923461914062 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 973.5557861328125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 791.5270385742188 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 5995.81787109375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1004.2501831054688 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 1262.513916015625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2158.411865234375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 4151.87890625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2042.9588623046875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 975.6299438476562 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 1399.4031982421875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 3300.540283203125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1411.3289794921875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 657.7373046875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 3520.884033203125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 5502.46142578125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 708.8303833007812 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 932.6707153320312 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 3213.159912109375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1756.3580322265625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 2017.2684326171875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1263.198974609375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9385, loss_val: nan, pos_over_neg: 822.3671264648438 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9446, loss_val: nan, pos_over_neg: 1144.3980712890625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 1262.6502685546875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1346.75341796875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 1033.703125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 666.7343139648438 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 1239.4705810546875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 688.6974487304688 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 11738.3828125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 780.9234619140625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 1200.760986328125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 1984.808837890625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 990.3109130859375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 837.6861572265625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 788.9046630859375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 1700.0103759765625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 966.126220703125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 13775.6884765625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 8459.6015625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 920.3148193359375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 861.4931030273438 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 994.1204223632812 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 666.2506713867188 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 2104.409912109375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 626.7174682617188 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 431.1825256347656 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1413.138671875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1273.21435546875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 1681.3017578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 1224.034423828125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9321, loss_val: nan, pos_over_neg: 1132.2237548828125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: -2336.535888671875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 1408.869140625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 7143.4970703125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 893.5311889648438 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 6272.26513671875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 848.4804077148438 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1188.6177978515625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1487.5750732421875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 3465.474365234375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 2779.07275390625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 4740.16552734375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 3835.8681640625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 3937.79541015625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1274.4361572265625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 10867.5888671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 3103.571044921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 1309.8433837890625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1839.878173828125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 580.0485229492188 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1829.4886474609375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 806.4744873046875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 719.6541137695312 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 740.3463134765625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 4916.109375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 865.37548828125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1022.0111083984375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 1182.7047119140625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1844.918212890625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1023.041748046875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 1655.90625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9421, loss_val: nan, pos_over_neg: 780.7415161132812 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 965.0693969726562 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 4993.07666015625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 21066.2578125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 985.26611328125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 597.9442138671875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1355.9244384765625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1738.6031494140625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 2414.142822265625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 3486.274658203125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.929, loss_val: nan, pos_over_neg: -15409.12109375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 890.0615234375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 1050.311767578125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 942.8245239257812 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1245.5458984375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 1360.27197265625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 2495.047119140625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 1215.8287353515625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9499, loss_val: nan, pos_over_neg: 1096.3648681640625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1569.7913818359375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 3520.581787109375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 1290.297119140625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 699.1799926757812 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 1112.5130615234375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9422, loss_val: nan, pos_over_neg: 921.8214111328125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 10376.9052734375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 1280.4114990234375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 892.2373657226562 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 2383.135498046875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1754.758056640625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 756.6754150390625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 812.5122680664062 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9445, loss_val: nan, pos_over_neg: 6001.81787109375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1283.1396484375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9343, loss_val: nan, pos_over_neg: 1095.75830078125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 510.69720458984375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1851.5521240234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 8678.3525390625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 1513.5548095703125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 1192.748046875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 8274.736328125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1097.9879150390625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1311.9097900390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: -12780.31640625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 3631.6650390625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 1001.7014770507812 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9482, loss_val: nan, pos_over_neg: 2961.439208984375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 1090.93994140625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 2323.123779296875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 1445.953857421875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9354, loss_val: nan, pos_over_neg: 667.8142700195312 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1002.0311279296875 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 3368.029541015625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1029.3765869140625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 1622.0657958984375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9353, loss_val: nan, pos_over_neg: 1460.4088134765625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1152.2877197265625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 2478.662353515625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: -7482.90380859375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 2043.4515380859375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 10989.4931640625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 8392.333984375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 46834.08984375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 4842.79296875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 8268.9052734375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1203.104736328125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1415.569091796875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1297.453125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 942.88818359375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 1721.5189208984375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1664.9736328125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 1873.44775390625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 2171.052490234375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1621.897216796875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1087.569580078125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 722.2381591796875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1451.9732666015625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 813.7532958984375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 1050.9312744140625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 5332.02294921875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 2722.942138671875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 1789903.875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 2816.533447265625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1463.282470703125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 2278.649169921875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 14058.341796875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 2833.265625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 1780.015625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1539.0828857421875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9343, loss_val: nan, pos_over_neg: 3238.00732421875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 4809.52490234375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1079.4810791015625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 13150.798828125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 2186.376708984375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: -25727.185546875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 13554.4453125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 2424.934326171875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 9256.3525390625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1426.6920166015625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 1310.150146484375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 1164.7200927734375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1584.717041015625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 2948.381103515625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 2674.318115234375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 1481.5772705078125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 686.3499755859375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1572.7557373046875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 1142.0367431640625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 522.8118286132812 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1608.1624755859375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 675.86083984375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 1711.2086181640625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 915.2471923828125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1087.042236328125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 2002.7882080078125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 2804.33251953125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: 1006.5231323242188 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9414, loss_val: nan, pos_over_neg: 504.07611083984375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 3157.619384765625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 2580.65869140625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 695.549560546875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 885.184814453125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 374.9563293457031 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 1577.3607177734375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 2054.73193359375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 1239.3145751953125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 1054.0711669921875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9377, loss_val: nan, pos_over_neg: 882.3514404296875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 2608.46435546875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 1142.235595703125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9406, loss_val: nan, pos_over_neg: 367.0581359863281 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1022.2699584960938 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1016.412109375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 5271.5673828125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 3177.868896484375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 885.5523681640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 989.8805541992188 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 1039.693115234375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 926.41650390625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 983.0025634765625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 860.1963500976562 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 822.387451171875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 612.775634765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 862.1099243164062 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 3155.101318359375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 4153.52001953125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9369, loss_val: nan, pos_over_neg: 875.3779907226562 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 506.45086669921875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 1690.995361328125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 3554.208251953125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 1596.824462890625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 465.1056213378906 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 1041.5555419921875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 2036.850830078125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 3866.702880859375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 2806.659912109375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1279.3936767578125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1862.64013671875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1973.5496826171875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 2049.565185546875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 4049.035888671875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 992.770751953125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2408.304443359375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 1088.30126953125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 860.5865478515625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 4268.40234375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: -3878.201904296875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 3107.37060546875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 892.7011108398438 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1408.802734375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9478, loss_val: nan, pos_over_neg: 1241.8643798828125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 1329.803955078125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1628.4837646484375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.932, loss_val: nan, pos_over_neg: 2326.516845703125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1085.3394775390625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 759.8336791992188 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 1455.1290283203125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1297.6905517578125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 697.4153442382812 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 873.1921997070312 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 799.1104736328125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1777.463623046875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1119.9305419921875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 725.8153076171875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 675.487548828125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1769.6422119140625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1526.0045166015625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1457.2359619140625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 891.0604248046875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 1857.7294921875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1444.6861572265625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 892.5005493164062 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 16322.376953125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 4481.8828125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1094.5936279296875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 876.356689453125 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.945, loss_val: nan, pos_over_neg: 782.2476196289062 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9352, loss_val: nan, pos_over_neg: 730.614501953125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 2534.55859375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 1177.1199951171875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 1056.003662109375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1184.91796875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 846.4423828125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 4244.1103515625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 1234.522216796875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 960.574951171875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 774.185302734375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 650.0800170898438 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 3079.971923828125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 9694.5947265625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9471, loss_val: nan, pos_over_neg: 3743.438232421875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 7662.6728515625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 2924.32666015625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 647.850341796875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 1213.6971435546875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 896.9457397460938 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 662.1317749023438 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1407.35400390625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 1438.0467529296875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1826.8740234375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 81046.4296875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 1280.172119140625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 938.9808349609375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 795.2916259765625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1053.6014404296875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 814.4799194335938 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1036.90478515625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1059.4105224609375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 2083.69873046875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.932, loss_val: nan, pos_over_neg: 1743.3404541015625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 2107.720458984375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 883.1650390625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1979.830322265625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 2110.80224609375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9363, loss_val: nan, pos_over_neg: 1020.7837524414062 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:19:53<99868:51:47, 1198.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 48492.91796875 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 995.76318359375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 873.7741088867188 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1251.0792236328125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 8663.65625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 1154.2357177734375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 1127.1385498046875 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 672.5905151367188 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 1064.2772216796875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 4653.38916015625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 3039.26953125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 970.346435546875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1235.2481689453125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9373, loss_val: nan, pos_over_neg: 768.8197021484375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 1035.2459716796875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 5866.29052734375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1632.6014404296875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 2603.80615234375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 503.3785095214844 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 1513.229248046875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1214.6639404296875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2359.0234375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1965.81005859375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 3131.961669921875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1778.5911865234375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1536.21728515625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1833.085205078125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 1507.3463134765625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9321, loss_val: nan, pos_over_neg: 699.5138549804688 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 2047.0721435546875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 1419.607666015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1493.1329345703125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1228.322509765625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 2532.5966796875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: -10132.2900390625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: -26472.46484375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 2946.6240234375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 2554.068359375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 902.687255859375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1869.5389404296875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: 938.3373413085938 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 1508.7235107421875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1502.18359375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 7628.58154296875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 2316.334716796875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 557.7623291015625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 2546.18310546875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 2814.12548828125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 3133.483642578125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 2540.983642578125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1104.4453125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 3074.88623046875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 2788.3798828125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 935.8934326171875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1310.021240234375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 2313.303955078125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 2399.023193359375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 1459.658447265625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.928, loss_val: nan, pos_over_neg: -53354.83203125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 3500.63525390625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 1616.4786376953125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 944.766845703125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1979.406494140625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 2029.9342041015625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 761.9880981445312 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 584.5611572265625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 1360.44921875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 23739.15234375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 1142.7696533203125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 776.3368530273438 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1333.1395263671875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 5893.6435546875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 8962.8798828125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 5436.1396484375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1568.5433349609375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 5607.044921875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 4241.80126953125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 1178.460693359375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: -58669.70703125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.928, loss_val: nan, pos_over_neg: 2093.128173828125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 4441.58056640625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 2891.11474609375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1592.381591796875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1186.788330078125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 2734.578125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 1039.6014404296875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 1161.35693359375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 6047.5478515625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 962.0239868164062 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 2575.94580078125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 3560.5380859375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1885.0618896484375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 3614.07080078125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1372.879638671875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 5459.341796875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1781.243408203125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9461, loss_val: nan, pos_over_neg: 1728.1446533203125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 2211.4765625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 522.3208618164062 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 2524.11474609375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1933.2613525390625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 826.2820434570312 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 4074.3037109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1028.72509765625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1511.4736328125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 1853.746337890625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1018.6098022460938 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 780.8635864257812 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 539.12744140625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 927.0059204101562 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 927.0687255859375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 2638.095458984375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 1584.7330322265625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 1535.123046875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9336, loss_val: nan, pos_over_neg: 834.3878173828125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9438, loss_val: nan, pos_over_neg: 1410.6295166015625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 4659.568359375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1428.6419677734375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1543.1495361328125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 5307.2744140625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2091.84716796875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 4991.7900390625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 1132.7230224609375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 860.5669555664062 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 10212.19921875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 3660.558837890625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 1453.6585693359375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 3383.529052734375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 451.4265441894531 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 2407.169677734375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 718.1851196289062 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1291.0699462890625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 3553.0732421875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1490.787109375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 11931.9755859375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1735.3572998046875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 2077.329833984375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1074.8358154296875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 2135.67138671875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 4063.100341796875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 1756.7713623046875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1353.531494140625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 6380.6884765625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 979.5673828125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 4277.36962890625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2304.937255859375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 815.9732055664062 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9391, loss_val: nan, pos_over_neg: 1103.44775390625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 740.3276977539062 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 3043.6767578125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1307.162353515625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1859.6259765625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 10738.5908203125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 2868.9345703125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 3600.160888671875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1476.0684814453125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: -4399.38134765625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1286.59912109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 1603.5838623046875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1754.577392578125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 13742.099609375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1274.819580078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9276, loss_val: nan, pos_over_neg: 11788.3427734375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: -11803.6591796875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 4837.205078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 9032.8974609375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1494.657470703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 1700.3974609375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: -12964.1171875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 2408.90185546875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9379, loss_val: nan, pos_over_neg: 1097.01171875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 908.1986083984375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2492.382080078125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2857.278564453125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.932, loss_val: nan, pos_over_neg: 1212.229248046875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 2330.336181640625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: -15678.8291015625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: -15929.90625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 1453.334716796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 8120.2275390625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1238.7010498046875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1445.409423828125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 940.3510131835938 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9419, loss_val: nan, pos_over_neg: 1457.8245849609375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1077.8021240234375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 1575.0277099609375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1518.95947265625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1852.7542724609375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 746.6199340820312 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 2214.05712890625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1097.090576171875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1712.2701416015625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2346.6767578125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1420.8990478515625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 4377.14453125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 3926.538330078125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 4577.11083984375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 2371.7861328125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 2197.348876953125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 701.3472290039062 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 805.0717163085938 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1678.4610595703125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1009.1533813476562 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 842.16796875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: -15829.111328125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 2082.912841796875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 4123.119140625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1188.5416259765625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 643.4766845703125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1192.2178955078125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1151.3345947265625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1156.795654296875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 1315.5692138671875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 943.7960205078125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 2567.231689453125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 7121.7333984375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 1445.1904296875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 37984.125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 11207742.0 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 4038.879638671875 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 2758.21435546875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1889.5045166015625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 858.49951171875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1874.628662109375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 4532.849609375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 872.4972534179688 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 1503.7645263671875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 1174.355224609375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 1193.9095458984375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 2963.826416015625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 6394.75439453125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 2122.63037109375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 1032.4022216796875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1269.089599609375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 1056.6251220703125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 618.9592895507812 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 726.036865234375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1419.5068359375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9442, loss_val: nan, pos_over_neg: 1601.678466796875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2248.9365234375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2598.068359375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 693.0538330078125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 803.1018676757812 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1245.2491455078125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 8715.8486328125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1415.7529296875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 1225.007568359375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 750.9813842773438 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 11752.0185546875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 949.5243530273438 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 675.8772583007812 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1563.0947265625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1521.6580810546875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 870.2276611328125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1093.904541015625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 837.3035888671875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 38093.6171875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1407.1971435546875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 2347.5048828125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 772.4057006835938 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1402.91943359375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1712.57275390625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1567.0185546875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 3901.6591796875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1706.458251953125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: -5301.60546875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2831.136962890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 982.0672607421875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 792.4070434570312 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 5741.19140625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 8572.6123046875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 18680.72265625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 1303.18994140625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1120.8807373046875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 15147.1064453125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1416.0030517578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 7127.421875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1678.6617431640625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 736.6547241210938 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2959.71728515625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 50629.5859375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 995.106689453125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 965.7114868164062 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 14231.625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 5565.7734375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2527.094482421875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 2751.365234375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 1363.8319091796875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 4834.12451171875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 1228.0418701171875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 1442.3543701171875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 1728.450927734375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 3873.818115234375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.93, loss_val: nan, pos_over_neg: -22205.27734375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 4695.86669921875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1161.9892578125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 7404.8603515625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 1928.6573486328125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1368.6595458984375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 1951.6961669921875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: -12785.771484375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 2310.660400390625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1759.395751953125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 1085.6761474609375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 7426.447265625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 4292.0615234375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 10762.4609375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 2958.269287109375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1507.7305908203125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2065.871826171875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 599.9520874023438 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 2345.46875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 1949.1583251953125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 3384.99658203125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 3065.729736328125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 5030.46337890625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 2933.730224609375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 2241.4375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 2123.963623046875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 3414.027099609375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1953.538330078125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 630.9663696289062 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1107.48974609375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 8551.2080078125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1079.839599609375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1275.7423095703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1779.116943359375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 3659.75 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 952.55908203125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 484.59466552734375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1149.4451904296875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9376, loss_val: nan, pos_over_neg: 543.2969360351562 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 553.3681030273438 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 837.3947143554688 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 1221.5748291015625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: -22289.609375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 2189.264892578125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 1304.2674560546875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 1166.185302734375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2268.1005859375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1251.9002685546875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1152.2374267578125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 1295.2769775390625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 3020.142822265625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 714.6735229492188 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 674.9212036132812 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1575.5555419921875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 4390.65576171875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 5741.13818359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 922.6244506835938 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1331.209716796875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 667.33056640625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 2346.536865234375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1469.2677001953125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 633.9132690429688 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 710.10888671875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 2606.386474609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: -18514.140625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 1045.3106689453125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1406.5091552734375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 1704.674560546875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1410.8814697265625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 4211.7119140625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 3402.40185546875 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1564.83447265625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1587.732177734375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1503.713623046875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 2595.22265625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 5763.62890625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1275.5299072265625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 910.214111328125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 2336.952880859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 4871.208984375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 5235.5810546875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 4212.85498046875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9345, loss_val: nan, pos_over_neg: 982.9132690429688 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 2862.254150390625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1200.1728515625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 1115.02294921875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 2685.5791015625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 873.0157470703125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1538.718505859375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 996.6250610351562 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 1862.9678955078125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 604.6114501953125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 129667.8125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1691.54345703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 6616.1328125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 3414.452880859375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: -13706.232421875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1421.7359619140625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 14642.0712890625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 791.0582885742188 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 1801.300048828125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 2280.310791015625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 2515.6806640625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 1209.798583984375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 3754.55126953125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 1625.51123046875 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 4647.50341796875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1047.4256591796875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 743.4452514648438 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1901.990234375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1538.7491455078125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1272.4927978515625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 785.8048095703125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 484.72930908203125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 2474.58740234375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: -8929.57421875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.939, loss_val: nan, pos_over_neg: 1600.67041015625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 2041.6475830078125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1055.3607177734375 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 575.9491577148438 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 963.7943725585938 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 694.1337280273438 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3922.369873046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 897.3397216796875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 1735.661376953125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 24910206.0 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 1639.2281494140625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 941.0905151367188 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1869.0845947265625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: -4777.20947265625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 2171.738525390625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 840.197265625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.946, loss_val: nan, pos_over_neg: 526.7701416015625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 1047.2969970703125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 4012.25537109375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1468.751953125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 31312.5 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: -12832.8544921875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: -4662.17041015625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 3046.057861328125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 32609.453125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 4654.29150390625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 4236.95947265625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1923.805419921875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 2445.348876953125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: -4168.9794921875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1755.2762451171875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 2173.43212890625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1216.5904541015625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1498.4664306640625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 12972.6884765625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 855.5279541015625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1704.7060546875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1521.0877685546875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 2030.934814453125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 1392.5489501953125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 2509.867919921875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 1487.1483154296875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 911.6470947265625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1950.9520263671875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1617.331787109375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1861.4979248046875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 2925.8486328125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1823.425537109375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 1088.912841796875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 1123.15478515625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2138.67041015625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1025.2073974609375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 866.4144897460938 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 782.0892333984375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1409.4130859375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1369.0506591796875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 2294.191162109375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 3141.100341796875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 1281.659423828125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 3031.1611328125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 13814.1455078125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 857.859130859375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9309, loss_val: nan, pos_over_neg: 741.2982788085938 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 2065.24462890625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1386.6796875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 6843.37841796875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 1002.5469360351562 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9417, loss_val: nan, pos_over_neg: 2626.812255859375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 574.3857421875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 657.3652954101562 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 1079.1339111328125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1091.5770263671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1542.2608642578125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 604.36474609375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 647.0189208984375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1077.320556640625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 2299.60595703125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 921.6267700195312 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1235.4420166015625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 997.0086669921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1239.065673828125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9455, loss_val: nan, pos_over_neg: 7336.451171875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 15234.9189453125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1598.2774658203125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2082.564208984375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 2033.182861328125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 3051.753662109375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 5196.38232421875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 3317.11474609375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 2815.31787109375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 3750.60791015625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 4789.41845703125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2567.82763671875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: -12801.6162109375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 4212.68359375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1859.9986572265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 3984.5224609375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 2666.058349609375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 849.395751953125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 650.6571655273438 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: -5277.21142578125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 23070.208984375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 2635.02197265625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 893.3275756835938 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 803.827392578125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 792.568115234375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 9667.36328125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1596.315185546875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1136.3970947265625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 1364.7880859375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 4033.996826171875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 925.4956665039062 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: -3421.960205078125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 944.90283203125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 792.3135986328125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1075.1224365234375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 4687.64208984375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1389.709228515625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 952.7484130859375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 8352.46484375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 2214.56201171875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 668.5968627929688 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: 1063.0926513671875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 800.4834594726562 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1637.704345703125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 654.6027221679688 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 804.8668212890625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: -423621.6875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 3865.00390625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1567.407470703125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 716.1768188476562 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 391.85064697265625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2260.30908203125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 3926.967041015625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 2601.17431640625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 724.188720703125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 3322.719482421875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 2700.450927734375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 1904.125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1739.6068115234375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 698.7560424804688 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1348.9114990234375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 9345.3525390625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1783.546875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 3276.172607421875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 1211.8658447265625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 1027.69580078125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1046.1943359375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 6201.0087890625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 3902.05126953125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 14960.4267578125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1106.498291015625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 4746.83056640625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 1251.3590087890625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 3807.91015625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 2191.6962890625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1242.6708984375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 2490.88427734375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1751.5784912109375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1335.853759765625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 2698.37451171875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1153.256591796875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 1456.8458251953125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: -5539.7724609375 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4549.3427734375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 933.2743530273438 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 3217.4736328125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: -5025.32373046875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 2885.8583984375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 2605.797119140625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 3103.9208984375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2225.093505859375 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 2950.040771484375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 3964.186279296875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1020.078857421875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 18716.30859375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 12999.9677734375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1717.344482421875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2223.25537109375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1054.1529541015625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1319.4014892578125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3504.05712890625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 771.1920166015625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 2428.400634765625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1421.925048828125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 2169.177001953125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 914.8570556640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1590.05029296875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 775.0942993164062 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 807.3688354492188 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1394.318359375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9319, loss_val: nan, pos_over_neg: 552.8455200195312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 519.7066040039062 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 535.0386962890625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 2129.062744140625 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 3651.335205078125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 1119.6392822265625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 442.94732666015625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 790.5437622070312 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1470.763916015625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 987.8992919921875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 817.1135864257812 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 773.7079467773438 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 1320.468017578125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 786.7185668945312 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 547.3032836914062 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 1053.7523193359375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 2270.457763671875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1459.603515625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 7701.009765625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 1990.5045166015625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 678.343505859375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 3104.794189453125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 44050.95703125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: -6541.99658203125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 2208.727783203125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1866.9869384765625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 3239.134521484375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 10118.8896484375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1961.099853515625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1140.77490234375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 740.2525024414062 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 686.4291381835938 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1721.2646484375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 6437.28271484375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1683.4610595703125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 1272.68603515625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 1816.7017822265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: -29751.7734375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 1115.8544921875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2417.785888671875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 1781.838623046875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 1220.028076171875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 1086.70751953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 734.6453857421875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 3247.6669921875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 2437.340576171875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: 924.2584228515625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1000.8640747070312 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 3102.49658203125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 8649.38671875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1272.2852783203125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 616.186279296875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 655.4506225585938 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 5421.55712890625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 2015.730712890625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1059.12109375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1209.8052978515625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 726.3782348632812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 2067.888916015625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1717.4732666015625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 4327.765625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1049.2994384765625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9395, loss_val: nan, pos_over_neg: 997.7797241210938 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 8885.421875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: -8164.3046875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 4838.46142578125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 980.4926147460938 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1049.1197509765625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1475.686767578125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 3259.759521484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 1131.81396484375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 1377.436279296875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -37462.03125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 665.8837280273438 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9324, loss_val: nan, pos_over_neg: 1213.925048828125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 766.4501953125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1356.6456298828125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 981.3973388671875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 2818.05126953125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 832.1109008789062 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 582.9229125976562 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 2537.19580078125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 1393.1251220703125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1139.917236328125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: -19268.53125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 777.8526611328125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 842.3336181640625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 1018.747314453125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 1000.1024169921875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1137.242431640625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.925, loss_val: nan, pos_over_neg: 1483.7474365234375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: -2411.908935546875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 2964.68505859375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 2982.201416015625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 6650.12744140625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:39:57<100015:47:28, 1200.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 2776.0302734375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 1851.7734375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 5073.083984375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 914.9872436523438 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1439.75390625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 2562.899658203125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 3986.3583984375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1590.9510498046875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 938.4153442382812 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: -4810.630859375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 2606.7080078125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1892.455810546875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1420.7562255859375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1175.7529296875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 648.43701171875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 2060.46630859375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 1384.7479248046875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.93, loss_val: nan, pos_over_neg: -3777.008056640625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 1394.45263671875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.922, loss_val: nan, pos_over_neg: -29905.30859375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 1367.61962890625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 1043.47412109375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 2521.769287109375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 7024.8037109375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1770.9638671875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1091.4542236328125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 31133.1640625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1960.3267822265625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 3602.5400390625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 8067.27294921875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 4643.3720703125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1688.933837890625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 1922.6328125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1394.9022216796875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: -6062.38720703125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 2208.546875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 16652.24609375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1273.1708984375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: -4836.62841796875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1443.066162109375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 2308.2734375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 6609.037109375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 1576.6298828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2332.611328125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 32051.9140625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: -63582.10546875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9311, loss_val: nan, pos_over_neg: 584.581787109375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: -7010.6884765625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 2529.537109375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 1833.679443359375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 2152.7080078125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 1685.814453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1389.2166748046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 284393.4375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: -13121.599609375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: -5694.3232421875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 1414.555908203125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1060.96533203125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.922, loss_val: nan, pos_over_neg: -13105.69140625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 15440.796875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1182.3492431640625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: -4466.580078125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 3314.990234375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 2729.72265625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4109.462890625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 3343.7685546875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: -2458.70166015625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 143851.0625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1928.560791015625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 3665.50927734375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 4757.14306640625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 5506.24072265625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 2424.142333984375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1657.49267578125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 2104.4365234375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 2549.803955078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1357.0653076171875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.91, loss_val: nan, pos_over_neg: -4558.78125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9328, loss_val: nan, pos_over_neg: 883.1714477539062 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 4762.2041015625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1784.82666015625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 595.7508544921875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 1691.0399169921875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 955.9569091796875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 915.3439331054688 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 793.2153930664062 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1148.1087646484375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 55933.41015625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 712.7410278320312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 2884.62353515625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: -33862.4296875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 2745.173828125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 4283.43310546875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 1137.95263671875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 3252.9345703125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1503.0029296875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 1553.856201171875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 3275.4794921875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 514.3841552734375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 2323.810546875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3156.2109375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1754.978271484375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 1070.92724609375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2215.968017578125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 33306.07421875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1174.3106689453125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 2212.11572265625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 976.9185791015625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 747.5553588867188 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 867.4927368164062 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 2302.036865234375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 1271.8341064453125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1139.240234375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2984.593994140625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 4207.3203125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.914, loss_val: nan, pos_over_neg: -5896.76318359375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.921, loss_val: nan, pos_over_neg: -5349.1630859375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1402.188232421875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 2072.63720703125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 2267.71826171875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 29803.693359375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3506.833984375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1295.21533203125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 920.2169799804688 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 4486.05078125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1005.2816772460938 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1803.7327880859375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1260.625732421875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1735.757568359375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1897.09765625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 2235.836669921875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 1726.016845703125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 3743.483642578125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1490.126708984375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1233.993408203125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 658.580322265625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 1482.0115966796875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1552.0902099609375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 2561.22021484375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 649.8106079101562 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 1463.8228759765625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 2330.173095703125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 898.3704223632812 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1429.2745361328125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1149.43310546875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1488.7239990234375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 2509.595703125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 869.0999755859375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 4573.84423828125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 980.1603393554688 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 1597.9263916015625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1026.72509765625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 2144.4697265625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1813.51806640625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 3401.42333984375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 815.4529418945312 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 24059.4921875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 948.21142578125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1347.399169921875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.914, loss_val: nan, pos_over_neg: -6758.0302734375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 1742.529541015625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 5680.587890625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1044.41552734375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 2299.10693359375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 394.83038330078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 398.8850402832031 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1158.72607421875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3905.697509765625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: 6686.6728515625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 8482.18359375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 13509.794921875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 2539.84521484375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 2166.411865234375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1683.5001220703125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2040.9404296875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1390.3189697265625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 594.362548828125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1628.8638916015625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 727.6414794921875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 447.33221435546875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 7724.51025390625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1621.1348876953125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1796.689697265625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1595.531005859375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 6211.55615234375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 1068.6614990234375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 658.5917358398438 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 3411.533447265625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 2055.234619140625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: -4926.97216796875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 4535.99267578125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1749.8531494140625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: -16162.65234375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 1984.5284423828125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 1250.3538818359375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 39325.52734375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 9797.595703125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1752.1248779296875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1790.514892578125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 935.4705200195312 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2802.374267578125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 623.8469848632812 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: -7741.6748046875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1071.2489013671875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 513.1222534179688 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3720.440185546875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1898.2911376953125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 1370.8685302734375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 671.8619384765625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 685.0542602539062 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 2708.3056640625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 5928.37158203125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 1307.04736328125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1074.91162109375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2248.546142578125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 947.1129150390625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1287.817138671875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 2228.78955078125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1947.761962890625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 1298.548583984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1450.6485595703125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 5590.5712890625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: -11324.7998046875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 2542.656982421875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 3041.324462890625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1603.2738037109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 1168.648193359375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2456.077392578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 6611.5166015625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 2452.875732421875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1923.033203125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -11857.1533203125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1655.9859619140625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1114.5562744140625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 4564.44775390625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: -3591.837158203125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 920.4867553710938 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 857.3189697265625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 35509.5 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 7077.5908203125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 5790.60791015625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 49370.265625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 845.3496704101562 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1205.43017578125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 10429.5693359375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 4193.8173828125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1094.0423583984375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 1101.117431640625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1214.835205078125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 5369.1669921875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 2172.921630859375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 1420.26611328125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 9569.939453125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.925, loss_val: nan, pos_over_neg: 1037.0911865234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 30402.09375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 6055.85400390625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 28318.767578125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1553.453857421875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1968.3056640625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1209.978271484375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1459.26171875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 2188.2509765625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 5082.38916015625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 807.2922973632812 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1641.120849609375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1338.85302734375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 3635.136962890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 3230.1044921875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.925, loss_val: nan, pos_over_neg: -4604.78759765625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 37612.01171875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1151.6417236328125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2848.66552734375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 2057.6767578125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1262.91845703125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1038.1392822265625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1102.067138671875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1430.990234375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 4772.21044921875 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 2517.73583984375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1243.435791015625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1739.04833984375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1894.721923828125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 792.6815185546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 6518.498046875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 1704.81640625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 2824.49072265625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1712.8192138671875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 1476.8565673828125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1712.5098876953125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1177.1795654296875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 2498.146240234375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.925, loss_val: nan, pos_over_neg: 12314.5361328125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1426.0474853515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1669.4942626953125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 2482.032470703125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2467.397216796875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -3861.405517578125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 7704.55517578125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 884.7332763671875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 775.93798828125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 25999.986328125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 4770.52880859375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 2052.269775390625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1253.3798828125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 27552.990234375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2803.83349609375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 1593.650390625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 1399.779296875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1491.963623046875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1068.3887939453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 4282.97412109375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2791.479248046875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 1003.596923828125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1437.1322021484375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 2031.3411865234375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 583.6765747070312 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 695.1117553710938 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 1113.984130859375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 2567.938232421875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 7547.71923828125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 33517.12109375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1477.3538818359375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1324.2562255859375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 3772.94384765625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 16449.119140625 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1655.8604736328125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 609.8065185546875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 781.5662841796875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 51751.09375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: -6030.7099609375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 7274.23779296875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 1378.1710205078125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 3002.158203125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 4864.02880859375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 2218.025390625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 2973.016845703125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1280.779052734375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1294.16796875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 1883.0262451171875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 913.7892456054688 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4374.537109375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 997.3294067382812 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: -30385.1640625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1781.48779296875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 3641.294677734375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: -15022.310546875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 902.02490234375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 737.4840087890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1240.11376953125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 1197.462158203125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 26738.291015625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1124.5626220703125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1796.0576171875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 1523.23193359375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2041.704345703125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 8152.5654296875 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.914, loss_val: nan, pos_over_neg: -9012.087890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1001.6215209960938 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1058.325927734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 5817.65087890625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 4016.723388671875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 1873.02197265625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 3799.92919921875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 1030.5560302734375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 1589.239013671875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 22680.3046875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 2109.097900390625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1944.9844970703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 2226.327392578125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 3585.323974609375 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: -76594.203125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 6403.234375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1209.7591552734375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1092.09033203125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3792.57666015625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8987, loss_val: nan, pos_over_neg: 2565.287841796875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 17725.95703125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 11780.162109375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 13330.77734375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9286, loss_val: nan, pos_over_neg: 2130.94970703125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 1533.2149658203125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2269.224609375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 758.1945190429688 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 8351.314453125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1663.5496826171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 500.3431091308594 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 3039.08056640625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 6181.73974609375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 972.29638671875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 4326.1533203125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 1050.77880859375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1132.9644775390625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1970.367431640625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1545.52294921875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 2020.933349609375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 5174.28173828125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 3291.482666015625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 3114.94580078125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 10078.8740234375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 2114.76123046875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 1592.6029052734375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1201.5216064453125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 594.057373046875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1709.15576171875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 782.169921875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1737.1322021484375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 2304.421630859375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 7530.72021484375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1868.0755615234375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: -11486.921875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 2512.089111328125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 2103.428466796875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1169.4654541015625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1461.7205810546875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 2558.055908203125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1230.9349365234375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 1200.8582763671875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 2408.95361328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1904.7708740234375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: -25957.744140625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: -19894.99609375 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 5395.5205078125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: -4871.2587890625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 2228.8056640625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 4446.19921875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 42552.1953125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 6519.19970703125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 2369.002685546875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1416.81884765625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 1321.56201171875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 976.3785400390625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2029.9696044921875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 1740.7298583984375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 223862.265625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1862.827880859375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 6149.43994140625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: -6692.2001953125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 1898.2691650390625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 1772.8380126953125 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 6810.07421875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 70330.9609375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 4044.204345703125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9321, loss_val: nan, pos_over_neg: 1376.6600341796875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1103.4508056640625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 910.0184936523438 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 25776.873046875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 2843.82177734375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 693.1392211914062 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1106.8671875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 619.9368286132812 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 527.0968627929688 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 658.1196899414062 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 634.4827270507812 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 1580.7578125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1983.4991455078125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 771.3433227539062 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1094.2867431640625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: -97637.640625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 782.6519775390625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 10811.10546875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 686.609130859375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1847.99462890625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: -13945.720703125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 2737.043212890625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1234.28369140625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 3358.79296875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2808.6494140625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 3654.068603515625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 6315.064453125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 3236.935546875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 4743.68505859375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1610.43896484375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 3572.563720703125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 2652.713134765625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 2624.84423828125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 4618.4765625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 4048.330810546875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 1733.679443359375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 6690.595703125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 1473.0521240234375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1469.2138671875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: -6462.5078125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 3625.138916015625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1826.9664306640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1671.1207275390625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.912, loss_val: nan, pos_over_neg: -8857.4267578125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 3145.560791015625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 12102.859375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1946.7861328125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 5570.3984375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2311.6767578125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9306, loss_val: nan, pos_over_neg: 1883.8489990234375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 932.4706420898438 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1638.407958984375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: -7017.72900390625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 12375.5654296875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 2451.239501953125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: -69455.3984375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 2713.873046875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 1064.890380859375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9298, loss_val: nan, pos_over_neg: 1644.660400390625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 4698288.0 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1085.3656005859375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 2067.384033203125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1010.4219360351562 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2981.951171875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 7615.3720703125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2269.5634765625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 1909.10498046875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 7257.62744140625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 2509.712158203125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9254, loss_val: nan, pos_over_neg: 4518.37939453125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 1929.8634033203125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 1024.482421875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1984.822021484375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1605.897216796875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: -16181.787109375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 6666.82470703125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1530.1561279296875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1800.85888671875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1756.2784423828125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 1214.1256103515625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: 1453.849609375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 1299.949462890625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 628.8189697265625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 8304.9189453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1631.981201171875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1669.130859375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 2877.56787109375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 11076.8681640625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 1963.6680908203125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 3620.09765625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1562.040283203125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 2489.271484375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1493.738525390625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 12266.537109375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 2878.30908203125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1077.1240234375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 3271.242919921875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 2990.249267578125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 1320.3721923828125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 720.6920776367188 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1982.1513671875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 2968.2783203125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 3208.653564453125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1128.8980712890625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 749.4777221679688 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1084.7734375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 2261.526611328125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9343, loss_val: nan, pos_over_neg: 2554.51220703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 719.2345581054688 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1095.9853515625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 5020.212890625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -18023.42578125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 32305.134765625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1511.84375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1884.1920166015625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1594.5904541015625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1684.5887451171875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: -26864.71484375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1431.425537109375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1043.4019775390625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: -7245.08251953125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 5125.2294921875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 1655.281982421875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1701.2032470703125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 1832.947998046875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1485.5091552734375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2130.880615234375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 2287.94921875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1155.4248046875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1774.1025390625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9349, loss_val: nan, pos_over_neg: 3310.25439453125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 4426.62744140625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 4464.791015625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 5678.8544921875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1659.6956787109375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1520.273193359375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 7099.95263671875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 2767.73974609375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 3452.2587890625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 6049.087890625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 2554.523193359375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 1519.78173828125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.929, loss_val: nan, pos_over_neg: 1220.4677734375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1836.9306640625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 4198.43359375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 6895.14208984375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 2266.600341796875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 882.5283813476562 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 1114.1060791015625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1344.3912353515625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1393.87939453125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1652.54052734375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 11872.9326171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1948.272705078125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 847.4385986328125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1255.2498779296875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1050.131591796875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 918.2046508789062 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9252, loss_val: nan, pos_over_neg: 892.9920043945312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 3998.1484375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 3662.3603515625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 2495.98876953125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1012.7833862304688 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 35811.60546875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9288, loss_val: nan, pos_over_neg: 24842.12109375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 2046.1973876953125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1192.362060546875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 11356.0400390625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1654.394287109375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 1148.4176025390625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1380.516845703125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 291028.625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1837.990478515625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1543.4205322265625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 1247.5986328125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: -3730.239990234375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 2171.042724609375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1168.2110595703125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2134.091552734375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 2267.85205078125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2813.239501953125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 5040.78125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 19100.8046875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 1147.09814453125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 8460.0517578125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 878.6343994140625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 4015.284912109375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 885.0553588867188 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 3891.833251953125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: -842751.625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 24595.9140625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 3083.489013671875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1369.4854736328125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1029.5947265625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 2855.08544921875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 2211.600830078125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 2919.945556640625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 983.709228515625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1000.2305297851562 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1708.657958984375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 12547.3173828125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 806.4009399414062 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 797.735595703125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2161.577880859375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 11591.3203125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 19710.869140625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 940.0065307617188 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 849.1981811523438 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: -12204.7470703125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 552484.5 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1236.696044921875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 957.748046875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1339.6551513671875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1563.805419921875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 5132.98779296875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 3110.40625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 985.3955688476562 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 1381.0634765625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: -26061.564453125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 3566.28173828125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 2098.823974609375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 2934.504638671875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 2167.744140625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8946, loss_val: nan, pos_over_neg: 3989.90869140625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1001.5339965820312 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 1075.217529296875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 1672.4764404296875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 1731.2918701171875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 3249.38525390625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 989.2283935546875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1540.43701171875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: -3709.119384765625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 3913.4970703125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1391.50244140625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1381.3978271484375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1736.90380859375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2030.8935546875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 2217.10205078125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.895, loss_val: nan, pos_over_neg: -2474.236083984375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 16812.69140625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.924, loss_val: nan, pos_over_neg: -338005.65625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: -1219289.375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 1456.5831298828125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 5274.005859375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 12609.9248046875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 2265.947265625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1184.5029296875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 11255.6337890625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1627.2081298828125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 3176.62451171875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 2595.142578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 1261.859375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 32559.42578125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:00:01<100122:16:23, 1201.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 915.0073852539062 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1190.4296875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: -9027.373046875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1812.66357421875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 1824.3536376953125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 3802.115234375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 2484.25537109375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 7818.12646484375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 4718.265625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: -23409.009765625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 3008.78564453125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 998.4008178710938 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: -4114.201171875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 3330.475341796875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 16388.4140625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 486.24273681640625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 220997.171875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9312, loss_val: nan, pos_over_neg: 1481627.75 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2551.81884765625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 2900.4853515625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 1282.052490234375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 951.1390991210938 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2160.34619140625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 554.3411254882812 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 1272.7413330078125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1517.784912109375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1569.71923828125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 9128.7236328125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 5858.33203125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: -9143.6796875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1302.4000244140625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 1660.477783203125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 3331.606689453125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9255, loss_val: nan, pos_over_neg: 520.1856079101562 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1966.9112548828125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2025.714111328125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: -17105.19140625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1006.8687133789062 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1232.1717529296875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9299, loss_val: nan, pos_over_neg: 916.5042724609375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 2112.87353515625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 5076.67529296875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -7552.48974609375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 3005.43896484375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 3795.646728515625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 3536.660888671875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 2013.8748779296875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: -10106.060546875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2905.103271484375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 14701.849609375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 4899.591796875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 4798.67431640625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 2476.834228515625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 7715.22216796875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 4843.24609375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 2005.243896484375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: -14571.6328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: -7648.1650390625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 4961.4189453125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 6564.65869140625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 3905.639892578125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 5150.9931640625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.911, loss_val: nan, pos_over_neg: -4055.68212890625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 4374.31103515625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 6588.64111328125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 11619.26953125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 7785.81201171875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 9513.75390625 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2572.632568359375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 2358.921142578125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 2123.1533203125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1810.4964599609375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1521.190673828125 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 1357.888427734375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1479.69677734375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 2711.494140625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 1061.2030029296875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 747.7968139648438 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: 1812.70166015625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1397.64697265625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 2401.808349609375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 1672.3056640625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1726.414306640625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1020.232177734375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 1328.261474609375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 4069.52978515625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 816.07666015625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 824.155029296875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1315.24853515625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: -11698.716796875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1158.8614501953125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 1657.39013671875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 2410.0380859375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 3833.122314453125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1535.2833251953125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 785.0856323242188 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 1866.0447998046875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 815.5452880859375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 3558.170166015625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9264, loss_val: nan, pos_over_neg: 3668.13623046875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 2280.628662109375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: -13272.8642578125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 5087.51220703125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1567.1693115234375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 1064.772216796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1533.8614501953125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 3126.39501953125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2261.40625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 1694.50341796875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 51288.99609375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1460.998779296875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 8236.7158203125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.901, loss_val: nan, pos_over_neg: -3469.379150390625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1838.49853515625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 2273.531982421875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 1505.7181396484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 1812.5517578125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 4914.20556640625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: -39158.5859375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 30588.181640625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 2342.617919921875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1762.157470703125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 3405.590576171875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 5375.54248046875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 6841.5654296875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 949.8619384765625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 5667.6748046875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 7420.5341796875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 810.1507568359375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1555.25390625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 1215.211669921875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1387.5325927734375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 1358.8685302734375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 4141.50732421875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1693.389892578125 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 507.6087951660156 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 759.5479736328125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 1882.5594482421875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 883.6702880859375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 750.10986328125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 713.5235595703125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: -39577.390625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 8539.3583984375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1346.3331298828125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1492.89208984375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 1649.2196044921875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 2142.850341796875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 3279.670166015625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 1573.257080078125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 959.7039794921875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 763.3081665039062 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 1539.8226318359375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 928.580078125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1834.255615234375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1190.510498046875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 1135.5494384765625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1986.39794921875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: -4880.8828125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 1915.8487548828125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1477.5162353515625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1728.68505859375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1468.1497802734375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1515.2969970703125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2050.26318359375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1449.381591796875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2280.0166015625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 1258.0872802734375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1407.0926513671875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1483.5245361328125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1017.80859375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 725.9194946289062 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 702.8294067382812 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1833.4493408203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 960.890625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 720.3832397460938 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1250.1458740234375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 3100.402587890625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 2849.52294921875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1569.6768798828125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 906.4702758789062 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2307.7958984375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1094.1572265625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1452.1820068359375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1648.074951171875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1183.9913330078125 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 2625.1669921875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.902, loss_val: nan, pos_over_neg: 4245.169921875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1842.0673828125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1230.97314453125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 756.2969970703125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 4528.9248046875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 1258.4454345703125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 4865.39111328125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 9801.2490234375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 2827.097412109375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 4610.2236328125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 5497.34423828125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 4080.6201171875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 2154.521240234375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: -5466.12841796875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 25856.328125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 6408.31689453125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2446.625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 2996.492919921875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 881.7698974609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: -105863.1171875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9273, loss_val: nan, pos_over_neg: -6735.32177734375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1731.5303955078125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 4675.37744140625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1110.62744140625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 1295.1483154296875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1157.8043212890625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 911.7153930664062 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 4109.341796875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 1660.618896484375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 1130.83935546875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1732.9013671875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1676.95654296875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 170258.0625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 5880.27197265625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1192.95361328125 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: -60045.3203125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1938.935302734375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2987.28662109375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 2540.7041015625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1306.118408203125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 2034.7996826171875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9238, loss_val: nan, pos_over_neg: 2193.929443359375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 3145.224609375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 3296.08203125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 646.7845458984375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1171.888671875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 1422.412109375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 9083.4833984375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1001.4627685546875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1654.1102294921875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1537.278076171875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 18916.177734375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 663.402587890625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1683.5152587890625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 735.0845947265625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 1382.91650390625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 2943.329345703125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1860.77490234375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1191.3856201171875 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 4624.0048828125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1204.39794921875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1720.263427734375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1195.0625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1811.259521484375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1261.89013671875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 1095.937255859375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 2302.83837890625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 2403.129150390625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 8402.6982421875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 6849.44775390625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 1437.849365234375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 14765.3603515625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 2466.085693359375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 14604.8173828125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 2184.681640625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 1800.5330810546875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1202.9720458984375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 2257.7041015625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 1389.5150146484375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1062.028076171875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 631.2564086914062 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 2440.521728515625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: -8136.7294921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.908, loss_val: nan, pos_over_neg: -15209.8271484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 895.9627685546875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 1550.2076416015625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 1967.87890625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1136.907470703125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 2656.593505859375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 916.0968017578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 1527.7476806640625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: -77816.859375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: 3285.695556640625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 1363.32421875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1701.6092529296875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1082.8316650390625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: -17971.076171875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 2685.18017578125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 2845.462646484375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 3943.134521484375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2255.9921875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 3396.46728515625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1449.782958984375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: 2571.857421875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 1931.0089111328125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 2001.2554931640625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2391.868408203125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 2787.588623046875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 3626.376708984375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 2442.442626953125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 3524.2587890625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 8682.7998046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 25561.33984375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1401.7166748046875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 714.6112060546875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 3006.981689453125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 6101.0234375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 3246.2001953125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1689.923583984375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 2696.237548828125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 961.740966796875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1613.0594482421875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1130.3409423828125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 955.56298828125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 554.6195678710938 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1138.2947998046875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1256.1685791015625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1358.884765625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 2107.66796875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1482.3663330078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 8899.615234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 1174.0804443359375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 2975.958984375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1285.661865234375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 4279.455078125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 16619.439453125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1394.9486083984375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 5665.435546875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 1601.0 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2754.055908203125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 4124.4560546875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 1023.2027587890625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2194.37353515625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 708.3528442382812 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 3468.5703125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 969.615234375 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9267, loss_val: nan, pos_over_neg: 1062.292236328125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1498.709716796875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 3283.10009765625 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 760.0953369140625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1883.380615234375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 1482.675048828125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1245.537353515625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 659.2952270507812 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 2903.152587890625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1003.2384643554688 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1424.4488525390625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 5717.294921875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 2043.1654052734375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 2672.783203125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1741.5516357421875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 925.0018310546875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1909.5068359375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 2129.097900390625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 2014.4532470703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 1330.1641845703125 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1121.14404296875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: -16638.64453125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 4804.56689453125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1761.931396484375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 1474.7379150390625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 2520.89208984375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 635.0744018554688 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 1933.4798583984375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1397.0367431640625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1576.932861328125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1527.0799560546875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1122.5159912109375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 894.201416015625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1120.5340576171875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 2659.113037109375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 1161.2518310546875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 792.1221313476562 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 1194.2857666015625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 4576.29345703125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 897.4149780273438 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 12600.1513671875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 2116.385986328125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 5641.18359375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 3102.47119140625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 3591.437255859375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 1351.4173583984375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1351.1273193359375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1226.1392822265625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 5773.1220703125 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 2529.63720703125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1535.00244140625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 2762.7890625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 738.2337036132812 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 3407.609130859375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1182.4713134765625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2030.945068359375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 680.1197509765625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2275.569091796875 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 3249.350830078125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 1049.147216796875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1707.6141357421875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1056.72216796875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9234, loss_val: nan, pos_over_neg: 615.9171752929688 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1796.50634765625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1881.3494873046875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1987.4696044921875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 3594.36474609375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: -201273.671875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 4456.09912109375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 5343.84521484375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 2481.97021484375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2044.7001953125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 3097.94580078125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1830.526123046875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 950.4401245117188 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 2590.06884765625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 2152.722900390625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: -49595.64453125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 1966.655517578125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 13132.294921875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 3276.591796875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: -11310.279296875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: -4246.38720703125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 2307.921875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 2203.870361328125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 6295.986328125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -21087.03125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1317.5311279296875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 724.4412841796875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 2818.140869140625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 8143.11328125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 2221.32861328125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1668.60791015625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 5471.07763671875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: -581562.125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 2998.663330078125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 939.3170166015625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 5629.60302734375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 2675.604248046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 4147.82421875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3859.947265625 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 1656.3812255859375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 1589.5640869140625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 1551.9063720703125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 4412.962890625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1802.0233154296875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 1518.7283935546875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 891.9575805664062 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1657.576904296875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1880.0333251953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 1898.671875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 5037.14306640625 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 1762.09033203125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 11857.8603515625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 38896.30859375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 6030.96435546875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 3407.9560546875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 4358.3759765625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 1715.0394287109375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 2566.30517578125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 1085.97705078125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 1659.8248291015625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 3584.138671875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 1627.3740234375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1080.4749755859375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1629.3812255859375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 3530.769287109375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 4617.7880859375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 2918.508056640625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 723.8969116210938 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2139.143798828125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 14239.1064453125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 799.10009765625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1867.2880859375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 682.2769775390625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 933.9096069335938 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: -308983.21875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1444.311767578125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 897.6822509765625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: -153311.453125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1773.477294921875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 3202.45654296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1486.60546875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 2302.331787109375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 2634.616943359375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1152.208984375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 3233.0947265625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 2475.61279296875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1483.2366943359375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1711.7349853515625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9231, loss_val: nan, pos_over_neg: 1955.4603271484375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1189.361572265625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 2326.912353515625 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: -7287.1689453125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 1735.3123779296875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 1233.095458984375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 4723.94921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 4202.88037109375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 2571.88525390625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 1476.180908203125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 3291.294189453125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -4127.4609375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 850.4931030273438 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1310.039794921875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 7857.337890625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 12023.6005859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: -8269.267578125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 5080.7001953125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 2816.30712890625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 10495.330078125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1627.1156005859375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 1317.6767578125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 789.0916137695312 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 3116.6220703125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 3034.035400390625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 789.9901123046875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 2046.4178466796875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 2686.720458984375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 861.2899169921875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1028.8773193359375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 948.6109619140625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 784.83154296875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 6356.93994140625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 1234.232666015625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8948, loss_val: nan, pos_over_neg: 2519.45166015625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9303, loss_val: nan, pos_over_neg: -14078.4765625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 1769.02978515625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 1427.1129150390625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: -9770.625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1480.54345703125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1702.468994140625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1728.220458984375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2704.40234375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 3345.138671875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 6676.97705078125 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 2005.6043701171875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 1038.1734619140625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2435.8095703125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: -2024.05029296875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 4071.16845703125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 574.9392700195312 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 824.1446533203125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.901, loss_val: nan, pos_over_neg: -2784.161865234375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.906, loss_val: nan, pos_over_neg: -6246.92431640625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 1879.5418701171875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: -42080.64453125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 1529.107177734375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: -62339.86328125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 3449.255615234375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 6251.8525390625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 12829.392578125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 3904.043212890625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1878.8983154296875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: -20342.43359375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1649.7470703125 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: -89171.59375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 4217.42724609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1237.432861328125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 2068.83203125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: -139794.828125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 2164.36767578125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1826.186279296875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1879.2227783203125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 13016.583984375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 3013.767578125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1163.45361328125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 2433.5791015625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: 3224.553955078125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 2002.0452880859375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 2262.13818359375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1649.5052490234375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 1248.47265625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 2950.054931640625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1547.506103515625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 1867.6705322265625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1678.8436279296875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1931.366943359375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1539.1494140625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 782.1441040039062 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 6479.794921875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 1741.8248291015625 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 2429.682861328125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 1210.217529296875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: -14579.376953125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 14984.8564453125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 4934.83203125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 2031.340087890625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1142.9010009765625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 1373.936767578125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 2208.063232421875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: -25734.07421875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8987, loss_val: nan, pos_over_neg: 2390.69677734375 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 794.244873046875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1315.880615234375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 27327.435546875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 10899.189453125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 8703.0546875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1214.167724609375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 1739.7015380859375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 849.9476318359375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.891, loss_val: nan, pos_over_neg: -14551.740234375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 12713.60546875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1186.075927734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: -4393.806640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 21374.806640625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: -10461.75390625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1235.6529541015625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 2015.07861328125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 1764.0272216796875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 6573.19091796875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1876.9620361328125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1537.7392578125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 1426.570556640625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1761.493408203125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 118066.375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1207.720703125 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 12510.3515625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 3574.673095703125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 1399.3341064453125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1666.4249267578125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: -24867.314453125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 1717.54638671875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 6721.17822265625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 5266.73974609375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2572.07470703125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9325, loss_val: nan, pos_over_neg: 809.098388671875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 2549.835205078125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: -45692.4609375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 827.2994995117188 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 1959.52099609375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 1389.140380859375 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 831.855224609375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 789.5621337890625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 407.4544372558594 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 392.701904296875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1055.6279296875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 1774.8272705078125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 1183.7176513671875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 1528.52099609375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 763.4108276367188 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 961.294189453125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1011.26611328125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1832.22314453125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1829.4505615234375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 1549.296142578125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 858.443603515625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1126.559814453125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1885.474853515625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 2372.068359375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 2142.20166015625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 1038.7933349609375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 1186.3387451171875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 1330.646484375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1303.593994140625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 3013.750732421875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 2347.5166015625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: -8340.8837890625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 366808.0 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 13190.8515625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 3979.416015625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1838.5792236328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 701.3015747070312 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 965.3842163085938 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 1460.6219482421875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1226.919677734375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 7391.16845703125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 766.322021484375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1570.879150390625 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 1976.4903564453125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 1672.171142578125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1524.4791259765625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1123.5982666015625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 793.5996704101562 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 1751.3271484375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 2399.244384765625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1093.57666015625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 689.6441040039062 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 866.33203125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1400.6966552734375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 676.330078125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1814.9246826171875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1106.14208984375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 973.310546875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 1265.9476318359375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1058.87939453125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2834.609619140625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 1158.918701171875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1046.8221435546875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 2921.814453125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 769.4741821289062 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 2266.807861328125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1792.5736083984375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 771.370849609375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1315.169189453125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1229.031005859375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: -8193.9287109375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.915, loss_val: nan, pos_over_neg: -18351.458984375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 9433.8974609375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 5429.8115234375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1182.464599609375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: -8964.6435546875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9242, loss_val: nan, pos_over_neg: 4459.79150390625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8875, loss_val: nan, pos_over_neg: 2223.722412109375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:20:04<100166:45:04, 1202.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 1829.0745849609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 2274.955322265625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 7070.23876953125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 837.2586669921875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 798.3116455078125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: -5539.25439453125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1090.5521240234375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 2152.70947265625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 2133.79248046875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: -2241926.0 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 12118.892578125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: -7432.04150390625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1228.2996826171875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1310.681640625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 3703.114013671875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 998.6375732421875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 4354.9443359375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 2696.554443359375 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: -20356.728515625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 5732.634765625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1289.53271484375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 2941.18017578125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 1030.8260498046875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 6617.72509765625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 4701.4541015625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1350.5989990234375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 10642.7060546875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1280.7938232421875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 2477.018798828125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 722.9835205078125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 5886.3447265625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 780.2947998046875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 3600.713623046875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 1642.55419921875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 2310.952392578125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: -3553.8173828125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 15703.4130859375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 1594.479736328125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 3042.795166015625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 10979.353515625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 3844.591796875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 8988.8779296875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.905, loss_val: nan, pos_over_neg: -10175.71484375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.905, loss_val: nan, pos_over_neg: -12904.099609375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 1641.989501953125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1072.5328369140625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: -11169.76953125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1495.90771484375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 880.910400390625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3169.80126953125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 3973.525146484375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 2181.6689453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 1971.2557373046875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 1223.3814697265625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: -90917.7109375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.909, loss_val: nan, pos_over_neg: -2947.03125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 623.484375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 1719.0821533203125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 5934.43017578125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: -4154.12744140625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 956.6080322265625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 962.9780883789062 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 1094.7969970703125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 6300.64990234375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 5895.39404296875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 2540.925048828125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 994.087646484375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1457.456298828125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8994, loss_val: nan, pos_over_neg: 3701.739013671875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 444546.84375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 1459.8935546875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 1945.1630859375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 916.4938354492188 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1499.2568359375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9222, loss_val: nan, pos_over_neg: 588.5422973632812 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 889.6013793945312 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 1188.0955810546875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8928, loss_val: nan, pos_over_neg: 1232.968994140625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1492.7545166015625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 2959.230712890625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 1140.5091552734375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8921, loss_val: nan, pos_over_neg: 2433.98828125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 1479.8763427734375 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 1252.067626953125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 703.6441040039062 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 1655.9373779296875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 1265.0870361328125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 1218.7420654296875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 1573.17626953125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 2021.027099609375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 5983.51123046875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8938, loss_val: nan, pos_over_neg: 1704.7244873046875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 2443.696533203125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1691.7052001953125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 824.821533203125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1462.1783447265625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 6531.30517578125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 601.3416748046875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 1566.56201171875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1614.0927734375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 802.0294799804688 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1260.7393798828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 3468.67138671875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 1860.177734375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2146.74560546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 1222.25732421875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 2415.472412109375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 2688.54638671875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: -120185.0859375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 49092.37109375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 5638.978515625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1480.395751953125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 20966.513671875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 3266.273681640625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 3141.49365234375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2998.107177734375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 16961.24609375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 5273.900390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1288.663330078125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8949, loss_val: nan, pos_over_neg: 4766.44580078125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 1401.4053955078125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 2621.2734375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 1050.8519287109375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1113.21875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1581.966064453125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: -38032.9140625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 2797.451904296875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 2181.33740234375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1786.4058837890625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 966.8658447265625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 4420.5263671875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1010.4531860351562 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1300.4879150390625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 912.2586059570312 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 1175.274169921875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: -6326.50732421875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: -4303.89990234375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 3486.89453125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 32324.109375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 5077.904296875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 2076.90234375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: -5256.38427734375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 4703.728515625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1641.3419189453125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1561.166748046875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 955.16748046875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 6962.2333984375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: -7255.318359375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2530.75830078125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8973, loss_val: nan, pos_over_neg: 1351.7359619140625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 5134.95751953125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 1691.50439453125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -17816.6796875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 4470.2353515625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 1043.1307373046875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 9206.6796875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 2434.349609375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 6154.52783203125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 12692.6474609375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 5196.2109375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: -17007.30078125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -17483.955078125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 2640.1962890625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 1993.657470703125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8986, loss_val: nan, pos_over_neg: 4625.35205078125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 830.9146728515625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: 4206.044921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 607.3087768554688 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: 2737.283935546875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 2994.76123046875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: 2667.708740234375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 3075.170166015625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 2702.85400390625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1313.4613037109375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9155, loss_val: nan, pos_over_neg: 9989.4619140625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 2662.2783203125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 1285.4627685546875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1998.81396484375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1050.2557373046875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: -301509.34375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: -5204.22119140625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2688.9462890625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 2170.69189453125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 3074.291015625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 1310.0023193359375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 1402.5421142578125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 1826.4462890625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 2091.248779296875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 1696.361083984375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 4360.01171875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 998.5374145507812 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 2300.67626953125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9211, loss_val: nan, pos_over_neg: 1130.6275634765625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 2892.377197265625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 4880.39599609375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 5197.76806640625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8894, loss_val: nan, pos_over_neg: 3994.51318359375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 2578.08837890625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 685.353515625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9148, loss_val: nan, pos_over_neg: 2742.16650390625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 7222.85205078125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1417.299072265625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 2685.5380859375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 1077.119873046875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 1134.8037109375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 1145.6705322265625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 1024.0682373046875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9214, loss_val: nan, pos_over_neg: 1808.808837890625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 3608.037353515625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1222.744384765625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 664.8082885742188 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 723.8626098632812 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1301.8499755859375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 8951.228515625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1425.7889404296875 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 2172.7998046875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1666.0423583984375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 3186.3642578125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 3827.60009765625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 2927.65478515625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 3986.337646484375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 2945.0087890625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 4762.17578125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 1601.706787109375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 4146.12451171875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.906, loss_val: nan, pos_over_neg: -13637.376953125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 2518.634521484375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 1881.2120361328125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 1065.7301025390625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 3309.47119140625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1963.19580078125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 1140.4261474609375 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 1124.552734375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 2003.8277587890625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 803.0118408203125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1345.0577392578125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 2013.3612060546875 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2281.922607421875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 3448.50927734375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: -5996.49755859375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 25058.1953125 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 7356.06396484375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 3818.845703125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 2337.901123046875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1319.003662109375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 979.7753295898438 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 3106.47607421875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 2295.43115234375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 1346.7379150390625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 2180.306640625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1511.3994140625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 7868.3505859375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 2340.271240234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 12037.5634765625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 14195.4833984375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1586.173583984375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 3319.374755859375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 33096.2265625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 1183.9749755859375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1977.546142578125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 13055.0244140625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 1657.7701416015625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8925, loss_val: nan, pos_over_neg: 38411.0703125 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.905, loss_val: nan, pos_over_neg: -41497.4375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 2149.077392578125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 1053.066650390625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 962.606689453125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 5618.59716796875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: -9658.294921875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 2754.353271484375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 4320.103515625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 47963.9375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: -2896.21240234375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 1223.661865234375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 3596.88427734375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: -4100.35546875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 2420.0546875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1422.5596923828125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 1137.3560791015625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 62328.1953125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: -4084.115966796875 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 4267.6279296875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 2396.060302734375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1586.778564453125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 3369.172119140625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 4185.62548828125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 2537.980224609375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 2293.6015625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1265.90380859375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 831.3299560546875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 1723.264404296875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 788.6373291015625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1930.6820068359375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 4572.95556640625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1910.4698486328125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 1316.3905029296875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 744.4866333007812 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 28602.767578125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 576.4666137695312 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 1231.009033203125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1261.15380859375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1751.274169921875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 1560.288818359375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 2557.719970703125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9247, loss_val: nan, pos_over_neg: 1606.217529296875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9241, loss_val: nan, pos_over_neg: 910.0381469726562 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 1228.1141357421875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1902.921142578125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: -31324.18359375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 3349.43896484375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 1044.2650146484375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 1091.1080322265625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: -8580.3203125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 4316.95703125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 1384.29248046875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 732.9551391601562 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 2009.2750244140625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: -7283.5068359375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 2397.7470703125 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2073.937255859375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.893, loss_val: nan, pos_over_neg: 980.4022827148438 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 3413.871337890625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 9296.8330078125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 933.98828125 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 1941.2484130859375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 3048.802734375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 823.0652465820312 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 974.4615478515625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 652.7904663085938 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 2102.68603515625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 2374.78369140625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 2911.48828125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 2136.22998046875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 3080.408935546875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 1997.2703857421875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 2158.3388671875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1297.18505859375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 2861.80029296875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 1547.536865234375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 2027.253662109375 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2424.608154296875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 2358.610107421875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 580.601318359375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 900.2257690429688 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 5218.59130859375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 2725.3720703125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1045.97900390625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 1220.9404296875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1357.8839111328125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: 1901.4727783203125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 2633.018310546875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: -10428.3662109375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 2316.362060546875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 1711.9365234375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1280.79736328125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1181.1412353515625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 630.5474243164062 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 5433.60791015625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 1534.91064453125 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 1483.240478515625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2025.6116943359375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1632.1177978515625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 2510.23974609375 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2582.26318359375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 6244.94140625 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 2819.10595703125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8945, loss_val: nan, pos_over_neg: 5163.84033203125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 9284.1005859375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: -13061.6767578125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 4145.525390625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8963, loss_val: nan, pos_over_neg: 3704.031005859375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1040.4554443359375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 618.0585327148438 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 1119.1065673828125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 485.8759765625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 42694.60546875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 1026.388916015625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 1395.490478515625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 3396.866455078125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9181, loss_val: nan, pos_over_neg: -32050.4140625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 960.2413940429688 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1519.0537109375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 1130.86083984375 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 1577.3135986328125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: -147837.15625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 2809.174072265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 1957.1396484375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 1554.6370849609375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 3058.36328125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 2674.6865234375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 2404.4423828125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 1320.8927001953125 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 1815.3753662109375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 1567.3807373046875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: 15577.9677734375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 3102.849365234375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: 1896.771240234375 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1201.7501220703125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: -10750.845703125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 5043.0654296875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 14240.1884765625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 2510.313720703125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 1744.2177734375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2173.33544921875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: -46758.3515625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 4105.5322265625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 12245.2958984375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 1520.7198486328125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 1774.542236328125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: -23437.40625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: -6090.36572265625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 1107.1036376953125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 609.2711791992188 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1006.8663330078125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 3132.701171875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 1486.4779052734375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 2300.815673828125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 6598.21435546875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 3694.2919921875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: -7621.373046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 2305.741455078125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 3086.840576171875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 1219.978759765625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 971.8916015625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 1754.7177734375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8966, loss_val: nan, pos_over_neg: -49501.203125 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 5870.5400390625 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 1384.454833984375 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: 432.4060974121094 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 1941.3681640625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 92632.625 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 3237.334716796875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 3790.282470703125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 19071.67578125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: 2428.383056640625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 8560.81640625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 8687.7548828125 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: -23514.9921875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 7056.54248046875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1139.7779541015625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 1647.1878662109375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 1988.0880126953125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 1669.3848876953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 4433.0205078125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 4555.703125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 2436.800048828125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8846, loss_val: nan, pos_over_neg: 4268.3486328125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 1265.01318359375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: -13514.326171875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 5965.3603515625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 1221.73779296875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 731.33154296875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 1419.516357421875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 998.2224731445312 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 1046.5643310546875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 759.4305419921875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 871.1375732421875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 682.8768920898438 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 1356.0108642578125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.899, loss_val: nan, pos_over_neg: 4983.3427734375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 721.065673828125 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1103.8382568359375 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 2063.28125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 2321.513427734375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 639.378662109375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 888.8326416015625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: -11712.25 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: -32570.173828125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 15981.2919921875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 943.3187255859375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 905.8831176757812 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 2109.689453125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: -26468.39453125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 499.6938171386719 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1626.7943115234375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 842.7264404296875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 37237.75 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 7182.5205078125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 1972.0101318359375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 3179.27880859375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 4858.40966796875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 14141.5419921875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 1483.6929931640625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: -7492.048828125 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 7873.5283203125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: -29918.3671875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1393.949462890625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 807.4339599609375 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 892.732666015625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 3390.253662109375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 2284.156982421875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 1264.8046875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 16776.126953125 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 24337.734375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 2850.341796875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 1318.778076171875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.913, loss_val: nan, pos_over_neg: -5061.52197265625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.911, loss_val: nan, pos_over_neg: -14386.8232421875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: -10363.416015625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 723.2781372070312 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 8381.765625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 2940.298583984375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: -16703.390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 3658.786376953125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9064, loss_val: nan, pos_over_neg: 1658.3524169921875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 1768.5506591796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 4454.3486328125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: -19708.962890625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 2373.463623046875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 4200.06982421875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 3065.068115234375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 7514.5380859375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: -5461.1748046875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 1648.9591064453125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9026, loss_val: nan, pos_over_neg: 3095.691162109375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 2835.5390625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8955, loss_val: nan, pos_over_neg: -6216.68505859375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 2362.765625 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 2345.215087890625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1294.083740234375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 7500.37841796875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 3000.775390625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 649.0760498046875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 2299.705810546875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 34760.24609375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: -169938.6875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1171.19140625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 8023.68359375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 1451.544677734375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1737.7578125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 4162.09375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 1113.0172119140625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 2044.9039306640625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 1386.1759033203125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 969.1260986328125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2971.572509765625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 3427.33349609375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1547.8333740234375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 1060.318115234375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 1526.124755859375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 928.4850463867188 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 2198.93994140625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 1256.2354736328125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 5813.734375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2052.5986328125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 4615.74951171875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1595.248291015625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 8740.4873046875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: -9085.8935546875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 27782.275390625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 3091.688232421875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 2740.810302734375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8906, loss_val: nan, pos_over_neg: 7107.1357421875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 2153.54052734375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1482.4149169921875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1488.3323974609375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 3296.687255859375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 1707.6544189453125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1664.9010009765625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 1955.52978515625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 2698.57568359375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 3635.1962890625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 13943.083984375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2478.134521484375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 4275.68408203125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 930.3877563476562 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 4432.73291015625 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 2037.5269775390625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 1187.89794921875 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: -24847.787109375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3922.40673828125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 125680.7890625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 2870.916748046875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 2815.11376953125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1788.7569580078125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 1072.9503173828125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3757.05810546875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1286.8797607421875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 31156.677734375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 1032.8726806640625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 7210.87841796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 516.450439453125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: -5082.478515625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 11319.4482421875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1782.9066162109375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9078, loss_val: nan, pos_over_neg: 2403.43505859375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 9293.568359375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9175, loss_val: nan, pos_over_neg: 6646.7451171875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 3481.203369140625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 988.7660522460938 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 3593.3154296875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 2546.225341796875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 1285.5159912109375 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 269676.34375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 1580.1258544921875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 4152.501953125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9158, loss_val: nan, pos_over_neg: 651.7778930664062 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.902, loss_val: nan, pos_over_neg: 1412.74365234375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: -6334.6845703125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 9217.5361328125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 1221.857177734375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1227.78515625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8973, loss_val: nan, pos_over_neg: 2007.1673583984375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: -3256.7255859375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 4025.320068359375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1008.9325561523438 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1007.8242797851562 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 18483.326171875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 2339.833251953125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 2620.644775390625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 3895.9404296875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 2134.922607421875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 1405.8492431640625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1428.8968505859375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 2715.97216796875 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 866.18359375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 2675.541015625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1930.1451416015625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 1585.2760009765625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 5792.65478515625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 5004.9140625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 2563.080322265625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 818.8170166015625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 2231.483642578125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8901, loss_val: nan, pos_over_neg: 2718.2314453125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 2021.72119140625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 608.2199096679688 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 822.8181762695312 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1245.3289794921875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 5373.31201171875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: 1787.406005859375 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 1532.38134765625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 1155.320556640625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 17406.0703125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.894, loss_val: nan, pos_over_neg: 3472.010498046875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1433.2066650390625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 792.0361328125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 1653.8433837890625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9094, loss_val: nan, pos_over_neg: 3857.8740234375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 1066.5875244140625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 2181.35546875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 926.4160766601562 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 2277.83544921875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.9118, loss_val: nan, pos_over_neg: 7504.22900390625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 1523.66162109375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 1565.6844482421875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 638.09033203125 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 1012.7573852539062 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 1003.9639282226562 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 1726.3172607421875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 928.2578125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9256, loss_val: nan, pos_over_neg: 1241.337158203125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9, loss_val: nan, pos_over_neg: -2937.84521484375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 4721.73388671875 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 3343.62548828125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 1119.185302734375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 2041.640625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: -10300.951171875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 1119.589111328125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 684.5072021484375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 1597.03759765625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3995.55712890625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2638.803955078125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: -7835.1240234375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: -96444.4375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1819.9873046875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 1119.2874755859375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 798.720458984375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9162, loss_val: nan, pos_over_neg: 1947.9324951171875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2136.998046875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 2247.41064453125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 30030.658203125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8937, loss_val: nan, pos_over_neg: 6392.87744140625 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 1603.5262451171875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 1439.4794921875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 1296.5792236328125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 2799.020263671875 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 1288.15576171875 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 2051.585205078125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2006.6102294921875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1341.2620849609375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9087, loss_val: nan, pos_over_neg: 3099.148681640625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 3476.0048828125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: -4752.03564453125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8956, loss_val: nan, pos_over_neg: 2723.375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 17281.302734375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 11010.71875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8888, loss_val: nan, pos_over_neg: 1856.9368896484375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: -6719.74169921875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: -61401.05859375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 2451.612548828125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 1062.1214599609375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 4024.02392578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 1719.674560546875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: -10867.4619140625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [2:40:08<100234:09:17, 1202.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 2494.420166015625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: -2554.18408203125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 3621.4375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9014, loss_val: nan, pos_over_neg: -16339.17578125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 2508.8515625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: -4552.7861328125 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 1530.356689453125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1073.409423828125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: 2549.42919921875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 963.4039306640625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 1379.15625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 1444.1549072265625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 1606.4625244140625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 1904.59619140625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 1190.239990234375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 2413.6162109375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 1426.511962890625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9172, loss_val: nan, pos_over_neg: -4833.99169921875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 9697.0361328125 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 1665.00439453125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: 1294.8514404296875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 1287.264892578125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1223.15576171875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 931.9474487304688 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 9888.83984375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 1417.6820068359375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 1831.97412109375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 2196.830078125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 15624.998046875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: 3147.9130859375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2518.1015625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8935, loss_val: nan, pos_over_neg: 2303.462890625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 2219.201904296875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 3836.653564453125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 4866.25341796875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 3333.3154296875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 859.0841674804688 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8892, loss_val: nan, pos_over_neg: 3451.740478515625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 2753.2109375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8972, loss_val: nan, pos_over_neg: 4971.9931640625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1754.9525146484375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 1542.3182373046875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 2007.079345703125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 2629.10791015625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 5156.150390625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: -3497.338134765625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 1247.93701171875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 1525.8057861328125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 1925.5186767578125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 2096.80859375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8866, loss_val: nan, pos_over_neg: 2907.915771484375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9132, loss_val: nan, pos_over_neg: 8808.5849609375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: -10309.4013671875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 1482.234130859375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 2449.1982421875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 3159.3994140625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 28040.8046875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 1850.8076171875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 939.6622314453125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 1724.8116455078125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 3310.609619140625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 637.0846557617188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 4861.330078125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9029, loss_val: nan, pos_over_neg: 1488.6351318359375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 11048.3916015625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 3389.1025390625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 1008.5672607421875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 3954.4404296875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 3549.99951171875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 1838.4879150390625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1252.8895263671875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 2331.3603515625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 1231.36474609375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8932, loss_val: nan, pos_over_neg: 2233.435791015625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 2432.920166015625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 1771.6361083984375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 2062.713623046875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 1455.1878662109375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1929.422119140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 1275.748046875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.887, loss_val: nan, pos_over_neg: -6039.21484375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 889.8907470703125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 1168.4991455078125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2187.322509765625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1440.47119140625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 8896.7998046875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 1370.45166015625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8938, loss_val: nan, pos_over_neg: 6658.9501953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 1874.5118408203125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9123, loss_val: nan, pos_over_neg: 1701.006103515625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 4284.7099609375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 3538.225341796875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 2689.694091796875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 2813.08447265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8942, loss_val: nan, pos_over_neg: 3238.827392578125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 1746.610107421875 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 576.9569702148438 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: -5175.8408203125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 4661.98876953125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 1593.5146484375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 4299.22607421875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8881, loss_val: nan, pos_over_neg: -147395.203125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 1108.2503662109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1027.6490478515625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 1175.6280517578125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1135.953125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8979, loss_val: nan, pos_over_neg: 3578.76904296875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 2616.381591796875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 2006.483154296875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 9459.7060546875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 67604.4453125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: -7917.48779296875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 1217.0472412109375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 2863.54150390625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8949, loss_val: nan, pos_over_neg: 2109.959716796875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 4374.28076171875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: -31576.26953125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 3099.301025390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 1450.8917236328125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 10065.70703125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.909, loss_val: nan, pos_over_neg: -3621.130859375 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 1553.1309814453125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9149, loss_val: nan, pos_over_neg: 3947.624755859375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 649.5860595703125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 3883.866943359375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: -29919.09765625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.899, loss_val: nan, pos_over_neg: -10392.7861328125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: -2042.8740234375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9097, loss_val: nan, pos_over_neg: -21831.466796875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 6097.3173828125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 4352.43701171875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: -15722.4541015625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 8114.41015625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 2982.81005859375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 6677.00244140625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 4716.9208984375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: -5417.689453125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2380.159912109375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: -22068.9765625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1628.353271484375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9066, loss_val: nan, pos_over_neg: 1901.181884765625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 1081.77001953125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: 1391.701171875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 8399.3125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 3746.99853515625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8951, loss_val: nan, pos_over_neg: 6552.06201171875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 17185.603515625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 53939.44921875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 4150.4306640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: -3840.572509765625 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: -21179.275390625 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: -34795.22265625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 22026.111328125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 7431.712890625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.899, loss_val: nan, pos_over_neg: 3583.53955078125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 8522.7265625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9117, loss_val: nan, pos_over_neg: 4756.6552734375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 4835.5458984375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1312.3377685546875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 3094.482421875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 1312.5198974609375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: -12303.091796875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 2477.642578125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9101, loss_val: nan, pos_over_neg: 65229.40234375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 8282.1962890625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 4581.1171875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 3018.10595703125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1465.74365234375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 7739.65087890625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 22048.103515625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 4696.6796875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 13197.029296875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1359.642822265625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 5144.2783203125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 972.3153686523438 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 3491.007080078125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 2355.248046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 2160.614501953125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 7819.41552734375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 3325.429443359375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: -4035.280029296875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 1610.9371337890625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8947, loss_val: nan, pos_over_neg: 1148.17724609375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 2236.683837890625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 4228.9052734375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.896, loss_val: nan, pos_over_neg: -51857.85546875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 2312.157958984375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 2647.567626953125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 4164.6953125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8952, loss_val: nan, pos_over_neg: 2812.973876953125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 2442.568115234375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8952, loss_val: nan, pos_over_neg: 13863.6689453125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 2078.467041015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 8017.7607421875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 3734.816162109375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 2833.859130859375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 13886.7998046875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.916, loss_val: nan, pos_over_neg: -491242.1875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 1296.59423828125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1658.1279296875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 3122.0849609375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 4663.87890625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 1620.8995361328125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 1728.732177734375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 5506.7763671875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: -15776.6572265625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 10108.0087890625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 3835.7314453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: -72729.765625 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss  Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix  Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE  RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "#                                                     pin_memory=True,\n",
    "#                                                     num_workers=32,\n",
    "#                                                     persistent_workers=True,\n",
    "#                                                     # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "#                                                     pin_memory=True,\n",
    "#                                                     num_workers=32,\n",
    "#                                                     persistent_workers=True,\n",
    "#                                                     # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:15<00:00,  3.27it/s]\n",
      "100%|| 50/50 [00:09<00:00,  5.20it/s]\n",
      "100%|| 50/50 [00:05<00:00,  9.32it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "splitter = ShuffleSplit(n_splits=50)\n",
    "all_split_inx = list(splitter.split(features_train))\n",
    "\n",
    "train_X = [features_train[_[0]] for _ in all_split_inx]\n",
    "train_y = [y_labeled_train[_[0]] for _ in all_split_inx]\n",
    "test_X = [features_train[_[1]] for _ in all_split_inx]\n",
    "test_y = [y_labeled_train[_[1]] for _ in all_split_inx]\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10,10))\n",
    "plt.suptitle(f'{model_file_name}')\n",
    "\n",
    "c_lst = [1e-1, 1e-2, 1e-3]\n",
    "for ic, c in enumerate(c_lst):\n",
    "    train_cms = []\n",
    "    test_cms = []\n",
    "    for inx_split in trange(len(train_X)):\n",
    "        tmp_train_X = train_X[inx_split]\n",
    "        tmp_train_y = train_y[inx_split]\n",
    "        \n",
    "        tmp_test_X = test_X[inx_split]\n",
    "        tmp_test_y = test_y[inx_split]\n",
    "        \n",
    "        logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=c).fit(tmp_train_X, tmp_train_y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        proba = logreg.predict_proba(tmp_train_X)\n",
    "\n",
    "        preds = np.argmax(proba, axis=1)\n",
    "        cm = classification.confusion_matrix(preds, tmp_train_y)\n",
    "        train_cms.append(cm)\n",
    "\n",
    "#         plt.figure()\n",
    "#         sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "#         plt.title('train');\n",
    "        \n",
    "        proba = logreg.predict_proba(tmp_test_X)\n",
    "        preds = np.argmax(proba, axis=1)\n",
    "        cm = classification.confusion_matrix(preds, tmp_test_y)\n",
    "        test_cms.append(cm)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "#         plt.title('val');\n",
    "\n",
    "    sns.heatmap(np.mean(train_cms,axis=0), annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'), ax=ax[ic, 0])\n",
    "    ax[ic, 0].set_title(f'train  C:{c}');\n",
    "\n",
    "    sns.heatmap(np.mean(test_cms,axis=0), annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'), ax=ax[ic, 1])\n",
    "    ax[ic, 1].set_title(f'val  C:{c}');\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-3)).fit(features_train, y_labeled_train)\n",
    "\n",
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
